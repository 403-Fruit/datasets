{"creation_date":"2016-03-17 09:51:08.0","id":23449,"title":"Use of java generics in hadoop map reduce jobs?","body":"<p>How often we use java generics in map reduce jobs.Your inputs is highly appreciated! </p>","tags":["MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-17 12:07:51.0","id":23467,"title":"Permission denied error during NameNode start","body":"<p>Today I observed that the NameNode was in red. I tried to restart the server, but I found the following errors. As per the suggestions mentioned in various threads, I formatted the NameNode, by using the command \"hadoop\nnamenode -format\". I was logged in as 'root', when I did it. When I saw this error, I reformatted the namenode as 'hdfs' user, but I still see the followign errors. Can anyone help me understand what is going wrong.</p><p>2016-03-17 17:27:02,305 WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(683)) - Encountered exception loading fsimage\njava.io.FileNotFoundException: /hadoop/hdfs/namenode/current/VERSION (Permission denied)</p>","tags":["ambari-2.1.2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-17 16:12:52.0","id":23502,"title":"Ranger HBase CLI command not working","body":"<p>User eve is a delegate admin for hbase all in all. However, she can't do grant on hbase cli:</p><pre>hbase(main):005:0&gt; grant 'mktg1', 'RW', 'test'\nERROR: DISABLED: Security features are not available</pre>","tags":["cli","Ranger","Hbase"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-17 08:43:52.0","id":23442,"title":"Sqoop incremental import when the source table has separate created and updated columns","body":"<p>I have to import daily data from SQL Server using Sqoop (1.4.6). Naturally, incremental imports would seem to be the way to go, but here's the thing - the source tables I have to import have separate 'created' and 'updated' columns. That is, newly-created records only have a timestamp value under the 'created' column, and records will only have a value for the 'updated' column if they are edited anytime after first creation. Looking through the documentation, it doesn't look like it's possible to have 2 check columns when doing incremental imports, so the only way I've managed to get this done is with 2 separate imports:</p><ol>\n<li>Incremental import with the primary key as check column for NEW records</li><li>Incremental import with the updated column as check column for UPDATED records</li></ol><p>This works, but I wonder if there's a better way to do this. Any thoughts?</p><p>Update: I tried the suggestion by <a rel=\"user\" href=\"/users/342/sluangsay.html\" nodeid=\"342\">@Sourygna Luangsay</a> to use free-form query imports, however the documentation quite clearly states that the query can't contain OR conditions in the WHERE clause. Besides, since these will be incremental imports, the output directory would exist and I'd still need a two-step workflow: the first step for importing, then the second for merging. </p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-16 07:41:34.0","id":23232,"title":"AMBARI METRICS restart randomly","body":"<p>Hi,</p><p>I built a new standalone hdp 2.3. This standalone are not yet sollicited by application but  the ambari metrics service shutdown and restart for no reason. It can happen twice/three or four times a week.</p><p>This is the logs of my ambari-metrics-collector.log:</p><p>2016-03-14 01:29:07,166 ERROR org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer: RECEIVED SIGNAL 15: SIGTERM 2016-03-14 01:29:07,169 INFO org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation: Closing master protocol: MasterService 2016-03-14 01:29:07,182 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:6188 2016-03-14 01:29:07,207 INFO org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x1536be5998a0001 2016-03-14 01:29:07,208 INFO org.apache.zookeeper.ZooKeeper: Session: 0x1536be5998a0001 closed 2016-03-14 01:29:07,208 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down 2016-03-14 01:29:07,286 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping phoenix metrics system... 2016-03-14 01:29:07,289 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: phoenix metrics system stopped. 2016-03-14 01:29:07,289 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: phoenix metrics system shutdown complete. 2016-03-14 01:29:07,290 INFO org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl: Stopping ApplicationHistory 2016-03-14 01:29:07,290 INFO org.apache.hadoop.ipc.Server: Stopping server on 60200 2016-03-14 01:29:07,294 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder 2016-03-14 01:29:07,294 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 60200 2016-03-14 01:29:07,295 INFO org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer: SHUTDOWN_MSG:</p><p>Do you know the reason of this restarts?</p><p>Thanks,</p><p>Gauthier</p>","tags":["hdp-2.3.0","ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-21 17:57:00.0","id":23932,"title":"connected to Beeline ,i see default database,and this default database  is empty","body":"","tags":["database","connection-refused"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-22 05:11:49.0","id":23993,"title":"sqoop list-databases  is showing only system databases or public.!!","body":"<p>Hi,</p><p>sqoop list-databases --connect jdbc:mysql://sandbox.hortonworks.com/hive --username hive --password hive is showing only system databases or public. How can i see the databases i created in hive??</p><p>Mamta</p>","tags":["Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-22 01:13:48.0","id":23988,"title":"Not able to run hive benchmark test","body":"<p>\n\tI am trying to do hive bench marking(https://github.com/hortonworks/hive-testbench)</p><p> but when I run setup script it loads data is some table but fails after sometime fails with the following error: </p><p>OK\nTime taken: 0.264 seconds\n+ '[' X = X ']'\n+ FORMAT=orc\n+ i=1\n+ total=24\n+ DATABASE=tpcds_bin_partitioned_orc_2\n+ for t in '${FACTS}'\n+ echo 'Optimizing table store_sales (1/24).'\nOptimizing table store_sales (1/24).\n+ COMMAND='hive -i settings/load-partitioned.sql -f ddl-tpcds/bin_partitioned/store_sales.sql     -d DB=tpcds_bin_partitioned_orc_2  -d SCALE=2     -d SOURCE=tpcds_text_2 -d BUCKETS=1     -d RETURN_BUCKETS=1 -d FILE=orc'\n+ runcommand 'hive -i settings/load-partitioned.sql -f ddl-tpcds/bin_partitioned/store_sales.sql     -d DB=tpcds_bin_partitioned_orc_2  -d SCALE=2     -d SOURCE=tpcds_text_2 -d BUCKETS=1     -d RETURN_BUCKETS=1 -d FILE=orc'\n+ '[' XON '!=' X ']'\n+ hive -i settings/load-partitioned.sql -f ddl-tpcds/bin_partitioned/store_sales.sql -d DB=tpcds_bin_partitioned_orc_2 -d SCALE=2 -d SOURCE=tpcds_text_2 -d BUCKETS=1 -d RETURN_BUCKETS=1 -d FILE=orc\nWARNING: Use \"yarn jar\" to launch YARN applications.\n\nLogging initialized using configuration in file:/etc/hive/2.4.0.0-169/0/hive-log4j.properties\n...</p><p>OK\nTime taken: 0.948 seconds\nOK\nTime taken: 0.238 seconds\nOK\nTime taken: 0.629 seconds\nOK\nTime taken: 0.248 seconds\nQuery ID = hdfs_20160322014240_60c3f689-816d-409e-b8c7-c6ea636fa12a\nTotal jobs = 1\nLaunching Job 1 out of 1\nDag submit failed due to Invalid TaskLaunchCmdOpts defined for Vertex Map 1 : Invalid/conflicting GC options found, cmdOpts=\"-server -Djava.net.preferIPv4Stack=true -Dhdp.version=2.4.0.0-169 -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/ -Dlog4j.configuratorClass=org.apache.tez.common.TezLog4jConfigurator -Dlog4j.configuration=tez-container-log4j.properties -Dyarn.app.container.log.dir=&lt;LOG_DIR&gt; -Dtez.root.logger=INFO,CLA \" stack trace: [org.apache.tez.dag.api.DAG.createDag(DAG.java:859), org.apache.tez.client.TezClientUtils.prepareAndCreateDAGPlan(TezClientUtils.java:694), org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:487), org.apache.tez.client.TezClient.submitDAG(TezClient.java:434), org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:439), org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:180), org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160), org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89), org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)] retrying...\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\n+ '[' 1 -ne 0 ']'\n+ echo 'Command failed, try '\\''export DEBUG_SCRIPT=ON'\\'' and re-running'\nCommand failed, try 'export DEBUG_SCRIPT=ON' and re-running\n+ exit 1</p><p>Not sure what is wrong. </p><p>Anyhelp is appreciated.</p>","tags":["Tez","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-22 10:37:01.0","id":24035,"title":"How to restart YARN Client, Tez Client and Pig Client","body":"<p>The Ambari Dashboard shows restart icon for Yarn Client and Tez client and a red triangle for Pig client. I have pressed the \"Restart component\" button but nothing happens. The process does not show any errors, but the state of these clients never change and they always remain the same. Can anyone tell me how to restart them manually or how do I know what prevents them from being restarted or run properly? I am pasting a screen shot of the display of these clients.<a href=\"/storage/attachments/2920-screenshot.png\">screenshot.png</a></p>","tags":["ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-21 11:04:20.0","id":23890,"title":"Error in connecting HDP 2.3.4 and SSRS","body":"<p>While connecting HDP and Microsoft SQL Server Report Builder, while doing settings in the Data Source Properties.</p><p>i get the following error:</p><p><strong>ERROR [IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified</strong></p><p><img src=\"/storage/attachments/2921-capture.png\" style=\"width: 595px;\"></p><p>ERROR [IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified </p><p>While clicking on Build, shown in the image above, I get connecting successful inside connection Properties:</p><p>But when I do test on Data Source properties I get the error mentioned above, what am I missing here?</p><p>Following is my Hortonworks Hive ODBC Driver Connection Dialog: </p><p><img src=\"/storage/attachments/2922-capture1.png\" style=\"width: 326px;\"></p>","tags":["hadoop","Hive","driver"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-03-22 07:28:48.0","id":24010,"title":"Mongodb with hive : Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. com/mongodb/hadoop/io/BSONWritable","body":"<p>Jar -&gt; mongo-hadoop-core-1.4.0,mongo-hadoop-hive-1.4.0,mongo-java-driver-2.10.1 </p><p>hive&gt; CREATE EXTERNAL TABLE minute_bars\n    &gt; (\n    &gt;\n    &gt; id STRING,\n    &gt;     Symbol STRING,\n    &gt;     `Timestamp` STRING,\n    &gt;     Day INT,\n    &gt;     Open DOUBLE,\n    &gt;     High DOUBLE,\n    &gt;     Low DOUBLE,\n    &gt;     Close DOUBLE,\n    &gt;     Volume INT\n    &gt; )\n    &gt; STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'\n    &gt; WITH SERDEPROPERTIES('mongo.columns.mapping'='{\"id\":\"_id\",\n    &gt;  \"Symbol\":\"Symbol\", \"Timestamp\":\"Timestamp\", \"Day\":\"Day\", \"Open\":\"Open\", \"High\":\"High\", \"Low\":\"Low\", \"Close\":\"Close\", \"Volume\":\"Volume\"}')\n    &gt; TBLPROPERTIES('mongo.uri'='mongodb://localhost:27017/marketdata.minbars');\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. com/mongodb/hadoop/io/BSONWritable\nhive&gt;</p>","tags":["Hive","mongodb"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-18 17:08:49.0","id":34058,"title":"Spark and HBase","body":"<p>I am trying to write to HBase from SparkStreaming job. </p><p>I get the following exception when calling: </p><p>table.put(_config,put); </p><p>java.lang.NullPointerException</p><p>at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.getMetaReplicaNodes(ZooKeeperWatcher.java:269)</p><p>I see that the configuration value is not correct but I tried to set up values I don't know exactly which values to setup \n</p><p>I am running on azure with hortonworks sandbox and running the job via spark-submit with --jars supplying the entire jars needed to run. (spark version 1.6)</p><p>The streaming is running perfect except that I cant write to HBase </p><p>Here is my code to put data:</p><pre>class Adapter extendsSerializable{</pre>\n<pre>var_conf = new HBaseConfiguration()</pre>\n<pre>val_admin = new HBaseAdmin(_conf)</pre>\n\n<pre>defPutData(tableName:String,columnFamily:String,key:String,data:String){</pre>\n<pre>//  _conf = HBaseConfiguration.create().asInstanceOf[HBaseConfiguration]</pre>\n\n<pre>        _conf.addResource(\"//etc//hbase//conf//hbase-site.xml\")</pre>\n<pre>        _conf.set(\"hbase.zookeeper.quorum\",\"sandbox.hortonworks.com\");</pre>\n<pre>        _conf.setInt(\"hbase.zookeeper.clientport\",2181);</pre>\n<pre>println(_conf.get(\"hbase.zookeeper.quorum\")); </pre>\n<pre>println(_conf.get(\"hbase.zookeeper.clientport\"));</pre>\n\n<pre>valtable = new HTable(_conf,tableName)</pre>\n<pre>valput = new Put(Bytes.toBytes(key));</pre>\n<pre>valobj = JSON.parseFull(data); </pre>\n\n<pre>objmatch {</pre>\n<pre>caseSome(m:Map[String,Any])=&gt;</pre>\n\n<pre>m.map((r)=&gt;{</pre>\n<pre>valv=r._2.asInstanceOf[String];</pre>\n<pre>put.add(Bytes.toBytes(columnFamily),Bytes.toBytes(r._1),Bytes.toBytes(v))</pre>\n<pre>//println(r._1+\":\"+v)</pre>\n<pre>          })</pre>\n<pre>      }      </pre>\n\n<pre>println(\"writing to HBase\"); </pre>\n<pre>table.put(put); </pre>\n<pre>  }</pre>\n<pre>}</pre>","tags":["Hbase","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-18 19:41:57.0","id":34104,"title":"Hi, I have hdfs directory with files like \"filename-yyyy-mm-ddThh:mm:ss\".  i wand to load these files into my hive table, between any two given dates. Please help.","body":"","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-18 15:16:41.0","id":34039,"title":"Can't delete rows in Hive table + complex hive request","body":"<p>Hi,</p><p>I would like to get rid of some duplicates in my Hive table. The duplicates differs by the variable DateExtraction.</p><p> DateExtraction is a date formated as \"yyyy/MM/dd\". In case of duplicate i would like to keep the most recent observation.</p><p>Any clues are most welcome :)</p><p>Here is the query i used to create my table:</p><pre>DROP TABLE IF EXISTS netodb.oneTable;\nCREATE EXTERNAL TABLE IF NOT EXISTS netodb.oneTable(\n  Entreprise string,Industrie string,Source string,Type string,Auteur string,\n  Id string,Texte string,DateCreation string,Likes bigint,Partages bigint,\n  ObjetParent string,Users_mentionned string,Hashtags_uses string,View_count string,\n  Dislike_count string,Comment_count string,DateExtraction date) \nROW FORMAT DELIMITED\nFIELDS TERMINATED BY \"|\"\nLOCATION \"/tmp/NetoDB/OneTable\" \nTBLPROPERTIES(\"transactional\"=\"true\");</pre>","tags":["Hive","hdfs-permissions","delete"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-20 13:46:14.0","id":34536,"title":"Need to move all the individual text files into one text file in HDFS","body":"<p>1) When I given the below command, I am getting a lot of text files. The size of the text files is very small, I need to move all the text files data into one text file. So that the size of the file is big.</p><p><strong>Command Given on the Terminal: </strong>hadoop fs -ls hadoopcli2/queue_paths</p><p>\nFound 13 items</p><p>\n-rw-r--r--   3 sg865w hdfs        113 2016-05-12 13:02 hadoopcli2/queue_paths/2016-02-12-.txt </p><p>-rw-r--r--   3 sg865w hdfs        114 2016-05-12 13:02 hadoopcli2/queue_paths/2016-02-12-01-02.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        114 2016-05-12 13:04 hadoopcli2/queue_paths/2016-04-12-01-4.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        112 2016-05-12 13:06 hadoopcli2/queue_paths/2016-05-12-01-06.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        111 2016-05-12 14:31 hadoopcli2/queue_paths/2016-05-12-01-17.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        112 2016-05-12 13:21 hadoopcli2/queue_paths/2016-05-12-01-21.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        111 2016-05-12 13:31 hadoopcli2/queue_paths/2016-05-12-01-31.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        113 2016-05-12 14:53 hadoopcli2/queue_paths/2016-05-12-02-53.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        112 2016-05-12 16:10 hadoopcli2/queue_paths/2016-05-12-04-10.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        112 2016-05-12 13:03 hadoopcli2/queue_paths/2016-3-12-01-03.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        113 2016-05-12 13:06 hadoopcli2/queue_paths/2016-5-12-01-06.txt </p><p>-rw-r--r--   3 sg865w hdfs        114 2016-05-12 12:57 hadoopcli2/queue_paths/2016-57-12-.txt</p><p>\n-rw-r--r--   3 sg865w hdfs        113 2016-05-12 12:58 hadoopcli2/queue_paths/2016-58-12-.txt</p><p>Now When I am giving this command to move all these text files into one text file, I am not getting the result. Please help me on this.</p><p><strong>Command I am Giving: </strong>hadoop fs -getmerge hadoopcli2/queue_paths/*.txt testing/queue_paths/2016-05-19-16-05.txt</p><p>2) All these text files which I have shown in the above are generating for every minute, if the above process is successful how can i write a cronjob for every 15 min for the above one. Can you please help me on this.</p>","tags":["filesystem"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-22 10:42:59.0","id":34745,"title":"Oozie - Is it OK to modify its database?","body":"<p>hi\nWe've got the following error:</p><p>org.apache.openjpa.persistence.PersistenceException: ERROR: value too long for type character varying(255)\n{prepstmnt 190305649 INSERT INTO WF_JOBS (id, app_name, app_path, conf, \ncreated_time, end_time, external_id, group_name, last_modified_time, \nlog_token, parent_id, proto_action_conf, run, sla_xml, start_time, \nstatus, user_name, wf_instance) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,\n ?, ?, ?, ?, ?, ?, ?)} [code=0, state=22001]</p><p>and to solve it we've altered the WF_JOBS table, resizing the app_path and app_name to be varchar(1024).</p><p>Is it acceptable to modify the oozie database?</p><p>Will it cause us problems in other parts of oozie?</p><p>We did  while using postgreSQL for oozie db.</p><p>Thanks</p><p>Doron</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-24 08:36:21.0","id":35138,"title":"Target Replicas is 5 but found 3 live replica(s)","body":"<p>Hello,</p><p>I have 1 NameNode and 3 DataNodes using the default in &gt;dfs.replication&lt; (3).</p><p>\"hdfs fsck /\" shows this example output:</p><p>........\n/apps/accumulo/data/tables/!0/table_info/A0000ncg.rf:  Under replicated BP-1501447354-10.79.210.78-1461068133478:blk_1073774243_33440. Target Replicas is 5 but found 3 live replica(s), 0 decommissioned replica(s) and 0 decommissioning replica(s).\n</p><p>.\n</p><p>.............................................................................................\n</p><p>/user/accumulo/.Trash/Current/apps/accumulo/data/tables/!0/table_info/A0000ncf.rf:  Under replicated BP-1501447354-10.79.210.78-1461068133478:blk_1073774242_33439. Target Replicas is 5 but found 3 live replica(s), 0 decommissioned replica(s) and 0 decommissioning replica(s).\n..........</p><p>After \"hdfs dfs -setrep -w 3 /\" the messages are gone but after a while shown again.</p><p>How and where can I define that accumulo will use 3 replicas or is it a config issue of HDFS?</p><p>:-) Klaus</p>","tags":["Accumulo","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-24 07:29:45.0","id":35125,"title":"Knox Error after configuring namenode HA","body":"<p>knox was running fine. I was able to check  Knox using curl comman.</p><p>After I configured name node ha, Knox is not working. can u pls suggest.</p>","tags":["Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-25 20:32:01.0","id":35623,"title":"Installing Zeppelin 0.6 on Sandbox","body":"<p>I am trying to update my Zeppelin in sandbox to get the last version. So removed the ambari service </p><pre>export SERVICE=ZEPPELIN\nexport PASSWORD=admin\nexport AMBARI_HOST=localhost\n\n#detect name of cluster\noutput=`curl -u admin:$PASSWORD -i -H 'X-Requested-By: ambari'  http://$AMBARI_HOST:8080/api/v1/clusters`\nCLUSTER=`echo $output | sed -n 's/.*\"cluster_name\" : \"\\([^\\\"]*\\)\".*/\\1/p'`\n\n#unregister the service from ambari\ncurl -u admin:$PASSWORD -i -H 'X-Requested-By: ambari' -X DELETE http://$AMBARI_HOST:8080/api/v1/clusters/$CLUSTER/services/$SERVICE\n\n#if above errors out, run below first to fully stop the service\n#curl -u admin:$PASSWORD -i -H 'X-Requested-By: ambari' -X PUT -d '{\"RequestInfo\": {\"context\" :\"Stop $SERVICE via REST\"}, \"Body\": {\"ServiceInfo\": {\"state\": \"INSTALLED\"}}}' http://$AMBARI_HOST:8080/api/v1/clusters/$CLUSTER/services/$SERVICE\n\nrm -rf /opt/incubator-zeppelin\nrm -rf /var/log/zeppelin*\nrm -rf /var/run/zeppelin*\nsudo -u hdfs hadoop fs -rmr /apps/zeppelin\nrm -rf /var/lib/ambari-server/resources/views/zeppelin-view-1.0-SNAPSHOT.jar\nuserdel -r zeppelin\nVERSION=`hdp-select status hadoop-client | sed 's/hadoop-client - \\([0-9]\\.[0-9]\\).*/\\1/'`\nrm -rf /var/lib/ambari-server/resources/stacks/HDP/$VERSION/services/ZEPPELIN\nrm -f /tmp/zeppelin.tar.gz\nservice ambari-server restart</pre><p>Then I tried to install the new version by following this guide: http://hortonworks.com/hadoop-tutorial/apache-zeppelin-hdp-2-4/</p><p>However the install fails and I am getting the below error</p><pre>resource_management.core.exceptions.Fail: Execution of '/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/setup_snapshot.sh /usr/hdp/current/zeppelin-server/lib sandbox.hortonworks.com 9083 10001 sandbox.hortonworks.com 9995 True /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package /usr/lib/jvm/java &gt;&gt; /var/log/zeppelin/zeppelin-setup.log' returned 1. /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/setup_snapshot.sh: line 71: conf/hive-site.xml: Permission denied</pre><p>It looks like the old files are still here and Ambari don't have the right to write. When I delete /usr/hdp/current/zeppelin-server/ the install fails too because the zeppelin-server is not created.</p><p>Can you help me ?</p><p>Thanks</p>","tags":["zeppelin","ambari-service","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-30 18:36:03.0","id":36480,"title":"Oozie work flow validation error.","body":"<p>Can anybody let me know where I am going wrong in the below workflow?</p><pre>&lt;workflow-app xmlns=\"uri:oozie:workflow:0.4\" name=\"wf\"&gt;\n &lt;start to=\"create\"/&gt;  \n  &lt;action name=\"create\"&gt;\n     &lt;pig&gt;\n         &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n         &lt;name-node&gt;${namenode}&lt;/name-node&gt;\n         &lt;prepare&gt;\n             &lt;delete path=\"${nameNode}/user/${appRoot}/Output\"/&gt;\n         &lt;/prepare&gt;\n         &lt;configuration&gt;\n             &lt;property&gt;\n                 &lt;name&gt;mapred.job.queue.name&lt;/name&gt;\n                 &lt;value&gt;${queueName}&lt;/value&gt;\n             &lt;/property&gt;\n             &lt;property&gt;\n                 &lt;name&gt;mapred.compress.map.output&lt;/name&gt;\n                 &lt;value&gt;true&lt;/value&gt;\n             &lt;/property&gt;\n         &lt;/configuration&gt;\n         &lt;script&gt;script.pig&lt;/script&gt;\n         &lt;param&gt;input=/user/${appRoot}/clean.log&lt;/param&gt;\n         &lt;param&gt;output=/user/${appRoot}/Output&lt;/param&gt;\n     &lt;/pig&gt;\n     &lt;ok to=\"end\"/&gt;\n     &lt;error to=\"fail\"/&gt;\n  &lt;/action&gt; \n  &lt;kill name=\"fail\"&gt;\n    &lt;message&gt;Process killed&lt;/message&gt;\n  &lt;/kill&gt;\n &lt;end name=\"end\"/&gt;\n&lt;/workflow-app&gt;</pre><p>I get the below error</p><pre>Error: E0701: XML schema error, /Oozie/app/test1.xml, org.xml.sax.SAXParseException; lineNumber: 31; columnNumber: 16; cvc-complex-type.2.3: Element 'workflow-app' cannot have character [children], because the type's content type is element-only.</pre>","tags":["Oozie","Pig","workflow"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-31 03:05:13.0","id":36495,"title":"ranger hdfs plugin not working","body":"<p>2016-05-05 16:11:22,416 ERROR util.PolicyRefresher (PolicyRefresher.java:loadPolicyfromPolicyAdmin(228)) - PolicyRefresher(serviceName=FSDP_hadoop): failed to refresh policies. Will continue to use last known version of policies (-1)\ncom.sun.jersey.api.client.ClientHandlerException: java.net.ConnectException: Connection refused\n        at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149)\n        at com.sun.jersey.api.client.Client.handle(Client.java:648)\n        at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670)\n        at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)\n        at com.sun.jersey.api.client.WebResource$Builder.get(WebResource.java:503)\n        at org.apache.ranger.admin.client.RangerAdminRESTClient.getServicePoliciesIfUpdated(RangerAdminRESTClient.java:71)\n        at org.apache.ranger.plugin.util.PolicyRefresher.loadPolicyfromPolicyAdmin(PolicyRefresher.java:205)\n        at org.apache.ranger.plugin.util.PolicyRefresher.loadPolicy(PolicyRefresher.java:175)\n        at org.apache.ranger.plugin.util.PolicyRefresher.run(PolicyRefresher.java:154)\nCaused by: java.net.ConnectException: Connection refused\n        at java.net.PlainSocketImpl.socketConnect(Native Method)\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n        at java.net.Socket.connect(Socket.java:579)\n        at sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\n        at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)\n        at sun.net.www.http.HttpClient.New(HttpClient.java:308)\n        at sun.net.www.http.HttpClient.New(HttpClient.java:326)\n        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)</p>","tags":["ranger-0.5.0"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-26 08:25:21.0","id":35720,"title":"query to find the null table detail in Phoenix","body":"<p>can any one help me with query to find the details with null or empty column value from pheniox .</p>","tags":["Phoenix"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-01 13:32:48.0","id":37025,"title":"Oozie workflow does not terminate even after completing successfully","body":"<pre>I have two workflows that run 4 minutes apart. I notice that when at a stage, usually after close of        business, the workflows do not terminate gracefully even after successfully completing the loading process. Is there any oozie parameter i'm missing? \n\n\n</pre>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-01 22:00:47.0","id":37159,"title":"Installation of Streamsets on HDP 2.4 sandbox","body":"<p>Hi,</p><p>I am bit new to  Streamsets. However, i want to try it out in a project and do  a comparison vs NIFI . </p><p>I have searched quite a lot but was not able to  find the installation procedure for streamsets on HDP sandbox. Can someone please help me  point to any resource or guide. </p><p>Thanks,</p><p>Raghvendra Singh</p>","tags":["streaming"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-02 01:27:31.0","id":37158,"title":"Does Ambari supports any kind of change management ?","body":"<p>Is there any possibility to track configuration changes in Ambari?</p>","tags":["Ambari"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-02 11:08:55.0","id":37217,"title":"for HDPCD:Java exam task, to be clear does \"Perform a join of two or more datasets\" means Reduce side join.  As there is another task specifically says \"Perform a map-side join of two datasets\"\"","body":"","tags":["hdpcd"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-02 18:35:14.0","id":37368,"title":"Filter empty Fields out of Tuple in Pig","body":"<p>Hi,</p><p>Anyone tried filtering out the empty fields out of a tuple in pig ? </p><p>I have tuples which are loaded with schema defined & need to filter out the null fields out of each tuple with updated schema in the new alias. </p><p>Eg : </p><p>I have a tuple (1,2,,,,6,,,9)  -&gt; output should be like (1,2,6,9) </p><p>or </p><p>(1,,,,5,,7,,9) -&gt; output should be like (1,5,7,9) </p><p>-Thanks</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-06 13:09:20.0","id":37843,"title":"HDP 2.4 installation on windows server 2008 R2","body":"<p>Hi:</p><p>\nI installation HDP 2.4,but it doesn't work.There is the install logs.I didn't find where it happened,please help me. </p><p><a href=\"/storage/attachments/4785-hdplog.txt\">hdplog.txt</a></p><p><a href=\"/storage/attachments/4785-hdplog.txt\"></a>Thanks.</p>","tags":["hdp-2.4.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-06 20:32:07.0","id":37970,"title":"unable to connect to host using Phoenix","body":"<p>Below is the error from our application logs:</p><p>Jun 06, 2016 12:32:25 PM org.apache.zookeeper.ZooKeeper &lt;init&gt;</p><pre>INFO: Initiating client connection,\nconnectString=nykdsr000003239.intranet.barcapint.com:2181,nykdsr000003238.intranet.barcapint.com:2181,nykdsr000003240.intranet.barcapint.com:2181\nsessionTimeout=60000 watcher=hconnection-0x755f37310x0,\nquorum=nykdsr000003239.intranet.barcapint.com:2181,nykdsr000003238.intranet.barcapint.com:2181,nykdsr000003240.intranet.barcapint.com:2181,\nbaseZNode=/hbase\nJun 06, 2016 12:32:25 PM org.apache.zookeeper.ClientCnxn$SendThread\nlogStartConnect\nINFO: Opening socket connection to server\nnykdsr000003239.intranet.barcapint.com/10.60.72.79:2181. Will not attempt to\nauthenticate using SASL (unknown error)\nJun 06, 2016 12:32:25 PM org.apache.zookeeper.ClientCnxn$SendThread\nprimeConnection\nINFO: Socket connection established, initiating session, client:\n/10.173.76.85:52746, server:\nnykdsr000003239.intranet.barcapint.com/10.60.72.79:2181\nJun 06, 2016 12:32:25 PM org.apache.zookeeper.ClientCnxn$SendThread\nonConnected\nINFO: Session establishment complete on server\nnykdsr000003239.intranet.barcapint.com/10.60.72.79:2181, sessionid =\n0x2552005d31512ac, negotiated timeout = 60000\nJun 06, 2016 12:32:31 PM org.apache.phoenix.metrics.Metrics initialize\nINFO: Initializing metrics system: phoenix\nJun 06, 2016 12:32:32 PM org.apache.hadoop.metrics2.impl.MetricsConfig\nloadFirst\nWARNING: Cannot locate configuration: tried\nhadoop-metrics2-phoenix.properties,hadoop-metrics2.properties\nJun 06, 2016 12:32:32 PM org.apache.hadoop.metrics2.impl.MetricsSystemImpl\nstartTimer\nINFO: Scheduled snapshot period at 10 second(s).\nJun 06, 2016 12:32:32 PM\norg.apache.hadoop.metrics2.impl.MetricsSystemImpl start\nINFO: phoenix metrics system started\nJun 06, 2016 12:32:34 PM org.apache.hadoop.conf.Configuration\nwarnOnceIfDeprecated\nINFO: hadoop.native.lib is deprecated. Instead, use\nio.native.lib.available\nJun 06, 2016 12:32:37 PM\norg.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper &lt;init&gt;\nINFO: Process identifier=hconnection-0x2b55d383 connecting to ZooKeeper\nensemble=nykdsr000003239.intranet.barcapint.com:2181,nykdsr000003238.intranet.barcapint.com:2181,nykdsr000003240.intranet.barcapint.com:2181\nJun 06, 2016 12:32:37 PM\norg.apache.zookeeper.ZooKeeper &lt;init&gt;\nINFO: Initiating client\nconnection, connectString=nykdsr000003239.intranet.barcapint.com:2181,nykdsr000003238.intranet.barcapint.com:2181,nykdsr000003240.intranet.barcapint.com:2181\nsessionTimeout=60000 watcher=hconnection-0x2b55d3830x0,\nquorum=nykdsr000003239.intranet.barcapint.com:2181,nykdsr000003238.intranet.barcapint.com:2181,nykdsr000003240.intranet.barcapint.com:2181,\nbaseZNode=/hbase\nJun 06, 2016 12:32:37 PM org.apache.zookeeper.ClientCnxn$SendThread\nlogStartConnect\nINFO: Opening socket connection to server\nnykdsr000003239.intranet.barcapint.com/10.60.72.79:2181. Will not attempt to\nauthenticate using SASL (unknown error)\nJun 06, 2016 12:32:37 PM org.apache.zookeeper.ClientCnxn$SendThread\nprimeConnection\nINFO: Socket connection established, initiating session, client:\n/10.173.76.85:52749, server: nykdsr000003239.intranet.barcapint.com/10.60.72.79:2181\nJun 06, 2016 12:32:37 PM org.apache.zookeeper.ClientCnxn$SendThread\nonConnected\nINFO: Session establishment complete on server\nnykdsr000003239.intranet.barcapint.com/10.60.72.79:2181, sessionid =\n0x2552005d31512ad, negotiated timeout = 60000\nJun 06, 2016 12:33:47 PM\norg.apache.hadoop.hbase.client.RpcRetryingCaller callWithRetries\nINFO: Call exception, tries=10,\nretries=35, started=68364 ms ago, cancelled=false, msg=row '' on table\n'SYSTEM.CATALOG' at\nregion=SYSTEM.CATALOG,,1444218102919.828f7e39c3ea24467feb470c07ca9e84.,\nhostname=nykdsr000003241.intranet.barcapint.com,60020,1452857807728, seqNum=311\nJun 06, 2016 12:34:07 PM\norg.apache.hadoop.hbase.client.RpcRetryingCaller callWithRetries\nINFO: Call exception, tries=11,\nretries=35, started=88541 ms ago, cancelled=false, msg=row '' on table\n'SYSTEM.CATALOG' at region=SYSTEM.CATALOG,,1444218102919.828f7e39c3ea24467feb470c07ca9e84.,\nhostname=nykdsr000003241.intranet.barcapint.com,60020,1452857807728, seqNum=311\nJun 06, 2016 12:34:27 PM\norg.apache.hadoop.hbase.client.RpcRetryingCaller callWithRetries\nINFO: Call exception, tries=12,\nretries=35, started=108617 ms ago, cancelled=false, msg=row '' on table\n'SYSTEM.CATALOG' at\nregion=SYSTEM.CATALOG,,1444218102919.828f7e39c3ea24467feb470c07ca9e84.,\nhostname=nykdsr000003241.intranet.barcapint.com,60020,1452857807728, seqNum=311\nJun 06, 2016 12:34:47 PM\norg.apache.hadoop.hbase.client.RpcRetryingCaller callWithRetries\nINFO: Call exception, tries=13,\nretries=35, started=128796 ms ago, cancelled=false, msg=row '' on table\n'SYSTEM.CATALOG' at\nregion=SYSTEM.CATALOG,,1444218102919.828f7e39c3ea24467feb470c07ca9e84.,\nhostname=nykdsr000003241.intranet.barcapint.com,60020,1452857807728, seqNum=311\nJun 06, 2016 12:35:07 PM\norg.apache.hadoop.hbase.client.RpcRetryingCaller callWithRetries\nINFO: Call exception, tries=14,\nretries=35, started=148878 ms ago, cancelled=false, msg=row '' on table\n'SYSTEM.CATALOG' at\nregion=SYSTEM.CATALOG,,1444218102919.828f7e39c3ea24467feb470c07ca9e84.,\nhostname=nykdsr000003241.intranet.barcapint.com,60020,1452857807728, seqNum=311\nJun 06, 2016 12:35:28 PM\norg.apache.hadoop.hbase.client.RpcRetryingCaller callWithRetries\nINFO: Call exception, tries=15,\nretries=35, started=169088 ms ago, cancelled=false, msg=row '' on table\n'SYSTEM.CATALOG' at\nregion=SYSTEM.CATALOG,,1444218102919.828f7e39c3ea24467feb470c07ca9e84.,\nhostname=nykdsr000003241.intranet.barcapint.com,60020,1452857807728, seqNum=311</pre>","tags":["Hbase","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-07 09:50:36.0","id":38112,"title":"SmartSense Bundle Capture Failure","body":"<p>In my cluster called 'Islero' (6 nodes) this SmartSense doesn't  work, Ambari give me this message related a single node:</p><p>Last SmartSense bundle was not successful. Bundle key:unspecified_islero_0_2016-06-05_02-00-01, status: failed.</p><p>I've tried to solve it with a simple service restart but i received the same response.</p><p>Please help me!!!!! :(</p><p>Thanks</p>","tags":["smartsense","Ambari"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-06-07 14:19:02.0","id":38187,"title":"HORTONWORKS HDP 2.3.2 Installation","body":"<p>Dear all,</p><p>I am installing HDP 2.3.2 stack in my computer . My Deployment architecture as follows : 2-Master nodes and 2- data nodes. I am using to create this cluster in MY Laptop using the virtuall box with 4-instance of Cent OS . I deployed Ambari server successfully and then I started to deploy HDP sack from Ambari automated install , I sucessfuly passed below steps in Ambari :</p><p>1.Get started</p><p>2. Select Stack</p><p>3.Install options</p><p>4.Conform hosts</p><p>5.Assign masters:</p><p>6. Assign salves and clients</p><p>7.Customize services</p><p>8.Review </p><p>9. Install, Start and Test  ---&gt; Currently I am in this step.</p><p>Even in above step, ambari shows sucellfully instlled all services in all 4-nodes . After that it has to start the service in each node, but from theire there is no prograss --- I even waited for more then 14-Hours after install still same position:</p><p>Pl find the screen shot below :</p><p><img style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\"></p><p></p><p><img src=\"/storage/attachments/4824-hdp-install.png\"></p><p><img src=\"/storage/attachments/4825-hdp.png\"></p><p>Can some one help us why was it not able to start any of service. Even I checked the logs no clue . You support on this really appreciate on this.</p><p>Br,</p><p>meethadoop</p>","tags":["hdp-2.3.2"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-06-08 11:14:29.0","id":38441,"title":"Logstash infrastructure to monitor my Hadoop/Hue and Hive .","body":"<p>I have already a server configured with logstash and also Kibana is configured and showing the logs currently on the server.\nNow in another server where Hadoop is running I want also to capture the logs generated by a certain component (e.g HDFS) and send the logs to the central server where logstash is running.</p><p>\nMy questions are:</p><p>\n - What do I need to install on the Hadoop server elasticsearch or some .jar files? And in what exactly will consists the configuration so that some logs (e.g in the directory \\vat\\log\\XX </p><p> - where to specify the agent configuration (remote logstash serve/output)?  </p>","tags":["HDFS","hadoop","logstash"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-08 12:41:43.0","id":38463,"title":"NameNode HA - Active / Standby Switch - Force them into roles?","body":"<p>Good day Hortonworks community!</p><p>Recently we've went from an extremely old version of Ambari/HDP to the bleeding edge latest edition.</p><p>Along the way I just about converted every service that could be made HA into HA.</p><p>Now here is an issue I get sometimes regarding to NN and SNN.</p><p>Sometimes NN/SNN will both become \"Active\" or \"Standby\" namenodes if one or the other crash.</p><p>Question: Is there some kind of setting to force them back into static roles when both NameNodes are online instead of having to manually restart the service of another?</p>","tags":["high-availability","bug","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-08 14:21:39.0","id":38516,"title":"Kerberos - Potential Security Threat","body":"<p>Hello experts,</p><p>I feel confident with Kerberos Authentication, however a recent article has created a panic among few customers, I would like to understand if there is a real threat and how others are thinking through it.</p><p>http://news4security.com/posts/2015/12/old-microsoft-kerberos-vulnerability-gets-new-spotlight/</p><p>The article talks about various ways to attack Kerberos and obtain or pass forged tickets.\n</p><p>Would be real helpful if security experts can clear the air, specially what these threat means in hadoop world (if any) </p><p>Thanks</p><p>Mayank</p>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-14 11:55:08.0","id":39631,"title":"Ambari Metrics distributed mode - HBASE service required?","body":"<p>Hi,</p><p>I have switched Ambari Metrics from \"embedded\" mode to \"distributed\" mode and it seems to work well. However I do not have HBASE service deployed in the cluster, but I assume that Ambari Metrics is using HBASE is standalone mode. </p><p>The question is wether Ambari Metrics can benefit somehow from the HBASE service running on my cluster.</p>","tags":["Hbase","metrics-collector"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-15 07:08:43.0","id":39852,"title":"How to combine Hive table files for input to mapreduce?","body":"<p>I have a HDP 2.0 cluster where I'm executing a mapreduce program which takes Hive(0.14) table as input. There are a large number of small files for the Hive table and hence large number of mapper containers are being requested. Please let me know if there is a way to combine small files before being input to mapreduce job?</p>","tags":["data","mapper","Hive","yarn-container"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-15 14:54:21.0","id":39949,"title":"Hadoop and Hortonworks help","body":"<p>I work with a consulting firm in Indianapolis, Centric Consulting.    I have a client that needs help with Hadoop and Hortonworks.  Is there anyone that would be willing to work in Indianapolis on a long term project with Hadoop?  If so, please send me an email at richard.mcgee@centricconsulting.com.</p>","tags":["connection"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-15 20:34:23.0","id":40027,"title":"Nifi and ISO 8583","body":"<p>Is there a way to use Nifi to convert a ISO 8583 message? Converting to json would be nice. Soon I'll be getting a feed from our Mainframe every hour and it will be in ISO 8583 format. I'll need to convert it and process it.</p><p>Thanks for any help,</p><p>Chris</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-15 19:15:41.0","id":40005,"title":"Is it possible to poll RDBMS tables for changed records in NiFi?","body":"","tags":["nifi-processor","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-09-25 17:58:11.0","id":294,"title":"Running distcp between two cluster: One Kerberized and the other is not","body":"<p>hadoop distcp  -i -log /tmp/ hdfs://xxx:8020/apps/yyyy hdfs://xxx_cid/tmp/</p><p>In this case the \"xxx\" is the \"un-secure\" cluster, while \"xxx_cid\" in the secure cluster.</p><p>We are launching the job from the Kerberos cluster, with the appropriate kinit for the user and getting the following error:</p><p>java.io.IOException: Failed on local exception: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.; Host Details : local host is: \"xxx<a href=\"http://abhdp-dn-a01.marketo.org/10.1.32.7\">/10.x.x.</a>x\"; destination host is: \"xxx\":8020;</p><p>...</p><p>Caused by: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.</p><p>I thought by launching the job from the secure cluster, that we could avoid any access issues.  But it appears that the processes are kicked off from the \"source\" cluster.  In this case, that's the insecure cluster.</p><p>Idea's on getting around this?</p>","tags":["distcp","kerberos","hdp-2.3.0"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-24 22:20:26.0","id":210,"title":"Nifi for RabbitMQ and Couchbase","body":"<p> Can someone please confirm if I can use a NiFi to load data into HDP 2.3 from </p><p>1.MS SQL (To Hive if possible). I know I can Sqoop it, but having another option to load data using NiFi would add more value to introduce NiFi. </p><p>2.CouchDB </p><p>\n3.RabbitMQ</p><p>Thanks</p><p>Sundar R</p>","tags":["database","couchbase","Nifi","rabbitmq"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-09-29 01:41:02.0","id":415,"title":"I upgraded Ambari to Ambari 2.1.0 and HDP to HDP2.3.0. Now YARN configs do not show up in Ambari UI. Please suggest.","body":"","tags":["hdp-2.3.4","upgrade","Ambari","ambari-2.1.0","YARN"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-30 13:20:33.0","id":607,"title":"How do you Remove a Service from HDP via Ambari","body":"<p>I no longer have a use for Accumulo on my cluster and would like to delete the service. But when I tried to do that in Ambari, I noticed I could only add a service, not remove one. </p>","tags":["Ambari","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 23:04:34.0","id":572,"title":"Does HDP 2.2 supports SAML integration?","body":"","tags":["saml"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-28 22:19:20.0","id":373,"title":"What methods are available for moving files into an HDFS Encryption Zone?","body":"<p>is distcp the best/only mechanism for moving files into an encryption zone? The documentation says so. But would an hdfs mv or cp also work just fine?</p>","tags":["encryption","security","HDFS"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 13:28:55.0","id":460,"title":"Ambari Kerberos Wizard: How to configure Active Directory LDAPS?","body":"<p>How is Active Directory configured to support LDAPS for the Ambari Kerberos wizard?</p>","tags":["Ambari","active-directory","security","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-03 13:10:16.0","id":840,"title":"Best practice around provenance and ingesting large files","body":"<p>Hi, what are the recommended approaches for handling the following scenario? NiFi is ingesting lots of files (say, pull from a remote system into the flow), and we care about file as a whole only, so flowfile content is the file, no further splits or row-by-row processing. The size of files can vary from few MBs to GBs, which is not the problem, but what happens when there are millions of files ingested this way? Say, they end up in HDFS in the dataflow.</p><p>Given that file content will be recorded in the content repository to enable data provenance, disk space may become an issue. Any way to control this purge/expiration on a more fine-grained level other than instance-wide journal settings?</p>","tags":["hdf","provenance","best-practices","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-04 19:00:31.0","id":857,"title":"HUE 3.8 installation on HDP 2.3 & SLES 11 OS","body":"<p>I am trying to install HUE 3.8.1 on HDP 2.3 cluster installed over SLES 11 by following this blog post “<a href=\"http://gethue.com/hadoop-hue-3-on-hdp-installation-tutorial/\">http://gethue.com/hadoop-hue-3-on-hdp-installation-tutorial/</a>”</p><p>But struggling to get all the prerequisite packages, below are packages which i’m not able to install with zypper(yast): krb5-devel , mysql-devel, openssl-devel , cyrus-sasl-devel , cyrus-sasl-gssapi, sqlite-devel, libtidy , libxml2-devel , libxslt-devel, openldap-devel, python-devel , python-setuptools</p><p>are there any online repos available from where i can get these. I was successfully in installing HUE on CentOS HDP cluster using same steps.\nRealy appreciate Any help and pointers on this.</p>","tags":["hdp-2.3.0","installation","hue"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-04 16:10:10.0","id":855,"title":"Rolling upgrade failing for a clean cluster from HDP 2.2.8 to HDP 2.3.2","body":"<p>Installed a fresh HDP 2.2.8 cluster, and then went through all the steps to perform a rolling upgrade to the new HDP 2.3.2 release, the service checks for MapReduce2, Tez and Pig all fail.</p><p>Can't see anything in the known issues of the release notes for this.</p><p>Once the upgrade runs all the way through I'll take a look for some debug logs, but for now just wanted to see if anyone else had seen this?</p>","tags":["upgrade","ambari-2.1.2","hdp-2.3.0","hdp-2.2.8"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-02 22:15:04.0","id":818,"title":"IPA Kerberos not liking my kinit Ticket","body":"<p>I've just installed and Kerberized my cluster:</p><p>Ambari 2.1.1</p><p>CentOS 7</p><p>IPA 4 for LDAP and Kerberos (IPA Clients Configured across the cluster hosts)</p><p>Oracle JDK 1.7.0_79 (with JCE)</p><p>HDP 2.3.0</p><p>The cluster comes up just fine and all the services seem to be happy talking to each other.  So I'm pretty convinced that all the keytabs are configured correctly.</p><p>From any node on the cluster, after getting a valid ticket (kinit) and trying to do a basic hdfs command, I get (kerberos debug enabled): <strong><em>ONLY HAPPENS FROM IPA Clients. Other host client access works fine (read on).</em></strong></p><pre>-sh-4.2$ klist\nTicket cache: KEYRING:persistent:100035:krb_ccache_T7mkWNw\nDefault principal: dstreev@HDP.LOCAL\n\n\nValid starting       Expires              Service principal\n10/02/2015 09:17:07  10/03/2015 09:17:04  krbtgt/HDP.LOCAL@HDP.LOCAL\n-sh-4.2$ hdfs dfs -ls .\nJava config name: null\nNative config name: /etc/krb5.conf\nLoaded from native config\n&gt;&gt;&gt;KinitOptions cache name is /tmp/krb5cc_100035\n15/10/02 18:07:48 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n15/10/02 18:07:48 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n15/10/02 18:07:48 INFO retry.RetryInvocationHandler: Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over m2.hdp.local/10.0.0.161:8020 after 1 fail over attempts. Trying to fail over immediately.\njava.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"m3.hdp.local/10.0.0.162\"; destination host is: \"m2.hdp.local\":8020;\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1431)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1358)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:252)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1655)\n\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:326)\n\tat org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:235)\n\tat org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:218)\n\tat org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:201)\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:165)\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:287)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:340)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n\tat org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:685)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:648)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:735)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:373)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1397)\n\t... 28 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n\tat com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)\n\tat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413)\n\tat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:558)\n\tat org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:373)\n\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:727)\n\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:723)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:722)\n\t... 31 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n\tat sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n\tat sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n\tat sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n\tat sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n\tat sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n\tat sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n\tat com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)\n\t... 40 more\n15/10/02 18:07:48 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n</pre><p>(it retries 15-20 times before quitting, which happens really fast.</p><p>If I try to access the cluster from a host that is NOT part of the IPA hosts (from my Mac, as an example). I do <strong>NOT</strong> get this error, and I can interact with the cluster.</p><pre>➜  conf  klist\nCredentials cache: API:D44F3F89-A095-40A5-AA7C-BD06698AA606\n        Principal: dstreev@HDP.LOCAL\n  Issued                Expires               Principal\nOct  2 17:52:13 2015  Oct  3 17:52:00 2015  krbtgt/HDP.LOCAL@HDP.LOCAL\nOct  2 18:06:53 2015  Oct  3 17:52:00 2015  host/m3.hdp.local@HDP.LOCAL\n➜  conf  hdfs dfs -ls /\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nJava config name: null\nNative config name: /etc/krb5.conf\nLoaded from native config\n15/10/02 18:10:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n&gt;&gt;&gt;KinitOptions cache name is /tmp/krb5cc_501\n&gt;&gt; Acquire default native Credentials\nUsing builtin default etypes for default_tkt_enctypes\ndefault etypes for default_tkt_enctypes: 18 17 16 23.\n&gt;&gt;&gt; Obtained TGT from LSA: Credentials:\n      client=dstreev@HDP.LOCAL\n      server=krbtgt/HDP.LOCAL@HDP.LOCAL\n    authTime=20151002215213Z\n   startTime=20151002215213Z\n     endTime=20151003215200Z\n   renewTill=20151009215200Z\n       flags=FORWARDABLE;RENEWABLE;INITIAL;PRE-AUTHENT\nEType (skey)=18\n   (tkt key)=18\n15/10/02 18:10:59 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\nFound ticket for dstreev@HDP.LOCAL to go to krbtgt/HDP.LOCAL@HDP.LOCAL expiring on Sat Oct 03 17:52:00 EDT 2015\nEntered Krb5Context.initSecContext with state=STATE_NEW\nFound ticket for dstreev@HDP.LOCAL to go to krbtgt/HDP.LOCAL@HDP.LOCAL expiring on Sat Oct 03 17:52:00 EDT 2015\nService ticket not found in the subject\n&gt;&gt;&gt; Credentials acquireServiceCreds: same realm\nUsing builtin default etypes for default_tgs_enctypes\ndefault etypes for default_tgs_enctypes: 18 17 16 23.\n&gt;&gt;&gt; CksumType: sun.security.krb5.internal.crypto.RsaMd5CksumType\n&gt;&gt;&gt; EType: sun.security.krb5.internal.crypto.Aes256CtsHmacSha1EType\n&gt;&gt;&gt; KdcAccessibility: reset\n&gt;&gt;&gt; KrbKdcReq send: kdc=m3.hdp.local UDP:88, timeout=30000, number of retries =3, #bytes=654\n&gt;&gt;&gt; KDCCommunication: kdc=m3.hdp.local UDP:88, timeout=30000,Attempt =1, #bytes=654\n&gt;&gt;&gt; KrbKdcReq send: #bytes read=637\n&gt;&gt;&gt; KdcAccessibility: remove m3.hdp.local\n&gt;&gt;&gt; EType: sun.security.krb5.internal.crypto.Aes256CtsHmacSha1EType\n&gt;&gt;&gt; KrbApReq: APOptions are 00100000 00000000 00000000 00000000\n&gt;&gt;&gt; EType: sun.security.krb5.internal.crypto.Aes256CtsHmacSha1EType\nKrb5Context setting mySeqNumber to: 227177742\nCreated InitSecContextToken:\n0000: 01 00 6E 82 02 3C 30 82   02 38 A0 03 02 01 05 A1  ..n..&lt;0..8......\n0010: 03 02 01 0E A2 07 03 05   00 20 00 00 00 A3 82 01  ......... ......\n...\n0230: 99 AC EE FB DF 86 B5 2A   19 CB A1 0B 8A 8E F7 9B  .......*........\n0240: 81 08                                              ..\n\nEntered Krb5Context.initSecContext with state=STATE_IN_PROCESS\n&gt;&gt;&gt; EType: sun.security.krb5.internal.crypto.Aes256CtsHmacSha1EType\nKrb5Context setting peerSeqNumber to: 40282898\nKrb5Context.unwrap: token=[05 04 01 ff 00 0c 00 00 00 00 00 00 02 66 ab 12 01 01 00 00 8e 14 7a df 34 d7 c5 3d 5d d1 ce b5 ]\nKrb5Context.unwrap: data=[01 01 00 00 ]\nKrb5Context.wrap: data=[01 01 00 00 ]\nKrb5Context.wrap: token=[05 04 00 ff 00 0c 00 00 00 00 00 00 0d 8a 75 0e 01 01 00 00 9c a5 73 25 59 0f b5 64 24 f0 a8 78 ]\nFound 8 items\ndrwxrwxrwx   - yarn   hadoop          0 2015-09-28 15:55 /app-logs\ndrwxr-xr-x   - hdfs   hdfs            0 2015-09-28 15:57 /apps\ndrwxr-xr-x   - hdfs   hdfs            0 2015-09-28 15:53 /hdp\ndrwxr-xr-x   - mapred hdfs            0 2015-09-28 15:53 /mapred\ndrwxrwxrwx   - mapred hadoop          0 2015-09-28 15:54 /mr-history\ndrwxr-xr-x   - hdfs   hdfs            0 2015-09-28 19:20 /ranger\ndrwxrwxrwx   - hdfs   hdfs            0 2015-09-29 13:09 /tmp\ndrwxr-xr-x   - hdfs   hdfs            0 2015-10-02 17:51 /user\n➜  conf\n</pre><p>Since I can get to the cluster and interact with it, from a host that hasn't been configured by the IPA client.  I'm pretty sure that my IPA environment is tweaked.</p><p>Any idea where to look in IPA to fix this for the hosts that are part of the IPA environment?</p>","tags":["kerberos","security","ipa"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-05 06:03:17.0","id":875,"title":"Is multi-homing supported for HDFS NN on HDP 2.1.10","body":"<p>Referring to the guidelines at \"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html\" , there are properties required to be configured (dfs.namenode.http-bind-host and dfs.namenode.https-bind-host) which got introduced in Hadoop ver. 2.5 (ref. https://issues.apache.org/jira/browse/HDFS-6273)</p><p>Since HDP 2.1.10 is based on Hadoop 2.4, does it support configuring multi-homed cluster in such an environment ?</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-06 15:36:22.0","id":989,"title":"Where can I find the SmartSense view?","body":"","tags":["ambari-views","operations","smartsense","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-06 20:01:41.0","id":1015,"title":"hipchat test","body":"<p>test test test</p><p>test</p>","tags":["hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-05 17:21:48.0","id":907,"title":"Falcon not killing Oozie Bundle / Coordinator","body":"<p>I'm running a Falcon process at a certain frequency - Say around every 1 hour. I had to make a change to the frequency to run every 30 mins. I suspended the process and resumed the same in Falcon. The underlying Oozie Bundle and Coordinators are not getting killed. This results in running the workflow twice every hour.  I had to kill the Oozie Bundle/ Coordinator manually to run the changed proces. Could you please let me know if there are any workarounds or Falcon killing the Oozie Coordinator automatically.  I'm running it on HDP 2.3.</p><p>thanks!</p>","tags":["Oozie","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-07 19:18:53.0","id":1085,"title":"Can you disable Column Pruner?","body":"<p>Hive v13,hitting the semantic error: </p><p>2015-10-07 05:12:40,988 ERROR ql.Driver (SessionState.java:printError(547)) - FAILED: NullPointerException null\njava.lang.NullPointerException\nat org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory$ColumnPrunerSelectProc.process(ColumnPrunerProcFactory.java:618)\nat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)\nat org.apache.hadoop.hive.ql.optimizer.ColumnPruner$ColumnPrunerWalker.walk(ColumnPruner.java:166)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)\nat org.apache.hadoop.hive.ql.optimizer.ColumnPruner.transform(ColumnPruner.java:129)\nat org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:146)\nat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9414)\nat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:427)\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:323)\nat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:980)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1045)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:916)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:906)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:359)\nat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:456)\nat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:466)\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:749)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:212)</p><p>Looks very similar to: </p><p>https://issues.apache.org/jira/browse/HIVE-9185</p><p>Looks like the ability to disable pruner was taken away on Hive13: </p>hive.optimize.cp=false<p>https://issues.apache.org/jira/browse/HIVE-4113</p><p>Is there another way to disable the pruner temporarily? </p>","tags":["hiveserver2","Tez","Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-07 20:05:11.0","id":1089,"title":"Bulk Ingesting from Documentum and Sharepoint","body":"<p>Hello Experts!</p><p>  Wanted to check if we have recommendations for doing bulk ingesting from EMC Documentum and MS Sharepoint. I know for Sharepoint we can do REST API’s , but want to know if that is the optimum way of doing bulk ingest operation?</p><p>Thanks!</p>","tags":["data-ingestion","sharepoint","documentum"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-07 02:11:35.0","id":1034,"title":"Unable to run CTAS query using external table with gzipped data.","body":"<p>I think either this is function is not supported or I am missing something very basic.. but here is the issue - </p><p>1) Uploaded a GZipped CSV format file to HDFS - No issues</p><p>2) Created an external table using CSV Serde pointing LOCATION to the file in step 1 above. Once the table is created I am able to run queries without any problems. </p><p>3) Running a CTAS query with the exact same table layout but in ORC format causes the error below. </p><p>Please help !</p><p>------- Error ------- </p><p>Caused by: java.io.IOException: incorrect header check</p><p>        at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</p><p>        at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:228)</p><p>        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</p><p>        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</p><p>        at java.io.InputStream.read(InputStream.java:101)</p><p>        at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180)</p><p>        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)</p><p>        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</p><p>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:246)</p><p>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:47)</p><p>        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)</p><p>        ... 22 more</p><p>]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1443886863664_0003_1_00 [Map 1] killed/failed due to:null]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0</p><p>        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:170)</p><p>        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)</p><p>        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)</p><p>        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1653)</p><p>        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1412)</p><p>        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)</p><p>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)</p><p>        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)</p><p>        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)</p><p>        ... 11 more</p>","tags":["Hive","ctas"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-07 18:55:26.0","id":1084,"title":"Ranger KMS for HDFS Transparent Data Encryption: Switching KMS Keys","body":"<p>Customer would like to know if they are able to switch the keys which are stored in the KMS without re-encrypting HDFS data? I believe this may also be referred to as the EEK (Encrypted Encryption Key)?</p><p><a href=\"https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html#Key_Management_Server_KeyProvider_EDEKs\">Documentation here</a></p>","tags":["kms","security","ranger-kms"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-05 15:43:26.0","id":897,"title":"Are there best practices for integrating HDP with FoxT?","body":"<p>I am working with a group who uses FoxT to secure the root login on all Unix/Linux systems. We are trying to determine the best way to integrate this tool with the Hortonworks stack when also using AD for authentication and Ranger for authorization. Has anyone performed this integration? What is the best way to set up authentication in this environment?</p>","tags":["Ranger","security","foxt"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-06 23:20:54.0","id":1021,"title":"How to Remove all External Users from the Ranger / Ranger usersync database","body":"<p>In the course of testing the usersync tool, we had some settings wrong and the users are mis-synced. We'd like to clear them all out and restart the sync. Is there an easy way/tool to remove all the external users from the Ranger / Ranger usersync database so we can resync with our new settings?</p>","tags":["security","Ranger","operations","ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-08 19:09:41.0","id":1156,"title":"What is the best way to implement row-based security in Hive?","body":"","tags":["Hive","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-09 17:02:02.0","id":1212,"title":"One node complaining HDFS client install failed","body":"<p>Tried to restart HDFS service, no help. Any idea how to solve it?</p>","tags":["HDFS","Ambari","ambari-2.1.1","operations"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-13 14:52:33.0","id":1337,"title":"Oozie Non Default Database Setup","body":"<p>From Ambari 1.7 doc <a target=\"_blank\" href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-1.7.0.0/Ambari_Doc_Suite/ADS_v170.html#ref-f6bcf79a-a84e-4881-bb9d-6c94b2cca79d\">http://docs.hortonworks.com/HDPDocuments/Ambari-1.7.0.0/Ambari_Doc_Suite/ADS_v170.html#ref-f6bcf79a-a84e-4881-bb9d-6c94b2cca79d</a>, why do Oozie user need to have all privileges (GRANT ALL)? Database admins might not want to set it up this way.</p>","tags":["operations","Oozie"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-07 16:20:17.0","id":1070,"title":"Is there a way to get Ticket expiration time from JAVA program using Hadoop APIs'?","body":"","tags":["hadoop","security","java","kerberos"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-15 06:40:41.0","id":1482,"title":"Will removing data from /apps/hbase/data/.hbck directory be a good option?","body":"<p>The situation is that disk space has been pretty much used up. And we found the data size under HDFS folder '/apps/hbase/data/.hbck' is quite large.</p><p>We're thinking of removing these data but not sure if it's good or not?</p><p>I think these data are the backup for offline meta repair. If my understanding is correct, removing the data means we don't have the option to do offline meta repair later on.</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-15 17:26:45.0","id":1528,"title":"Tez application stuck on Mapper for 20 hours, fails intermittently","body":"<p>A job was kicked off and stuck on last mapper after 23 hrs, had to be killed. This is failing intermittently.</p><p>Error seen hundreds of times: </p><pre>2015-09-15 00:13:01,313 INFO [fetcher [Map_1] #30] shuffle.HttpConnection: for url=http://badnode.mydns.com:13562/mapOutput?job=job_1441668191237_52949&reduce=132&map=attempt_1441668191237_52949_1_00_001066_0_10003,attempt_1441668191237_52949_1_00_001850_0_10003,attempt_1441668191237_5\n2949_1_00_001633_0_10003,attempt_1441668191237_52949_1_00_001361_0_10003,attempt_1441668191237_52949_1_00_001208_2_10003,attempt_1441668191237_52949_1_00_000474_0_10003,attempt_1441668191237_52949_1_00_000770_0_10002,attempt_1441668191237_52949_1_00_000178_0_10003 sent hash and receievd rep\nly 0 ms\n2015-09-15 00:13:01,314 WARN [fetcher [Map_1] #30] orderedgrouped.FetcherOrderedGrouped: Invalid map id: TTP/1.1 500 Internal Server Error\nContent-Type: text/plain; charset=UTF, expected to start with attempt, partition: 13\n2015-09-15 00:13:01,314 WARN [fetcher [Map_1] #30] orderedgrouped.FetcherOrderedGrouped: copyMapOutput failed for tasks [InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=1066], attemptNumber=0, pathComponent=attempt_1441668191237_52949_1_00_001066_0_10003]]\n2015-09-15 00:13:01,314 INFO [fetcher [Map_1] #30] orderedgrouped.ShuffleScheduler: Reporting fetch failure for InputIdentifier: InputAttemptIdentifier [inputIdentifier=InputIdentifier [inputIndex=1066], attemptNumber=0, pathComponent=attempt_1441668191237_52949_1_00_001066_0_10003] taskAttemptIdentifier: Map 1_001066_00 to AM.\n2015-09-15 00:13:01,314 INFO [fetcher [Map_1] #30] orderedgrouped.ShuffleScheduler: badnode.mydns.com:13562 freed by fetcher [Map_1] #30 in 4ms</pre><p>Why would the task attempt not start with \"attempt_\" ??</p><p>This is always the same node. badnode.mydns.com</p><p>Thanks </p>","tags":["Tez","YARN"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-14 14:59:56.0","id":1446,"title":"How does Amazon EC2 Enhanced Networking impact HDP clusters hosted on AWS?","body":"<p>Amazon offers <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\">Enhanced Networking</a> on certain EC2 instance types. Are any benchmarks available to compare performance with and without this feature? Can it cause any issues, or is there anything to be aware of when configuring this for a VPC used to host an HDP cluster?</p>","tags":["performance","network"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-16 10:20:51.0","id":1572,"title":"Kafka Partitioning Class : Clarification","body":"<p>We create Kafka Partition using kafka-topics.sh.  Would like to validate my understanding is correct.\nPartitioning strategy is actually performed by Kafka rather than Producer.\nAs per http://kafka.apache.org/07/configuration.html (Important configuration properties for Kafka broker:)\nproducer.DefaultPartitioner&lt;T&gt; - uses the partitioning strategy hash(key)%num_partitions. If key is null, \nthen it picks a random partition.\nProducer would either pass a key or not which is then used by Kafka Broker to determine partition strategy \nwhether to use Hash Partitioning or Random Key.</p>","tags":["Kafka","partitioning"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-16 13:45:50.0","id":1586,"title":"Pig scripts with RANK function fails with error after upgrading HDP cluster from 1.3.10 to HDP2.2.6.","body":"<p>2015-10-15 23:55:48,533 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job</p><p>2015-10-15 23:55:48,542 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.</p><p>2015-10-15 23:55:48,542 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche</p><p>2015-10-15 23:55:48,542 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []</p><p>2015-10-15 23:55:48,614 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.</p><p>2015-10-15 23:55:48,776 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl - Timeline service address: http://host:8188/ws/v1/timeline/</p><p>2015-10-15 23:55:48,928 [JobControl] WARN  org.apache.hadoop.mapreduce.JobSubmitter - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).</p><p>2015-10-15 23:55:49,082 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 18</p><p>2015-10-15 23:55:49,082 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 18</p><p>2015-10-15 23:55:49,103 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 348</p><p>2015-10-15 23:55:49,236 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:348</p><p>2015-10-15 23:55:49,421 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1444950185789_0011</p><p>2015-10-15 23:55:49,593 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.</p><p>2015-10-15 23:55:49,660 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1444950185789_0011</p><p>2015-10-15 23:55:49,700 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://host:8088/proxy/application_1444950185789_0011/</p><p>2015-10-15 23:55:49,700 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_1444950185789_0011</p><p>2015-10-15 23:55:49,700 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases H1,frhnj_temp2</p><p>2015-10-15 23:55:49,700 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: frhnj_temp2[22,13],frhnj_temp2[-1,-1],H1[23,4] C:  R: </p><p>2015-10-15 23:55:49,708 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete</p><p>2015-10-15 23:55:49,708 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1444950185789_0011]</p><p>2015-10-15 23:56:19,800 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 4% complete</p><p>2015-10-15 23:56:19,800 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1444950185789_0011]</p><p>2015-10-15 23:56:26,808 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 9% complete</p><p>2015-10-15 23:56:26,808 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1444950185789_0011]</p><p>2015-10-15 23:56:33,814 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 15% complete</p><p>2015-10-15 23:56:33,815 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1444950185789_0011]</p><p>2015-10-15 23:56:41,821 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 20% complete</p><p>2015-10-15 23:56:41,821 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1444950185789_0011]</p><p>2015-10-15 23:57:09,343 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 24% complete</p><p>2015-10-15 23:57:09,344 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1444950185789_0011]</p><p>2015-10-15 23:57:41,471 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server</p><p>2015-10-15 23:57:41,813 [main] INFO  org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl - Timeline service address: http://host:8188/ws/v1/timeline/</p><p>2015-10-15 23:57:41,821 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server</p><p>2015-10-15 23:57:42,472 [main] INFO  org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl - Timeline service address: http://host:8188/ws/v1/timeline/</p><p>2015-10-15 23:57:42,477 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server</p><p>2015-10-15 23:57:42,933 [main] INFO  org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl - Timeline service address: http://host:8188/ws/v1/timeline/</p><p>2015-10-15 23:57:42,938 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server</p><p>2015-10-15 23:57:42,972 [main] WARN  org.apache.pig.tools.pigstats.mapreduce.MRJobStats - Unable to get job counters</p><p>java.io.IOException: java.lang.NullPointerException</p><p>at org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.getCounters(HadoopShims.java:132)</p><p>at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addCounters(MRJobStats.java:284)</p><p>at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:235)</p><p>at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:165)</p><p>at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:360)</p><p>at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:282)</p><p>at org.apache.pig.PigServer.launchPlan(PigServer.java:1390)</p><p>at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1375)</p><p>at org.apache.pig.PigServer.execute(PigServer.java:1364)</p><p>at org.apache.pig.PigServer.access$500(PigServer.java:113)</p><p>at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1689)</p><p>at org.apache.pig.PigServer.registerQuery(PigServer.java:623)</p><p>at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1063)</p><p>at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:501)</p><p>at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)</p><p>at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)</p><p>at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)</p><p>at org.apache.pig.Main.run(Main.java:558)</p><p>at org.apache.pig.Main.main(Main.java:170)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</p><p>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>at java.lang.reflect.Method.invoke(Method.java:606)</p><p>at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</p><p>at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p><p>Caused by: java.lang.NullPointerException</p><p>at org.apache.hadoop.mapreduce.counters.AbstractCounters.&lt;init&gt;(AbstractCounters.java:108)</p><p>at org.apache.hadoop.mapred.Counters.&lt;init&gt;(Counters.java:79)</p><p>at org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.getCounters(HadoopShims.java:130)</p>","tags":["upgrade","Pig","error"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-21 13:47:41.0","id":1780,"title":"Ranger HA - Documentation","body":"<p>I'm reading ranger user guide and it's not clear for me what I need to do to setup ranger with HA:</p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Ranger_User_Guide/content/ch09.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Ranger_User_Guide/content/ch09.html</a></p><p>Do I need to change any settings on Ranger or on HDFS?</p><p>Thanks.</p>","tags":["Ranger","high-availability","namenode-ha"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-21 14:57:43.0","id":1798,"title":"Enable Kerberos wizard created AD SamAccountName as $K5V500-1ET1B4KFE6C4, how to change to regular username","body":"<p>How to change SamAccountName  : $K5V500-1ET1B4KFE6C4 to username storm-poc.</p><p>Ambari 2.1.2 enable kerberos wizard created AD account SamAccountName : $K5V500-1ET1B4KFE6C4.</p><pre>Get-ADUser -Identity '$K5V500-1ET1B4KFE6C4' -Properties *\nAccountExpirationDate  :\naccountExpires  : 0\nAccountLockoutTime  :\nAccountNotDelegated  : False\nAllowReversiblePasswordEncryption  : False\nBadLogonCount  : 0\nbadPasswordTime  : 0\nbadPwdCount  : 0\nCannotChangePassword  : False\nCanonicalName  : ldap.customer.com/HDP/Domain Accounts/Service Accounts/storm-poc\nCertificates  : {}\nCity  :\nCN  : storm-poc\ncodePage  : 0\nCompany  :\nCountry  :\ncountryCode  : 0\nCreated  : 10/16/2015 12:54:07 PM\ncreateTimeStamp  : 10/16/2015 12:54:07 PM\nDeleted  :\nDepartment  :\nDescription  :\nDisplayName  :\nDistinguishedName  : CN=storm-poc,OU=Service Accounts,OU=Domain Accounts,OU=HDP,DC=poc,DC=customer,DC=com\nDivision  :\nDoesNotRequirePreAuth  : False\ndSCorePropagationData  : {12/31/1600 7:00:00 PM}\nEmailAddress  :\nEmployeeID  :\nEmployeeNumber  :\nEnabled  : True\nFax  :\nGivenName  :\nHomeDirectory  :\nHomedirRequired  : False\nHomeDrive  :\nHomePage  :\nHomePhone  :\nInitials  :\ninstanceType  : 4\nisDeleted  :\nLastBadPasswordAttempt  :\nLastKnownParent  :\nlastLogoff  : 0\nlastLogon  : 130898381140333887\nLastLogonDate  : 10/16/2015 12:54:07 PM\nlastLogonTimestamp  : 130894880477406005\nLockedOut  : False\nlogonCount  : 60159\nLogonWorkstations  :\nManager  :\nMemberOf  : {}\nMNSLogonAccount  : False\nMobilePhone  :\nModified  : 10/16/2015 12:54:07 PM\nmodifyTimeStamp  : 10/16/2015 12:54:07 PM\nmsDS-User-Account-Control-Computed : 0\nName  : storm-poc\nnTSecurityDescriptor  : System.DirectoryServices.ActiveDirectorySecurity\nObjectCategory  : CN=Person,CN=Schema,CN=Configuration,DC=poc,DC=customer,DC=com\nObjectClass  : user\nObjectGUID  : 6d7826eb-4729-4074-8e4a-3705c9adcd40\nobjectSid  : S-1-5-21-568884682-143551100-1954249272-195764\nOffice  :\nOfficePhone  :\nOrganization  :\nOtherName  :\nPasswordExpired  : False\nPasswordLastSet  : 10/16/2015 12:54:07 PM\nPasswordNeverExpires  : True\nPasswordNotRequired  : False\nPOBox  :\nPostalCode  :\nPrimaryGroup  : CN=Domain Users,CN=Users,DC=poc,DC=customer,DC=com\nprimaryGroupID  : 513\nProfilePath  :\nProtectedFromAccidentalDeletion  : False\npwdLastSet  : 130894880476781969\nSamAccountName  : $K5V500-1ET1B4KFE6C4\nsAMAccountType  : 805306368\nScriptPath  :\nsDRightsEffective  : 15\nServicePrincipalNames  : {}\nSID  : S-1-5-21-568884682-143551100-1954249272-195764\nSIDHistory  : {}\nSmartcardLogonRequired  : False\nState  :\nStreetAddress  :\nSurname  :\nTitle  :\nTrustedForDelegation  : False\nTrustedToAuthForDelegation  : False\nUseDESKeyOnly  : False\nuserAccountControl  : 66048\nuserCertificate  : {}\nUserPrincipalName  : storm-poc@ldap.customer.com\nuSNChanged  : 9889735\nuSNCreated  : 9889732\nwhenChanged  : 10/16/2015 12:54:07 PM\nwhenCreated  : 10/16/2015 12:54:07 PM</pre>","tags":["kerberos","active-directory"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-21 21:40:36.0","id":1858,"title":"Known performance  issue while using Knox","body":"<p>Any one notice any performance for degradation while having all clients accessing services via rest over KNOX?  It see for me Knox is not a good solution for low latency applications.  thoughts?</p>","tags":["performance","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-22 20:20:59.0","id":1926,"title":"Accessing HDFS in Namenode HA environment","body":"<p>In HA environment, we can access HDFS by referring the active namenode directory but I am interested in finding if there is a way to access HDFS using nameservice id such that if and when the HDFS fails over to the passive namenode, then the client can just continue to use HDFS without manually changing the configuration.</p>","tags":["namenode","HDFS","namenode-ha"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-22 15:55:43.0","id":1892,"title":"What node(s) should Slider be installed on for use by Hbase for HDP 2.2.8?","body":"","tags":["slider"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-22 01:32:23.0","id":1868,"title":"Exception in ExecuteSQL processor","body":"<p>After fixing the empty schema bug, when trying to pull data from Oracle server to PutFile I get the following exception:</p><pre>2015-10-20 15:59:59,859 ERROR [Timer-Driven Process Thread-9] o.a.nifi.processors.standard.ExecuteSQL ExecuteSQL[id=f92d313d-fc87-42a9-ac24-ce7d9b6972c9] ExecuteSQL[id=f92d313d-fc87-42a9-ac24-ce7d9b6972c9] failed to process due to org.apache.avro.file.DataFileWriter$AppendWriteException: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.CharSequence; rolling back session: org.apache.avro.file.DataFileWriter$AppendWriteException: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.CharSequence\n2015-10-20 15:59:59,860 ERROR [Timer-Driven Process Thread-9] o.a.nifi.processors.standard.ExecuteSQL ExecuteSQL[id=f92d313d-fc87-42a9-ac24-ce7d9b6972c9] ExecuteSQL[id=f92d313d-fc87-42a9-ac24-ce7d9b6972c9] failed to process session due to org.apache.avro.file.DataFileWriter$AppendWriteException: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.CharSequence: org.apache.avro.file.DataFileWriter$AppendWriteException: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.CharSequence\n2015-10-20 15:59:59,860 WARN [Timer-Driven Process Thread-9] o.a.nifi.processors.standard.ExecuteSQL ExecuteSQL[id=f92d313d-fc87-42a9-ac24-ce7d9b6972c9] Processor Administratively Yielded for 1 sec due to processing failure\n2015-10-20 15:59:59,860 WARN [Timer-Driven Process Thread-9] o.a.n.c.t.ContinuallyRunProcessorTask Administratively Yielding ExecuteSQL[id=f92d313d-fc87-42a9-ac24-ce7d9b6972c9] due to uncaught Exception: org.apache.avro.file.DataFileWriter$AppendWriteException: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.CharSequence\n2015-10-20 15:59:59,864 WARN [Timer-Driven Process Thread-9] o.a.n.c.t.ContinuallyRunProcessorTask\norg.apache.avro.file.DataFileWriter$AppendWriteException: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.CharSequence\n  at org.apache.avro.file.DataFileWriter.append(DataFileWriter.java:296) ~[na:na]\n  at org.apache.nifi.processors.standard.util.JdbcCommon.convertToAvroStream(JdbcCommon.java:87) ~[na:na]\n  at org.apache.nifi.processors.standard.ExecuteSQL$1.process(ExecuteSQL.java:142) ~[na:na]\n  at org.apache.nifi.controller.repository.StandardProcessSession.write(StandardProcessSession.java:1937) ~[nifi-framework-core-0.3.0.jar:0.3.0]\n  at org.apache.nifi.processors.standard.ExecuteSQL.onTrigger(ExecuteSQL.java:136) ~[na:na]\n  at org.apache.nifi.processor.AbstractProcessor.onTrigger(AbstractProcessor.java:27) ~[nifi-api-0.3.0.jar:0.3.0]\n  at org.apache.nifi.controller.StandardProcessorNode.onTrigger(StandardProcessorNode.java:1077) ~[nifi-framework-core-0.3.0.jar:0.3.0]\n  at org.apache.nifi.controller.tasks.ContinuallyRunProcessorTask.call(ContinuallyRunProcessorTask.java:127) [nifi-framework-core-0.3.0.jar:0.3.0]\n  at org.apache.nifi.controller.tasks.ContinuallyRunProcessorTask.call(ContinuallyRunProcessorTask.java:49) [nifi-framework-core-0.3.0.jar:0.3.0]\n  at org.apache.nifi.controller.scheduling.TimerDrivenSchedulingAgent$1.run(TimerDrivenSchedulingAgent.java:119) [nifi-framework-core-0.3.0.jar:0.3.0]\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_79]\n  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) [na:1.7.0_79]\n  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_79]\n  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.7.0_79]\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_79]\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_79]\n  at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79]\nCaused by: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.CharSequence\n  at org.apache.avro.generic.GenericDatumWriter.writeString(GenericDatumWriter.java:213) ~[na:na]\n  at org.apache.avro.generic.GenericDatumWriter.writeString(GenericDatumWriter.java:208) ~[na:na]\n  at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:76) ~[na:na]\n  at org.apache.avro.generic.GenericDatumWriter.writeField(GenericDatumWriter.java:114) ~[na:na]\n  at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:104) ~[na:na]\n  at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:66) ~[na:na]\n  at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:58) ~[na:na]\n  at org.apache.avro.file.DataFileWriter.append(DataFileWriter.java:290) ~[na:na]</pre>","tags":["Nifi","hdf","dataflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-23 07:11:20.0","id":1970,"title":"Should the Service User have default shell for executing the scripts and commands ?","body":"<p>As per security, service user created as part of rpm/deb install should have default shell ?</p>","tags":["operations"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-23 09:18:03.0","id":1975,"title":"Configuration objects fail to initialize in HDP 2.2.4.8+ because /etc/hadoop/conf in no longer in CLASSPATH","body":"<p>In order to facilitate rolling upgrades,  /etc/hadoop/conf is not part of the CLASSPATH env var that is constructed for a job.\n\nHowever when jobs are using a Configuration object: \n\nConfiguration conf2 = new Configuration(); \n\n<a href=\"https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html\">https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html</a>\n\nIt tries to read the core-site.xml from the CLASSPATH</p><p>How can this be addressed?</p>","tags":["help","MapReduce","configuration"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-23 23:56:34.0","id":2040,"title":"Hbase snapshot Timeout exception","body":"<p>Table Sizes:</p><p>==========</p><p>11.8 T   /apps/hbase/data/data/default/CAR_IMAGES</p><p>29.4 T   /apps/hbase/data/data/default/CAR_IMAGES_ARCHIVE</p><p>Error Log:</p><p>=========</p><p>hdpslave53.bigdataprod1.wh.xxxxorp.com,60020,1445642866970] }</p><p>        at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:366)</p><p>        at org.apache.hadoop.hbase.master.HMaster.isSnapshotDone(HMaster.java:2993)</p><p>        at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:38245)</p><p>        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)</p><p>        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)</p><p>        at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73)</p><p>        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)</p><p>        at java.util.concurrent.FutureTask.run(FutureTask.java:262)</p><p>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</p><p>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</p><p>        at java.lang.Thread.run(Thread.java:745)</p><p>Caused by: org.apache.hadoop.hbase.errorhandling.TimeoutException via timer-java.util.Timer@595cf39b:org.apache.hadoop.hbase.errorhandling.TimeoutException: Timeout elapsed! Source:Timeout caused Foreign Exception Start:1445643328484, End:1445643388484, diff:60000, max:60000 ms</p><p>        at org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.rethrowException(ForeignExceptionDispatcher.java:83)</p><p>        at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.rethrowExceptionIfFailed(TakeSnapshotHandler.java:318)</p><p>        at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:356)</p><p>        ... 10 more</p><p>Caused by: org.apache.hadoop.hbase.errorhandling.TimeoutException: Timeout elapsed! Source:Timeout caused Foreign Exception Start:1445643328484, End:1445643388484, diff:60000, max:60000 ms</p><p>        at org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector$1.run(TimeoutExceptionInjector.java:67)</p><p>        at java.util.TimerThread.mainLoop(Timer.java:555)</p><p>        at java.util.TimerThread.run(Timer.java:505)</p>","tags":["snapshot","Hbase"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-23 15:07:28.0","id":1998,"title":"Heterogeneous worker nodes with HDP2.3","body":"<p>What precautions & extra configurations, if any, are needed when adding worker nodes with different capacity to a cluster? My understanding is that YARN will be able to just manage the nodes without anything special. </p><p>For e.g - Any issues with adding 3 nodes with following config to an existing POC cluster that has similar nodes 8 cores, 32 Gigs, 3 TB DAS for data- </p><pre>node1 - 8 cores, 64GB RAM, NO STORAGE \n\nnode2 - 8 cores, 64GB RAM, 2 TB\n\nnode3 - 8 cores, 64GB RAM, 2TB</pre><p>Also, how do you configure YARN to utilize different amount of memory on these heterogeneous boxes? </p>","tags":["hdp-2.3.0","memory","configuration","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-26 19:56:23.0","id":2137,"title":"How To Best Resolve - RMStateStore FENCED?","body":"<p>When doing a restart of all services after Kerberos setup, we ran into the following exception:</p><pre>File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'yarn resourcemanager -format-state-store' returned 255. 15/10/26 16:11:16 INFO resourcemanager.ResourceManager: STARTUP_MSG:\n\n15/10/26 16:11:17 INFO recovery.ZKRMStateStore: org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$VerifyActiveStatusThread thread interrupted! Exiting!\n15/10/26 16:11:17 INFO zookeeper.ZooKeeper: Session: 0x150a4b3429b0002 closed\n15/10/26 16:11:17 FATAL resourcemanager.ResourceManager: Error starting ResourceManager\norg.apache.zookeeper.KeeperException$NotEmptyException: KeeperErrorCode = Directory not empty for /rmstore/ZKRMStateRoot/RMAppRoot\n                at org.apache.zookeeper.KeeperException.create(KeeperException.java:125)\n                at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n                at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.recursiveDeleteWithRetriesHelper(ZKRMStateStore.java:1049)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.recursiveDeleteWithRetriesHelper(ZKRMStateStore.java:1045)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.access$500(ZKRMStateStore.java:89)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$10.run(ZKRMStateStore.java:1032)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$10.run(ZKRMStateStore.java:1029)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1104)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1125)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.deleteWithRetries(ZKRMStateStore.java:1029)\n                at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.deleteStore(ZKRMStateStore.java:825)\n                at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.deleteRMStateStore(ResourceManager.java:1267)\n                at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1190)\n15/10/26 16:11:17 INFO zookeeper.ClientCnxn: EventThread shut down\n15/10/26 16:11:17 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG:</pre><p>What are the root cause of this and how to best resovle/avoid this from happening?</p>","tags":["resource-manager","namenode-ha","YARN","kerberos"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-28 18:53:21.0","id":2279,"title":"Best practices for spanning AWS availability zones (or equivalent at other cloud providers)","body":"<p>Are there HDP applications where latency between availability zones (AZ) (approx. 1 ms) is significant? It seems like <a href=\"http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/RackAwareness.html \">rack awareness</a> could be used, treating each AZ as a different rack.</p><ul>\n<li>Is this the common way to handle this in practice?</li><li>Does anyone have examples of SLAs for clusters with and without multiple AZs?</li><li>Anything else to be aware of regarding EC2 AZs (or the equivalents at other cloud providers)?</li></ul>","tags":["best-practices","namenode-ha","HDFS","aws"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-29 14:40:22.0","id":2346,"title":"Bug in HDP 2.3.2 and earlier releases, Ambari Pig View helper pointing to wrong HCatalogLoader package","body":"<p>The package name for HCatLoader function is wrong. Results in class not found error. This is not a correct path in Hive 1.2.1</p><p><img src=\"/storage/attachments/353-wrong-path.png\"></p><p>This is correct</p><p><img src=\"/storage/attachments/354-correct-path.png\"></p>","tags":["ambari-views","hcatalog","Ambari","bug","Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-29 20:29:42.0","id":2435,"title":"Why is kinit with a headless keytab failing?","body":"<p>The commands being ran are below.  Both fail.</p><pre>[root@host1 ~]# sudo -u hdfs /usr/bin/kinit -k -t /etc/security/keytabs/hdfs.headless.keytab hdfs/host1.prod.myclient.com@CORP.DS.MYCLIENT.COM\nkinit: Keytab contains no suitable keys for hdfs/host1.prod.myclient.com@CORP.DS.MYCLIENT.COM while getting initial credentials </pre><p><strong>and</strong> </p><pre>[user1@host2.prod /var/www/html]$ sudo -u hdfs /usr/bin/kinit -k -t /etc/security/keytabs/hdfs.headless.keytab\nkinit: Client not found in Kerberos database while getting initial credentials</pre>","tags":["kerberos","active-directory"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-30 12:57:00.0","id":2513,"title":"Ambari and Postgres HA","body":"<p>I have a customer asking about support for, configuration of, or known gotchas when using Postgres HA with Ambari. From the context, the customer's primary corporate DB is postgres, and they have entire farms of DB server instances. I don't see anything obvious that would prevent integrating the two, but wanted to check with the larger community to see if anyone had any specific experience or learnings related to this configuration.</p>","tags":["postgres","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-30 14:42:18.0","id":2517,"title":"Maximum Hive Table Partitions allowed & recommended","body":"<p>What is the maximum number of partitions allowed for a Hive table? E.g. 2k ... 10k?</p><p>Are there any performance implications we should consider as we get close to this number?</p>","tags":["partitioning","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-03 18:40:28.0","id":2690,"title":"How to bind host for oozie service in a Multihome environment","body":"","tags":["Oozie"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-11-04 23:37:03.0","id":2838,"title":"Has anyone integrated (for demo purposes only) the Knox LDAP demo server with the Ambari 2.1.1 Server? I am not sure that it can be done, but need the instructions if it can be done. I only need to be able to log in to Ambari using the LDAP users.","body":"","tags":["Knox","test","Ambari","ldap"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-10 18:05:39.0","id":3147,"title":"Beeline sample with -u -f -n -p","body":"<p>Client tries to execute hive sql inside a file via Beeline.</p><p>Anyone has working sample of how to use \"beeline -u &lt;url&gt; -n &lt;user&gt; -f &lt;file&gt; -p &lt;password&gt;\"? I have tried this, but does not work.</p>","tags":["Hive","beeline"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-19 18:44:04.0","id":1675,"title":"Sizing for Master/Edges servers","body":"<p>I'm helping a prospect expansion from current 6 nodes hadoop cluster to plans of more than 1PB and hundred nodes. I gave him some hints:</p><p>- master and edges nodes running in virtual environment (as they do not require high I/O and virtual environment can increase availability)</p><p>- knox as security perimeter gateway</p><p>- dedicated database nodes with high availability</p><p>I need help with recommended sizing and notes for items below:</p><p>- <strong>Master nodes</strong>, what is recommended RAM for maste<strong>r</strong>? Prospect asked me to consider that virtualized usually runs on machines with 512GB of RAM and usually they don't allocate more than 64GB virtual hosts.</p><p>- <strong>Edges nodes</strong></p><p><strong></strong>- <strong>Knox</strong>, do we have any sizing for Knox?</p><p>- <strong>Database servers</strong>, do we have any sizing for dedicated database servers(for metadata: Ambari, Hue, Hive Metastore, Oozie, etc)? </p><p>Thanks.</p><p>Guilherme.</p>","tags":["edge","sizing","master"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-11-09 21:23:59.0","id":3112,"title":"Alert Configuration on Ambari","body":"<p>How to adjust thresholds on Ambari alerts?</p><p>For example, the following Ambari Agent Disk Usage alert has 50% WARNING and 80% CRITICAL threasholds but what do i need to do if i would like to make 60% usage as CRITICAL?</p><p><strong>This host-level alert is triggered if the amount of disk space used on a host goes above specific thresholds. The default values are 50% for WARNING and 80% for CRITICAL.</strong></p><p>and also can i make these alerts specific to mount/volume on Ambari host server?</p><p>Thanks,</p>","tags":["ambarialerts","ambari-metrics","monitoring"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-10 20:41:43.0","id":3173,"title":"When adding new service , Knox tab is requiring master secret but the field is grayed out.","body":"<p>I am trying to add Jupyter service, but the knox tab is forcing to input the master secret, but the field is greyed out and readonly.  Anyone ran into this issue also?</p><p><img src=\"/storage/attachments/433-screen-shot-2015-11-10-at-34052-pm.png\"></p>","tags":["jupyter","service","Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-11-11 13:21:46.0","id":3219,"title":"Do the Spark REPLs have a way to list current variables?","body":"<p>I'm specifically using pyspark and I'm wondering if there is something similar to Pig's \"aliases\" command that shows all currently available variables.  If there is something like that in pyspark, I'm just missing it and I hope someone straightens me out! ;-)  I'm not using spark-shell much, but knowing how to do this in that REPL would be useful, too.</p>","tags":["Pig","pyspark","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-11 00:21:41.0","id":3197,"title":"Spark 1.5.1 Tech Preview","body":"<p>Team,</p><p>Congratulations. We just published Spark 1.5.1 TP (http://hortonworks.com/hadoop-tutorial/apache-spark-1-5-1-technical-preview-with-hdp-2-3/)</p><p>Next stop on this journey is Spark 1.5.1 GA with HDP.</p><p>Please take it out for a spin. </p>","tags":["zeppelin","sparksql","Spark","data-science","sparkr"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-13 18:17:56.0","id":3380,"title":"Implications of switching HiveServer2 from Binary to HTTP Mode","body":"<p>Our customer is interested in changing all their existing applications and users from connecting to HS2 in Binary (default) mode to <a href=\"https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-RunninginHTTPmode\">HTTP mode</a> in order to take advantage of Knox integration. Are there any impacts to normal operation the customer should consider:</p><ul><li><a href=\"https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-ConnectionURLWhenHiveServer2IsRunninginHTTPMode\">Changing the JDBC URL</a></li><li>Performance implications</li><li>Anything else?</li></ul><p>Thanks!</p>","tags":["Hive","Knox","hiveserver2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-17 11:50:11.0","id":4097,"title":"Ambari View: How many users per server","body":"<p>Do we have a guideline on how many users which should be running per ambari view servers ? What are the guidelines ? Do we have a spec for a \"server\" ? I was thinking to install Ambari View on a VM.</p>","tags":["ambari-views","best-practices"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-18 17:18:47.0","id":4249,"title":"Can Microsoft/Hortonworks ODBC Driver provide bi-directional connectivity with Microsoft Access?","body":"<p>Customer has an Access database application and they would like to (a) connect to Hive and extract data, and also (b) take resulting Access DB data and ODBC connect & load it into an existing Hive table from within Microsoft Access (not via Sqoop or via flat file).  I am currently installing the driver and will soon test out option (a), but would like to know if option (b) is possible.   </p><p>Thanks in advance!</p>","tags":["microsof","windows","driver","microsoft","connector","odbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-18 23:49:27.0","id":4280,"title":"HBase client jar requires an older version of Hadoop jars in HDP 2.2.4","body":"<p>I'm working on a Java application that utilizes HDFS and HBase APIs and connects to an HDP 2.2.4 cluster.  This version of HDP ships with HBase v0.98.4 and Hadoop v2.6.0.  I see that hbase-client jar requires in an older version (v2.2.0) of hadoop-auth, hadoop-common, hadoop-mapreduce-client-core, and hadoop-annotations than what is shipped with HDP 2.2.4.  </p><p>Will HBase client have any issues if I exclude the older dependencies and manually specify the correct versions? </p>","tags":["hadoop","Hbase","hdp-2.2.4"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-19 18:53:12.0","id":4343,"title":"What is the best approach for migrating (no data loss) the Hive metastore database from MySQL to Oracle?","body":"<p>The move from MySQL to Oracle is mandatory.  Creating the new schema in Oracle is not the issue, but the migration of the data from MySQL is the issue. </p>","tags":["metastore","migration","oracle","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-19 22:38:13.0","id":4367,"title":"HDF - Social Media (Non-Twitter) Examples","body":"<p>Hi,</p><p>Does anyone have any examples/templates for using HDF to connect to a social media stream that isn't Twitter (e.g. Facebook, Google+, YouTube, Instagram, etc) ?</p><p>Thanks,</p><p>Andrew</p>","tags":["api","hdf","social","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-21 23:27:21.0","id":4546,"title":"Execute an action on NiFi startup","body":"<p>What is the best way to execute a one-time action during a NiFi startup? E.g. call home and report the (floating) IP address of the instance, or update a registry, etc.</p><p>Contrary to a reporting task, the idea is to fire once, not be running continuously while NiFi is up and have no requirement to be configured by a user explicitly (other than an admin deploying the code).</p>","tags":["Nifi","startup","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-20 20:22:18.0","id":4451,"title":"SmartSense Agent failed to stay up","body":"<p>We started SmartSense server fine. But for the agents, they started but after a few seconds, went down again. See the screen capture for error in hst-agent.log.</p><p><a href=\"http://community.hortonworks.com/storage/attachments/541-smartsense-error-in-agent-hst.pdf\">smartsense-error-in-agent-hst.pdf</a></p><p>O/S: RHEL 7, </p><p>HDP 2.3.2.0-2950</p><p>Ambari 2.1.2</p>","tags":["smartsense"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-19 20:57:55.0","id":4359,"title":"Custom SuperUserGroup - Implications for other Hadoop Services?","body":"<p>If we set a custom value for dfs.permissions.superusergroup do we need to add any other Hadoop services to that group in order for the cluster to function normally?</p><p>For example, if we set \"dfs.permissions.superusergroup=blueManGroup\" would we need to add service accounts such as hive, hdfs, oozie, etc. to the \"blueManGroup\"?</p><p>\nCouldn't find the answer in the Hadoop documentation: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#The_Super-User\n</p>","tags":["HDFS","superuser"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-25 22:29:54.0","id":4838,"title":"java.lang.NoSuchMethodError: org.apache.hadoop.hive.serde2.lazy.LazyUtils.getByte(Ljava/lang/String;B)B","body":"<p>HDP 2.3.0</p><p>Originally running query in spark shell got the following error:</p><pre>scala&gt; sqlContext.sql(\"select count(*) from hbase_game_telemetry\").collect().foreach(println) 15/11/19 17:12:14 INFO ParseDriver: Parsing command: select count(*) from hbase_game_telemetry 15/11/19 17:12:14 INFO ParseDriver: Parse Completed java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/util/Bytes</pre><p>Then we tried the following:</p><p>-- adding libs to the '--jars' parameter, like: </p><pre>spark-shell —jars /usr/hdp/&lt;hdp version&gt;/hive/lib/hive-hbase-handler.jar,/usr/hdp/&lt;hdp version&gt;/hbase/lib/hbase-common.jar --master yarn-client </pre><p>-- adding spark.executor.extraClassPath to spark-defaults.conf, like:</p><pre>spark.executor.extraClassPath /usr/hdp/&lt;hdp version&gt;/hive/lib/hive-hbase-handler.jar:/usr/hdp/&lt;hdp version&gt;/hbase/lib/hbase-common.jar </pre><p>This seems to resolve the \"NoClassDefFoundError\" issue, but then we got \"NoSuchMethodError\"</p><pre>scala&gt; sqlContext.sql(\"select count(*) from hbase_game_telemetry\").collect().foreach(println) 15/11/23 21:02:41 INFO ParseDriver: Parsing command: select count(*) from hbase_game_telemetry 15/11/23 21:02:42 INFO ParseDriver: Parse Completed java.lang.NoSuchMethodError: org.apache.hadoop.hive.serde2.lazy.LazyUtils.getByte(Ljava/lang/String;B)B </pre><p>How can we address this?  Is that because Hive 0.13.1 in Spark 1.3.1 does not support this?  </p><p>Thank you</p>","tags":["Hive","Spark"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-30 19:32:19.0","id":4939,"title":"Kafka Monitoring","body":"<p>Wondering if anyone has any tools they use for monitoring Kafka besides Ambari Metrics and Yammer Metrics.  For example:</p><p>https://github.com/damienclaveau/kafka-graphite</p><p>Wondering if we advise customers to create their own monitoring through Yammer Metrics or if there are other tools that we utilize or recommend.</p><p>Many thanks,</p><p>Chris</p>","tags":["Kafka"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-01 22:23:49.0","id":5003,"title":"MIT principal and keytab management via Amabari 2.1.2","body":"<p>Can I use Ambari 2.1.2 (API or other) to manage principals and keytab deployments for non-ambari controlled services? </p>","tags":["ambari-2.1.2","kerberos","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-30 06:02:39.0","id":4928,"title":"Java MapReduce and HCatalog with Dynamic Partitions","body":"<p>Couldn’t get Java MapReduce work with HCatalog in last HDP release, before HDP 2.3\n </p><p>Can anyone indicate if</p><ol>\n<li> writing to Hive tables with HCatalog using Java MapReduce work in HDP2.3  (an example would be great) and</li><li>writing to dynamic partitions with Java MapReduce is feasible (an example would be great)?</li></ol><p><strong>Background: HCatalog Limitations with Java MapReduce</strong></p><p><strong><em>Dynamic Partitions (at Write):</em></strong></p><ul><li>Null Pointer Exception in DynamicPartFileRecordWriterContainer on null part-keys (perhaps this issue: https://issues.apache.org/jira/browse/HIVE-11470 )</li></ul><ul><li>Partition column cannot be empty or NULL</li></ul><p>Error:</p><pre>15/11/30 15:11:56 INFO mapreduce.Job: Running job: job_1448337121999_0211\n15/11/30 15:12:01 INFO mapreduce.Job: Job job_1448337121999_0211 running in uber mode : false\n15/11/30 15:12:01 INFO mapreduce.Job:  map 0% reduce 0%\n15/11/30 15:12:05 INFO mapreduce.Job:  map 100% reduce 0%\n15/11/30 15:12:09 INFO mapreduce.Job: Task Id : attempt_1448337121999_0211_r_000000_0, Status : FAILED</pre><pre>Error: java.lang.NullPointerException \nat org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:139)\nat org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)\nat org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)\nat org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)\nat org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\nat org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)\nat com.kgi.test.mapreduce.hcatalog.DynamicPartitionTest$Reduce.reduce(DynamicPartitionTest.java:107)\nat com.kgi.test.mapreduce.hcatalog.DynamicPartitionTest$Reduce.reduce(DynamicPartitionTest.java:1)\nat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\nat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</pre><p>The <a target=\"_blank\" href=\"https://community.hortonworks.com/storage/attachments/600-dynamicpartitiontestjava.txt\">DynamicPartitionTestJava</a> package is attached</p>","tags":["Hive","MapReduce","dynamic","partitioning","hcatalog"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-02 21:15:22.0","id":5086,"title":"Ext4 vs XFS Filesystem - Survey of Popularity","body":"<p>Wanted to see how many people have clusters where the HDFS DataNodes are running on XFS vs Ext4 filesystems? I'm trying to get a sense for which filesystem is chosen most often in the wild. Feel free to comment if you have a preference for one vs the other.</p><p>Thanks!</p>","tags":["HDFS","filesystem","datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-07 17:39:27.0","id":5322,"title":"Ranger user sync error:  javax.naming.AuthenticationException: [LDAP: error code 49 - 80090308: LdapErr: DSID-0C0903CF","body":"<pre>07 Dec 2015 11:33:12  INFO UserGroupSync [UnixUserSyncThread] - initializing sink: org.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder\n07 Dec 2015 11:33:13  INFO LdapUserGroupBuilder [UnixUserSyncThread] - LdapUserGroupBuilder created\n07 Dec 2015 11:33:13  INFO UserGroupSync [UnixUserSyncThread] - initializing source: org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder\n07 Dec 2015 11:33:13  INFO UserGroupSync [UnixUserSyncThread] - Begin: initial load of user/group from source==&gt;sink\n07 Dec 2015 11:33:13  INFO LdapUserGroupBuilder [UnixUserSyncThread] - LDAPUserGroupBuilder updateSink started\n07 Dec 2015 11:33:13  INFO LdapUserGroupBuilder [UnixUserSyncThread] - LdapUserGroupBuilder initialization started\n07 Dec 2015 11:33:13 ERROR UserGroupSync [UnixUserSyncThread] - Failed to initialize UserGroup source/sink. Will retry after 30000 milliseconds. Error details:\njavax.naming.AuthenticationException: [LDAP: error code 49 - 80090308: LdapErr: DSID-0C0903CF, comment: AcceptSecurityContext error, data 52e, v2580^@]\n  at com.sun.jndi.ldap.LdapCtx.mapErrorCode(LdapCtx.java:3135)\n  at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:3081)\n  at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:2883)\n  at com.sun.jndi.ldap.LdapCtx.connect(LdapCtx.java:2797)\n  at com.sun.jndi.ldap.LdapCtx.&lt;init&gt;(LdapCtx.java:319)\n  at com.sun.jndi.ldap.LdapCtxFactory.getUsingURL(LdapCtxFactory.java:192)\n  at com.sun.jndi.ldap.LdapCtxFactory.getUsingURLs(LdapCtxFactory.java:210)\n  at com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxInstance(LdapCtxFactory.java:153)\n  at com.sun.jndi.ldap.LdapCtxFactory.getInitialContext(LdapCtxFactory.java:83)\n  at javax.naming.spi.NamingManager.getInitialContext(NamingManager.java:684)\n  at javax.naming.InitialContext.getDefaultInitCtx(InitialContext.java:313)\n  at javax.naming.InitialContext.init(InitialContext.java:244)\n  at javax.naming.ldap.InitialLdapContext.&lt;init&gt;(InitialLdapContext.java:154)\n  at org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder.createLdapContext(LdapUserGroupBuilder.java:149)\n  at org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder.updateSink(LdapUserGroupBuilder.java:262)\n  at org.apache.ranger.usergroupsync.UserGroupSync.run(UserGroupSync.java:58)\n  at java.lang.Thread.run(Thread.java:745)</pre>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-07 20:35:35.0","id":5345,"title":"French Bilingual support for Hive","body":"<p>Customer is facing issues with French character set, when data is populated to Hive. </p><p>Records are getting split when French characters are encountered.</p><p>Checking on internet blogs, the recommendation I can find is to implement custom Serde's .</p><p>Are there any options to handle french characters in Hive after loading data ? </p><p>Or is it recommended to pre-process French characters prior to loading ? </p>","tags":["help","french","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-07 22:36:23.0","id":5372,"title":"Disable Hive Metastore Statistics","body":"<p>Hi All,</p><p>We currently have a Hive Dynamic Partitioning INSERT INTO job which runs very quickly (minutes) in the YARN/MR stage but takes a VERY long time (hours) when its loading the partitions. After reviewing the logs I believe this is happening at the Hive Metastore level where its attempting to calculate the statistics for each partition involved in the load. </p><p>Can anyone please tell us how to disable the stats generation part of the insert process with Hive? </p><p>We have attempted to use the following properties and they do not work prevent the Metastore from attempting to generate stats as its not an INSERT OVERWRITE query but an INSERT INTO (append.)  We validated that the jobs properties where being taken correct by searching the configuration provided for the app in the YARN UI. </p><p>#Not Working Properties</p><pre>set hive.stats.autogather=false;\n\nset hive.stats.collect.rawdatasize=false;\n\nset hive.analyze.stmt.collect.partlevel.stats=false;</pre><p>If I am understanding this correctly its this Jira that is creating my problem as it does not provide a way to go back to the default behaviour. <a href=\"https://issues.apache.org/jira/browse/HIVE-3959\">https://issues.apache.org/jira/browse/HIVE-3959</a></p><p>PS - each partition will have gigs to terabytes of data... We are just trying to run this right now on a smaller amount and its creating issues...</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-08 23:10:27.0","id":5482,"title":"how to fetch service level configuration using REST API","body":"<p>Can someone help me to fetch service level config using ambari REST API?</p><p>For example, we would like to have the following config value pulled using ambari REST API calls.</p><p>hive.server2.transport.mode</p><p>hive.txn.timeout</p><p>Thanks,</p>","tags":["api","configuration","Ambari"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-09 07:15:05.0","id":5514,"title":"Implementing Expectation Maximization and a Minimal Spanning tree clustering algorithm in hadoop","body":"<p>Hi, I am fairly new to the hadoop, and I don't know what tool to use to implement EM and clustering algorithms in hadoop, </p><p>so far what I have studied I figured out that I have to use Apache Spark for clustering and Map Reduce to implement EM, but I am not sure, what I need to know is that how to implement any algorithm in both Sparc and Map reduce, I have downloaded the Sandbox, but now I don't know how to write my algorithms in these frameworks. Please advise. </p>","tags":["hadoop","Spark","MapReduce"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-09 13:10:04.0","id":5529,"title":"How to extract Bz file in hdfs","body":"<p>i already try this comment not working fine .....</p><p>hadoop fs -unzip /path/fileinRC_2015-01.bz2</p><p>hadoop fs -bunzip /path/fileinRC_2015-01.bz2</p>","tags":["help","rcfile","HDFS","hadoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-04 19:52:55.0","id":5653,"title":"Mkdirs failed to create D:\\....\\META-INF\\license","body":"<p>I am going to be crazy because of this error. Please help me. I have a jar file and when I want execute it I get error in the title. I am sure about the permissions. They are all ok.. Please tell me something.</p>","tags":["hadoop","YARN","MapReduce"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-09 22:46:46.0","id":5708,"title":"Do we have Sandbox for windows 10?","body":"<p>I understand current version of Sandbox works on Windows 7, Windows 8 and Mac OSX. </p><p>Do we have Sandbox for windows 10? If not then is there any plan for the same?</p><p>Thanks</p>","tags":["windows","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-09 16:29:36.0","id":5594,"title":"Query with transform clause is disallowed in current configuration","body":"<pre>2015-12-09 09:14:54,306 ERROR [HiveServer2-Background-Pool: Thread-60853]: ql.Driver (SessionState.java:printError(833)) - FAILED: Hive Internal Error: org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException(Query with transform clause is disallowed in current configuration.)\norg.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException: Query with transform clause is disallowed in current configuration.\n        at org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook.run(DisallowTransformHook.java:34)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1304)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1177)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:999)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:144)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:69)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:196)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:208)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)</pre>","tags":["Ranger","hiveserver2","transformer","beeline"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-10 10:43:29.0","id":5792,"title":"ResourceManager YARN cannot started","body":"<p>Hi I am using the Sandbox 2.3.2</p><p>I just download it 2 weeks ago. </p><p>At the first week before I am running with 4 GB RAM. Then i request it to my team to upgrade  memory to 8 GB.</p><p>But the Resource Manager YARN and Ambari Server Alert cannot be started. always CRIT </p><p>Please what do i miss ?</p><p><img src=\"/storage/attachments/740-0001.jpg\"></p><p><img src=\"/storage/attachments/751-0002.jpg\"></p><p>Thank you </p><p>yan</p>","tags":["YARN"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-10 06:11:19.0","id":5775,"title":"Best practice for extract/output data (generated by Pig script) to be stored in a single file?","body":"<p>AIM: To grab a daily extract of data stored in HDFS/Hive, process it using Pig, then make results available externally as a single CSV file (automated using bash script).</p><p>OPTIONS:</p><p>1. Force output from Pig script to be stored as one file using 'PARALLEL 1' and then copy out using '-copyToLocal'</p><pre>extractAlias = ORDER stuff BY something ASC;\nSTORE extractAlias INTO '/hdfs/output/path' USING CSVExcelStorage() PARALLEL 1;</pre><p>2. Allow default parallelism during Pig STORE and use '-getmerge' when copying out extract results</p><pre>hdfs dfs -getmerge '/hdfs/output/path' '/local/dest/path'</pre><p>QUESTION:</p><p>Which way is more efficient/practical and why? Are there any other ways?</p>","tags":["Pig","HDFS","best-practices"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-10 14:05:43.0","id":5834,"title":"solr indexing from folder in hdfs","body":"<p>Hi,</p><p>I tried to index the files in a folder on HDFS; my solr configuration is the following:</p><pre>./solr start -cloud -s ../server/solr -p 8983 -z 10.0.2.15:2181 -Dsolr.directoryFactory=HdfsDirectoryFactory -Dsolr.lock.type=hdfs -Dsolr.data.dir=hdfs://10.0.2.15:8020/user/solr -Dsolr.updatelog=hdfs://10.0.2.15:8020/user/solr</pre><p>when I launch:</p><pre>hadoop jar /opt/lucidworks-hdpsearch/job/lucidworks-hadoop-job-2.0.3.jar com.lucidworks.hadoop.ingest.IngestJob -Dlww.commit.on.close=true -cls com.lucidworks.hadoop.ingest.DirectoryIngestMapper -c Collezione -i /user/solr/documents -of com.lucidworks.hadoop.io.LWMapRedOutputFormat -zk 10.0.2.15:2181/solr</pre><p>I get the following error:</p><pre>Solr server not available on: &lt;a href=\"http://10.0.2.15:2181/solr\"&gt;http://10.0.2.15:2181/solr&lt;/a&gt;\nMake sure that collection [Collezione] exists</pre><p>The collection exists and is valid, but it looks like it is not able to contact the server.</p><p>I'd really appreciate some help in solving this problem.</p><p>Davide</p>","tags":["HDFS","solrcloud","MapReduce"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-10 17:32:37.0","id":5865,"title":"Installing Solr on yarn  using Slider","body":"<p>Hi,</p><p>I am trying to run solr on yarn using the link <a target=\"_blank\" href=\"https://github.com/LucidWorks/solr-slider\">lucidworksSolrSlider</a>, apart from taking help from <a target=\"_blank\" href=\"http://slider.incubator.apache.org/docs/getting_started.html#resspec\">slider.incubator.apache.org/docs/getting_started.html</a></p><p>Here is my folder structure:</p><pre>[solrs@ip-10-0-0-217 solr-slider]$ ls -lrt\ntotal 131744\n-rw-rw-r--. 1 solrs solrs      3182 Dec 10 01:17 README.md\ndrwxrwxr-x. 4 solrs solrs        32 Dec 10 01:17 package\n-rw-rw-r--. 1 solrs solrs      2089 Dec 10 01:17 metainfo.xml\n-rw-rw-r--. 1 solrs solrs     11358 Dec 10 01:17 LICENSE\n-rw-rw-r--. 1 solrs solrs 134874517 Dec 10 01:37 solr-on-yarn.zip\n-rw-rw-r--. 1 solrs solrs       277 Dec 10 01:49 resources-default.json\n-rw-rw-r--. 1 solrs solrs      1355 Dec 10 15:33 appConfig-default.json\n</pre><p>appConfig-default.json:</p><pre>{\n  \"schema\": \"http://example.org/specification/v2.0.0\",\n  \"metadata\": {\n  },\n  \"global\": {\n    \"application.def\": \"/user/solrs/.slider/package/solryarn/solr-on-yarn.zip\",\n    \"java_home\": \"/usr/jdk64/jdk1.8.0_40\",\n    \"site.global.app_root\": \"${AGENT_WORK_ROOT}/app/install/solr-5.2.0-SNAPSHOT\",\n    \"site.global.zk_host\": \"localhost:2181\",\n    \"site.global.solr_host\": \"${SOLR_HOST}\",\n    \"site.global.listen_port\": \"${SOLR.ALLOCATED_PORT}\",\n    \"site.global.xmx_val\": \"1g\",\n    \"site.global.xms_val\": \"1g\",\n    \"site.global.gc_tune\": \"-XX:NewRatio=3 -XX:SurvivorRatio=4 -XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=8 -XX:+UseConcMarkSweepGC -XX:+UseParNewG$\n    \"site.global.zk_timeout\": \"15000\",\n    \"site.global.server_module\": \"--module=http\",\n    \"site.global.stop_key\": \"solrrocks\",\n    \"site.global.solr_opts\": \"\"\n  },\n  \"components\": {\n    \"slider-appmaster\": {\n      \"jvm.heapsize\": \"512M\"\n    },\n    \"SOLR\": {\n    }\n  }\n}\n</pre><p>resources-default.json:</p><pre>{\n  \"schema\" : \"http://example.org/specification/v2.0.0\",\n  \"metadata\" : {\n  },\n  \"global\" : {\n  },\n  \"components\": {\n    \"slider-appmaster\": {\n    },\n    \"SOLR\": {\n      \"yarn.role.priority\": \"1\",\n      \"yarn.component.instances\": \"3\",\n      \"yarn.memory\": \"1024\"\n    }\n  }\n}\n</pre><p>Could you please suggest me what will be the value of below parameters in appConfig-default.json file:</p><pre>\"site.global.app_root\": \"${AGENT_WORK_ROOT}/app/install/solr-5.2.0-SNAPSHOT\",\n\"site.global.solr_host\": \"${SOLR_HOST}\",\n\"site.global.listen_port\": \"${SOLR.ALLOCATED_PORT}\",</pre><p>Basically where should I find \"/app/install/solr-5.2.0-SNAPSHOT\"??</p><p>My Environment: HDP 2.3, Slider Core-0.80.0.2.3.2.0-2950</p><p>Thanks, hoping a quick reply.</p>","tags":["SOLR","slider"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-14 05:37:48.0","id":6294,"title":"Why Sqoop Executing same query for both MySql and Oracle..","body":"<p>sqoop import --driver oracle.jdbc.driver.OracleDriver --connect jdbc:oracle:thin:@ipaddress:1521:orcl --username username --password password  --table T_PROJECT --delete-target-dir --warehouse-dir /foo</p><p>              The above one is simple sqoop command i executed.This tries to execute following command to know the columns.\"SELECT t.* FROM T_PROJECT AS t WHERE 1=0\" (False condition to know columns of table) .Unfortunately this won't work in oracle.If i changed the database to mysql,It is working well.Where is the logic.How to execute above command for oracle database.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-15 15:16:52.0","id":6530,"title":"Why does the install of Accumulo not work with HDP Sandbox?","body":"<p>I get the following error trying to install Accumulo on the HDP Sandbox. I need Accumulo to run sqoop.</p><p>Does anyone know how to fix this Accumulo installation error - seems like a repository error.</p><pre>stderr: \nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/ACCUMULO/1.6.1.2.2.0/package/scripts/accumulo_client.py\", line 65, in &lt;module&gt;\n    AccumuloClient().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/ACCUMULO/1.6.1.2.2.0/package/scripts/accumulo_client.py\", line 36, in install\n    self.install_packages(env)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 395, in install_packages\n    Package(name)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 152, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 118, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py\", line 45, in action_install\n    self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py\", line 49, in install_package\n    shell.checked_call(cmd, sudo=True, logoutput=self.get_logoutput())\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of '/usr/bin/yum -d 0 -e 0 -y install 'accumulo_2_3_*'' returned 1. Error: Cannot find a valid baseurl for repo: base\nCould not retrieve mirrorlist <a href=\"http://mirrorlist.centos.org/?release=6&arch=x86_...\">http://mirrorlist.centos.org/?release=6&arch=x86_...</a> error was\n14: PYCURL ERROR 6 - \"Couldn't resolve host 'mirrorlist.centos.org'\"\n\n\n\n\n\n\n\n\n\n\n stdout:\n2015-12-15 15:04:12,132 - Group['hadoop'] {}\n2015-12-15 15:04:12,133 - Group['users'] {}\n2015-12-15 15:04:12,133 - Group['zeppelin'] {}\n2015-12-15 15:04:12,134 - Group['knox'] {}\n2015-12-15 15:04:12,134 - Group['ranger'] {}\n2015-12-15 15:04:12,134 - Group['spark'] {}\n2015-12-15 15:04:12,134 - User['oozie'] {'gid': 'hadoop', 'groups': ['users']}\n2015-12-15 15:04:12,135 - User['hive'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,135 - User['zeppelin'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,136 - User['ambari-qa'] {'gid': 'hadoop', 'groups': ['users']}\n2015-12-15 15:04:12,136 - User['flume'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,137 - User['hdfs'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,137 - User['knox'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,138 - User['ranger'] {'gid': 'hadoop', 'groups': ['ranger']}\n2015-12-15 15:04:12,138 - User['storm'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,139 - User['spark'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,140 - User['mapred'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,140 - User['accumulo'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,140 - Adding user User['accumulo']\n2015-12-15 15:04:12,227 - User['hbase'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,228 - User['tez'] {'gid': 'hadoop', 'groups': ['users']}\n2015-12-15 15:04:12,229 - User['zookeeper'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,229 - User['kafka'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,230 - User['falcon'] {'gid': 'hadoop', 'groups': ['users']}\n2015-12-15 15:04:12,230 - User['sqoop'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,231 - User['yarn'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,231 - User['hcat'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,232 - User['ams'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,232 - User['atlas'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2015-12-15 15:04:12,233 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2015-12-15 15:04:12,234 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2015-12-15 15:04:12,238 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2015-12-15 15:04:12,238 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'recursive': True, 'mode': 0775, 'cd_access': 'a'}\n2015-12-15 15:04:12,239 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2015-12-15 15:04:12,240 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}\n2015-12-15 15:04:12,243 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if\n2015-12-15 15:04:12,244 - Group['hdfs'] {'ignore_failures': False}\n2015-12-15 15:04:12,244 - User['hdfs'] {'ignore_failures': False, 'groups': ['hadoop', 'hdfs']}\n2015-12-15 15:04:12,245 - Directory['/etc/hadoop'] {'mode': 0755}\n2015-12-15 15:04:12,256 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2015-12-15 15:04:12,256 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}\n2015-12-15 15:04:12,267 - Repository['HDP-2.3'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.2.0/', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP', 'mirror_list': None}\n2015-12-15 15:04:12,319 - File['/etc/yum.repos.d/HDP.repo'] {'content': InlineTemplate(...)}\n2015-12-15 15:04:12,332 - Repository['HDP-UTILS-1.1.0.20'] {'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP-UTILS', 'mirror_list': None}\n2015-12-15 15:04:12,334 - File['/etc/yum.repos.d/HDP-UTILS.repo'] {'content': InlineTemplate(...)}\n2015-12-15 15:04:12,346 - Package['unzip'] {}\n2015-12-15 15:04:12,482 - Skipping installation of existing package unzip\n2015-12-15 15:04:12,482 - Package['curl'] {}\n2015-12-15 15:04:12,553 - Skipping installation of existing package curl\n2015-12-15 15:04:12,553 - Package['hdp-select'] {}\n2015-12-15 15:04:12,623 - Skipping installation of existing package hdp-select\n2015-12-15 15:04:12,755 - Package['accumulo_2_3_*'] {}\n2015-12-15 15:04:12,890 - Installing package accumulo_2_3_* ('/usr/bin/yum -d 0 -e 0 -y install 'accumulo_2_3_*'')</pre><p>I think this might be the problem</p><pre>14: PYCURL ERROR 6 - \"Couldn't resolve host 'mirrorlist.centos.org'\"</pre>","tags":["Accumulo","Sandbox","Sqoop"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-15 08:22:45.0","id":6488,"title":"YARN JMX access","body":"<p>Hello,</p><p>I try to connect to the JMX console of the ResourceManager  via some tools like Jconsole, but it fails establish a proper connection : </p><p><em>Exception in thread \"main\" java.io.IOException: Failed to retrieve RMIServer stub: javax.naming.CommunicationException [Root exception is java.rmi.onnectIOException: non-JRMP server at remote endpoint] at javax.management.remote.rmi.RMIConnector.connect(RMIConnector.java:369) at javax.management.remote.JMXConnectorFactory.connect(JMXConnectorFactory.java:270) at org.archive.jmx.Client.execute(Client.java:225) at org.archive.jmx.Client.main(Client.java:154)</em></p>....though the JMX data are visible via the URL : <p><a href=\"http://&lt;ResourceManager\">http://&lt;ResourceManager&gt;:8088/jmx</a></p><p>Any idea on how to enable a correct JMX port for the Yarn ResourceManager ?</p><p>Thanks in advance.</p><p>Kind regards</p><p>LC</p>","tags":["resource-manager","YARN","jmx"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-15 21:11:00.0","id":6593,"title":"Editing firewall settings in HDInsight over Azure?","body":"<p>How do I view firewall settings in HDInsight over Azure?</p>","tags":["azure","hdinsight","firewall"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-16 00:19:48.0","id":6652,"title":"Not able to fnd JDBC driver in HDP 2.2 Sandbox","body":"<p>The JDBC call to hive database is falling with error \" no suitable driver found for \"jdbc:hive2://localhost:10000/default\"  The hive-jdbc jar is available in /usr/hdp/2.2*/hive/lib path. I am calling \"org.apache.hive.jdbc.HiveDriver\" driver class.</p><p> What could be issue here?</p>","tags":["hive-jdbc","Hive","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-15 18:38:05.0","id":6570,"title":"How to Load a Textfile bcp'ed out of MS SQL Server table  to Hive External table in ORC format HDFS","body":"<p>I extracted comma delimited Text file via MS SQL Server bcp utility for a given table which I need to load into Hive ORC format External table. I can do it if I use 'TextFIle' Format but not if I use 'ORC format' while creating External Table in Hive.</p><p>Any ideas ?</p><p>Thanks.</p>","tags":["Hive","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-15 02:58:57.0","id":6463,"title":"Do we have a walk through for Ranger and Knox?","body":"","tags":["how-to-tutorial","Ranger","security","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-16 14:46:53.0","id":6796,"title":"How to increase DataNode filesystem size","body":"<p>We have a small cluster for testing. I used lvm for the filesystems on the datanodes. I increased the size of the lv on the datanodes but the NameNode has not picked up the change. </p><p>I tried hdfs dfsadmin -refreshNodes</p><p>I also decommissioned the node and then recommissioned it.</p><p>The NameNode still does not see the additional space.</p>","tags":["filesystem","datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-16 13:06:35.0","id":6746,"title":"Hive jdbc connection string","body":"<p>I am trying to connect to hive through a java program using the below connection string. It was working fine until the hdp upgrade to 2.3.</p><p>jdbc:hive2://knoxserver.net:8443/default;ssl=false;transportMode=http;httpPath=knox/sandbox/hive\",\n                \"knoxusername\", \"knoxuserpwd\"</p><p>I am able to execute the connection string through beeline as below.</p><p>\"jdbc:hive2://m1.hdp.local:10010/&lt;db&gt;;principal=hive/_HOST@HDP.LOCAL;transportMode=http;httpPath=cliservice\"</p><p>but that works only when I do a kinit from the edge node. But I want the coneection string to work in the java program without doing kinit. Also the connection string I mentioned in my question is giving a invalid connection string error in beeline. In java program I am getting the below error.</p><p>Exception in thread \"main\" java.sql.SQLException: Could not open connection to jdbc:hive2://hive2://knoxserver.net:8443/default;ssl=false;transportMode=http;httpPath=knox/sandbox/hive: null at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:206) at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:178) at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) at java.sql.DriverManager.getConnection(Unknown Source) at java.sql.DriverManager.getConnection(Unknown Source) at find_file_by_filename.main(find_file_by_filename.java:51) Caused by: org.apache.thrift.transport.TTransportException at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84) at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:178) at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:288) at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:203) ... 5 more</p>","tags":["Hive","Knox","hiveserver2","jdbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 19:51:00.0","id":7117,"title":"HIVE QUERY OUTPUT FOR EXTERNAL Vs INTERNAL TABLE","body":"<p>Folks, Running a query against External Table - based on Textfile and Internal Table is ORC format with snappy compression (Insert/Update/Delete) - output of the below query is totally different - wondering why? please post your feedback on this - it's much appreciated.</p><p>Select min(sno) from table1 where scode in ('100',200','53','52); - For External table output is '10' , Internal Table output is sometime '1' and sometime '3' - don't know what's causing this?</p><p>Thanks</p><p>Babu Gopal</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 22:21:44.0","id":7133,"title":"Solr mapreduce with kerberized backend fails","body":"<p>\n\tDear all,</p><p>\n\tWe have job which runs `MapReduceIndexerTool` in kerebized environment. With couple tweaks we managed to get it running and even successing map/reduce phase, however it fails at go live stage while inserting data:</p>\n<pre>--- bunch of earlier log entries ---\n15/12/17 19:33:08 INFO mapreduce.Job:  map 100% reduce 99%\n15/12/17 19:33:28 INFO mapreduce.Job:  map 100% reduce 100%\n15/12/17 19:34:58 INFO mapreduce.Job: Job job_1450203660079_0013 completed successfully\n15/12/17 19:34:58 INFO mapreduce.Job: Counters: 52\n\tFile System Counters\n\t\tFILE: Number of bytes read=1933903322\n\t\tFILE: Number of bytes written=3643256225\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=13020909852\n\t\tHDFS: Number of bytes written=20619046734\n\t\tHDFS: Number of read operations=10964\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=1344\n\tJob Counters \n\t\tLaunched map tasks=236\n\t\tLaunched reduce tasks=24\n\t\tOther local map tasks=236\n\t\tTotal time spent by all maps in occupied slots (ms)=5822436\n\t\tTotal time spent by all reduces in occupied slots (ms)=15745656\n\t\tTotal time spent by all map tasks (ms)=5822436\n\t\tTotal time spent by all reduce tasks (ms)=7872828\n\t\tTotal vcore-seconds taken by all map tasks=5822436\n\t\tTotal vcore-seconds taken by all reduce tasks=7872828\n\t\tTotal megabyte-seconds taken by all map tasks=14905436160\n\t\tTotal megabyte-seconds taken by all reduce tasks=40308879360\n\tMap-Reduce Framework\n\t\tMap input records=1886\n\t\tMap output records=16964842\n\t\tMap output bytes=11060997974\n\t\tMap output materialized bytes=1650839353\n\t\tInput split bytes=41536\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=16964842\n\t\tReduce shuffle bytes=1650839353\n\t\tReduce input records=16964842\n\t\tReduce output records=16964842\n\t\tSpilled Records=35286185\n\t\tShuffled Maps =5664\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=5664\n\t\tGC time elapsed (ms)=313229\n\t\tCPU time spent (ms)=8043320\n\t\tPhysical memory (bytes) snapshot=479611183104\n\t\tVirtual memory (bytes) snapshot=818600177664\n\t\tTotal committed heap usage (bytes)=530422693888\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=20537547\n\tFile Output Format Counters \n\t\tBytes Written=20619046734\n\torg.apache.solr.hadoop.SolrCounters\n\t\tSolrReducer: Number of document batches processed=848257\n\t\tSolrReducer: Number of documents processed=16964842\n\t\tSolrReducer: Time spent by reducers on physical merges [ms]=1316244849188\n15/12/17 19:34:58 INFO hadoop.MapReduceIndexerTool: Done. Indexing 1886 files using 236 real mappers into 24 reducers took 3.31220419E11 secs\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merging of output shards into Solr cluster...\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00000 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00003 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00005 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00001 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00006 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00002 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00004 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00011 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00012 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00010 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00009 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00008 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00007 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00014 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00013 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00015 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00016 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00017 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00018 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00019 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00020 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00021 into &lt;a href=\"http://hdp-2.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00022 into &lt;a href=\"http://hdp-3.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:58 INFO hadoop.GoLive: Live merge hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676/results/part-00023 into &lt;a href=\"http://hdp-1.magic.com:8983/solr\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt;\n15/12/17 19:34:59 ERROR hadoop.GoLive: Error sending live merge command\njava.util.concurrent.ExecutionException: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at &lt;a href=\"http://hdp-1.magic.com:8983/solr:\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt; Expected mime type application/octet-stream but got text/html. &lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/&gt;\n&lt;title&gt;Error 401 Authentication required&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;h2&gt;HTTP ERROR 401&lt;/h2&gt;\n&lt;p&gt;Problem accessing /solr/admin/cores. Reason:\n&lt;pre&gt;    Authentication required&lt;/pre&gt;&lt;/p&gt;&lt;hr&gt;&lt;i&gt;&lt;small&gt;Powered by Jetty://&lt;/small&gt;&lt;/i&gt;&lt;hr/&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:188)\n\tat org.apache.solr.hadoop.GoLive.goLive(GoLive.java:118)\n\tat org.apache.solr.hadoop.MapReduceIndexerTool.run(MapReduceIndexerTool.java:866)\n\tat org.apache.solr.hadoop.MapReduceIndexerTool.run(MapReduceIndexerTool.java:608)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.solr.hadoop.MapReduceIndexerTool.main(MapReduceIndexerTool.java:595)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at &lt;a href=\"http://hdp-1.magic.com:8983/solr:\"&gt;http://hdp-2.magic.com:8983/solr&lt;/a&gt; Expected mime type application/octet-stream but got text/html. &lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/&gt;\n&lt;title&gt;Error 401 Authentication required&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;h2&gt;HTTP ERROR 401&lt;/h2&gt;\n&lt;p&gt;Problem accessing /solr/admin/cores. Reason:\n&lt;pre&gt;    Authentication required&lt;/pre&gt;&lt;/p&gt;&lt;hr&gt;&lt;i&gt;&lt;small&gt;Powered by Jetty://&lt;/small&gt;&lt;/i&gt;&lt;hr/&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:527)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:214)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:210)\n\tat org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:131)\n\tat org.apache.solr.hadoop.GoLive$1.call(GoLive.java:99)\n\tat org.apache.solr.hadoop.GoLive$1.call(GoLive.java:90)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor$1.run(ExecutorUtil.java:148)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n15/12/17 19:34:59 INFO hadoop.GoLive: Live merging of index shards into Solr cluster took 9.355796E7 secs\n15/12/17 19:34:59 INFO hadoop.GoLive: Live merging failed\nJob failed, leaving temporary directory: hdfs://ambari.magic.com:8020/user/banana/mapreduceindexer-temp/temp-29676\n</pre><p>We had some issues with other places which were calling solr REST services but we have fixed that by using Krb5HttpClientConfigurer, however in this case we can't change code which is coming from Solr codebase.</p>","tags":["MapReduce","SOLR","kerberos"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-16 19:36:31.0","id":6900,"title":"I am getting  following error when i try to load the data into elastic search \"Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row\"","body":"<p>also in the same log i am getting below error too</p><p>\"Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -20\"</p>","tags":["elasticsearch","data-ingestion","data"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-07 04:08:53.0","id":9045,"title":"Where Does the atlas meta store located","body":"<p><strong>ATLAS METADATA :</strong></p><ul><li>Where does atlas metadata reside inside hadoop cluster.</li><li>How to access the metadata.</li></ul>","tags":["hadoop","Atlas","hadoop-ecosystem"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-07 03:04:49.0","id":9033,"title":"How to add custom alerts in Ambari","body":"<p>I would like to add a custom alert for single or multiple nodes in the cluster apart from hadoop services which are already defined under \"Alert\" section of Ambari. I know that we can do this using Nagios however just wanted to know if it is possible via Ambari?</p><p>e.g. I would like to monitor disk space of my /xyz directory which contains input data to some hadoop job.</p>","tags":["Ambari","ambari-alerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-02 21:26:46.0","id":25658,"title":"How can I specify table output directory using SparkSQL?","body":"<p>I'm using SparkSQL (local mode) in Zeppelin for development work. As I am not running on a cluster, I do not have /user/hive/warehouse directories. If I'm using strictly SQL, is there a way I can specify the directory of my newly created tables? How about setting the default output directories?</p><p>Failing Example:</p><pre>%sql\ncreate table pings as\nselect\n  split(time, \" \")[0] as month,\n  split(time, \" \")[2] as year,\n  split(split(time, \" \")[3], \":\")[0] as hour,\n  split(split(time, \" \")[3], \":\")[1] as minute,\n  split(split(split(time, \" \")[3], \":\")[2], \"\\\\.\")[0] as second,\n  substr(split(split(split(time, \" \")[3], \":\")[2], \"\\\\.\")[1],0, 3) as ms,\n  *\nfrom pings_raw\n</pre><pre>MetaException(message:file:/user/hive/warehouse/pings is not a directory or unable to create one)\nset zeppelin.spark.sql.stacktrace = true to see full stacktrace</pre>","tags":["Spark","spark-sql"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-04 07:11:53.0","id":25759,"title":"Spark - Execute in the worker URL","body":"<p>How to submit jobs to the slave node (worker node) from master node in spark.</p><p>I started the nodes successfully,</p><p>I executed the following commands</p><table><tbody><tr><td><p><strong>./spark-submit --class \"SimpleApp\" --master \"spark://worker@192.168.117.141:35941\" /home/ssbatch4/.sbt/0.13/staging/b081b85b0b35b548b3ca/simpleproj/target/scala-2.10/simple-project_2.10-1.0.jar</strong></p></td></tr></tbody></table><table><tbody><tr><td><p><strong>./spark-submit --class \"SimpleApp\" --master \"spark://sridhar25.sridhar.com:35941\" /home/ssbatch4/.sbt/0.13/staging/b081b85b0b35b548b3ca/simpleproj/target/scala-2.10/simple-project_2.10-1.0.jar</strong></p></td></tr></tbody></table><p>It is creating exceptions for the above commands.</p><p>there is master URL in the master node,</p><p>there is no master URL in the slave node</p>","tags":["spark-streaming","Spark","spark-csv"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-29 14:40:49.0","id":24998,"title":"Has anyone tried to use Apache Ignite on Yarn with HDFS?","body":"<p>Has anyone tried to use Apache Ignite on Yarn with HDFS? Specifically the HDFS acceleration feature (I am guessing similar to Tachyon). </p>","tags":["memory","data-management","HDFS"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-06 00:41:16.0","id":26032,"title":"HDP 2.4  rolling upgrade","body":"<p>I am planning to upgrade from HDP 2.2.6 to HDP 2.4. I am also running Ambari 2.1.0 version. What is the best way to upgrade 2.4.0 ? </p><p>Is there a direct way to upgrade to 2.4 from 2.2.6  ?  This is what I am planning to upgrade . </p><p>\nStep 1 : Upgrade Ambari 2.2.x version</p><p>Step 2  : Upgrade to HDP 2.3.x version  ( I can skip this if direct rolling upgrade is available from 2.2 to 2.4)</p><p>Step 3 :Upgrade to HDP 2.4 version. </p><p>Please give your thoughts and suggestions on this .</p><p>I see documents for manual upgrade from 2.2 to 2.4 but I haven't seen documents for rolling upgrade. </p>","tags":["hdp-2.4.0","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-06 10:59:35.0","id":26152,"title":"Ambari 2.2.1.1 STORM widget not found","body":"<p>Hi,</p><p>I have Ambari 2.2.1.1 on HDP 2.4.0.0-169.</p><p>If I try to add a widget to STORM It tells me:</p><p><img src=\"https://community.hortonworks.com/storage/attachments/3237-cattura.png\"></p>","tags":["Ambari","metrics-collector","Storm"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-06 06:07:18.0","id":26110,"title":"Hbase tall-narrow or flat wide design","body":"<p>Can anyone throw some light on the hbase table design. which one should one use \"tall-narrow or flat wide design\" and for which use case.</p>","tags":["Hbase","development","nosql"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-06 08:01:39.0","id":26121,"title":"not able to schedule falcon feed","body":"<p>Team:</p><p>I am following http://hortonworks.com/blog/introduction-apache-falcon-hadoop/ link and trying to implement example 3 but I am not able to schedule feed entity. I am getting below error, I tried to check falcon_application.log file but unfortunately it is not getting generated. </p><p>s0998dnz@lxhdpmasttst001 falconReplicationDemo]$ falcon entity -type feed -schedule -name replication-feed</p><p>ERROR: Bad Request;default/org.apache.falcon.FalconWebException::org.apache.falcon.FalconException: Entity schedule failed for feed: replication-feed</p>","tags":["cluster","replication","feed"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-07 08:29:07.0","id":26335,"title":"how to upgrade HDP2.4's knox0.6 to knox 0.7?","body":"<p>We find that HDP2.4 ship knox of 0.6 , which is not  ideal one. We want o to upgrade HDP2.4 cluster 's knox 0.6 to 0.7.  How to do that?</p>","tags":["Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-07 10:47:49.0","id":26353,"title":"Upgrade custom Ambari service separately","body":"<p>How can I upgrade a custom Ambari service independently from HDP and other services? Let's say the third-party service has a new version that runs on the same HDP version. It seems that on the Ambari Web UI I can only upgrade/downgrade to HDP versions. I've checked the code for other services (like HBase), but I think it's upgraded only when the whole cluster is upgraded.</p><p>Is there an option for this like for Cloudera parcels? Or should I manually copy the new service version to the cluster, stop and delete the original, and add the new version on the Web UI?</p>","tags":["ambari-service","Ambari","upgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-07 10:57:46.0","id":26354,"title":"Hive garbage collector best practice","body":"<p>Hi,</p><p>I see an increase of Old Generation memory from jmap for Hiverser2. </p><p>This is my configuration:</p><p>/usr/jdk64/jdk1.8.0_40/bin/java -Xmx2048m -Dhdp.version=2.3.2.0-2950 -Djava.net.preferIPv4Stack=true -Dhdp.version=2.3.2.0-2950 -Dhadoop.log.dir=/var/log/hadoop/hive -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.3.2.0-2950/hadoop -Dhadoop.id.str=hive -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/hdp/2.3.2.0-2950/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx2048m -Xmx8192m -<strong>XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/log/hive/oom/ -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:ErrorFile=/var/log/hive/oom/hs2jvmerror%p.log</strong> -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /usr/hdp/2.3.2.0-2950/hive/lib/hive-service-1.2.1.2.3.2.0-2950.jar org.apache.hive.service.server.HiveServer2 --hiveconf hive.aux.jars.path=file:///usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar -hiveconf hive.metastore.uris=  -hiveconf hive.log.file=hiveserver2.log -hiveconf hive.log.dir=/var/log/hive</p><p>jmap -heap output is:</p><p>JVM version is 25.40-b25 </p><p>using thread-local object allocation. </p><p>Parallel GC with 8 thread(s)</p><p>Heap Configuration: </p><p>   MinHeapFreeRatio         = 0</p><p>\n   MaxHeapFreeRatio         = 100</p><p>\n   MaxHeapSize              = 8589934592 (8192.0MB) </p><p>   NewSize                  = 335544320 (320.0MB) </p><p>   MaxNewSize               = 2863136768 (2730.5MB)</p><p>\n   OldSize                  = 671088640 (640.0MB)</p><p>\n   NewRatio                 = 2</p><p>\n   SurvivorRatio            = 8 </p><p>   MetaspaceSize            = 21807104 (20.796875MB) </p><p>   CompressedClassSpaceSize = 1073741824 (1024.0MB) </p><p>   MaxMetaspaceSize         = 17592186044415 MB </p><p>   G1HeapRegionSize         = 0 (0.0MB) </p><p>Heap Usage: </p><p>PS Young Generation</p><p>\nEden Space:\n   capacity = 354418688 (338.0MB)</p><p>\n   used     = 70879872 (67.5963134765625MB) </p><p>   free     = 283538816 (270.4036865234375MB) </p><p>   19.998909312592456% used </p><p>From Space: </p><p>   capacity = 14680064 (14.0MB) </p><p>   used     = 14244056 (13.584190368652344MB)</p><p>\n   free     = 436008 (0.41580963134765625MB) </p><p>   97.0299312046596% used </p><p>To Space: </p><p>   capacity = 67108864 (64.0MB)</p><p>\n   used     = 0 (0.0MB)</p><p>\n   free     = 67108864 (64.0MB)</p><p>\n   0.0% used </p><p>PS Old Generation </p><p>   capacity = 3139960832 (2994.5MB) </p><p>   used     = 2938877792 (2802.7322692871094MB) </p><p>   free     = 201083040 (191.76773071289062MB)</p><p>\n   93.59600164592116% used </p><p>39810 interned Strings occupying 4013432 bytes.</p><p>Do you have any best practice for tuning this?</p>","tags":["best-practices","garbage-collector","hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-07 17:00:34.0","id":26445,"title":"HUE upgrade","body":"<p>I am running HUE 2.6 on HDP 2.3. Need to upgrade HUE to 3.8. Where can I find the instructions. How to perform in-place upgrade. </p>","tags":["hue"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-07 21:22:35.0","id":26525,"title":"HDP deployment - App Timeline Install error","body":"<p>Here is the error message:</p><pre>resource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.4 | tail -1`' returned 1. Traceback (most recent call last):\n  File \"/usr/bin/hdp-select\", line 375, in &lt;module&gt;\n    setPackages(pkgs, args[2], options.rpm_mode)\n  File \"/usr/bin/hdp-select\", line 268, in setPackages\n    os.symlink(target + \"/\" + leaves[pkg], linkname)\nOSError: [Errno 17] File exists</pre><p>Can someone help me with what is going on?</p>","tags":["hdp-2.4"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-07 14:47:48.0","id":26404,"title":"Hive query out of memory","body":"<p>Hi All,</p><p>I have a hive query running on top of HBase (I have to use hive and not phoenix). This query failed with an exception 'out of memory'.</p><p>Do you have an idea should I have a look to solve the issue?</p><p>In attachment is the full exception stack.</p><p>Thanks in advance,</p><p>Michel</p>","tags":["Hive","query"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-08 05:05:07.0","id":26570,"title":"Hadoop Query Issue - Different Result on TEZ and MR Engine","body":"<p>Hi Everyone, </p><p>I am getting different result (count output) when i run same HIVE query on MR and TEZ.  In MR i get greater value which is actually correct and when i use Tez engine for the same query  iam getting lower value which is wrong. </p><p># Iam not using any compression on data </p><p><strong>Query for your reference::::::::::::::::</strong></p><p>select count(1)</p><p>FROM</p><p>(select * from\nt_attr</p><p>WHERE mthly_ext_stat_cde IN ('R','A')</p><p>and perf_mthly_dt = '2015-09-30'</p><p>) a</p><p>INNER JOIN\nt_sot_dir_bnk_card.curr_acct c</p><p>ON a.acct_key=c.acct_key;</p><p># I tried spitting the query by running just select * from table using both the engine iam getting same result, But when i use JOIN iam getting different value. </p><p>\nCan someone please help me to solve the issue. </p><p><strong>TEZ version : 0.7.0</strong></p><p><strong>HIVE  : 1.2.1</strong></p><p><strong>HDP : 2.7.1</strong></p><p>Regards</p><p>Suresh bk </p>","tags":["Tez","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-07 11:12:48.0","id":26371,"title":"Is it possible to execute Hive queries parallelly (single conection jdbc)","body":"<p>i try to develop  application java that manage data  stored in hdfs by using  hive jdbc to analyse  this data ,  after that i create webservice that connect to hive and  select the data that i needed  , when  i tested my webservice   it's work  but the problem when i try to call my webservice  multiple  time parallaly  ,some of it give error and  it d'ont return a result  \"java.sql.SQLException: org.apache.thrift.transport.TTransportException: SASL authentication not complete \"   error  Socket closed\" or \"Cannot read from null inputStream\"   and this error from log file \"    Error running hive query:\",  i thought that  hive not support parrallel  execution  concurrency query for single connexion  jdbc , i need to know more about this issue , ,any explenation is appreciated ;</p><h1></h1>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-08 15:44:27.0","id":26661,"title":"LDAPS connection failure while using Ambari Kerberos wizard","body":"<p>We are having trouble using the Kerberos wizard in Ambari\nwhen testing the connection to our AD domain controllers over LDAPS.  They sit behind a load-balancer which is\nsecured using a third-party trusted certificate.  Originally we thought that the certificate was\nat issue as testing with an openssl client was producing a “self-signed”\nwarning.  This was corrected though when\nwe updated the underlying OS software and it presumably updated the root\ncertificate.</p><p>The errors we receive in the log are the following:</p><pre>ERROR [ambari-kdc-verify] KdcConnection:380 - Authentication failed\nERROR [ambari-kdc-verify] KdcConnection:380 - Authentication failed\nWARN [qtp-ambari-client-23]KdcServerConnectionVerification:167 - Failed to connect to the KDC server at &lt;servername&gt;:636 over TCP \nWARN [qtp-ambari-client-23] KdcServerConnectionVerification:197 - Timeout occurred while attempting to communicate with KDC server at &lt;servername&gt;:636 over UDP \nERROR[qtp-ambari-client-23] KdcServerConnectionVerification:113 - Failed to connect to\nthe KDC at &lt;servername&gt;:636 using either TCP or UDP</pre><p>We have tested the port on the load balancer using netcat/openssl\nand ran a search using ldapsearch, they were all able to connect to that port\nand ldapsearch returned results.  Using\nthe test option in the wizard also works when the connection is to a standard\ndomain controller over port 389.  We’ve\nalso been able to setup LDAPS authentication for the Ambari web console to the\nsame load balancer address which also works fine.</p><p>Any insights into what might be wrong or should we move forward\nwith manual creation/distribution of keytabs and principals?</p>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-07 16:42:10.0","id":26435,"title":"Error on \"A Lap Around Apache Spark\"","body":"<p>Im getting this error when im trying to run the Pi test ?? anyone know how to solve this problem ?</p><p>16/04/07 16:12:03 INFO Client: Application report for application_1460045249798_0001 (state: ACCEPTED)</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-11 09:32:36.0","id":26893,"title":"Steps to install HDP cluster on Google cloud","body":"<p>Are their any resources/steps to configure HDP cluster (using Ambari) on Google cloud?</p><p>PS- I am looking to deploy a full blown multi-node cluster not a single node setup.</p><p>Thanks</p><p>Amit</p>","tags":["cloud","google"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-16 15:57:47.0","id":33533,"title":"Race Condition where Task is Preempted, but Job fails from InvalidStateTransitonException.","body":"<p>\n\tDiagnosing a Race condition where Job Finishes with a FINISHED State, but FAILS because of a  org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at COMMITTING. In reviewing the Job logs of the Application All of the other tasks are still RUNNING when the Preemption takes place, but one of the tasks has already completed and is causing this to go into a Invalid state. I'm trying to see if there are any recommended ways to work around this. </p><p>Below is a clip of the Job Log:</p><pre>2016-05-13 12:10:44,686 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_e594_1462333361639_127115_01_000076 taskAttempt attempt_1462333361639_127115_m_000000_3\n\n2016-05-13 12:10:44,693 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1462333361639_127115_m_000000_3\n\n2016-05-13 12:10:44,694 INFO [ContainerLauncher #7] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : &lt;hostName&gt;:45454\n\n2016-05-13 12:10:44,708 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1462333361639_127115_m_000000_3 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n\n2016-05-13 12:10:44,709 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1462333361639_127115_m_000000_3\n\n2016-05-13 12:10:44,710 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1462333361639_127115_m_000000 Task Transitioned from RUNNING to SUCCEEDED\n\n2016-05-13 12:10:44,710 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 1\n\n2016-05-13 12:10:44,711 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1462333361639_127115Job Transitioned from RUNNING to COMMITTING\n\n2016-05-13 12:10:44,712 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_COMMIT\n\n2016-05-13 12:10:44,787 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 1\n\n2016-05-13 12:10:45,098 INFO [CommitterEvent Processor #4] hive.metastore: Trying to connect to metastore with URI thrift://&lt;hostName&gt;:9083\n\n2016-05-13 12:10:45,157 INFO [CommitterEvent Processor #4] hive.metastore: Connected to metastore.\n\n2016-05-13 12:10:45,293 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:0 RackLocal:0\n\n2016-05-13 12:10:45,294 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_e594_1462333361639_127115_01_000076\n\n2016-05-13 12:10:45,295 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:0 RackLocal:0\n\n2016-05-13 12:10:45,295 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1462333361639_127115_m_000000_3 TaskAttempt Transitioned from SUCCEEDED to KILLED\n\n2016-05-13 12:10:45,295 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1462333361639_127115_m_000000_3: Container preempted by scheduler\n\n2016-05-13 12:10:45,297 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved &lt;hostName&gt; to /rack13\n\n2016-05-13 12:10:45,297 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved &lt;hostname&gt; to /rack6\n\n2016-05-13 12:10:45,298 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved &lt;hostname&gt; to /rack6\n\n2016-05-13 12:10:45,298 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1462333361639_127115_m_000000 Task Transitioned from SUCCEEDED to SCHEDULED\n\n2016-05-13 12:10:45,298 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at COMMITTING\n\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:996)\n\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:138)\n\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1289)\n\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1285)\n\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:182)\n\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)\n\n\tat java.lang.Thread.run(Thread.java:744)\n\n2016-05-13 12:10:45,299 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at COMMITTING\n\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:996)\n\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:138)\n\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1289)\n\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1285)\n\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:182)\n\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)\n\n\tat java.lang.Thread.run(Thread.java:744)\n\n2016-05-13 12:10:45,299 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1462333361639_127115_m_000000_4 TaskAttempt Transitioned from NEW to UNASSIGNED\n\n2016-05-13 12:10:45,302 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1462333361639_127115Job Transitioned from COMMITTING to ERROR\n\n2016-05-13 12:10:45,303 INFO [Thread-87] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: We are finishing cleanly so this is the last retry\n\n2016-05-13 12:10:45,303 INFO [Thread-87] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true\n\n2016-05-13 12:10:45,303 INFO [Thread-87] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true\n\n2016-05-13 12:10:45,304 INFO [Thread-87] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true\n\n2016-05-13 12:10:45,304 INFO [Thread-87] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true\n\n2016-05-13 12:10:45,304 INFO [Thread-87] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Calling stop for all the services\n\n2016-05-13 12:10:45,305 INFO [Thread-87] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0\n</pre>","tags":["YARN","MapReduce","preemption"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-16 23:59:49.0","id":33618,"title":"How to loop using Pig for unique values","body":"<p>Hi,</p><p>I have a file that contains the census information which I would like to query using Pig.</p><p>The file format is as follows:</p><p>ID Name Year Gender State Count</p><p>1 Jones 1980    M         MA    100</p><p>I would like to get the percentage for each name for that state in that year for each year in the file</p><p>How can I loop through each of the years and calculate for each state the percentage of occurrences of each name?</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-17 05:17:02.0","id":33661,"title":"Analytics in hadoop","body":"<p>This is more of a general structure question but here it is. How and where do analytics people fit into hadoop environment? These people usually work with analysis stuff using Excel. Hive looks like an option but again are these people expected to know SQL querying sort of thing?</p><p>Also,let's say hive part is what they manage. How would graphical reports be generated? Do we need to use R for that?</p><p>R is again a programming thing. Hive does generate charts for queries executed but these can not really be downloaded, right? What are other good alternatives for this? </p>","tags":["Sqoop"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-17 19:18:27.0","id":33838,"title":"How to read specific jar from classpath (having multiple version of common-codec in classpath)","body":"<p>My application need common-codec1.10 version but on cluster i have 1.4, when i launch yarn application using hadoop_classpath=/home/*/common-codec-1.10.jar:o yarn jar &lt;*.jar&gt;\nyarn picks up 1.4 first then after it has 1.10. so i am getting exception on method not found.\n\nHow can tell yarn application to read through full classpath instead to read till 1.4 and stop.</p>","tags":["help","YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-16 10:43:06.0","id":33468,"title":"Reading/Parsing aggregated yarn app log file","body":"<p>Is there any way to read a aggregated yarn app log file as a text, without using 'yarn logs'? I have a environment which does not allow me to use 'yarn logs' but 'hdfs dfs -{text|get}'. So I tired the command below but it did not work.</p><pre>hdfs dfs -text /app-logs/hive/logs/application_1463130014242_0012/ip-10-0-1-133.ap-northeast-1.compute.internal_45454 | gunzip</pre><p>Why I suppose the file is encoded is here</p><p><img src=\"/storage/attachments/4233-logcompression.png\"></p>","tags":["YARN","logs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-17 15:20:05.0","id":33769,"title":"Sandbox HDP 2.4 & Azure","body":"<p>when I looked into Azure marketplace, I was only able to find 2.2 & 2.3 version.</p>","tags":["azure","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-18 13:41:45.0","id":33995,"title":"Missing kafka.consumer.* MBeans","body":"<p>What am I missing? I'm using JConsole connected to my Kafka 0.9.0 broker to find this:</p><p><em><strong>kafka.consumer:type=ConsumerFetcherManager,name=MaxLag,clientId=([-.\\w]+)</strong></em></p><p>However, there is no kafka.consumer.* (nor kafka.producer.* for that matter). </p><p>Thanks.</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-18 18:13:59.0","id":34079,"title":"Can the default encryption type (3des) for ambari security  be changes to AES?","body":"<p>\n\twhen \"Encrypt passwords stored in ambari.properties file\" using <strong>ambari-server setup-security</strong>. i know that the passwords are stored in a Java Keystore implementation (JCEKS) which uses 3DES in CBC mode with PKCS #5 padding to encrypt its keys. </p><p>since JCEKS also support AES with the JCE installed, can the default encryption type (3des) be changes to AES? if yes, how do we do this?</p><p><a href=\"https://ambari.apache.org/1.2.5/installing-hadoop-using-ambari/content/ambari-secure-passwords.html\">https://ambari.apache.org/1.2.5/installing-hadoop-using-ambari/content/ambari-secure-passwords.html</a></p>","tags":["Ambari","security","encryption"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-18 20:57:07.0","id":34132,"title":"Ranger HA setup documentation","body":"<p>Hi, I am trying to set up the Ranger HA ie. HA for Ranger Admin. I know that to set up it needs to have a loadbalancer configured and give that for the external URL. </p><p>However, could not find any instructions on how to set up the Load balancer for Ranger HA. Can some one provide load balancer configurations for Ranger HA. </p><p>I am trying with HAProxy as Load balancer. With my set up, when I try to log in to the Load balancer url that is provided, it just hangs at Loading.... does not bring up the repository page. </p><p>So are there any instructions or suggestions to how to setup the load balancer for Ranger HA?  </p>","tags":["ranger-admin"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-18 19:07:38.0","id":34099,"title":"Spark =Streaming job submit as a service","body":"<p>I have a spark streaming running on azure with Hortonworks sandbox . </p><p>I run it via spark-submit from ssh . </p><p>but when I close ssh the job stops.</p><p>How can I sumbit the spark job as a service? </p><p>TIA</p>","tags":["spark-streaming"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-19 07:50:50.0","id":34241,"title":"namenode format effect","body":"<p>I understand that namenode format will not delete the data but will not know about that data anymore.</p><p>but let's say I executed namenode format command. Then hadoop fs -ls will give me the same data that was in there before formatting or it will give empty dirs?</p>","tags":["format","namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-23 10:40:47.0","id":34880,"title":"Setting up HDP 2.3 cluster with 3 nodes","body":"<p>Hi,</p><p>We are planning to install a HDP 2.3 cluster with 3 nodes.  Kindly let me know the service distribution among them.  We are building it for SAP HANA Vora POC.</p><p>Regards,</p><p>Subramanian S.</p>","tags":["installation"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-05-24 14:24:41.0","id":35234,"title":"Hive settings for mapjoin","body":"<p style=\"margin-left: 60px;\">currently I am having issue with mapjoin when we run queries with mapjoin as true hitting into error with mapjoinmemory exhausted ..so thought of increaing the noconditionaltask.size to the small table size...but how can we derive the maximum size of this property ..do we need to consider heap size or anything else.</p>","tags":["Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-24 07:46:25.0","id":35126,"title":"Keberose authenication issue","body":"<p>Hi,</p><p>I am getting \"No key to store\" exception in kerberized cluster while restarting ranger kms service.</p><pre>INFO  SessionIdGenerator - Creation of SecureRandom instance for session ID generation using [SHA1PRNG] took [374] milliseconds.\n2016-05-24 07:45:20,404 ERROR [/kms] - Exception starting filter authFilter\njavax.servlet.ServletException: org.apache.hadoop.security.authentication.client.AuthenticationException: javax.security.auth.login.LoginException: No key to store\n  at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:241)\n  at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.init(DelegationTokenAuthenticationHandler.java:117)\n  at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:248)\n  at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.initializeAuthHandler(DelegationTokenAuthenticationFilter.java:195)\n  at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:234)\n  at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)\n  at org.apache.catalina.core.ApplicationFilterConfig.initFilter(ApplicationFilterConfig.java:279)\n  at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:260)\n  at org.apache.catalina.core.ApplicationFilterConfig.&lt;init&gt;(ApplicationFilterConfig.java:105)\n  at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:4828)\n  at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5508)\n  at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)\n  at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)\n  at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.security.authentication.client.AuthenticationException: javax.security.auth.login.LoginException: No key to store\n  at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:225)\n  ... 17 more\nCaused by: javax.security.auth.login.LoginException: No key to store\n  at com.sun.security.auth.module.Krb5LoginModule.commit(Krb5LoginModule.java:1119)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:497)\n  at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)\n  at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)\n  at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)\n  at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)\n  at javax.security.auth.login.LoginContext.login(LoginContext.java:588)\n  at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:222)\n  ... 17 more\n2016-05-24 07:45:20,405 ERROR StandardContext - Error filterStart\n2016-05-24 07:45:20,406 ERROR StandardContext - Context [/kms] startup failed due to previous errors\n2016-05-24 07:45:20,408 INFO  KMSWebApp - KMS Stopped\n2016-05-24 07:45:20,410 ERROR WebappClassLoader - The web application [/kms] appears to have started a thread named [FileWatchdog] but has failed to stop it. This is very likely to create a memory leak.\n2016-05-24 07:45:20,411 ERROR WebappClassLoader - The web application [/kms] appears to have started a thread named [Abandoned connection cleanup thread] but has failed to stop it. This is very likely to create a memory leak.\n2016-05-24 07:45:20,412 ERROR WebappClassLoader - The web application [/kms] appears to have started a thread named [kms.async.multi_dest.batch_kms.async.multi_dest.batch.db_destWriter] but has failed to stop it. This is very likely to create a memory leak.</pre><p>Can someone please give me pointer on this?</p><p>Thanks in advance.</p>","tags":["Ranger","ranger-kms","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-25 02:31:08.0","id":35407,"title":"oozie / MySQl - Qiestion","body":"<p>I am trying to use MySql instead of derby to be used for oozie. I installed MySql on oozie server host and also created a database. Now on Ambari during installation it says below. Ambari-server is already setup and I am trying to add services on multiple nodes now. Do I have to install MySql on Ambari-server also. This is not clear to me. </p><p>Also if I use derby to start with, can I changed it to MySql later ?</p><pre>Be sure you have run:\nambari-server setup --jdbc-db=mysql --jdbc-driver=/path/to/mysql/mysql-connector-java.jar on the Ambari Server host to make the JDBC driver available and to enable testing the database connection.\n</pre>","tags":["Oozie"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-25 08:33:07.0","id":35442,"title":"Is Apache Drill working on Sandbox 2.4 ??? If it works how can??? and please guide me the installation procedure.....","body":"","tags":["drill","hdp-2.4.0","installation"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-25 12:15:23.0","id":35504,"title":"Unable to start hive metastore","body":"<p>Hi,</p><p>I'm trying to install and run Hive on my test environment.</p><p>During Hive startup I get the message, that Hive Metastore startup has failed.</p><p>Here's an output:</p><p><strong>stderr:</strong></p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py\", line 245, in &lt;module&gt;\n    HiveMetastore().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py\", line 60, in start\n    hive_service('metastore', action='start', upgrade_type=upgrade_type)\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py\", line 89, in thunk\n    return fn(*args, **kwargs)\n  File \"/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_service.py\", line 68, in hive_service\n    pid = get_user_call_output.get_user_call_output(format(\"cat {pid_file}\"), user=params.hive_user, is_checked_call=False)[1]\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/get_user_call_output.py\", line 58, in get_user_call_output\n    err_msg = Logger.filter_text((\"Execution of '%s' returned %d. %s\") % (command_string, code, all_output))\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/logger.py\", line 101, in filter_text\n    text = text.replace(unprotected_string, protected_string)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd0 in position 117: ordinal not in range(128)\n</pre><p><strong>stdout:</strong></p><pre>2016-05-25 14:58:18,934 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.4.0.0-169\n2016-05-25 14:58:18,935 - Checking if need to create versioned conf dir /etc/hadoop/2.4.0.0-169/0\n2016-05-25 14:58:18,935 - call['conf-select create-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-05-25 14:58:18,955 - call returned (1, '/etc/hadoop/2.4.0.0-169/0 exist already', '')\n2016-05-25 14:58:18,956 - checked_call['conf-select set-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-05-25 14:58:18,976 - checked_call returned (0, '/usr/hdp/2.4.0.0-169/hadoop/conf -&gt; /etc/hadoop/2.4.0.0-169/0')\n2016-05-25 14:58:18,976 - Ensuring that hadoop has the correct symlink structure\n2016-05-25 14:58:18,976 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-05-25 14:58:19,065 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.4.0.0-169\n2016-05-25 14:58:19,065 - Checking if need to create versioned conf dir /etc/hadoop/2.4.0.0-169/0\n2016-05-25 14:58:19,065 - call['conf-select create-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-05-25 14:58:19,085 - call returned (1, '/etc/hadoop/2.4.0.0-169/0 exist already', '')\n2016-05-25 14:58:19,086 - checked_call['conf-select set-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-05-25 14:58:19,105 - checked_call returned (0, '/usr/hdp/2.4.0.0-169/hadoop/conf -&gt; /etc/hadoop/2.4.0.0-169/0')\n2016-05-25 14:58:19,105 - Ensuring that hadoop has the correct symlink structure\n2016-05-25 14:58:19,105 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-05-25 14:58:19,107 - Group['spark'] {}\n2016-05-25 14:58:19,108 - Group['hadoop'] {}\n2016-05-25 14:58:19,108 - Group['users'] {}\n2016-05-25 14:58:19,108 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,109 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,109 - User['oozie'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-05-25 14:58:19,110 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,110 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-05-25 14:58:19,111 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,111 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-05-25 14:58:19,112 - User['flume'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,112 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,113 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,113 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,114 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,114 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,115 - User['hcat'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-05-25 14:58:19,115 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-05-25 14:58:19,117 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2016-05-25 14:58:19,122 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2016-05-25 14:58:19,123 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'recursive': True, 'mode': 0775, 'cd_access': 'a'}\n2016-05-25 14:58:19,123 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-05-25 14:58:19,124 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}\n2016-05-25 14:58:19,130 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if\n2016-05-25 14:58:19,130 - Group['hdfs'] {}\n2016-05-25 14:58:19,130 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'hdfs']}\n2016-05-25 14:58:19,131 - FS Type: \n2016-05-25 14:58:19,131 - Directory['/etc/hadoop'] {'mode': 0755}\n2016-05-25 14:58:19,143 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-05-25 14:58:19,144 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}\n2016-05-25 14:58:19,155 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}\n2016-05-25 14:58:19,162 - Skipping Execute[('setenforce', '0')] due to not_if\n2016-05-25 14:58:19,163 - Directory['/var/log/hadoop'] {'owner': 'root', 'mode': 0775, 'group': 'hadoop', 'recursive': True, 'cd_access': 'a'}\n2016-05-25 14:58:19,164 - Directory['/var/run/hadoop'] {'owner': 'root', 'group': 'root', 'recursive': True, 'cd_access': 'a'}\n2016-05-25 14:58:19,165 - Directory['/tmp/hadoop-hdfs'] {'owner': 'hdfs', 'recursive': True, 'cd_access': 'a'}\n2016-05-25 14:58:19,168 - File['/usr/hdp/current/hadoop-client/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'hdfs'}\n2016-05-25 14:58:19,170 - File['/usr/hdp/current/hadoop-client/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'hdfs'}\n2016-05-25 14:58:19,171 - File['/usr/hdp/current/hadoop-client/conf/log4j.properties'] {'content': ..., 'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}\n2016-05-25 14:58:19,182 - File['/usr/hdp/current/hadoop-client/conf/hadoop-metrics2.properties'] {'content': Template('hadoop-metrics2.properties.j2'), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-05-25 14:58:19,183 - File['/usr/hdp/current/hadoop-client/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}\n2016-05-25 14:58:19,187 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop'}\n2016-05-25 14:58:19,193 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}\n2016-05-25 14:58:19,355 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.4.0.0-169\n2016-05-25 14:58:19,355 - Checking if need to create versioned conf dir /etc/hadoop/2.4.0.0-169/0\n2016-05-25 14:58:19,356 - call['conf-select create-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-05-25 14:58:19,377 - call returned (1, '/etc/hadoop/2.4.0.0-169/0 exist already', '')\n2016-05-25 14:58:19,377 - checked_call['conf-select set-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-05-25 14:58:19,397 - checked_call returned (0, '/usr/hdp/2.4.0.0-169/hadoop/conf -&gt; /etc/hadoop/2.4.0.0-169/0')\n2016-05-25 14:58:19,397 - Ensuring that hadoop has the correct symlink structure\n2016-05-25 14:58:19,397 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-05-25 14:58:19,436 - Directory['/etc/hive'] {'mode': 0755}\n2016-05-25 14:58:19,437 - Directory['/usr/hdp/current/hive-metastore/conf'] {'owner': 'hive', 'group': 'hadoop', 'recursive': True}\n2016-05-25 14:58:19,438 - XmlConfig['mapred-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hive-metastore/conf', 'mode': 0644, 'configuration_attributes': {}, 'owner': 'hive', 'configurations': ...}\n2016-05-25 14:58:19,452 - Generating config: /usr/hdp/current/hive-metastore/conf/mapred-site.xml\n2016-05-25 14:58:19,452 - File['/usr/hdp/current/hive-metastore/conf/mapred-site.xml'] {'owner': 'hive', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0644, 'encoding': 'UTF-8'}\n2016-05-25 14:58:19,485 - File['/usr/hdp/current/hive-metastore/conf/hive-default.xml.template'] {'owner': 'hive', 'group': 'hadoop'}\n2016-05-25 14:58:19,485 - File['/usr/hdp/current/hive-metastore/conf/hive-env.sh.template'] {'owner': 'hive', 'group': 'hadoop'}\n2016-05-25 14:58:19,485 - File['/usr/hdp/current/hive-metastore/conf/hive-exec-log4j.properties'] {'content': ..., 'owner': 'hive', 'group': 'hadoop', 'mode': 0644}\n2016-05-25 14:58:19,486 - File['/usr/hdp/current/hive-metastore/conf/hive-log4j.properties'] {'content': ..., 'owner': 'hive', 'group': 'hadoop', 'mode': 0644}\n2016-05-25 14:58:19,486 - Directory['/usr/hdp/current/hive-metastore/conf/conf.server'] {'owner': 'hive', 'group': 'hadoop', 'recursive': True}\n2016-05-25 14:58:19,486 - XmlConfig['mapred-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hive-metastore/conf/conf.server', 'mode': 0644, 'configuration_attributes': {}, 'owner': 'hive', 'configurations': ...}\n2016-05-25 14:58:19,494 - Generating config: /usr/hdp/current/hive-metastore/conf/conf.server/mapred-site.xml\n2016-05-25 14:58:19,494 - File['/usr/hdp/current/hive-metastore/conf/conf.server/mapred-site.xml'] {'owner': 'hive', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0644, 'encoding': 'UTF-8'}\n2016-05-25 14:58:19,525 - File['/usr/hdp/current/hive-metastore/conf/conf.server/hive-default.xml.template'] {'owner': 'hive', 'group': 'hadoop'}\n2016-05-25 14:58:19,525 - File['/usr/hdp/current/hive-metastore/conf/conf.server/hive-env.sh.template'] {'owner': 'hive', 'group': 'hadoop'}\n2016-05-25 14:58:19,526 - File['/usr/hdp/current/hive-metastore/conf/conf.server/hive-exec-log4j.properties'] {'content': ..., 'owner': 'hive', 'group': 'hadoop', 'mode': 0644}\n2016-05-25 14:58:19,526 - File['/usr/hdp/current/hive-metastore/conf/conf.server/hive-log4j.properties'] {'content': ..., 'owner': 'hive', 'group': 'hadoop', 'mode': 0644}\n2016-05-25 14:58:19,527 - XmlConfig['hive-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hive-metastore/conf/conf.server', 'mode': 0644, 'configuration_attributes': {}, 'owner': 'hive', 'configurations': ...}\n2016-05-25 14:58:19,534 - Generating config: /usr/hdp/current/hive-metastore/conf/conf.server/hive-site.xml\n2016-05-25 14:58:19,534 - File['/usr/hdp/current/hive-metastore/conf/conf.server/hive-site.xml'] {'owner': 'hive', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0644, 'encoding': 'UTF-8'}\n2016-05-25 14:58:19,638 - File['/usr/hdp/current/hive-metastore/conf/conf.server/hive-env.sh'] {'content': InlineTemplate(...), 'owner': 'hive', 'group': 'hadoop'}\n2016-05-25 14:58:19,638 - Directory['/etc/security/limits.d'] {'owner': 'root', 'group': 'root', 'recursive': True}\n2016-05-25 14:58:19,641 - File['/etc/security/limits.d/hive.conf'] {'content': Template('hive.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}\n2016-05-25 14:58:19,642 - File['/usr/lib/ambari-agent/DBConnectionVerification.jar'] {'content': DownloadSource('http://h1.sdd.d4.org:8080/resources/DBConnectionVerification.jar'), 'mode': 0644}\n2016-05-25 14:58:19,642 - Not downloading the file from http://h1.sdd.d4.org:8080/resources/DBConnectionVerification.jar, because /var/lib/ambari-agent/tmp/DBConnectionVerification.jar already exists\n2016-05-25 14:58:19,642 - File['/var/lib/ambari-agent/tmp/start_metastore_script'] {'content': StaticFile('startMetastore.sh'), 'mode': 0755}\n2016-05-25 14:58:19,644 - Execute['export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-metastore/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED]'] {'not_if': \"ambari-sudo.sh su hive -l -s /bin/bash -c 'export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-metastore/bin/schematool -info -dbType mysql -userName hive -passWord [PROTECTED]'\", 'user': 'hive'}\n2016-05-25 14:58:23,524 - Skipping Execute['export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-metastore/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED]'] due to not_if\n2016-05-25 14:58:23,525 - Directory['/var/run/hive'] {'owner': 'hive', 'mode': 0755, 'group': 'hadoop', 'recursive': True, 'cd_access': 'a'}\n2016-05-25 14:58:23,525 - Directory['/var/log/hive'] {'owner': 'hive', 'mode': 0755, 'group': 'hadoop', 'recursive': True, 'cd_access': 'a'}\n2016-05-25 14:58:23,526 - Directory['/var/lib/hive'] {'owner': 'hive', 'mode': 0755, 'group': 'hadoop', 'recursive': True, 'cd_access': 'a'}\n2016-05-25 14:58:23,527 - call['ambari-sudo.sh su hive -l -s /bin/bash -c 'cat /var/run/hive/hive.pid 1&gt;/tmp/tmppLcf6w 2&gt;/tmp/tmpw1ccfI''] {'quiet': False}\n2016-05-25 14:58:23,565 - call returned (1, '')\n</pre><p>I'm using HDP-2.4.0.0-169 stack with Ambari-2.2.2.0</p>","tags":["hdp-2.4.0","metastore","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-25 09:29:30.0","id":35453,"title":"How to run one nifi processor as super user","body":"<p>Hi,</p><p>I have one nifi flow in that I am using 8-10 processor. In that 1 processor, I want to execute as superuser and rest as normal user.</p><p>Can you please help me if is there any way to do this?</p><p>Thanks</p>","tags":["Nifi","nifi-templates","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-25 13:22:11.0","id":35517,"title":"FLUME + Kafka skin in hpd 2.3.4.7","body":"<p>We would like to get help from you for one challenge\nin our HDP 2.3.4 Environment.  Please guide us to resolve issue.</p><p>We install HDP 2.3.4 version and configured HA for\nmaster services (Namenode, Resource Manager, HIVE, Hbase, oozie), Kerberos\n(Active directory), Ranger and Knox for HDP services.</p><p>We tried to feed input to Kafka topics through flume\napplication.  It is failing after enable Kerberos.</p><p>We create require policy on Ranger for users. The same\nuser is able to access Kafka producer and consumer through Kafka commands.</p><p>The same user is able to execute flume relate task\nexpect input feed to Kafka topics task.</p><p>I am attached two text file which has flume\nconfiguration details and error message here. Please find this and help us.</p><p>We noticed that, once we enabled Kerberos, the Kafka\nbroker details in zookeeper value is changed like below. Please confirm is it\nright or not.</p><p>[zk: host1.test.com(CONNECTED) 1] get\n/brokers/ids/1003</p><p>{\"jmx_port\":-1,\"timestamp\":\"1461845457345\",\"endpoints\":[\"PLAINTEXTSASL://host4.test.com:6667\"],\"host\":null,\"version\":2,\"port\":-1}</p><p>cZxid = 0x600000062</p><p>ctime = Thu Apr 28 12:11:02 UTC 2016</p><p>mZxid = 0x600000062</p><p>mtime = Thu Apr 28 12:11:02 UTC 2016</p><p>pZxid = 0x600000062</p><p>cversion = 0</p><p>dataVersion = 0</p><p>aclVersion = 0</p><p>ephemeralOwner = 0x2545cc704190001</p><p>dataLength = 138</p><p>numChildren = 0</p><p>[zk: host1.test.com(CONNECTED) 2] quit</p><p>flume conf file details:</p><p> tier1.sources  = source1 </p><p> tier1.channels = channel1 </p><p> tier1.sinks = sink1 </p><p> tier1.sources.source1.type = exec </p><p> tier1.sources.source1.command = /usr/bin/vmstat </p><p> tier1.sources.source1.channels = channel1 </p><p> tier1.channels.channel1.type = memory </p><p> tier1.channels.channel1.capacity = 10000 </p><p>  tier1.channels.channel1.transactionCapacity = 1000 </p><p> tier1.sinks.sink1.type=org.apache.flume.sink.kafka.KafkaSink </p><p> tier1.sinks.sink1.topic=testtopic </p><p> tier1.sinks.sink1.brokerList=host4.test.com:6667 </p><p>  tier1.sinks.sink1.channel=channel1 </p><p>tier1.sinks.sink1.max.message.size=1000000</p><p>\ntier1.sinks.sink1.requiredAcks=1\n tier1.sinks.sink1.batchSize=20000 </p><p>tier1.sinks.skink1.security.protocol=PLAINTEXTSASL </p><p>tier1.sinks.skink1.serializer.class=kafka.serializer.StringEncoder</p><p>\ntier1.sinks.skin1.controller.socket.timeout.ms=30000 </p><p>tier1.sinks.skin1.request.timeout.ms=30000</p><p>\ntier1.sinks.skin1.zookeeper.session.timeout.ms=60000</p><p>\ntier1.sinks.skin1.zookeeper.connection.timeout.ms=60000</p><p>\ntier1.sinks.skin1.topic.partitions=0 </p><p>tier1.sinks.skin1.socket.request.max.bytes=1048576000 </p><p>tier1.sinks.skin1.socket.send.buffer.bytes=1024000 </p><p>tier1.sinks.skin1.socket.receive.buffer.bytes=1024000 </p><p>tier1.sinks.skin1.client.id=console-consumer-75344</p><p>tier1.sinks.skin1.auto.commit.enable=false</p><p>\ntier1.sinks.skin1.dual.commit.enabled=false</p>","tags":["flume-1.6"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-03 13:42:28.0","id":37575,"title":"Hive View Error -\"H100 Unable to submit statement show databases like '*':\"","body":"<p>Hi,</p><p>As suggested in another posts, I restarted all the service and initiated a new session.  Still I am getting the error in Hive View.  How to address this.</p><p>Regards,</p><p>Subramanian S.</p>","tags":["hive-views"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-04 05:28:10.0","id":37689,"title":"Converting back to  list from raw in R","body":"<p>I have my data in a list which I can access like:</p><ol>\n<li>print(max(gaDataEcommerce4$date))</li></ol><p>Now, I save it in a text file in HDFS and what I read back from this file is a raw object.</p><p>I then convert it to a list but on trying to access $date get null.</p><pre>&lt;code&gt;getLastDataImportDate =function(){\n  hdfs.init()\n  f = hdfs.file(\"/user/rstudio/gaDataEcommerce4\",\"r\")\n  m = hdfs.read(f)\n  print(typeof(m))//raw\n  m1 &lt;- m\n  mnull &lt;- m == as.raw(0)//What is happening here?\n  m1[mnull]&lt;- as.raw(20)//and here?\n  c &lt;- rawToChar(m1)\n  print(c)\n  print(typeof(c))//character\n  df = as.list(c)\n  print(df)\n  print(df$date)//NULL}</pre>\n<ol><li>I could not directly use <code>rawToChar</code> function because I have nulls in the data.</li></ol><p>So found this for the fix but I would really like to know what exactly is happening at these 2 lines.</p><pre>&lt;code&gt;  mnull &lt;- m == as.raw(0)//What is happening here?\n  m1[mnull]&lt;- as.raw(20)//and here?</pre>\n<p>Also, I tried converting it to a list but</p><p><code>df$date</code> gives NULL. Why exactly is that? I need to get max(df$date), how else would I get it?</p>","tags":["r"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-04 07:24:44.0","id":37686,"title":"webhdfs FAILS to GET file content","body":"<p>I am successfully using webhdfs to list hdfs folders over knox.</p><p>When I try to do GET file content  using OPEN operation, webhdfs consistently returns file not found.</p><p>This is the url:</p><p>curl -i -L -k -u USER-NAME:PASSWORD  'https://knox-endpoint:8443/gateway/default/webhdfs/v1/FILE-PATH?op=OPEN'</p><p>Is this feature supported?</p><p>thanks,</p>","tags":["HDFS","Knox","webhdfs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-06 09:52:51.0","id":37831,"title":"Failed to run Zeppelin notebook demo Spark Streaming","body":"<p>I tried to run the note book demo available on Zeppelin in Hortonworks sandbox 2.4 (Notebook named twitter) to learn SparkStreaming. According the instruction on the top of notebook (/* BEFORE START....), I logged on Ambari to modify the configuration of Yarn service. </p><p>- CPU =&gt; Container: Minimum Container Size (VCores) 4; Maximum Container Size (Vcores): 8 </p><p>- Memory</p><p style=\"margin-left: 20px;\">+ Node: 2250MB</p><p style=\"margin-left: 20px;\">+ Container: Minimum Container Size: 768MB; Maximum Container Size: 2250MB</p><p style=\"margin-left: 20px;\">All services are restarted after modifying but when I came back to Zeppelin to run the notebook, the second paragraph (/* UPDATE YOUR TWITTER CREDENTIALS */....) was always on the state \"running\" but never \"finished\". All twitter credentials are already updated. </p><p style=\"margin-left: 20px;\">P/S: without modifying the YARN configuration, I could run the second paragraph, but when running the 3rd, It was always \"running\" but never \"finished\"</p><p style=\"margin-left: 20px;\">Thanks for any suggestions</p>","tags":["yarn-container","zeppelin-notebook","vcores","spark-notebook"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-08 01:14:34.0","id":38358,"title":"Not enough disk space on host . A minimum of 2GB is required for \"/\" mount.Can we change this default directory because i am getting this warning at the time of cluster setup.","body":"<p><a href=\"/storage/attachments/4816-error.png\">error.png</a><a href=\"/storage/attachments/4817-issue-sandpit.png\">issue-sandpit.png</a>I have attached the snapshot , please suggest as currently we are installing HDP 2.4.0.0 on a test environment but Ambari shows this warning.</p>","tags":["ambari-2.1.2","ambari-service","hadoop-ecosystem"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-08 10:17:28.0","id":38431,"title":"Hive error","body":"<p>How to move the files from hdfs to hive dynamically. please help me </p>","tags":["Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-12 16:59:53.0","id":1293,"title":"How to Make Kylin work with HDP 2.3","body":"<p>Here is the issue with Kylin (http://kylin.incubator.apache.org/).  For now it only supports HDP 2.2.x\n\nIn HDP 2.3, Hbase has upgraded to 1.0 and APIS have changed. See <a href=\"https://issues.apache.org/jira/browse/KYLIN-892\">https://issues.apache.org/jira/browse/KYLIN-892</a> and <a href=\"https://issues.apache.org/jira/browse/KYLIN-920\">https://issues.apache.org/jira/browse/KYLIN-920</a></p><p>Kyle did not keep up to date with the changes and  have not migrated to HBase v1.0 APIs. <a href=\"https://issues.apache.org/jira/browse/KYLIN-782\">https://issues.apache.org/jira/browse/KYLIN-782</a></p>","tags":["hdp-2.3.0","Hbase","kylin"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-10-09 00:18:12.0","id":1180,"title":"When will the Kafka log be deleted in this case?","body":"<p>This is a bit confusing. But reading the configs and logs, I'm not able to understand why the log was deleted at such time point.</p><p>topic 1_persistent was created on 3rd Oct. </p><p>A new log segment was rolled once the log size reached above 1 GB on 5th Oct: \n[2015-10-05 11:55:08,329] INFO Rolled new log segment for '1_persistent-0' in 31 ms. (kafka.log.Log) </p><p>However,  log shows that the first log segment for 1_persistent got deleted after 4 days (7th Oct).</p><p>Can anyone help me understand why? Thank you.</p><p>[server.properties]</p><pre># Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n# \n#    http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# see kafka.server.KafkaConfig for additional details and defaults\n\n\n############################# Server Basics #############################\n\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id=2041775\n\n\n############################# Socket Server Settings #############################\n\n\nlisteners=PLAINTEXT://:9092\n\n\n# The port the socket server listens on\n#port=9092\n\n\n# Hostname the broker will bind to. If not set, the server will bind to all interfaces\nhost.name=10.204.177.5\n\n\n# Hostname the broker will advertise to producers and consumers. If not set, it uses the\n# value for \"host.name\" if configured.  Otherwise, it will use the value returned from\n# java.net.InetAddress.getCanonicalHostName().\nadvertised.host.name=10.204.177.5\n\n\n# The port to publish to ZooKeeper for clients to use. If this is not set,\n# it will publish the same port that the broker binds to.\nadvertised.port=9092\n\n\n# The number of threads handling network requests\nnum.network.threads=3\n \n# The number of threads doing disk I/O\nnum.io.threads=8\n\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes=102400\n\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes=102400\n\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes=104857600\n\n\n\n\n############################# Log Basics #############################\n\n\n# A comma seperated list of directories under which to store log files\nlog.dirs=/var/spool/backup/kafka_logs\n\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions=1\n\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir=1\n\n\n############################# Log Flush Policy #############################\n\n\n# Messages are immediately written to the filesystem but by default we only fsync() to sync\n# the OS cache lazily. The following configurations control the flush of data to disk. \n# There are a few important trade-offs here:\n#    1. Durability: Unflushed data may be lost if you are not using replication.\n#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.\n#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks. \n# The settings below allow one to configure the flush policy to flush data after a period of time or\n# every N messages (or both). This can be done globally and overridden on a per-topic basis.\n\n\n# The number of messages to accept before forcing a flush of data to disk\n#log.flush.interval.messages=10000\n\n\n# The maximum amount of time a message can sit in a log before we force a flush\n#log.flush.interval.ms=1000\n\n\n############################# Log Retention Policy #############################\n\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n\n# The minimum age of a log file to be eligible for deletion\nlog.retention.hours=168\n\n\n# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining\n# segments don't drop below log.retention.bytes.\n#log.retention.bytes=1073741824\n\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes=1073741824\n\n\n# The interval at which log segments are checked to see if they can be deleted according \n# to the retention policies\nlog.retention.check.interval.ms=300000\n\n\n# By default the log cleaner is disabled and the log retention policy will default to just delete segments after their retention expires.\n# If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.\nlog.cleaner.enable=false\n\n\n############################# Zookeeper #############################\n\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect=10.204.177.4:2181\n\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=60000\ndelete.topic.enable=true\ncontrolled.shutdown.enable=true\nlog.retention.bytes=1073741824\nreserved.broker.max.id=2147483647\n\n\n</pre><p>[Kafka log]</p><pre>[2015-10-03 17:52:41,580] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-03 17:52:41,626] INFO Completed load of log 1_persistent-0 with log end offset 0 (kafka.log.Log)\n[2015-10-03 17:52:41,631] INFO Created log for partition [1_persistent,0] in /var/spool/backup/kafka_logs with properties {segment.index.bytes -&gt; 10485760, file.delete.delay.ms -&gt; 60000, segment.bytes -&gt; 1073741824, flush.ms -&gt; 9223372036854775807, delete.retention.ms -&gt; 86400000, index.interval.bytes -&gt; 4096, retention.bytes -&gt; 1073741824, min.insync.replicas -&gt; 1, cleanup.policy -&gt; delete, unclean.leader.election.enable -&gt; true, segment.ms -&gt; 604800000, max.message.bytes -&gt; 1000012, flush.messages -&gt; 9223372036854775807, compression.type -&gt; producer, min.cleanable.dirty.ratio -&gt; 0.5, retention.ms -&gt; 604800000, segment.jitter.ms -&gt; 0}. (kafka.log.LogManager)\n[2015-10-03 17:52:41,632] INFO Partition [1_persistent,0] on broker 2041775: No checkpointed highwatermark is found for partition [1_persistent,0] (kafka.cluster.Partition)\n[2015-10-03 17:53:02,066] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-03 17:53:02,271] INFO Partition [1_persistent,0] on broker 2041775: Expanding ISR for partition [1_persistent,0] from 2041775 to 2041775,2041776 (kafka.cluster.Partition)\n[2015-10-03 17:53:02,307] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-03 17:53:15,170] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-03 17:57:43,288] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-03 17:57:43,288] INFO Truncating log 1_persistent-0 to offset 689. (kafka.log.Log)\n[2015-10-03 17:57:43,301] INFO [ReplicaFetcherManager on broker 2041775] Added fetcher for partitions List([[1_persistent,0], initOffset 689 to broker BrokerEndPoint(2041776,10.204.177.6,9092)] ) (kafka.server.ReplicaFetcherManager)\n[2015-10-05 10:00:15,530] INFO [KafkaApi-2041775] Auto creation of topic 1_persistent_Temp with 1 partitions and replication factor 1 is successful! (kafka.server.KafkaApis)\n[2015-10-05 10:00:15,831] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent_Temp,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-05 10:00:15,833] INFO Completed load of log 1_persistent_Temp-0 with log end offset 0 (kafka.log.Log)\n[2015-10-05 10:00:15,834] INFO Created log for partition [1_persistent_Temp,0] in /var/spool/backup/kafka_logs with properties {segment.index.bytes -&gt; 10485760, file.delete.delay.ms -&gt; 60000, segment.bytes -&gt; 1073741824, flush.ms -&gt; 9223372036854775807, delete.retention.ms -&gt; 86400000, index.interval.bytes -&gt; 4096, retention.bytes -&gt; 1073741824, min.insync.replicas -&gt; 1, cleanup.policy -&gt; delete, unclean.leader.election.enable -&gt; true, segment.ms -&gt; 604800000, max.message.bytes -&gt; 1000012, flush.messages -&gt; 9223372036854775807, compression.type -&gt; producer, min.cleanable.dirty.ratio -&gt; 0.5, retention.ms -&gt; 604800000, segment.jitter.ms -&gt; 0}. (kafka.log.LogManager)\n[2015-10-05 10:00:15,836] INFO Partition [1_persistent_Temp,0] on broker 2041775: No checkpointed highwatermark is found for partition [1_persistent_Temp,0] (kafka.cluster.Partition)\n[2015-10-05 10:00:22,016] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent_Temp,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-05 10:00:22,301] INFO Partition [1_persistent_Temp,0] on broker 2041775: Expanding ISR for partition [1_persistent_Temp,0] from 2041775 to 2041775,2041776 (kafka.cluster.Partition)\n[2015-10-05 10:00:22,307] INFO [ReplicaFetcherManager on broker 2041775] Removed fetcher for partitions [1_persistent_Temp,0] (kafka.server.ReplicaFetcherManager)\n[2015-10-05 11:55:08,329] INFO Rolled new log segment for '1_persistent-0' in 31 ms. (kafka.log.Log)\n[2015-10-07 09:59:22,150] INFO Rolled new log segment for '1_persistent-0' in 9 ms. (kafka.log.Log)\n[2015-10-07 10:03:08,055] INFO Scheduling log segment 0 for log 1_persistent-0 for deletion. (kafka.log.Log)\n[2015-10-07 10:04:08,067] INFO Deleting segment 0 from log 1_persistent-0. (kafka.log.Log)\n[2015-10-07 10:04:09,073] INFO Deleting index /var/spool/backup/kafka_logs/1_persistent-0/00000000000000000000.index.deleted (kafka.log.OffsetIndex)\n\n\n</pre>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-09 19:16:06.0","id":1235,"title":"What is the recommended method to ingest Binary files from Mainframes ? Can sqoop handle this type of transfer","body":"","tags":["help","Sqoop","mainframe"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-12 19:01:15.0","id":1305,"title":"What is the best way to debug cloudbreak","body":"<p>What's the best way to debug cloudbreak?  Are there certain logs you have found more helpful than others?</p>","tags":["Cloudbreak","docker"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-12 18:02:13.0","id":1300,"title":"Ambari Metrics and Region Servers","body":"<p>If I'm setting up a cluster, which does not have an HBASE use case, but I want to configure Ambari Metrics in distributed mode, is there a guideline as to how many region servers I should have?</p>","tags":["Ambari","Hbase","ams","operations","ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-08 21:00:38.0","id":1173,"title":"hbase.bucketcache.size increase from 2G to 4G cause RegionServer Out of Memory","body":"<p>It is a dedicated HBase cluster, each node has 128GB RAM. It is HDP 2.3 and we follow HBase optimizing IO instruction: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Sys_Admin_Guides/content/ref-db219cd6-c586-49c1-bc56-c9c1c5475276.1.html</p>","tags":["performance","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-14 02:26:37.0","id":1400,"title":"Unable to drop Hive table due to corrupt partition.","body":"<p>When running a Hive CTAS query that was using wrong serde (accidently) the query was killed in the middle which caused a few partitions to get created but the partition looks corrupted.. Notice the non-ascii character in the partition name.</p><pre>/apps/hive/warehouse/mydb.db/mytbl\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__/created_mo=__HIVE_DEFAULT_PARTITION__\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__/created_mo=__HIVE_DEFAULT_PARTITION__/equip_init_f1=ϧ\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__/created_mo=__HIVE_DEFAULT_PARTITION__/equip_init_f1=ϧ/equip_nbr_l1=__HIVE_DEFAULT_PARTITION__\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__/created_mo=__HIVE_DEFAULT_PARTITION__/equip_init_f1=ϧ/equip_nbr_l1=__HIVE_DEFAULT_PARTITION__/000004_0\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__/created_mo=__HIVE_DEFAULT_PARTITION__/equip_init_f1=?\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__/created_mo=__HIVE_DEFAULT_PARTITION__/equip_init_f1=?/equip_nbr_l1=__HIVE_DEFAULT_PARTITION__\n/apps/hive/warehouse/mydb.db/mytbl/created_yr=__HIVE_DEFAULT_PARTITION__/created_mo=__HIVE_DEFAULT_PARTITION__/equip_init_f1=?/equip_nbr_l1=__HIVE_DEFAULT_PARTITION__/000083_0\n</pre><p>When running a DROP table statement is run, following exception appears in the metastore.log </p><pre>2015-10-13 17:55:50,660 ERROR [pool-3-thread-35]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(151)) - Error happens in method drop_table_with_environment_context: MetaException(message:Timeout when executing method: drop_table_with_environment_context)\n\tat org.apache.hadoop.hive.metastore.Deadline.newMetaException(Deadline.java:187)\n\tat org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:177)\n\tat org.apache.hadoop.hive.metastore.Deadline.checkTimeout(Deadline.java:160)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1820)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1807)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.access$200(ObjectStore.java:160)\n\tat org.apache.hadoop.hive.metastore.ObjectStore$2.getJdoResult(ObjectStore.java:1734)\n\tat org.apache.hadoop.hive.metastore.ObjectStore$2.getJdoResult(ObjectStore.java:1725)\n\tat org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2391)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1725)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1719)\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)\n\tat com.sun.proxy.$Proxy0.getPartitions(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.dropPartitionsAndGetLocations(HiveMetaStore.java:1693)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1532)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1737)\n\tat sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy5.drop_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9256)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:9240)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: drop_table_with_environment_context\n\tat org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)\n\t... 35 more\n</pre>","tags":["ddl","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-17 03:27:02.0","id":1624,"title":"HBase regionserver reporting wal.FSHLog: Slow sync cost. Searching so far points towards Datanode XMX tuning, any other suggestions?","body":"","tags":["performance","hadoop","Hbase","HDFS"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-18 05:43:27.0","id":1639,"title":"Falcon replication and mirroring between two Kerberized clusters?","body":"<p>Does Falcon support replication and mirroring for HDFS and Hive between two Kerberized clusters (Prod and DR)?</p>","tags":["disaster-recovery","replication","Falcon","kerberos"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-19 17:54:22.0","id":1667,"title":"Can Ranger KMS be Active Active or be configured for HA?","body":"","tags":["high-availability","ranger-kms","security","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-20 01:49:26.0","id":1698,"title":"Hive View crashing Ambari Server","body":"<p>I found an unpleasant way to crash ambari-server entirely, I have a virtual machine where this situation can be reproduced as many times as we want and it also happened with me in front of a customer, after installing hdp 2.3 / ambari 2.1.2.</p><p>Here are the steps;</p><p>1- Install Ambari 2.1.2 / HDP 2.3</p><p>2- Add Hive View and File View with default settings</p><p>3- Install Ranger and enable Hive-Ranger plugin</p><p>4- Logon to ambari with a user that has no permission in hive (default after ranger installation)</p><p>5- Open hive view (there will be permission error)</p><p>6- Open file view (or tez view)</p><p>7- Done, ambari-server is now inaccessible, you have to execute ambari-server restart.</p><p>Here a logs from ambari-server.log after step 5:</p><pre> H110 Unable to submit statement. Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [admin] does not have [USE] privilege on [null] [ERROR_STATUS]\n \n   org.apache.ambari.view.hive.client.HiveErrorStatusException: H110 Unable to submit statement. Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [admin] does not have [USE] privilege on [null] [ERROR_STATUS]\n\n\norg.apache.ambari.view.hive.client.HiveErrorStatusException: H110 Unable to submit statement. Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [admin] does not have [USE] privilege on [null] [ERROR_STATUS]\n\tat org.apache.ambari.view.hive.client.Utils.verifySuccess(Utils.java:29)\n\tat org.apache.ambari.view.hive.client.Connection.execute(Connection.java:345)\n\tat org.apache.ambari.view.hive.client.Connection.executeSync(Connection.java:360)\n\tat org.apache.ambari.view.hive.client.DDLDelegator.getDBListCursor(DDLDelegator.java:76)\n\tat org.apache.ambari.view.hive.client.DDLDelegator.getDBList(DDLDelegator.java:65)\n\tat org.apache.ambari.view.hive.resources.browser.HiveBrowserService.databases(HiveBrowserService.java:88)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:770)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:182)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:198)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:145)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\n</pre><p>and after step 6:</p><pre>\t14 Oct 2015 16:19:24,744  WARN [qtp-client-25] ServletHandler:563 - /api/v1/views/HIVE/versions/1.0.0/instances/hive/resources/ddl/database\njava.lang.OutOfMemoryError: PermGen space\n\tat org.apache.ambari.server.view.ViewRegistry.createAmbariStreamProvider(ViewRegistry.java:1722)\n\tat org.apache.ambari.server.view.ViewContextImpl.getAmbariStreamProvider(ViewContextImpl.java:262)\n\tat org.apache.ambari.view.utils.ambari.AmbariApi.getUrlStreamProviderBasicAuth(AmbariApi.java:296)\n\tat org.apache.ambari.view.utils.ambari.AmbariApi.requestClusterAPI(AmbariApi.java:156)\n\tat org.apache.ambari.view.utils.ambari.AmbariApi.requestClusterAPI(AmbariApi.java:142)\n\tat org.apache.ambari.view.utils.ambari.AmbariApi.getHostsWithComponent(AmbariApi.java:99)\n\tat org.apache.ambari.view.hive.client.ConnectionFactory.getHiveHost(ConnectionFactory.java:62)\n\tat org.apache.ambari.view.hive.client.ConnectionFactory.create(ConnectionFactory.java:51)\n\tat org.apache.ambari.view.hive.client.UserLocalConnection.initialValue(UserLocalConnection.java:42)\n\tat org.apache.ambari.view.hive.client.UserLocalConnection.initialValue(UserLocalConnection.java:26)\n\tat org.apache.ambari.view.utils.UserLocal.get(UserLocal.java:65)\n\tat org.apache.ambari.view.hive.resources.browser.HiveBrowserService.databases(HiveBrowserService.java:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n14 Oct 2015 16:19:37,941  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:131 - ERROR\njava.lang.OutOfMemoryError: PermGen space\n14 Oct 2015 16:19:40,983  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:131 - ERROR\njava.lang.OutOfMemoryError: PermGen space\n14 Oct 2015 16:19:44,184  WARN [qtp-ambari-agent-90] ServletHandler:590 - Error for /agent/v1/heartbeat/hdp23.hadoop.braccialli\njava.lang.OutOfMemoryError: PermGen space\n14 Oct 2015 16:19:50,011  WARN [qtp-ambari-agent-90] AbstractHttpConnection:552 - https://hdp23.hadoop.braccialli:8441/agent/v1/heartbeat/hdp23.hadoop.braccialli\njava.lang.OutOfMemoryError: PermGen space\n14 Oct 2015 16:19:54,278  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:131 - ERROR\njava.lang.OutOfMemoryError: PermGen space\n14 Oct 2015 16:20:05,070  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:131 - ERROR\njava.lang.OutOfMemoryError: PermGen space\n</pre>","tags":["ambari-views","hdp-2.3.0","Hive","Ambari","error"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-20 11:48:52.0","id":1709,"title":"Kafka MirrorMaker compression over the wire","body":"<p>Mirror Maker works by consuming a source Kafka and producing into a destination Kafka. </p><p>If I am producing messages with compression enabled into the source Kafka, is there a way to consume them in Mirror Maker without decompression, ie, just grab the raw compressed bits, and pass those on the wire to the target Kafka, or will the Consumer force decompression and recompression at the other end (meaning uncompressed data goes over the wire)?</p>","tags":["mirroring","Kafka","compression"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-20 14:58:59.0","id":1719,"title":"Does anyone have experience using vSphere with HDP (https://www.vmware.com/products/vsphere) ?","body":"","tags":["vmware","virtualization"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 21:05:40.0","id":553,"title":"Hive and Google Cloud Storage","body":"<p>Google’s Cloud Platform provides the infrastructure to perform MapReduce data analysis using open source software such as Hadoop with Hive and Pig. Google's Compute Engine provides the compute power and Cloud Storage is used to store the input and output of the MapReduce jobs.</p><p>HDP deployment using CloudBreak. </p><p>Before we deploy HDP in GCE, we need to setup account in GCE and CloudBreak.</p><p>Signup for free trial account on <a href=\"https://cloud.google.com/free-trial/\">https://cloud.google.com/free-trial/</a></p><p> Step1) Login into your google dashboard and then click Create a project. For example: I created a project called hadoop-install</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAVmAAAAJGNiMDRiOGU5LTJhYjAtNDY5ZC04OGIzLTBhNmQ0NGRjYThhOQ.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p>Step 2) Create credentials.</p><p>Click Create new Client ID and then choose Service account.</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAZRAAAAJDFlNWJjOGM2LTI5NjctNDk3Ni05YWJjLTdkNTZmYmZlMzVkYQ.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"> </p><p>Click Okay got it and it will download JSON key (We won’t be using this file). You will see Client ID, Email address and Certificate fingerprints in the same window after downloading JSON key. There will be an option to Generate new P12 key.</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAbvAAAAJGYxNDllZTk1LWU0YjctNDY4NC04NjQ2LTE3YTczNmUwMTEyMA.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p>Step 3) Enable API</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAR7AAAAJDRkODIzOGUxLWNjZmUtNGNjYi1hMDQ2LTAzNTg5MTQ4NDA5Mg.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"> Default API when you login</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAUYAAAAJDM2ODBkZWQ4LTNiY2ItNGQwNS04MDI0LTIwOGUwOTQ3OTUzZg.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"> Search google compute and click Google Compute Engine. You will see an option to Enable API that you need to click.</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAPAAAAAJGFjYmY0ZTU4LWNlODYtNGQ0ZC04MWY2LTA4ZTRhMmJmYjUzYw.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"> These are the API that I have with enabled status </p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAR_AAAAJGRkOTZhYmQwLWQzYjktNGUxNy1hZThmLTFlMGI5NmM0MTJkNA.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p>For HDP deployment, you would need Project-id, Email address and P12 key file.</p><p>GCE setup completed so let’s move on to CloudBreak setup.</p><p><strong>Signup for CloudBreak account.</strong></p><p><a href=\"https://accounts.sequenceiq.com/\">https://accounts.sequenceiq.com/</a></p><p><strong>Login url</strong></p><p><a href=\"https://cloudbreak.sequenceiq.com/\"><strong>https://cloudbreak.sequenceiq.com/</strong></a></p><p>Once you are logged into the Cloudbreak UI then setup GCP credentials</p><p>You will need project id and following details from the Credentials tab </p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAXZAAAAJDdiYjhiYjQyLTI3OWUtNGFkOS1iMGY4LTdkNzY3N2IyZDE5Mw.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p>Email address</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAUVAAAAJDNjNTQxMGJlLWE4MzAtNDcwYy04Njk0LWYwMDRiYmM4YmFjMg.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"> </p><p>My Cloudbreak UI looks like the following. We will be creating credentials, template and blueprint for HDP deployment and this is only one time process. </p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAALdAAAAJGM1OTg1Y2ZhLTVmYzMtNGY2ZS05MzBjLTY5MzAwZjAwZGQ4MA.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p><strong>Credentials:</strong></p><p>Under manage credentials, choose GCP. </p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAANOAAAAJGFiNjQ1OWJlLTIzZGEtNDY1MS05NjI0LTAzOTgxZDc2NjJjNQ.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p> <strong>Name</strong> – Credential name</p><p><strong>Description</strong> – As you like</p><p><strong>Project ID</strong> – hadoop-install (get this value from google dashboard)</p><p><strong>Service Account Email Address</strong> – Credentials tab in google dashboard “Email address” under Service account</p><p><strong>Service Account Key</strong> – Upload the file that you did rename as hadoop.12</p><p><strong>SSH public key</strong> – Mac users can copy the content of id_rsa.pub. Windows users needs to get this from putty (google search – putty public ssh keys)</p><p><strong>Template:</strong></p><p> Next step is to manage resources (create template)</p><p><strong>Name</strong> – Template name</p><p><strong>Description</strong> – As you like</p><p><strong>Instance-Type</strong> – You can choose as per your requirement (I chose n1-standard-2 for this test)</p><p><strong>Volume Type</strong> – Magnetic/SSD</p><p><strong>Attached volumes per instance</strong> – 1 for this test</p><p><strong>Volume Size</strong> – 100GB (Increase this value as per your requirement)</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAIdAAAAJDdhMmUxMGMyLWUzZTgtNDM4Zi1hNTNkLWEwMjRhZTBlMDA0Ng.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"> </p><p><strong>Blueprint</strong></p><p>You can download the blueprint from <a href=\"https://github.com/nsabharwal/ambariblueprintcollection/blob/master/hivegooglecloudstorage\">here</a>. Copy the content and paste it into the create blueprint window.</p><p>I am saving the blueprint as hivegoogle. In case, you receive blue print error while creating blueprint in CloudBreak then you can use <a href=\"http://codebeautify.org/jsonvalidate\">jsonvalidate</a> to validate/format the blueprint.</p><p><strong>Cluster Deployment</strong></p><p>Select your credentials</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAXkAAAAJGYyNGNmNGI4LWRmMzUtNDA5Zi05OTM1LWIyZWY5NjAxNTYxYQ.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p>Click create cluster</p><p>Clustername: Name your cluster</p><p>Region: Choose region to deploy the cluster</p><p>Network: Choose network</p><p>Blueprint: Choose blueprint created, hivegoogle</p><p>Hostgroup configuration:</p><p>cbgateway , master and slave – I am using minviable-gcp but you can choose the template as per you own choice.</p><p>Click “create and start cluster” </p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAXhAAAAJGJhNDdjZjNmLWFiYmMtNDA4OS05ZjgyLTVkNGQyZmFkYjFhOA.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p> You can see the progress in the Event history.</p><p>Final snapshot of the cluster looks like this: </p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAALYAAAAJDhhZGExZmZlLTFjMzUtNGUwMy04ZGVkLTVkNzdlOTQ3MDc3ZA.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p>Verify google cloud related settings and <strong>provide project.id & google cloud service email</strong>. You can find these details from the google dashboard.</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAb5AAAAJDVhOGQ4MGI0LTZjYjQtNGM1NS05OGJlLWJhOGQxNzA5NjgwZA.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"> Verify tez.aux.uris and make sure to copy gcs connector at this location. I have covered copy process in the environment setup section as below.</p><p> <img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAWnAAAAJDRlNTMwZjExLWViYWItNDRhYS04NTZlLWFkZGEwZmZiNzk1Nw.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p><p>Let’s setup the environment setup before running hdfs and hive commands.</p><p>We need hadoop.p12 and gcs connector in all the nodes.</p><ul>\n<li>Copy hadoop.p12 as defined in Ambari parameter       google.cloud.auth.service.account.keyfile</li><li>You can upload hadoop.p12 in dropbox and do wget or you can copy from your localhost.</li><li>Copy hadoop.p12 from local machine to VM instance. Cloudbreak uses docker containers to deploy the cluster so we need to copy file from local desktop to vm instance then copy it into the container.</li></ul><p>First, from the localhost to the vm instance (External IP can be found from google dashboard under VM Instances)</p><p>HW11326:.ssh nsabharwal$ scp ~/Downloads/hadoop.p12 cloudbreak@130.211.184.135:/tmp</p><p>hadoop.p12   100% 2572  2.5KB/s  00:00 </p><p>HW11326:.ssh nsabharwal$</p><p>Login to vm instance</p><p>HW11326:.ssh nsabharwal$ ssh <a href=\"https://cloud.google.com/hadoop/google-cloud-storage-connector#manualinstallation\">location</a></p><p>[hdfs@hdpgcp-1-1435537523061 ~]$ wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar</p><p>--2015-06-28 21:05:59-- https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar</p><p>Resolving storage.googleapis.com... 74.125.201.128, 2607:f8b0:4001:c01::80</p><p>Connecting to storage.googleapis.com|74.125.201.128|:443... connected.</p><p>HTTP request sent, awaiting response... 200 OK</p><p>Length: 2494559 (2.4M) [application/java-archive]</p><p>Saving to: `gcs-connector-latest-hadoop2.jar'</p><p> 100%[============================================================================================================================================================&gt;] 2,494,559  7.30M/s  in 0.3s  </p><p> 2015-06-28 21:05:59 (7.30 MB/s) - `gcs-connector-latest-hadoop2.jar' saved [2494559/2494559]</p><p>Copy the connector to HDFS location</p><p>[hdfs@hdpgcp-1-1435537523061 ~]$ hdfs dfs -put gcs-connector-latest-hadoop2.jar /apps/tez/aux-jars/</p><p>[hdfs@hdpgcp-1-1435537523061 ~]$</p><p>Let’s create storage bucket called hivetest in the google storage.</p><p> Login into your google compute engine account and click Storage.</p><p><strong>HDFS test</strong></p><p>We need to copy the connector into the hadoop-client location otherwise you will hit error “Google FileSystem not found”</p><p>cp gcs-connector-latest-hadoop2.jar /usr/hdp/current/hadoop-client/lib/</p><p>[hdfs@hdpgcp-1-1435537523061 ~]$ hdfs dfs -ls gs://hivetest/</p><p>15/06/28 21:15:32 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.4.0-hadoop2</p><p>15/06/28 21:15:33 WARN gcs.GoogleHadoopFileSystemBase: No working directory configured, using default: 'gs://hivetest/'</p><p>Found 3 items</p><p>drwx------  - hdfs hdfs  0 2015-06-28 15:29 gs://hivetest/ns</p><p>drwx------  - hdfs hdfs  0 2015-06-28 12:44 gs://hivetest/test</p><p>drwx------  - hdfs hdfs  0 2015-06-28 15:30 gs://hivetest/tmp</p><p>[hdfs@hdpgcp-1-1435537523061 ~]$</p><p><strong>Hive test</strong></p><p>bash-4.1# su - hive</p><p>[hive@hdpgcptest-1-1435590069329 ~]$ hive</p><p>hive&gt; create table testns ( info string) location 'gs://hivetest/testns';</p><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found)</p><p>hive&gt;</p><p>To avoid the above error, we have to copy gcs connector into all the nodes under hive-client</p><p> cp /tmp/gcs-connector-latest-hadoop2.jar /usr/hdp/current/hive-client/lib</p><p> Let’s run following Apache Hive test</p><p>Data Set: http://seanlahman.com/files/database/lahman591-csv.zip</p><p> We are writing to gs://hivetest</p><p> hive&gt; create table batting (col_value STRING) location '<strong>gs://hivetest/batting'</strong>;</p><p>OK</p><p>Time taken: 1.518 seconds</p><p>Run the following command to verify the location, <strong>'gs://hivetest/batting'</strong></p><p> hive&gt; show create table batting;</p><p>OK</p><p>CREATE TABLE `batting`(</p><p>`col_value` string)</p><p>ROW FORMAT SERDE</p><p>'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'</p><p>STORED AS INPUTFORMAT</p><p>'org.apache.hadoop.mapred.TextInputFormat'</p><p>OUTPUTFORMAT</p><p>'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</p><p>LOCATION</p><p><strong>'gs://hivetest/batting'</strong></p><p>TBLPROPERTIES (</p><p>'transient_lastDdlTime'='1435766262')</p><p>Time taken: 0.981 seconds, Fetched: 12 row(s)</p><p>hive&gt; select count(1) from batting;</p><p>Upload Batting.csv</p><p>hive&gt; drop table batting;</p><p>You will notice that Batting.csv is deleted from the storage, as it was locally managed table.</p><p>In case of external table, Batting.csv won’t be removed from the storage bucket.</p><p><strong>In case you want to test MR using Hive</strong></p><p>hive&gt; add jar /usr/hdp/current/hive-client/lib/gcs-connector-latest-hadoop2.jar;</p><p>Added [/usr/hdp/current/hive-client/lib/gcs-connector-latest-hadoop2.jar] to class path</p><p>Added resources: [/usr/hdp/current/hive-client/lib/gcs-connector-latest-hadoop2.jar]</p><p>hive&gt; select count(1) from batting; </p><p>Query ID = hive_20150702095454_c17ae70f-b77e-4599-87e6-022d9bb9a00d</p><p>Total jobs = 1</p><p>Launching Job 1 out of 1</p><p>Number of reduce tasks determined at compile time: 1</p><p>In order to change the average load for a reducer (in bytes):</p><p>set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</p><p>In order to limit the maximum number of reducers:</p><p>set hive.exec.reducers.max=&lt;number&gt;</p><p>In order to set a constant number of reducers:</p><p>set mapreduce.job.reduces=&lt;number&gt;</p><p>Starting Job = job_1435841827745_0003, Tracking URL = http://hdpgcptest-1-1435590069329.node.dc1.consul:8088/proxy/application_1435841827745_0003/</p><p>Kill Command = /usr/hdp/2.2.6.0-2800/hadoop/bin/hadoop job -kill job_1435841827745_0003</p><p>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</p><p>2015-07-02 09:54:33,468 Stage-1 map = 0%, reduce = 0%</p><p>2015-07-02 09:54:42,947 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.2 sec</p><p>2015-07-02 09:54:51,719 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.6 sec</p><p>MapReduce Total cumulative CPU time: 4 seconds 600 msec</p><p>Ended Job = job_1435841827745_0003</p><p>MapReduce Jobs Launched:</p><p>Stage-Stage-1: Map: 1 Reduce: 1  Cumulative CPU: 4.6 sec  HDFS Read: 187 HDFS Write: 6 SUCCESS</p><p>Total MapReduce CPU Time Spent: 4 seconds 600 msec</p><p>OK</p><p>95196</p><p>Time taken: 29.855 seconds, Fetched: 1 row(s)</p><p>hive&gt;</p><p><strong>Sparksql</strong></p><p>First, copy gcs connector to spark-historyserver to avoid “Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found”</p><p>export SPARK_CLASSPATH=/usr/hdp/current/spark-historyserver/lib/gcs-connector-latest-hadoop2.jar</p><p>I am following <a href=\"http://hortonworks.com/hadoop-tutorial/using-apache-spark-hdp/\">this</a> article for Spark test</p><p>scala&gt; val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)</p><p>sqlContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@140dcdc5</p><p> scala&gt;</p><p>scala&gt; sqlContext.sql(\"CREATE TABLE IF NOT EXISTS batting ( col_value STRING) location 'gs://hivetest/batting' \")</p><p> scala&gt; sqlContext.sql(\"select count(*) from batting\").collect().foreach(println)</p><p>15/07/01 15:38:42 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 187 bytes</p><p>15/07/01 15:38:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 286 ms on hdpgcptest-2-1435590069361.node.dc1.consul (1/1)</p><p>15/07/01 15:38:42 INFO YarnClientClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool</p><p>15/07/01 15:38:42 INFO DAGScheduler: Stage 1 (collect at SparkPlan.scala:84) finished in 0.295 s</p><p>[95196]15/07/01 15:38:42 INFO DAGScheduler: Job 0 finished: collect at SparkPlan.scala:84, took 8.872396 s</p><p><img src=\"https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAATUAAAAJDI4MDczYzA1LWNiZTgtNDM0Mi1hMjk1LWMwZmI5MTM1ZGZiOA.png\" alt=\"\" style=\"margin: 30px auto; padding: 0px; font-weight: inherit; font-style: inherit; font-family: inherit; vertical-align: baseline; font-variant: inherit; font-stretch: inherit; outline: 0px; text-align: center; display: block; background: transparent;\"></p>","tags":["cloud","Hive","Cloudbreak","gce","google"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-10-25 15:53:22.0","id":2092,"title":"Does Ambari needs SSH User Equivalance to send the repo file to Ambari agents during Initial installation ?","body":"<p>I understand SSH user equivalance is needed for installing ambari agent through Ambari UI. If we manage to install ambari agent manually Do we still need password less SSH requirement from Ambari server to Agents to transfer the repo files ?</p>","tags":["installation","Ambari","passwordless","ssh"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-26 13:31:00.0","id":2115,"title":"Unable to edit settings for a new Configuration Group","body":"<p>When a new Configuration group is created in Ambari and a newly added host is added to that group, I am unable to edit the settings / properties / configurations for that group. Everything is greyed out. </p><p>Verified that admin user is logged in. Also tried creating a brand new configuration group as well as duplicating an existing configuration group but no luck. </p><p>What else needs to be done to be able to edit the properties? </p><p>I am specifically looking to change the namenode and datanode directories for HDFS. The existing nodes have /grid/[0-3] but the new node only has 1 disk thats mounted to /grid/0 so no [1-3]. </p>","tags":["configuration","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-27 16:58:26.0","id":2218,"title":"Falcon mirroring assumptions and guarantees","body":"<p>Do we have a detailed technical write-up on Falcon mirroring? It uses distcp under the hood, and I can only assume it uses the -update option, but are there any exceptions to how precisely it follows the distcp docs/functionality? I'm mostly concerned with partially-completed jobs that might have tmp files hanging around when the copy kicks off. I have a use case where the user would like to use mirroring to replicate 1..n feeds within a directory instead of setting up fine-grained feed replication, e.g. </p><p>mirror job 1=</p><p>- /data/cust/cust1</p><p>    - /feed-1</p><p>    - /feed-n</p><p>mirror job 2=</p><p>- /data/cust/cust2</p><p>    - /feed-1</p><p>    - /feed-n</p><p>Any info is appreciated.</p>","tags":["disaster-recovery","mirroring","replication","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-28 00:05:02.0","id":2263,"title":"How to set falcon retention policy for unconventional folder name ? \" startday=2014-10-01\"","body":"<p>I'm trying to set Falcon retention policy for a set of folders with unique naming convention. Almost all the demos and references documentation use simple folder names like \"${YEAR}-${MONTH}-${DAY}\" and I cant seem to figure out how to make this work. The folders are named \"startday=2014-10-01\", the later part is the year,month,date.</p><p>\nBasically there's a fixed string in front of the date, and I'd like to be able to mention in the location tag. I tried the following but it didnt work.</p><p>&lt;location type=\"data\"\n                path=\"/user/falcon/retentiondata/#startday=${YEAR}-${MONTH}-${DAY}\"/&gt;\n</p>","tags":["Falcon","data-retention"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-27 08:55:33.0","id":2179,"title":"Recommended config: mapreduce.input.fileinputformat.split.[minsize|maxsize]","body":"<p>What is the purpose of the following two configuration parameters in <em>mapred-size.xml</em>? What are recommended values?</p><pre>mapreduce.input.fileinputformat.split.minsize\nmapreduce.input.fileinputformat.split.maxsize</pre><p>Thanks :)</p>","tags":["MapReduce","best-practices","configuration"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-06 19:43:19.0","id":55172,"title":"I was getting this error while i was running hive,tez job.","body":"<p>\"tez.am.resource.memory.mb\" : \"6144\"</p><p>\"tez.task.resource.memory.mb\" : \"6144\"</p><p>        \"tez.am.launch.cmd-opts\" : \"4915\", </p><p>        \"tez.runtime.unordered.output.buffer.size-mb\"  : \"614\"</p><p>\"hive.tez.container.size\" : \"6144\", </p><p>         \"hive.tez.java.opts\" : \"-Xmx4915m\"</p><p>I was keep changing memory  from 4gb to 10gb. but still failing, can anyone of you able to help me. </p><p>############################################################################################</p><p>Status: Failed\nVertex failed, vertexName=Reducer 3, vertexId=vertex_1472766011697_0003_1_04, diagnostics=[Task failed, taskId=task_1472766011697_0003_1_04_000001, diagnostics=[TaskAttempt 0 failed, info=[Container container_e02_1472766011697_0003_01_000010 finished with diagnostics set to [Container failed, exitCode=1. Exception from container-launch.\nContainer id: container_e02_1472766011697_0003_01_000010\nExit code: 1\nStack trace: ExitCodeException exitCode=1:\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:576)\n        at org.apache.hadoop.util.Shell.run(Shell.java:487)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nContainer exited with a non-zero exit code 1\n]], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.OutOfMemoryError: GC overhead limit exceeded\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:159)\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)\n        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)</p>","tags":["hadoop","Tez","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-26 09:08:21.0","id":47251,"title":"Should I separate my storage nodes from my computation nodes?","body":"<p>I am running an HDP cluster on AWS using Ambari, and I find myself struggling with an architectural dilemma.</p><p>Since our HDFS volumes are mounted on EBS, my computation and storage are separated by default, and so the closest I can bring these two units together (e.g a spark job and it's destination HDFS node) is running the job on the same EC2 instance the EBS volume is attached to.</p><p>Now, in order to be able to scale my computational components and my storage components separately, I have divided the default blueprint provided by Ambari to instances containing the datanode and hbase_regionserver components, and instances containing only the nodemanager component (and metrics_monitor on all nodes, naturally).</p><p>The purpose of this setup is to be able to launch spark jobs on machines designed for computation purposes only, allowing me to downscale quickly without having to redistribute my HDFS data, and scaling my HDFS nodes up only when I'm running low on space.</p><p>Does this design make any sense, or does it go against HDP best-practices?</p><p>Thanks,</p><p>Yaron.</p>","tags":["Ambari","hdp-2.4.0","hadoop"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-07-26 06:16:00.0","id":47204,"title":"While starting Namenode checking as below HDP 2.+ (Safe Mode cannoot renew lease for dfsclinet) namenodelogfile","body":"<p>2016-07-26 0000 INFO ipc.Server (Server.java:run(2034)) - IPC Server handler 57 on 8020, \ncall org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease fromto 000.0.0.0 Call#2478456 Retry#3 org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: \nCannot renew lease for DFSClient_task1468766070206_82805_m_000013_0_1388738965_1. \nName node is in safe mode. The reported blocks 68892652 needs additional 1837307 \nblocks to reach the threshold 1.0000 of total blocks 70729958. The number of live \ndatanodes 12 has reached the minimum number 0. Safe mode will be turned off automatically \nonce the thresholds have been reached. at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1206)      \n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:4133) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:767) at </p>","tags":["hdfs-ha","hdfs-maintenance"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-26 17:01:48.0","id":47408,"title":"Virtual screen resize","body":"<p>Hi</p><p>I am using the sandbox in my Virtual box 5.1. However, I am not able to resize the virtual screen. The resize options (resolutions) are not getting enabled when I start the sandbox.</p><p>In the Devices menu, I added the VBoxGuestAdditions.iso but not able to resize even after that. Its too tedious to check and work in a small screen.</p><p style=\"margin-left: 20px;\">Please help.</p>","tags":["virtualbox","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-26 23:06:04.0","id":47484,"title":"question on Ambari services","body":"<p>I was given an environment to support recently without proper documentation and knowledge transfer. I am looking to document something like </p><table><tbody><tr><td>Node Name</td><td>List of services it runs</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><p>Is there any easy way to get the list of services rather than logging into each box ?</p><p>Specifically I am looking for the box which acts as Ambari server and I believe this is where Ambari Metrics collector process runs and all Ambari metrics monitor process pushes the data to this server. Please correct me if I am wrong.</p>","tags":["hadoop-maintenance","hadoop-ecosystem","ambari-metrics","metrics-collector","documentation","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-26 15:32:30.0","id":47382,"title":"YARN components fail after AMBARI Metrics collector migration( failed)","body":"<p>I was trying to migrate AMC from a particular node to another node and this failed. But after this, all the YARN components are failing with the following error message :</p><p>Connection Failed to http :&lt;&lt;&lt;&lt;Host Name &gt;&gt;&gt;&gt;:8088 urlopen error. Connection refused.</p>","tags":["YARN","yarn-container","hadoop","hadoop-core","yarn-node-labels"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-26 19:17:07.0","id":47449,"title":"Hive Metastore SchemaTool","body":"<p>I'm trying to set up my Hive 2.1.0</p><p>When I try to run the Hive CLI i get an error saying I do not have the schema initlized so I went to use the schema tool </p><p>and set up all the hive-site.xml settings but I keep getting this error I do not understand. </p><p>--------------------------------------------------------------------------------------------------</p><p>[root@sandbox apache-hive-2.1.0-bin]# schematool -dbType mysql -initSchema\nWARNING: Use \"yarn jar\" to launch YARN applications.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/root/apache-hive-2.1.0-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\nMetastore connection URL:        jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true\nMetastore Connection Driver :    com.mysql.jdbc.Driver\nMetastore connection User:       root\nStarting metastore schema initialization to 2.1.0\nInitialization script hive-schema-2.1.0.mysql.sql\nError: Specified key was too long; max key length is 1000 bytes (state=42000,code=1071)\norg.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!\nUnderlying cause: java.io.IOException : Schema script failed, errorcode 2\nUse --verbose for detailed stacktrace.\n*** schemaTool failed ***</p>","tags":["metastore","Hive","schema","hadoop","mysql"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-26 17:24:55.0","id":47406,"title":"Distcp between secured clusters","body":"<p>Hi,</p><p>We\nhave two secured clusters with namenode HA setup. Let's name them as PRIMARY and DR. We are now\nimplementing a DR solution between the clusters using HDFS snapshots and distcp\n(We are on HDP2.4.2 and Falcon doesn't support HDFS snapshots till HDP2.5. So\nhad to use HDFS snapshots with distcp) to replicate the data from PRIMARY to DR\ncluster. All the Hadoop daemon accounts on the clusters are appended with the\ncluster name. For example, PRIMARY-hdfs, DR-yarn etc.\nI have few questions in this regard:</p><ul><li>Q: On which node should the\n     distcp job be running?<ul><li>My Understanding: For DR\n      purposes, distcp job should ideally be run on one of the machines on the\n      DR cluster as it has unused YARN capacity. The requirement for the node\n      is to have hadoop client libraries available for it to run distcp. For\n      example, assume the node as dr-host1@HADOOP.COM</li></ul></li><li>Which user should the distcp\n     job be running as? Is it someone with hdfs privileges (For example, DR-hdfs@HADOOP.COM) or any other user for\n     example, a new user created for this purpose -replication-user (replication-user@HADOOP.COM)</li><li>If its hdfs user (DR-hdfs@HADOOP.COM), how to ensure the user is\n     allowed access on the PRIMARY cluster? (probably through auth_to_local\n     settings like below?)<ul><li>RULE:\n      [1:$1@$0] (.*-hdfs@HADOOP.COM) s/.*/PRIMARY-hdfs/</li></ul></li><li>If it’s a non-standard user\n     like replication-user, what are the considerations to be taken? Is it\n     required / recommended to have the same user replication-user on both the clusters and\n     have auth_to_local setting similar to above?</li><li>As the clusters are secured\n     by Kerberos and the principals are going to be different on the clusters,\n     how to make this work? The replication-user's keytab file is going to be\n     different on PRIMARY and DR cluster. What is the best approach to handle\n     this?</li><li>What's the impact on the\n     solution if the both the clusters are part of separate Kerberos realms\n     like PRIMARY.HADOOP.COM and DR.HADOOP.COM?</li></ul><p>Apologies if some of\nthese are trivial. Hadoop security is still a grey-area for me and hence\nmajority of these surround security.</p><p>Thanks</p><p>Vijay</p>","tags":["distcp","sme-security","kerberos","security","disaster-recovery"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-07-26 19:09:00.0","id":47446,"title":"How can i import JSON field data from mySql server into Hive table using sqoop.","body":"<p>I have a table name EmpData in mySql server with two field as EmpID (INT) and Details (nvarchar). Deatils field contain <strong>JSON </strong>string . </p><table><tbody><tr><td>EmpID</td><td>Details</td></tr><tr><td>567</td><td><p>{ \"name\": \"Michel\",\"address\":{\"house no\":\"12\",\"street\":\"Johnson road\",\"city\":\"London\",\"country\":\"UK\"}}</p></td></tr><tr><td>927</td><td><p>{ \"name\": \"John\",\"address\":{\"house no\":\"99\",\"street\":\"Johnson road\",\"city\":\"London\",\"country\":\"UK\"}}</p></td></tr></tbody></table><p>I want to import \"Details\" field JSON data into HDP hive table EmpHiveStore and query on hive table like </p><p><em>SELECT name,address.street from EmpHiveStore;</em></p><p>Is there any way to import JSON field data into Hive table using sqoop ? </p><p>Thank You.</p>","tags":["Hive","mysql","Sqoop","json"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-27 03:17:26.0","id":47515,"title":"Hive Error (RuntimeException org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException)","body":"<p>select \n* from database.table_name limit 2;</p><p>FAILED: <strong>RuntimeException\norg.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException:\nFailed to retrieve roles for a1272279: java.net.SocketTimeoutException: Read\ntimed out</strong></p>","tags":["hiveserver2","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-27 05:06:23.0","id":47528,"title":"how to rerun failed tpch data generation","body":"<p>The tpch-setup.sh has errored out in the middle of data generation and on investigation it is due to name-node going down</p><p>Name node shutdown was due to non-availability of quorum.</p><p>2016-07-26 08:51:14,250 FATAL namenode.FSEditLog (JournalSet.java:mapJournalsAndReportErrors(398)) - Error: flush failed for required journal (JournalAndStream(mgr=QJM to [, stream=QuorumOutputStream starting at txid 102237))</p><p>org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:</p><p>Should we re-run the tpch-setup again ? If so how — I see there are couple of tables created under tpch_flat_orc_1000 database. Should we drop them and re-run ?</p>","tags":["journalnode","Hive","hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-27 07:11:29.0","id":47551,"title":"Azure HDInsight HBase Phoenix Query Server details","body":"<p>Hi, How do we make a request to HDInsight HBase Phoenix Query Server, There are libraries written in Java (Phoenix thin client) and .Net (https://github.com/Azure/hdinsight-phoenix-sharp). But this requires some kind of connection string. I couldn't find the connection string. Please guide me on this.</p>","tags":["queryserver","Hbase","phoenix4.4","hdinsight","azure"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-27 09:17:53.0","id":47564,"title":"Using Apache Kafka  to read files from a folder in a UNIX file system and write into Hadoop file system","body":"<p>I have a use case where I need to read files from a folder in Unix and write the data into Hadoop File System. Files will be generated in the folder by a downstream process real-time. Once a file has been generated the data should be moved into Hadoop. I am using Apache Kafka for the process. I need to know how to implement this use case.</p><ol>\n<li>How to read only the newly created files from the folder using the Kafka producer?(Any examples/Java Classes to use)</li><li>How to write the consumer to write the files into Hadoop File system?(Any examples/Java Classes to use)</li><li>Is there any other technology like NIFI /Apache storm  I need to use along with Kafka to obtain the results or this can be implemented entirely using Kafka?</li></ol>","tags":["hadoop","Storm","Nifi","nifi-streaming","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-27 04:04:21.0","id":47519,"title":"nifi gettwitter works intermittently","body":"<p>Get Twitter is working some but not all times.  If I connect to the sample endpoint, it retrieves data.  If I connect to the filter endpoint with a valid filter that contains popular topics, it retrieves nothing.  If I go back to the sample endpoint, it eventually stops getting any data. If I regenerate my twitter keys, it seems to work for a while and then stops getting data.  I saw the issue regarding server clock synchronization and I have made sure that nntp is running properly and the clock is right. not sure what else to adjust.</p>","tags":["Nifi","twitter","nifi-templates"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-27 14:53:55.0","id":47686,"title":"Zeppelin - Can I export data into csv - HDP 2.4,Zeppelin - can I export data to csv not exist in HDP 2.4?","body":"<p>I am using zeppelin as a service with ambari agent 2.2, and is working just fine.</p><p>I want to export the returned result from zeppelin to a csv file, but i didn't find this feature, however this feature is added to zeppelin code <a href=\"https://github.com/apache/zeppelin/pull/1008\">here </a>, it looks like HDP 2.4 is not picking this change.</p><p>So how can I add this feature? is there a way to update zeppelin in ambari, or shall I install it as a stand alone to clone the desired version?</p>,<p>Can I export the result returned from zeppelin to csv, i found this feature is release <a href=\"https://github.com/apache/zeppelin/pull/1008\">here</a></p><p>But I can't see it when I install zeppelin via ambari, it looks like ambari is getting an older code base.</p><p>How can I get this feature in zeppelin under ambari?, or should i install it as stand alone?</p>","tags":["zeppelin-notebook","zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-27 19:04:52.0","id":47762,"title":"Installing HDF on AWS","body":"<p>I am wondering if Cloudbreak or another tool can easily setup an HDF cluster in AWS?</p>","tags":["installation","aws","hdf","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-27 13:46:23.0","id":47665,"title":"Running Spark beeline and it's giving error \"native snappy library not available: this version of libhadoop was built without snappy support.\"","body":"<p>Apologies if same issue has been posted anywhere in this portal.</p><p>We are using HDP-2.4.0 and Spark version 1.6.0 with Kerberos enabled.</p><p>We have updated the Spark configuration as suggested on below link but it still gives error when we try to run the query on beeline and Tableau. We have imported table in Metastore using Sqoop with compression as snappy.</p><p>https://community.hortonworks.com/questions/38416/spark-saveastextfile-with-snappycodec-on-yarn-gett.html</p><p>https://community.hortonworks.com/questions/18903/this-version-of-libhadoop-was-built-without-snappy.html</p><p>Connected to: Spark SQL (version 1.6.0)</p><p>Driver: Hive JDBC (version 1.2.1000.2.4.0.0-169)\nTransaction isolation: TRANSACTION_REPEATABLE_READ</p><p>0: jdbc:hive2://localhost:&gt; select * from table_1 limit 10; </p><p>Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 32, localhost): java.lang.RuntimeException: native snappy library not available: this version of libhadoop was built without snappy support.\n        at org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:65)\n        at org.apache.hadoop.io.compress.SnappyCodec.getDecompressorType(SnappyCodec.java:193)\n        at org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:178)\n        at org.apache.hadoop.mapred.LineRecordReader.&lt;init&gt;(LineRecordReader.java:111)\n        at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n        at org.apache.spark.rdd.HadoopRDD$anon$1.&lt;init&gt;(HadoopRDD.scala:237)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n        at org.apache.spark.scheduler.Task.run(Task.scala:89)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nDriver stacktrace: (state=,code=0) </p><p>Am I missing anything which causes issue. Could you please help us to resolve the same.</p><p>Please note: we are able to run the same query on hive using beeline, hive cli and Spark shell.</p><p>Here is the output of \"hadoop checknative -a\"command:</p><p>16/07/27 09:43:14 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native</p><p>16/07/27 09:43:14 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\nNative library checking:</p><p>hadoop:  true /usr/hdp/2.4.0.0-169/hadoop/lib/native/libhadoop.so.1.0.0</p><p>zlib:    true /lib64/libz.so.1</p><p>snappy:  true /usr/hdp/2.4.0.0-169/hadoop/lib/native/libsnappy.so.1</p><p>lz4:     true revision:99</p><p>bzip2:   true /lib64/libbz2.so.1</p><p>openssl: true /usr/lib64/libcrypto.so</p><p>Thanks in advance.</p>","tags":["Spark","snappy","beeline"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-27 14:59:11.0","id":47689,"title":"Sandbox with security","body":"<p>Is there a kerberised version of HDP Sandbox image available which can be used for proof of concept purposes on AWS? I am planning to have two secured sandboxes on AWS and then play around with some functionality and hence trying to understand whats the best way to get around this.</p><p>Thanks</p>","tags":["kerberos","Sandbox","aws","security"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-28 01:45:25.0","id":47838,"title":"Is it possible to define max memory size for Spark executor and driver in Yarn","body":"<p>Just starting to understand Spark memory management on yarn and got few questions that I thought would be better to ask experts here.</p><p>1. Is there a way to restrict max size that users can use for Spark executor and Driver when submitting jobs on Yarn cluster? </p><p>2. What the best practice around determining number of executor required for a job? Is there a max limit that users can be restricted to?</p><p>3. How RM handles resource allocation if most of the resources are consumed by Spark jobs in a queue? How preemption is handled?</p>","tags":["spark-streaming","Spark","yarn-scheduler","YARN"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-28 07:50:08.0","id":47863,"title":"I am trying to trigger oozie workflow for hive tables but facing below error:","body":"<p>Error: E0803 : E0803: IO error, java.lang.RuntimeException: Unable to instantiate org.apache.hive.hcatalog.common.HiveClientCache$CacheableHiveMetaStoreClient</p><p>I have added following properties in workflow:</p><p>&lt;credentials&gt;\n    &lt;credential name=\"hcat\" type=\"hcat\"&gt;</p><p style=\"margin-left: 40px;\">\n       &lt;property&gt; </p><p style=\"margin-left: 120px;\">               &lt;name&gt;hcat.metastore.uri&lt;/name&gt; </p><p style=\"margin-left: 120px;\">&lt;value&gt;thrift://&lt;Host_Name&gt;:9083&lt;/value&gt; </p><p style=\"margin-left: 40px;\">      &lt;/property&gt; </p><p style=\"margin-left: 40px;\">        &lt;property&gt; </p><p style=\"margin-left: 120px;\">                &lt;name&gt;hcat.metastore.principal&lt;/name&gt; </p><p style=\"margin-left: 120px;\">                &lt;value&gt;hive/_HOST@EXAMPLE.COM&lt;/value&gt; </p><p style=\"margin-left: 40px;\">&lt;/property&gt;</p><p style=\"margin-left: 40px;\"> &lt;/credential&gt;</p><p style=\"margin-left: 40px;\"> &lt;/credentials&gt;</p><p>Does anyone has any idea about this error ?</p><p>Any help much apprecisted!</p>","tags":["hadoop","hiveserver2","Oozie","hcatalog","oozie-hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-27 21:02:00.0","id":47798,"title":"HBase graphical client","body":"<p>Hi,</p><p>Does anyone know a good GUI for HBase for creating and querying tables ? something like Hue HBase browser for HDP ? any plan for HBase Ambari view ?</p><p>Thanks</p>","tags":["tool","Hbase","view"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-28 08:35:48.0","id":47883,"title":"Pig Json import to HBase error -TableOutputFormat InvocationTargetException and ERROR 1002: Unable to store alias","body":"<p>Hi</p><p>I am trying to process a XML file on pig and make an Json output, my initial idea to import json directly to HBase but seems its not supporting. Then with elephant bird, I converted the Json for HBase ready.</p><p>But I see below errors. Environment is single node and like;</p><p>Pig: 0.16.0</p><p>HBase: 1.1.5</p><p>Zookeeper: 3.4.6</p><p>Hadoop: 2.6.4</p><p>Ubuntu 12.04 32 bit</p><pre>2016-07-27 15:47:29,606 [main] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0xf057b40x0, quorum=localhost:2181, baseZNode=/hbase\n2016-07-27 15:47:29,709 [main] ERROR org.apache.hadoop.hbase.mapreduce.TableOutputFormat - java.io.IOException: java.lang.reflect.InvocationTargetException\n2016-07-27 15:47:29,741 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1002: Unable to store alias T\nDetails at logfile: /usr/local/Pig/bin/pig_1469627167519.log\ngrunt&gt; 2016-07-27 15:47:29,772 [main-SendThread(localhost:2181)] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)</pre><pre>2016-07-27 15:47:29,774 [main-SendThread(localhost:2181)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established to localhost/127.0.0.1:2181, initiating session\n2016-07-27 15:47:29,990 [main-SendThread(localhost:2181)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x156277837ff000e, negotiated timeout = 40000</pre><pre>REGISTER /usr/local/Pig/lib/piggybank.jar\nREGISTER /usr/local/Pig/lib/json-simple-1.1.jar\nREGISTER /usr/local/Pig/lib/elephant-bird-pig-4.14.jar\nREGISTER /usr/local/Pig/lib/elephant-bird-core-4.14.jar\nREGISTER /usr/local/Pig/lib/elephant-bird-hadoop-compat-4.14.jar\nREGISTER /usr/local/Pig/lib/google-collections-1.0.jar\nREGISTER /usr/local/Hbase/lib/zookeeper-3.4.6.jar\nREGISTER /usr/local/Hbase/lib/protobuf-java-2.5.0.jar\nREGISTER /usr/local/Hbase/lib/hbase-common-1.1.5.jar\nREGISTER /usr/local/Hbase/lib/guava-12.0.1.jar\nDEFINE XPath org.apache.pig.piggybank.evaluation.xml.XPath();</pre><pre>S = LOAD '/usr/local/hadoop/data/untar/result/final/part-m-00000' USING com.twitter.elephantbird.pig.load.JsonLoader() AS (json: map[]);</pre><pre>T = FOREACH S GENERATE (int)json#'Oam_HardwareRfFailure_Sum' as Oam_HardwareRfFailure_Sum,(int)json#'Oam_HardwareRfTempOut_Sum' as Oam_HardwareRfTempOut_Sum............</pre><pre>STORE T INTO 'hbase://KPIKPI' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('info: Oam_HardwareRfFailure_Sum info: Oam_HardwareRfTempOut_Sum info: VS_NumFreqChanges info:........</pre>","tags":["HDFS","hadoop","hadoop-ecosystem","Hbase","Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-28 10:00:44.0","id":47897,"title":"IllegalArgumentException when select with where clause on hive external table pointing to parquet data","body":"<p>Followed is\nthe workflow</p><p>Hive&gt;create external table if not exists table1 (\nC1 string,\nC2 string,\nC3 int)</p><p>  stored as parquet</p><p> location 'hdfs:/project/table1.parquet';</p><p>Parquet\ndata are created by spark as</p><p>  df.write.mode(\"overwrite\").parquet('hdfs:/project/table1.parquet')</p><p>Hive&gt;select * from table1 where C1='toto';</p><p>OK</p><p>SLF4J:\nFailed to load class \"org.slf4j.impl.StaticLoggerBinder\".</p><p>SLF4J:\nDefaulting to no-operation (NOP) logger implementation</p><p>SLF4J: See <a href=\"http://www.slf4j.org/codes.html#StaticLoggerBinder\">http://www.slf4j.org/codes.html#StaticLoggerBinder</a>\nfor further details.</p><p>Failed with exception java.io.IOException:java.lang.IllegalArgumentException:\nColumn [c1] was not found in schema!</p><p>Time taken: 0.254 seconds</p>","tags":["illegalargumentexception","parquet","Hive","error"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-28 12:20:37.0","id":47941,"title":"In Hive via HiveServer2, the user has access to and can query views built off of tables. However, when they use a union with a view, the query fails due to not having permission to the table.","body":"<p>Hello, this is somewhat confusing so let me try to explain this as clearly as possible. </p><p>\nIn Hive, we have a schema called 'svct' which is where our tables sit and a schema called 'svcv' where we have views built on the tables in 'svct'. For instance, we have a table called svct.predictive_customers and view called svcv.predictive_customers_vw. svcv.predictive_customers_vw was created as \"create view svcv.predictive_customers_vw as select * from svct.predictive_customers;\". We also only give our users access to these views via Ranger Hive policies and Ranger HDFS policies. Each table in 'svct' is an external table with the data sitting in the location /hive/data/pre-app/[TABLE_NAME]. In our ranger policies we have given each user select access to all tables and columns in 'svcv' in the hive policy and read and execute access to the following HDFS directories: </p><pre>/apps/hive/warehouse/svcv.db \n/apps/hive/warehouse/svct.db \n/hive/data/pre-app \n/hive/data/ter-app \n/hive/data/msd-app </pre><p>We dont want users to have direct access to the tables, and only want them to have select access to the views. </p><p>Now on to the problem. A user can run a query against the 'svcv' views just fine. For instance, the following works just fine: </p><pre>select raw_customercode as cid, \nregion, \ncustomer_name \nfrom svcv.predictive_customers_vw; </pre><p>However, once they use a union in their query, it fails saying they don't have select access to the tables in 'svct'. For instance, the following query fails: </p><pre>select raw_customercode as cid, \nregion, \ncustomer_name \nfrom svcv.predictive_customers_vw \nUNION \nselect prod_customercode as cid, \nregion, \ncustomer_name \nfrom svcv.predictive_customers_vw; </pre><p>Here is the error we receive: </p><pre>\"[Error Code: 40000, SQL State: 42000] Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [ha230160] does not have [SELECT] privilege on [svct/predictive_customers/customer_name,prod_customercode,raw_customercode,region]\" </pre><p>So it appears to me that the UNION is converting the views to the tables they select from at run time, which our users don't have access to.</p><p>\nAlso, if I run the same query via HiveServer2 with an ID that DOES have permission to the tables, the query works. Also, if I go into Ranger and change the Hive policy where the users have select access to * tables and * columns in svcv and add svct to it, the query works. But once again, we do not want to give users select access to the tables. </p><p> What could be done to get past this? </p><p>Some other info: </p><ul><li>We are using Hive version: 1.2.1.2.3 </li><li>UNION ALL fails with the same error </li><li>We have tried multiple SQL client tools to connect via HiveServer2 and all are failing with the same error.</li></ul>","tags":["hiveserver2","Ranger","Hive","access","permission-denied","query","view"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-28 13:20:01.0","id":47973,"title":"I downloaded the latest HDP 2.4, I can't see tables in database explorer and get error \"H100 Unable to .TTransportException: java.net.SocketException: Broken pipe\". I can't proceed with any VM! neither for admin nor for developer, please HELP!","body":"<p>@neerajsabharwal</p>","tags":["Hive","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-28 15:34:31.0","id":48004,"title":"hive orc table error... When i use beeline it is working and when i use hive -e , it is not working and giving the error message. What will be the reason?","body":"","tags":["error","orc","beeline","Hive","hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-28 19:45:00.0","id":48079,"title":"WebHDFS: HTTP error 500","body":"<p>I configured HTTP authentication for HDFS (following this <a target=\"_blank\" href=\"https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.2.0/bk_Ambari_Security_Guide/content/_configuring_http_authentication_for_HDFS_YARN_MapReduce2_HBase_Oozie_Falcon_and_Storm.html\">document</a>) and I use it as</p><pre>kinit -V -kt spnego.service.keytab HTTP/example.com@EXAMPLE.COM\ncurl -s --negotiate -u : \"http://example.com:50070/webhdfs/v1/user/test?op=LISTSTATUS\"</pre><p>And I get this message error</p><pre>HTTP ERROR 500\n\nProblem accessing /webhdfs/v1/. Reason:\n    INTERNAL_SERVER_ERROR\n\nCaused by:\njava.lang.NullPointerException\n\tat org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:377)\n\tat org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:349)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:349)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:538)\n\tat org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:88)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1243)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)</pre><p>Thanks in advance.</p>","tags":["webhdfs","authorization","curl","http","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-07-29 15:00:19.0","id":48250,"title":"scala> header :26: error: not found: value header","body":"<p>The value of \"header\" is not found in scala using the spark</p>","tags":["scala"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-30 03:44:33.0","id":48304,"title":"Hue(beewax) hive error: Internal error processing query in PROD server","body":"<p>Hi..i am using HDP 2.1.2 and Hue(beewax hive UI) ..while i run simple hive query(show databases) its show internal error processing query error</p><p>But i could see all data bases and tables in hue and also i could run hive queries in command line..please help me get out of this..</p><p>Hive: 0.13.0.2.1</p><p>Tez:0.4.0.2.1</p>","tags":["hadoop-maintenance","hue","hadoop-ecosystem","hadoop","Hive","hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-31 10:07:34.0","id":48392,"title":"Learning the Ropes of the Hortonworks Sandbox (tutorial) SECTION 2 :1.4 SEND DATA BETWEEN AZURE SANDBOX & LOCAL MACHINE,Learning the ropes of the HortonWorks Sandbox  (tutorial) - SECTION 2- 1.4 SEND DATA BETWEEN AZURE SANDBOX & LOCAL MACHINE doesnt work","body":"<p>According to the tutorial one can use GiT Bash. The first image clearly shows the file is there.</p><p>The second image shows my steps in trying to use the tutorial and the results of those efforts. </p><p>azure is my user name and 24.176.105.180 is my IP on the azure cloud. I am using an acer laptop with Windows 10.</p><p>When I use port 2222 I'm rejected.  When I use port 22 ... no such file or directory...</p><p><img src=\"/storage/attachments/6211-sanbox-assistance-a.png\"></p><p>,\n</p><p>According to the tutorial, the commands should work in Git Bash. The top image shows the file. </p><p>The second image shows me following the directions of the tutorial and the results. </p><p>I am using an acer laptop with  windows 10. </p><p><img src=\"/storage/attachments/6180-sanbox-assistance-a.png\"></p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-31 19:23:34.0","id":48418,"title":"Big Data Analytics - Approach for Data Quality phase","body":"<p>I doing a small project in Hadoop which the main goal is create some KPI in Hive. However I needed to do some ETL jobs using Pig to clean my data and I put the transformed files into a new directory in HDFS. To ensure that all the files are in correct form, I want to create some data quality activities in Java or Python. I tried to to use PIG UDFs to achieve this but I couldn't connect the Jar file with Pig. Since I can't use PIG UDFs, I'm planning a new approach to do the data quality phase:\n\n1) Run the PIG scripts to clean the data and extract the new files into a new directory in HDFS\n2) Put Java/Python independentely read the new files and perform the data quality activities\n3) If the Data Quality tests return sucessfully load the files into Hive\n\nIn your opinion this a good approach for a Big Data project? I'm new in this topic... If not, what a good alternative for perform data quality jobs in this project?\n\nMany thanks for your help!</p>","tags":["java","data-processing","python","Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-01 06:11:42.0","id":48444,"title":"can i use the same disk with diff mount points to store Data nodes data, NN namenode data, SN data, JT data (master node data) and /usr and /var","body":"<p>Hi Team,</p><p>I have 3 virtual machines in HDP cluster ,if i have huge capacity in data nodes disk in TBs\nso can i use the same disk with diff mount points to store Data nodes data, NN namenode data, SN data, JT data (master node data) and /usr and /var .</p><p>I know then if my disk has some issue then all data will be affected </p><p>\nbasically i wanted to know if my data node disks have lot of space in TBs, so do you recommend creating diff mounts on same data node disks for diff purposes like /usr,/var and storing NN SN JT data</p><p>Also each HDP version data is in /usr/hdp</p>","tags":["Hbase","installation","hbase-namespace","high-availability","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-01 10:57:10.0","id":48510,"title":"How can we configure controller services  for \"DetectDuplicate\" processor in Apache Nifi ?","body":"<p>I want to use \"DetectDuplicate\" processor to remove duplicate JSON content or duplicate tweets and merge into a single file.</p><p>Can someone help me in this .<a href=\"https://community.hortonworks.com/users/166/jdyer.html\">@Jeremy Dyer</a>,@<a href=\"https://community.hortonworks.com/users/641/mburgess.html\">Matt Burgess</a></p><p>Thanks in advance.</p>","tags":["hdf","Nifi","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-03 15:55:25.0","id":49365,"title":"Apache NIFI - What is the difference between ConsumeKafka and GetKafka,Apache Nifi - What is the difference between ConsumeKafka and GetKafka","body":"","tags":["apache-nifi","nifi-processor","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-04 05:21:00.0","id":49797,"title":"geolocation_stage and truks_stage don't appear in Database Explorer even after execution success and refresh.","body":"<p><img src=\"/storage/attachments/6364-untitled.png\"></p>","tags":["hdp-2.4.0","tutorial-100"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-08-04 10:49:02.0","id":49844,"title":"Telecommunications Dataset - Link Analysis Project","body":"<p>Hi,</p><p>I'm doing my master thesis on topic of Big Data Analytics using Hortonworks environment. I want to do a social network analysis based on telecommunication industry. There exists any dataset that allows me to make a Link Analysis based on Telecommunication Industry (Interaction with Users via SMS, etc)? I've some urgency to find a telecommunication dataset to my master thesis.\n\nI already do a lot of search but I can't found anything :(</p><p>Many Thanks</p>","tags":["social","links","database","data"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-03 21:17:53.0","id":49745,"title":"Dedicated edge nodes","body":"<p>Hi</p><p>We are thinking on having dedicated edge per project for our data lake. Each project will have a vm on which we install the required clients.</p><p>Anyone is doing this ? Any problems or issues that we should be aware of with this configuration ?</p>","tags":["architecture","edge"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-08-04 15:59:28.0","id":49901,"title":"Ambari shows HiveServer2 and Hive Metastore is stopped but they are running","body":"<p>Ambari dashboard is showing HiveServer2 and Hive Metastore is stopped, but from the command line I am able to login into hive cli and hiveserver2 using beeline client also.</p><p>I have 6 node cluster in which I have installed hive server in 2 nodes and hive client is installed in all 6 nodes. In node1 ambari dashboard it was showing hiveserver2 and hive metastore is stopped but in node2 it was started.</p><p>In node1:</p><p><img src=\"/storage/attachments/6381-hive-node1.png\"></p><p>In node2:</p><p><img src=\"/storage/attachments/6382-hive-node2.png\"></p><p>When checked the process id which was available in /var/run/hive/hive.pid and hive-server.pid, they are same.</p><pre>[node1 ~]$ cd /var/run/hive\n[node1.staging.iad hive]$ ls\nhive.pid  hive-server.pid\n[node1 hive]$ cat hive.pid\n14037\n[node1 hive]$ cat hive-server.pid\n16654\n[node1 hive]$ ps -ef| grep 14037\ndinesh     564 32124  0 15:10 pts/0    00:00:00 grep 14037\nhive     14037     1  0 Aug03 ?        00:01:45 /usr/lib/jvm/jre/bin/java -Xmx1024m -Dhdp.version=2.3.4.0-3485 -Djava.net.preferIPv4Stack=true -Dhdp.version=2.3.4.0-3485 -Dhadoop.log.dir=/var/log/hadoop/hive -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.3.4.0-3485/hadoop -Dhadoop.id.str=hive -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/hdp/2.3.4.0-3485/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx1024m -XX:MaxPermSize=512m -Xmx1024m -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /usr/hdp/2.3.4.0-3485/hive/lib/hive-service-1.2.1.2.3.4.0-3485.jar org.apache.hadoop.hive.metastore.HiveMetaStore -hiveconf hive.log.file=hivemetastore.log -hiveconf hive.log.dir=/var/log/hive\n[node1 hive]$ ps -ef| grep 16654\ndinesh     621 32124  0 15:10 pts/0    00:00:00 grep 16654\nhive     16654     1  0 Aug03 ?        00:01:32 /usr/lib/jvm/jre/bin/java -Xmx1024m -Dhdp.version=2.3.4.0-3485 -Djava.net.preferIPv4Stack=true -Dhdp.version=2.3.4.0-3485 -Dhadoop.log.dir=/var/log/hadoop/hive -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.3.4.0-3485/hadoop -Dhadoop.id.str=hive -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/hdp/2.3.4.0-3485/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx1024m -XX:MaxPermSize=512m -Xmx512m -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /usr/hdp/2.3.4.0-3485/hive/lib/hive-service-1.2.1.2.3.4.0-3485.jar org.apache.hive.service.server.HiveServer2 --hiveconf hive.aux.jars.path=file:///usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar -hiveconf hive.metastore.uris=  -hiveconf hive.log.file=hiveserver2.log -hiveconf hive.log.dir=/var/log/hive\n</pre>","tags":["ambari-2.2.2","hadoop","Ambari","ambari-agent","Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-04 11:44:38.0","id":49845,"title":"Ambari Install, Start, and Test Errors","body":"<p>Hello,</p><p>So I'm quite lost in the errors in my cluster at this point. I'm hoping someone would be kind enough to take a look at the problems below and perhaps give me a couple of pointers or somewhere to start looking.</p><p>Background: I used the hortonworks documentation guidelines to setup ambari server and agents, along with a local repository. I am able to access the local repository through my browser as well as a wget from each node. The registration step goes smoothly and no warnings remain in that step. I am using a proxy and added the appropriate -DhttpProxyHost and -DhttpProxyPort in the way recommended to each host. </p><p>I have gone through and tested the network configurations given by both of the following links: </p><p>https://community.hortonworks.com/storage/attachments/2326-network-and-prereq-setup.pdf</p><p>http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.2.0/bk_Installing_HDP_AMB/content/_prepare_the_environment.html</p><p>1. The \"Select Stack\" step from the install wizard returns a 404 for my local repository base URL, which wasn't happening before but I don't recall exactly when it stopped working (initially the green checkmarks appeared and it failed later on)</p><p>2. The 404 repository base url problem seems to be in the logs for the installation errors in the final step as well, after successfully installing accumulo client and server as well as the data node for the appropriate nodes</p><p>3. The first failure is the Falcon server, which is the only \"Failure encountered\" and seems to trigger the warnings for all of the preceding steps.</p><p>4. wget to the baseurl works from each node, however wget to the repomd.xml times out and returns the 404 in the end. </p><p>Is there anything obvious that I could be missing or an important step that is not included in the documentation? Any advice or ideas are appreciated.</p><p>Thanks in advance,</p><p>Savanna</p>","tags":["setup","ambari-server","proxy","Ambari","repository","installation","network"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-04 18:29:40.0","id":49959,"title":"Even when i ran balancer, load one data node is 84%. What should be the reason?","body":"","tags":["usage","HDFS","balancer","hadoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-04 14:07:14.0","id":49875,"title":"Nifi: How to check if large file has completely been written to directory without using Minimum File Age setting?","body":"<p>Is the Minimum File Age setting on a GetFile processor the only way to check if large files have been completely written to its directory? </p><p>Instead of using a timer, is there a way to ask for the current filesize attribute after I use GetFile? I was trying to use UpdateAttribtue/RouteOnStrategy processors to repeatedly check the filesize attribute until it's been completed.</p><p>Thanks</p>","tags":["files","file","Nifi","filesize"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-05 03:54:12.0","id":50001,"title":"How to clear / remove the failed host components / services after failed installation","body":"<p>HI </p><p>I tried installing the services HIVE, TEZ, PIG to the existing cluster and it got failed. I wanted to remove / clean up the components and services. In the Since the installation is failed, i could not find the component to decommission and then remove. Only HCAT got installed. Tried through API to remove the service, it is getting failed with the below error.</p><p>CSRF option is not in ambari.properties file as well. </p><p>$ cat /etc/ambari-server/conf/ambari.properties | grep -i CSR\n[root@ip-172-27-3-42.ap-southeast-1.compute.internal]:/home/ambari\n$</p><pre>$ curl -u admin:xxxx -H “X-Requested-By: ambari” -X GET http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE\ncurl: (6) Could not resolve host: xn--ambari-1i0c; Name or service not known\n{\n  \"href\" : \"http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE\",\n  \"ServiceInfo\" : {\n    \"cluster_name\" : \"eim_edl_dev_cluster_1\",\n    \"maintenance_state\" : \"OFF\",\n    \"service_name\" : \"HIVE\",\n    \"state\" : \"INSTALL_FAILED\"\n  },\n  \"alerts_summary\" : {\n    \"CRITICAL\" : 0,\n    \"MAINTENANCE\" : 0,\n    \"OK\" : 0,\n    \"UNKNOWN\" : 0,\n    \"WARNING\" : 0\n  },\n  \"alerts\" : [ ],\n  \"components\" : [\n    {\n      \"href\" : \"http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE/components/HCAT\",\n      \"ServiceComponentInfo\" : {\n        \"cluster_name\" : \"eim_edl_dev_cluster_1\",\n        \"component_name\" : \"HCAT\",\n        \"service_name\" : \"HIVE\"\n      }\n    },\n    {\n      \"href\" : \"http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE/components/HIVE_CLIENT\",\n      \"ServiceComponentInfo\" : {\n        \"cluster_name\" : \"eim_edl_dev_cluster_1\",\n        \"component_name\" : \"HIVE_CLIENT\",\n        \"service_name\" : \"HIVE\"\n      }\n    },\n    {\n      \"href\" : \"http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE/components/HIVE_METASTORE\",\n      \"ServiceComponentInfo\" : {\n        \"cluster_name\" : \"eim_edl_dev_cluster_1\",\n        \"component_name\" : \"HIVE_METASTORE\",\n        \"service_name\" : \"HIVE\"\n      }\n    },\n    {\n      \"href\" : \"http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE/components/HIVE_SERVER\",\n      \"ServiceComponentInfo\" : {\n        \"cluster_name\" : \"eim_edl_dev_cluster_1\",\n        \"component_name\" : \"HIVE_SERVER\",\n        \"service_name\" : \"HIVE\"\n      }\n    },\n    {\n      \"href\" : \"http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE/components/MYSQL_SERVER\",\n      \"ServiceComponentInfo\" : {\n        \"cluster_name\" : \"eim_edl_dev_cluster_1\",\n        \"component_name\" : \"MYSQL_SERVER\",\n        \"service_name\" : \"HIVE\"\n      }\n    },\n    {\n      \"href\" : \"http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE/components/WEBHCAT_SERVER\",\n      \"ServiceComponentInfo\" : {\n        \"cluster_name\" : \"eim_edl_dev_cluster_1\",\n        \"component_name\" : \"WEBHCAT_SERVER\",\n        \"service_name\" : \"HIVE\"\n      }\n    }\n  ],\n  \"artifacts\" : [ ]\n\n======================================\n\n$ curl -u admin:xxxxx -H “X-Requested-By: ambari” -X DELETE http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE\ncurl: (6) Could not resolve host: xn--ambari-1i0c; Name or service not known\n{\n  \"status\" : 400,\n  \"message\" : \"CSRF protection is turned on. X-Requested-By HTTP header is required.\"\n\n$ curl -u admin:xxxxx -H “X-Requested-By: ambari” -X DELETE -d ‘{“RequestInfo”:{“state”:”INSTALL_FAILED”}}’ http://172.27.3.42:8080/api/v1/clusters/eim_edl_dev_cluster_1/services/HIVE\ncurl: (6) Could not resolve host: xn--ambari-1i0c; Name or service not known\n{\n  \"status\" : 400,\n  \"message\" : \"CSRF protection is turned on. X-Requested-By HTTP header is required.</pre>","tags":["Ambari","services","Hive","Tez","Pig"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-08-05 04:20:56.0","id":50013,"title":"How to Query Hbase Snapshot (in HDFS) from Spark or PySpark?","body":"<p><strong>Does anyone have sample PySpark or Spark code to query an Hbase Snapshot?</strong></p><p>I've created an Hbase table, loaded data, and then took a snapshot. I then moved the snapshot into HDFS. Now I would like to query this table to analyze the data and (more specifically) filter based on timestamp. </p><p>Does anyone have any code or best practices/advice for doing this? </p><p>Thanks!</p>","tags":["Hbase","Spark","snapshot","pyspark"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-08 05:56:33.0","id":50277,"title":"How to enable and install Kerberos on HDP, installed on Ubuntu 14.04. Standard steps that I can find are for CentOS/RedHat.I have a 3 node HDP cluster- 2.3","body":"","tags":["ubuntu","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-08-08 13:44:48.0","id":50349,"title":"How to store the output to a variable based file in pig?","body":"<p>Hi,</p><p>I am using pig to process some data and I am successfully able to store the data to a folder name I gave. After that with below command  I am able to move the file to a desired location with a desired name. </p><pre>fs -mv /usr/local/hadoop/data/result/tmp/tmp8/part-m-00000 /usr/local/hadoop/data/result/Final_KPI/femto.json;   </pre><p>What I want to do is, I want to give dynamic names instead of femto.json and the names will be based on some pig script outputs like below</p><pre>XML_MAIN_1 = LOAD '/usr/local/hadoop/data/tar/*' using org.apache.pig.piggybank.storage.XMLLoader('md') as (x:chararray);\n\nSERIAL_AND_DATE = FOREACH XML_MAIN_1 GENERATE XPath(x, 'md/neun'), XPath(x, 'md/mts');\n\nSERIAL_T = FOREACH SERIAL_AND_DATE GENERATE REGEX_EXTRACT(REGEX_EXTRACT($0, '(.*),bSRName=(.*)', 1), '(.*)Fsn=(.*)', 2);\n\nSERIAL_F = LIMIT SERIAL_T 1;   \n\nSTORE SERIAL_F INTO '/usr/local/hadoop/data/result/tmp/tmp1';\n\nSERIAL_C = LOAD '/usr/local/hadoop/data/result/tmp/tmp1/part-r-00000' as (Serial:chararray);</pre><p>Imagine I will have a file under Final_KPI as the name with the result of the dump of SERIAL_C. How can I do that?</p>","tags":["hadoop-ecosystem","hadoop","Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-08 16:30:23.0","id":50387,"title":"Apache PIG - Create a Schema or the Schema is already created?","body":"<p>Hi experts,\n\nProbably is a dummy question (but since I have :) ).\n\nI want to know how Pig read the headers from the following dataset that is stored in .csv:\n\n</p><p>ID,Name,Function\n\n1,Johnny,Student\n\n2,Peter,Engineer\n\n3,Cloud,Teacher\n\n4,Angel,Consultant\n\nI want to have the first row as a Header of my file. There I need to put:\nA = LOAD 'file' using PIGStorage(',') as (ID:Int,....etc) ?\n\nOr I only need to put:\n\nA = LOAD 'file' using PIGStorage(',')</p><p>And only with this pache PIG already know that the first line are the headers of my table.</p><p>Thanks!</p>","tags":["Pig","header","csv"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-11 13:18:23.0","id":51020,"title":"where to add external jars in nifi setup.","body":"<p><a href=\"https://community.hortonworks.com/users/525/mclark.html\">@mclark</a> @<a href=\"https://community.hortonworks.com/users/2761/pam.html\">PJ Moutrie</a> @<a href=\"https://community.hortonworks.com/users/5078/pvillard.html\">Pierre Villard</a></p><p>I have created a custom processor which accept messages from getKafka processor and store in hadoop data lake. This is data lake is a existing project. having multiple third party jars, config files and scripts. my question is where should deploy this project under nifi setup and how to set classpath for that so that my custom processor can easily access all required jars/classes.</p>","tags":["apache-nifi","nifi-processor","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-11 11:19:17.0","id":50982,"title":"MRUnit wih DistributedCache support","body":"<p>Hi All , </p><p>I have a simple mapper which read some data from a log file and do some join operation with a another file data and send that combined output to reducer for further processing. </p><p>In mapper I have used DistributedCache as the file is small one. Its working properly. </p><p>Now I have to write some MRUnit test cases for that mapper. Can any one help me out with some code example how to write MRUnit with DistributedCache support. </p><p>I am using Hadoop2 and MRUnit version is as follows ....</p><pre>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt;\n    &lt;artifactId&gt;mrunit&lt;/artifactId&gt;\n    &lt;version&gt;1.1.0&lt;/version&gt;\n    &lt;classifier&gt;hadoop2&lt;/classifier&gt; \n&lt;/dependency&gt;</pre><p>In Driver class I have added for DistributedCache (this is just to explain how I added cache in MR)Job job = Job.getInstance(conf);job.setJarByClass(ReportDriver.class);</p><pre>job.setJobName(\"Report\");\njob.addCacheFile(new Path(\"zone.txt\").toUri());\nFileInputFormat.addInputPath(job, new Path(args[0]));\nFileOutputFormat.setOutputPath(job, new Path(args[1]));\njob.setMapperClass(ReportMapper.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(Text.class);\njob.setReducerClass(ReportReducer.class);\njob.setNumReduceTasks(3);\n//job.setCombinerClass(ReportReducer.class);\nlogger.info(\"map job started ---------------\");\nSystem.exit(job.waitForCompletion(true) ? 0 : 1);</pre><p>In Mapper class I am fetching the cases file like this ....</p><pre>@Override\nprotected void setup(Context context) throws IOException, InterruptedException \n{\n        URI[] localPaths = context.getCacheFiles();\n}\n</pre><p>Please help me out if any one use DistributedCache with MRUnit with some code example...</p><p>Thanks a lot ....</p>","tags":["unit-test","hadoop","MapReduce"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-11 19:02:47.0","id":51117,"title":"Nifi: Execute *multiple* SQL commands in sequence","body":"<p>I have four SQL commands that I need to execute in order:</p><p>1) drop a temp table if it exists</p><p>2) create a temp table</p><p>3) copy into table from S3</p><p>4) insert select to final table</p><p>I was hoping to include all of these in one ExecuteSQL processor (...yes, I know it says it's for SELECT queries only, but seems to work with other commands just fine...), but an error message informed me that support for multiple SQL commands in a single ExecuteSQL processor is not currently available. So, I did the next obvious thing and broke up my series of commands to each have their own ExecuteSQL processor. This seems to work somewhat, but it does not behave as I would like, e.g., sometimes INSERT is attempted when the temp table doesn't exist, which should never happen if they are executed in order. The worst would be if DROP ran after COPY but before INSERT. Yikes! Fyi - I have set each of these ExecuteSQL processors to run on Primary node as to not end up with redundant data in my table(s).</p><p><strong>What is the best approach to ensuring that these commands always execute to completion <em>in order, </em>given that it seems I need to use multiple ExecuteSQL processors?</strong></p><p><strong>Allowing multiple commands in ExecuteSQL would be the easiest / safest way to do this -- Is there a workaround?</strong></p><p><strong>UPDATE:</strong> Setting this up as Event Driven seems to help somewhat. DROP, CREATE, COPY all execute once in order and then <strong>five</strong> (which happens to be the number of slave nodes on our staging cluster) INSERTS. Not sure why the switch from 1x to 5x though.</p>","tags":["apache-nifi","nifi-templates","Nifi","data","sql","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-16 02:38:17.0","id":51649,"title":"export Phoenix table using sqoop?","body":"<p>Is it possible to export phoenix data into RDBMS (ie oracle,mysql,teradata,etc)?   I know this can be done via apache NiFi. Wondering if possible using sqoop...?  I don't think so but asking if I am overlooking something</p>","tags":["Phoenix","export","Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-16 14:21:23.0","id":51800,"title":"How to run pig script in tez mode from command line","body":"<p>I have a pig script stored in HDFS that i need to run from the local filesystem.</p><p>I'm trying this but it doesn't work:  pig -exectype tez -file /path/to/script/script.pig</p><p>It returns an error saying the script does not exist.</p><p>Since I am executing in Tez mode, i would assume that it looks in HDFS for the script and not the local filesystem.</p>","tags":["Tez","Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-16 21:57:43.0","id":51885,"title":"I keep getting a GetTwitter[id=1fc710cb-5776-3bf0-bb15-f3ebd4d17b7f] CONNECTION_ERROR: stream.twitter.com. Failed to establish connection properly. Will attempt to reconnect.","body":"<p>I added valid access,consumer, access secret, and consumer secret tokens. Yet I still receive this error! </p>","tags":["nifi-processor","nifi-streaming","access","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-16 18:27:14.0","id":51869,"title":"Connecting to phoenix command line on kerberized cluster","body":"<p>I am on HDP 2.4.2 kerberized cluster trying to connect to phoenix sqlline.py with no success.  I have 3 ZK quorum </p><p>I have tried this:</p><p>./sqlline myserver.com,myserver.com,myserver.com:2181:/hbase-secure</p><p>and this</p><p>./sqlline myserver.com,myserver.com,myserver.com:2181:/hbase-secure:/home/smanjee/smanjee.headless.keytab@smanjee@MYSERVER.COM</p><p>and I get error:</p><p>java.lang.UnsupportedClassVersionError: org/apache/phoenix/jdbc/PhoenixDriver : Unsupported major.minor version 51.0\n       at java.lang.ClassLoader.defineClass1(Native Method)\n       at java.lang.ClassLoader.defineClass(ClassLoader.java:643)\n       at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</p><p>Any ideas?  I suspect my connecting string format is incorrect.</p>","tags":["Phoenix","connection"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-17 01:03:16.0","id":51944,"title":"Operating model opinion","body":"<p>I am debating a couple different architectures for processing analytics against e-mail content (fed via EML files), and I was curious if someone can give me some feedback.</p><p>Model 1:</p><p>Agent (windows) --&gt; Receiver (Linux) --&gt; Tika --&gt; Kafka --&gt; Spark --&gt; entity extraction</p><p>Model 2:</p><p>Agent (windows) --&gt; Receiver (Linux) --&gt; Kafka --&gt; Spark --&gt; Tika --&gt; entity extraction</p><p>I feel, and correct me if I am wrong, but if I do text extraction via tika, and segment the eml into the individual components, it would perform better in kafka's queue process for spark to consume it. If doing Tika after (via spark) I feel the block processing model would get hung up and slow the rest of the processes down. A developer I spoke to feels it's the other way around due to sparks processing model in the cluster, but my understanding is Tika doesn't operate within the distributed processing cluster no matter what.</p><p>Please let me know if I am incorrect or if the models above are not good.</p>","tags":["Spark","Kafka","tika"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-11-28 13:07:24.0","id":68687,"title":"Ambari Hive View (1.5.0) + LDAP ask for password","body":"<p>Hi guys,</p><p>I have Ambari version 2.4.0 synced with ldap. I want to configure Hive View version 1.5.0 with ldap users but I got the error validating the login. On Hive View version 1.0.0 (The same Ambari) it works fine because of use of this parameter:</p><pre>password=${ask_password}</pre><p>but in version 1.5.0 it does not work - Ambari does not ask for a password anymore. Is this parameter changed, or disabled?</p>","tags":["Ambari","ambari-ldap-sync","ambari-views"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-27 22:23:56.0","id":68603,"title":"Unable to create core [tweets_shard1_replica1] Caused by: Element type \"arr\" must be followed by either attribute specifications, \">\" or \"/>\".}","body":"<p>Hi</p><p>In reference to the following tutorial: http://hortonworks.com/hadoop-tutorial/how-to-refine-and-visualize-sentiment-data/</p><p>I am unable to run command: /opt/lucidworks-hdpsearch/solr/bin/solr create -c tweets -d tweet_configs -s 1-rf 1-p 8983</p><p>I have throughly searched through the solrconfig.xml file for a missing '&lt;' before and after 'are' but cannot find a problem with the solrcongfig.xml</p><p>Does anyone know where I'm going wrong with this please?</p><p>Thanks!</p><p>[solr@sandbox dashboards]$ ERROR: Failed to create collection 'tweets' due to: {sandbox.hortonworks.com:8983_solr=org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException:Error from server at http://sandbox.hortonworks.com:8983/solr: Error CREATEing SolrCore 'tweets_shard1_replica1': Unable to create core [tweets_shard1_replica1] Caused by: Element type \"arr\" must be followed by either attribute specifications, \"&gt;\" or \"/&gt;\".}</p>","tags":["SOLR","tweets","array","banana"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-28 13:42:07.0","id":68701,"title":"Cannot install HDP 2.5 on CentOS 7","body":"<p>Hi ,</p><p>\nI am trying to setup HDP 2.5 on a CentOS 7 virtual machine via Ambari. I chose the Public repository option where the urls mentioned are :</p><p><a href=\"http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.5.0.0\">http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.5.0.0</a>\nand</p><p> <a href=\"http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\">http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7</a></p><p><a href=\"http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\"></a>\nBut both of the URLs seems not to work , I get this error via Ambari : \"Please make sure all repository URLs are valid before proceeding \", when I am going to the URL via a browser I get a 404. </p><p>Am I using an incorrect URL, if so please can someone let me know the correct url. </p><p>Regards,</p><p>Saby</p>","tags":["centos","hdp-2.5.0","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-27 04:19:55.0","id":68524,"title":"I am trying to execute a small oozie job on my cluster but it end up with the same error for every task.","body":"<p>I have submitted the a simple sqoop job in oozie the workflow is </p><pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;workflow-app xmlns=\"uri:oozie:workflow:0.2\" name=\"mainjob-wf\"&gt;\n    &lt;start to=\"sqoop-process\"/&gt;\n\n\n    &lt;action name=\"sqoop-process\"&gt;\n        &lt;sqoop xmlns=\"uri:oozie:sqoop-action:0.2\"&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n            &lt;prepare&gt;\n                &lt;delete path=\"${myRoot}/mainjob/jb/sqoop\"/&gt;\n                &lt;mkdir path=\"${myRoot}/mainjob/jb\"/&gt;\n            &lt;/prepare&gt;\n            &lt;configuration&gt;\n                &lt;property&gt;\n                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;\n                    &lt;value&gt;${queueName}&lt;/value&gt;\n                &lt;/property&gt;\n            &lt;/configuration&gt;\n\t    &lt;command&gt;import --connect jdbc:mysql://localhost/db --table ${tableName} --username usr --password sqp --target-dir ${myRoot}/mainjob/jb/sqoop -m 1 &lt;/command&gt;\n        &lt;/sqoop&gt;\n        &lt;ok to=\"end\"/&gt;\n        &lt;error to=\"fail\"/&gt;\n    &lt;/action&gt;\n \n&lt;kill name=\"fail\"&gt;\n        &lt;message&gt;Job failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n    &lt;/kill&gt;\n\n\n    &lt;end name=\"end\"/&gt;\n&lt;/workflow-app&gt;\n\n\n</pre><p>Stderr:</p><p>Nov 26, 2016 6:42:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register</p><pre>INFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver as a provider class\nNov 26, 2016 6:42:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\nNov 26, 2016 6:42:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\nINFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices as a root resource class\nNov 26, 2016 6:42:19 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\nINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\nNov 26, 2016 6:42:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\nNov 26, 2016 6:42:19 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\nNov 26, 2016 6:42:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\nINFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices to GuiceManagedComponentProvider with the scope \"PerRequest\"\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.mapreduce.v2.app.MRAppMaster).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</pre><p>This is the common error for every job. Can any body please elaborate the solution. I have already done my search but found no luck. I am not much familiar with log4j</p>","tags":["Oozie","hadoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-28 03:17:19.0","id":68618,"title":"How GIT & Jenkins are related to Hadoop/Spark jobs?","body":"<p>Hi there,</p><p>I know Jenkins & GIt in general. But, I'm not aware of how Jenkins/GIT plays role in Hadoop projects..</p><p>Please let me know your information on this. Thanks in advance.</p><p>Regards,</p><p>Jee </p>","tags":["hadoop","hadoop-maintenance"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-28 10:38:05.0","id":68677,"title":"hortonworks sandbox 2.5 taking too much time to on virtual box","body":"<p>hortonworks sandbox taking too much time to start (Assigned Ram 5 GB )and when the virtual box starting two OS version coming which one needs to be selected . And its getting stuck after starting container .</p><p><img src=\"/storage/attachments/9806-hortonworks.png\"></p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-28 13:24:00.0","id":68690,"title":"pig nested for each clarification","body":"<p>I am new to pig and any input is really appreciated</p><p>source file:</p><pre>Exchange,Symbol,date,open,high,low,close,volume,adj_close\nNASDAQ,JDAS,2010-01-29,26.91,27.53,26.02,26.21,883100,26.21\nNASDAQ,JDAS,2010-01-28,29.86,27.97,26.84,26.88,1272600,26.88\nNASDAQ,JDAS,2010-01-27,27.48,27.93,27.20,27.68,560100,27.68\nICICI,JDAS,2010-02-08,25.41,26.59,25.15,26.46,488900,26.46\nICICI,JDAS,2010-01-29,26.91,27.53,26.02,26.21,883100,26.21\nICICI,JDAS,2010-01-28,27.86,27.97,26.84,26.88,1272600,26.88\nNASDAQ,JDAS,2010-01-29,26.91,27.53,26.02,26.21,883100,26.21\nNASDAQ,JDAS,2010-01-28,27.86,27.97,26.84,26.88,1272600,26.88\nNASDAQ,JDAS,2010-01-27,27.48,27.93,27.20,27.68,560100,27.68\nNASDAQ,JDAS,2010-02-08,25.41,26.59,25.15,26.46,488900,26.46\nNASDAQ,JDAS,2010-02-05,25.42,25.84,24.94,25.49,1121700,25.49\nNASDAQ,JDAS,2010-02-04,26.53,26.61,25.46,25.46,574900,25.46\nNASDAQ,JDAS,2009-12-31,25.97,26.13,25.47,25.47,283600,25.47\nNASDAQ,JDAS,2009-12-30,25.74,26.25,25.61,26.05,236300,26.05\nNASDAQ,JDAS,2009-12-29,25.98,25.98,25.52,25.76,238600,25.76\nNASDAQ,JDAS,2009-11-30,23.39,23.65,22.78,23.48,522000,23.48\nNASDAQ,JDAS,2009-11-27,23.12,23.71,23.10,23.54,144900,23.54\nNASDAQ,JDAS,2009-11-25,23.96,24.00,23.59,23.82,220400,23.82\nNASDAQ,JOEZ,2010-01-29,1.68,1.69,1.60,1.60,158900,1.60\nNASDAQ,JOEZ,2010-01-28,1.64,1.70,1.61,1.62,250700,1.62\nNASDAQ,JOEZ,2010-01-27,1.73,1.76,1.63,1.64,329200,1.64\nNASDAQ,JOEZ,2010-01-26,1.70,1.76,1.66,1.70,509100,1.70\nNASDAQ,JOEZ,2010-01-25,1.64,1.68,1.60,1.68,169600,1.68\nNASDAQ,JOEZ,2010-02-08,1.80,2.04,1.76,1.93,1712200,1.93\nNASDAQ,JOEZ,2010-02-05,1.84,1.88,1.70,1.80,1044700,1.80\nNASDAQ,JOEZ,2010-02-04,1.96,1.97,1.74,1.88,3758600,1.88\nNASDAQ,JOEZ,2010-02-03,1.73,1.79,1.68,1.72,1211700,1.72\nNASDAQ,JOEZ,2010-02-02,1.59,1.72,1.51,1.70,909400,1.70\nNASDAQ,JOEZ,2009-07-15,1.00,1.05,0.75,0.81,1215200,0.81\nNASDAQ,JOEZ,2009-07-14,0.80,0.95,0.80,0.93,580000,0.93\nNASDAQ,JOEZ,2009-07-13,0.80,0.83,0.75,0.79,148100,0.79\nNASDAQ,JOEZ,2009-05-06,0.56,0.67,0.55,0.58,83800,0.58\nNASDAQ,JOEZ,2009-05-05,0.63,0.63,0.58,0.58,68700,0.58\nNASDAQ,JOEZ,2009-05-04,0.62,0.68,0.60,0.63,134400,0.63\n\n\n</pre>\n<pre>x = LOAD '/home/prime23/source.txt' using PigStorage(',') As (exchange:chararray, symbol:chararray, date:chararray, open:double, high:double, low:double, close:double, volume:long, adj_close:double);\n\n\nquery:-For each symbol get me all distinct exchanges:\n\ny = GROUP x by symbol;\nz1 = foreach y {\n     t = distinct x.exchange;\n     generate group, t;\n}\n\n</pre><p><strong>clarifications: </strong></p><p>1)Here we have two symbols(JOEZ,JDAS) so nested foreach will iterate for two times.Please correct me if i am wrong?</p><p>2)How to get schema of t relation.describe is not  working.</p><p>\n3)last statement is not clear:</p><p>y relation contains only(group,x) fields.How can we select t field which is not present in y relation.</p>","tags":["Pig"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 15:13:32.0","id":68714,"title":"start and stop HDC cloud controller from command line.","body":"<p>Can someone please point out where the web server is running for cloud controller? I would like to start and stop HDC cloud controller from command line.</p>","tags":["cloud-controller"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-27 01:45:49.0","id":68521,"title":"HDP 2.4 - Kerberos install issue","body":"<p>Hi All,</p><p>I'm trying to install Kerberos on HDP 2.4, and running into issues  while installing Kerberos Client on Ambari UI</p><p>here is the error snippet :</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/hook.py\", line 37, in &lt;module&gt;\n    AfterInstallHook().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/hook.py\", line 31, in hook\n    setup_hdp_symlinks()\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/shared_initialization.py\", line 44, in setup_hdp_symlinks\n    hdp_select.select_all(version)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/hdp_select.py\", line 122, in select_all\n    Execute(command, only_if = only_if_command)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 238, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.4.0.0-169 | tail -1`' returned 1. Traceback (most recent call last):\n  File \"/usr/bin/hdp-select\", line 379, in &lt;module&gt;\n    printVersions()\n  File \"/usr/bin/hdp-select\", line 236, in printVersions\n    result[tuple(map(int, versionRegex.split(f)))] = f\nValueError: invalid literal for int() with base 10: 'kafka'\nERROR: set command takes 2 parameters, instead of 1</pre><p>I've installed MIT KDC, and created the Principal,</p><p>i get the error when enabling Kerberos using Ambari.</p><p>Pls let me know if you have faced this issue.</p><p>Thanks!</p>","tags":["security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-28 11:46:14.0","id":68689,"title":"ListenHttp Apache NIfi generic basepath","body":"<p>I'm trying to use a more generic basepath in the listenHTTP processor\n are they any way to specify for this webservice to accept all call \nincoming to port 9090 or specify a more generic basepath  like data/:type/:date , in this case \nthis is possible how can I use this basepath on routeonAttribute ?</p>","tags":["nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-26 21:30:58.0","id":68497,"title":"Kafka -  Error while fetching metadata [{TopicMetadata for topic kafka1_topic ->  No partition metadata for topic kafka1_topic due to kafka.common.TopicAuthorizationException}]","body":"<p><a href=\"/storage/attachments/9779-screen-shot-2016-11-26-at-12938-pm.png\">screen-shot-2016-11-26-at-12938-pm.png</a>\n</p><p>Hi All,</p><p>On HDP 2.4 - I'm getting error on Kafka after enabling Ranger-Kafka plugin.</p><p>Here is what is done :</p><p>1) Enabled Ranger-Kafka plugin (on Ranger UI)</p><p>2) Using Ranger, created user kafka1, and gave it permission to publish to topic *kafka1* (attached screenshot)</p><p>3) Created topic kafka1_topic, and published messages</p><p>Error obtained -&gt;</p><p>Error while fetching metadata [{TopicMetadata for topic kafka1_topic -&gt; \nNo partition metadata for topic kafka1_topic due to kafka.common.TopicAuthorizationException}] for topic [kafka1_topic]: class kafka.common.TopicAuthorizationException  (kafka.producer.BrokerPartitionInfo)</p><p>Zookeper nodes permission :</p><p>[kafka1@sandbox ~]$ ls -lrt /hadoop/zookeeper/\ntotal 8\n-rw-r--r-- 1 root  root  1 2016-03-14 14:17 myid\ndrwxr-xr-x 2 zookeeper hadoop 4096 2016-11-26 19:44 version-2</p><p>kafka1 User permission (on hdfs) :</p><p>[kafka1@sandbox ~]$ hadoop fs -ls /user/\nFound 11 items\ndrwxrwx---  - ambari-qa hdfs  0 2016-03-14 14:18 /user/ambari-qa\ndrwxr-xr-x  - hcat  hdfs  0 2016-03-14 14:23 /user/hcat\ndrwxr-xr-x  - hive  hdfs  0 2016-03-14 14:23 /user/hive\ndrwxr-xr-x  - kafka1  hdfs  0 2016-11-26 20:31 /user/kafka1\ndrwxr-xr-x  - kafka2  hdfs  0 2016-11-26 20:32 /user/kafka2</p><p>Any ideas on what needs to be changed, to enable this ?</p><p>Appreciate your quick response.</p><p>Pls note -&gt; this is a fresh HDP2.4 instance, and Kerberos is not yet enabled.</p>","tags":["Kafka","Ranger"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 08:48:18.0","id":68660,"title":"Hi. I have installed Metron 0.3.0 and have added squid telemetry and enrichment to it. Why am I  getting a null when trying to retrieve 'whois' value from 'enrichment' table.","body":"<p>The squid topology has gotten loaded and indexes were getting generated in elasticsearch. However, when I added the squid enrichment config as specified in https://cwiki.apache.org/confluence/display/METRON/Enriching+Telemetry+Events, I am getting an error </p><p>o.a.h.h.c.RpcRetryingCaller [INFO] Call exception, tries=34, retries=35, started=592066 ms ago, cancelled=false, msg=row 'hrP��^W�F�^@���`!^@^Ewhois^@  cisco.com' on table 'enrichment' at null</p><p>Please suggest how I can correct this. Thankyou</p>","tags":["Metron"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-11-28 15:28:11.0","id":68695,"title":"Mapper tasks get containers and then suddenly gets KILLED.","body":"<p>We have a 39-nodes cluster and we have been observing a peculiar behavior in MapReduce jobs. The mappers get containers and move to RUNNING STATE and then immediately get KILLED (although there is no other process which is sending the KILL signal). Following are logs from node manager from one of the nodes -</p><pre>2016-11-28 16:26:02,521 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000002 transitioned from LOCALIZED to RUNNING\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000004\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000002\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000003\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:02,646 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19599 for container-id container_e40_1479381018014_95355_01_000004: 2\n4.6 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:02,681 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19597 for container-id container_e40_1479381018014_95355_01_000005: 30.0 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:02,717 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19600 for container-id container_e40_1479381018014_95355_01_000002: 39.3 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:02,760 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19598 for container-id container_e40_1479381018014_95355_01_000003: 50.3 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:03,698 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\n2016-11-28 16:26:03,699 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\net:s016-11-28 16:26:03,700 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,700 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:03,701 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:03,701 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000005 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,701 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(371)) - Cleaning up container container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:03,702 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\n2016-11-28 16:26:03,702 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\n2016-11-28 16:26:03,702 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,703 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000004\n2016-11-28 16:26:03,703 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000004\n2016-11-28 16:26:03,703 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,704 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000002\n2016-11-28 16:26:03,704 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000002\n2016-11-28 16:26:03,704 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,704 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000003\n2016-11-28 16:26:03,704 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000003\n2016-11-28 16:26:03,706 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:launchContainer(381)) - Exit code from container container_e40_1479381018014_95355_01_000005 is : 143\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000004 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000002 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000003 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000005 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL\n2016-11-28 16:26:03,716 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(371)) - Cleaning up container container_e40_1479381018014_95355_01_000004</pre>","tags":["yarn-scheduler"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-28 06:11:05.0","id":68636,"title":"could not list the files in remote HDFS cluster","body":"<p><strong>More info. on the cluster:</strong></p><p>we have cluster A & B.</p><p>cluster A - HDP 2.3 - not kerberised</p><p>cluster B - HDP 2.4 - kerberised</p><p><strong>Action: </strong></p><ul><li>we are trying to list the directories in cluster A by issuing the command from one of the client machine of cluster B.</li></ul><p style=\"margin-left: 40px;\">\"<em>hadoop fs -Dfs.defaultFS=&lt;A namenode IP:port&gt; -ls / </em>\" </p><p style=\"margin-left: 40px;\">OR </p><p style=\"margin-left: 40px;\">\"<em>hadoop fs -ls hdfs://&lt;A namenode IP:port&gt;/</em>\" </p><ul><li>we are able to list the directories of cluster B from cluster A client machine.</li></ul><p><strong>Expectation Result:</strong></p><p>should list the directories from cluster A HDFS</p><p><strong>Actual Result:</strong></p><p>we are getting the list of files in the current B instead of cluster A. Reverse listing is working fine!</p><p><strong>Request:</strong></p><p>Please provide some pointers on what could have been the issue to enable -D option or what actually is stopping us to list the directories in the remote cluster.</p>","tags":["hadoopkerberosname","hadoop-core","HDFS","hadoop-ecosystem","hdfs-permissions"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-28 16:17:08.0","id":68739,"title":"Unable to launch sandbox 2.5","body":"<p>I get stuck when i launch HortonWorks Inside VirtualBox. the host is a CentOS 7 system. I installed the 5.1.6 virtualbox version. execution stop and block at the point: Starting SandboxContainer.</p><p> I have tried VM Ware as well but it is also showing the same behavior. </p><p>I am running Windows8 with 8Gb of RAM. For the VM i reduced the allocated 8Gb RAM to 6Gb as wel</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-28 10:33:37.0","id":68683,"title":"NiFi scheduling query","body":"<p>Hi,</p><p>I am trying to start a NiFi flow by a HTTP call using HandleHttpRequest</p><p>Once the flow being invoked, I want it to keep running until stopped</p><p>I have a InvokeHttp processor in the flow..where I am putting Scheduling - Run Schedule of 1 sec, Time driven</p><p>But this processor is not repeating the task. any suggestions.</p><p>Thanks,</p><p>Avijeet</p>","tags":["Nifi","scheduling"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-29 16:57:00.0","id":68997,"title":"Metron tutorial 1 Storm topology empty","body":"<p>Hi Metron gurus, I am following this tutorial 1 from metron wiki to create a squid topology</p><p>https://cwiki.apache.org/confluence/display/METRON/2016/04/25/Metron+Tutorial+-+Fundamentals+Part+1%3A+Creating+a+New+Telemetry</p><p>The Storm topology from Storm UI is empty without spout or bolt</p><p><img src=\"/storage/attachments/9886-screen-shot-2016-11-29-at-115042-am.png\"></p><p>The json looks ok</p><pre>[root@qwang-metron3 ~]# cat /usr/metron/0.3.0/config/zookeeper/parsers/squid.json\n{\n  \"parserClassName\": \"org.apache.metron.parsers.GrokParser\",\n  \"sensorTopic\": \"squid\",\n  \"parserConfig\": {\n    \"grokPath\": \"/patterns/squid\",\n    \"patternLabel\": \"SQUID_DELIMITED\",\n    \"timestampField\": \"timestamp\"\n  },\n  \"fieldTransformations\" : [\n    {\n      \"transformation\" : \"STELLAR\"\n    ,\"output\" : [ \"full_hostname\", \"domain_without_subdomains\" ]\n    ,\"config\" : {\n      \"full_hostname\" : \"URL_TO_HOST(url)\"\n      ,\"domain_without_subdomains\" : \"DOMAIN_REMOVE_SUBDOMAINS(full_hostname)\"\n                }\n    }\n                           ]\n\n</pre><p>Also the kafks topics are correctly created</p><pre>[root@qwang-metron3 ~]# /usr/hdp/current/kafka-broker/bin/kafka-topics.sh --zookeeper $ZOOKEEPER --list\nambari_kafka_service_check\nbro\nenrichments\nenrichments_error\nindexing\nparser_error\nparser_invalid\nsnort\nsquid\nthreatintel_error\nyaf\n</pre><p>The config in Zookeeper looks fine as well</p><pre>[root@qwang-metron3 ~]# /usr/metron/0.3.0/bin/zk_load_configs.sh -m DUMP -z $ZOOKEEPER\nlog4j:WARN No appenders could be found for logger (org.apache.curator.framework.imps.CuratorFrameworkImpl).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nGLOBAL Config: global\n\n\n{\n\"es.clustername\": \"metron\",\n\"es.ip\": \"qwang-metron3.field.hortonworks.com:9300\",\n\"es.date.format\": \"yyyy.MM.dd.HH\",\n\"fieldValidations\" : [{\n\"input\" : [ \"ip_src_addr\", \"ip_dst_addr\" ],\n\"validation\" : \"IP\",\n\"config\" : {\"type\" : \"IPV4\"}\n}]\n}\n\n\nPARSER Config: websphere\n{\n  \"parserClassName\":\"org.apache.metron.parsers.websphere.GrokWebSphereParser\",\n  \"sensorTopic\":\"websphere\",\n  \"parserConfig\":\n  {\n    \"grokPath\":\"/patterns/websphere\",\n    \"patternLabel\":\"WEBSPHERE\",\n    \"timestampField\":\"timestamp_string\",\n    \"dateFormat\":\"yyyy MMM dd HH:mm:ss\"\n  }\n}\nPARSER Config: jsonMap\n{\n  \"parserClassName\":\"org.apache.metron.parsers.json.JSONMapParser\",\n  \"sensorTopic\":\"jsonMap\"\n}\nPARSER Config: squid\n{\n  \"parserClassName\": \"org.apache.metron.parsers.GrokParser\",\n  \"sensorTopic\": \"squid\",\n  \"parserConfig\": {\n    \"grokPath\": \"/patterns/squid\",\n    \"patternLabel\": \"SQUID_DELIMITED\",\n    \"timestampField\": \"timestamp\"\n  },\n  \"fieldTransformations\" : [\n    {\n      \"transformation\" : \"STELLAR\"\n    ,\"output\" : [ \"full_hostname\", \"domain_without_subdomains\" ]\n    ,\"config\" : {\n      \"full_hostname\" : \"URL_TO_HOST(url)\"\n      ,\"domain_without_subdomains\" : \"DOMAIN_REMOVE_SUBDOMAINS(full_hostname)\"\n                }\n    }\n                           ]\n}\nPARSER Config: asa\n{\n  \"parserClassName\": \"org.apache.metron.parsers.asa.BasicAsaParser\",\n  \"sensorTopic\": \"asa\",\n  \"parserConfig\": {\n    \"deviceTimeZone\": \"UTC\"\n  }\n}\n\n\nPARSER Config: bro\n{\n  \"parserClassName\":\"org.apache.metron.parsers.bro.BasicBroParser\",\n  \"sensorTopic\":\"bro\",\n  \"parserConfig\": {}\n}\nPARSER Config: snort\n{\n  \"parserClassName\":\"org.apache.metron.parsers.snort.BasicSnortParser\",\n  \"sensorTopic\":\"snort\",\n  \"parserConfig\": {}\n}\nPARSER Config: yaf\n{\n  \"parserClassName\":\"org.apache.metron.parsers.GrokParser\",\n  \"sensorTopic\":\"yaf\",\n  \"fieldTransformations\" : [\n                    {\n                      \"input\" : \"protocol\"\n                     ,\"transformation\": \"IP_PROTOCOL\"\n                    }\n                    ],\n  \"parserConfig\":\n  {\n    \"grokPath\":\"/patterns/yaf\",\n    \"patternLabel\":\"YAF_DELIMITED\",\n    \"timestampField\":\"start_time\",\n    \"timeFields\": [\"start_time\", \"end_time\"],\n    \"dateFormat\":\"yyyy-MM-dd HH:mm:ss.S\"\n  }\n}\nENRICHMENT Config: websphere\n{\n  \"index\": \"websphere\",\n  \"batchSize\": 5,\n  \"enrichment\": {\n    \"fieldMap\": {\n      \"geo\": [\n        \"ip_src_addr\"\n      ],\n      \"host\": [\n        \"ip_src_addr\"\n      ]\n    },\n  \"fieldToTypeMap\": {\n      \"ip_src_addr\": [\n        \"playful_classification\"\n      ]\n    }\n  }\n}\n\n\n\n\nENRICHMENT Config: asa\n{\n    \"index\": \"asa\",\n    \"batchSize\": 5,\n    \"enrichment\" : {\n        \"fieldMap\": {\n            \"geo\": [\"ip_dst_addr\", \"ip_src_addr\"]\n        }\n    }\n}\n\n\n\n\nENRICHMENT Config: bro\n{\n  \"index\": \"bro\",\n  \"batchSize\": 5,\n  \"enrichment\" : {\n    \"fieldMap\": {\n      \"geo\": [\"ip_dst_addr\", \"ip_src_addr\"],\n      \"host\": [\"host\"]\n    }\n  },\n  \"threatIntel\": {\n    \"fieldMap\": {\n      \"hbaseThreatIntel\": [\"ip_src_addr\", \"ip_dst_addr\"]\n    },\n    \"fieldToTypeMap\": {\n      \"ip_src_addr\" : [\"malicious_ip\"],\n      \"ip_dst_addr\" : [\"malicious_ip\"]\n    }\n  }\n}\n\n\n\n\nENRICHMENT Config: snort\n{\n  \"index\": \"snort\",\n  \"batchSize\": 1,\n  \"enrichment\" : {\n    \"fieldMap\":\n      {\n      \"geo\": [\"ip_dst_addr\", \"ip_src_addr\"],\n      \"host\": [\"host\"]\n    }\n  },\n  \"threatIntel\" : {\n    \"fieldMap\":\n      {\n      \"hbaseThreatIntel\": [\"ip_src_addr\", \"ip_dst_addr\"]\n    },\n    \"fieldToTypeMap\":\n      {\n      \"ip_src_addr\" : [\"malicious_ip\"],\n      \"ip_dst_addr\" : [\"malicious_ip\"]\n    },\n    \"triageConfig\" : {\n      \"riskLevelRules\" : {\n        \"not(IN_SUBNET(ip_dst_addr, '192.168.0.0/24'))\" : 10\n      },\n      \"aggregator\" : \"MAX\"\n    }\n  }\n}\n\n\nENRICHMENT Config: yaf\n{\n  \"index\": \"yaf\",\n  \"batchSize\": 5,\n  \"enrichment\" : {\n    \"fieldMap\":\n      {\n      \"geo\": [\"ip_dst_addr\", \"ip_src_addr\"],\n      \"host\": [\"host\"]\n    }\n  },\n  \"threatIntel\": {\n    \"fieldMap\":\n      {\n      \"hbaseThreatIntel\": [\"ip_src_addr\", \"ip_dst_addr\"]\n    },\n    \"fieldToTypeMap\":\n      {\n      \"ip_src_addr\" : [\"malicious_ip\"],\n      \"ip_dst_addr\" : [\"malicious_ip\"]\n    }\n  }\n}\n</pre><p>here is what the accesslog looks like</p><pre>[root@qwang-metron2 ~]# tail /var/log/squid/access.log\n1480439014.080     59 ::1 TCP_MISS/200 183045 GET http://www.brightsideofthesun.com/2016/6/25/12027078/anatomy-of-a-deal-phoenix-suns-pick-bender-chriss - HIER_DIRECT/151.101.53.52 text/html\n1480439016.029    636 ::1 TCP_MISS/200 391870 GET http://www.aliexpress.com/af/shoes.html - HIER_DIRECT/23.56.117.241 text/html\n1480439017.544   1485 ::1 TCP_MISS/200 179097 GET http://www.pravda.ru/science/ - HIER_DIRECT/185.103.135.90 text/html\n1480439018.085    528 ::1 TCP_MISS/200 392358 GET http://www.aliexpress.com/af/shoes.html - HIER_DIRECT/23.56.117.241 text/html\n1480439018.878    749 ::1 TCP_MISS/200 102538 GET https://tfl.gov.uk/plan-a-journey/ - HIER_DIRECT/54.72.58.0 text/html\n1480439019.029    133 ::1 TCP_MISS/302 1907 GET http://www.autonews.com/article/20151115/RETAIL04/311169971/toyota-fj-cruiser-is-scarce-hot-and-high-priced - HIER_DIRECT/52.203.71.197 text/html\n1480439019.788    748 ::1 TCP_MISS/200 102538 GET https://tfl.gov.uk/plan-a-journey/ - HIER_DIRECT/54.72.58.0 text/html\n1480439019.800      0 ::1 TCP_MEM_HIT/301 701 GET https://www.microsoftstore.com/ - HIER_NONE/- text/html\n1480439020.722    913 ::1 TCP_MISS/200 442496 GET https://www.facebook.com/Africa-Bike-Week-1550200608567001/ - HIER_DIRECT/31.13.70.36 text/html\n1480439020.810     74 ::1 TCP_MISS/200 183034 GET http://www.brightsideofthesun.com/2016/6/25/12027078/anatomy-of-a-deal-phoenix-suns-pick-bender-chriss - HIER_DIRECT/151.101.53.52 text/html\n</pre><p>And cmd to pump message to kafks</p><pre>tail /var/log/squid/access.log | /usr/hdp/current/kafka-broker/bin/kafka-console-producer.sh --broker-list $BROKERLIST --topic squid</pre><p>Is there anything else I should check?</p>","tags":["Metron"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-11-27 08:56:18.0","id":68559,"title":"Groovy ExcuteScript example ?","body":"<p>Hello forum,</p><p>I got the below example from funnifi blog, thanks to Matt :-)</p><p>I have 2 questions on this ?</p><p>1- Groovy supposed to be Java like language, classes and methods... but I can't see this in the below example ? no class or method declaration ?</p><p>2- If I want to add conditions to the code and modify the output based on this.</p><p>e.g. </p><p>if field 3 (column C) starts with 3300, replace 33 with 22</p><p>if field 3 (column C) starts with 0, replace this 0 with 00212</p><p>how to achieve this ?</p><p>Input file:</p><pre>a1|b1|c1|d1\na2|b2|c2|d2\na3|b3|c3|d3</pre><p>Desired output:</p><pre>b1 c1\nb2 c2\nb3 c3</pre><p>script:</p><pre>import java.nio.charset.StandardCharsets\n\ndef flowFile = session.get()\nif(!flowFile) return\n\nflowFile = session.write(flowFile, {inputStream, outputStream -&gt;\n   inputStream.eachLine { line -&gt;\n   a = line.tokenize('|')\n   outputStream.write(\"${a[1]} ${a[2]}\\n\".toString().getBytes(StandardCharsets.UTF_8))\n   }\n} as StreamCallback)\n\nsession.transfer(flowFile, REL_SUCCESS)</pre>","tags":["nifi-processor","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 11:20:06.0","id":68684,"title":"Is it possible to limit the number of executor allocated in a queue?  And how can I manage this in Ambari? Thanks!","body":"","tags":["hdp-2.5.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-28 08:32:17.0","id":68659,"title":"External ORC Table : count not Matches","body":"<p>I have a big orc table :</p><p>1) i have deleted the hdfs location </p><p>2) delete from table_name where dt=201611</p><p>3) insert into table_name partition(dt) select * from table2 where dt=201611;</p><p>then ORC count was double , </p><p>i have tried with a) delete from table then also its showing same count </p><p>ANALYZE TABLE table_name PARTITION(dt) COMPUTE STATISTICS;</p><p>1/ does this command update hive metastore ?</p><p>2/ is this hive metastore issue ? if yes then how to solve this?</p><p>3/ msck repair table table_name  :ERROR\n\"FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask\" </p>","tags":["Hive","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-28 05:31:44.0","id":68634,"title":"Apache Ranger : Issues in Syncing the UI policy to beeline and setting up of metastore plugin","body":"<p>I am trying to implement  the following : </p><p>1) Ranger Hive plugin  </p><p>2) Set up ranger plugin for metastore </p><p>using HDP 2.5, ranger 0.6.0</p><p>Below are some of the settings in hive-site.xml :</p><p>hive.metastore.pre.event.listeners=org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener\n  hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider</p><p>hive.users.in.admin.role=admin,root</p><p>hive.server2.enable.doAs=true\nhive.security.authorization.enabled=true\n  hive.security.authorization.manager=org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizerFactory\n  hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator\n  hive.conf.restricted.list=hive.security.authorization.enabled,hive.security.authorization.manager,hive.security.authenticator.manager</p><p>Following are the questions that I have for the above implementation : </p><p>1) When I create a policy in the Ranger Policy Manger UI and try to test it out from beeline, it is not working. Are the above settings in the hive-site.xml correct?</p><p>2) If I create a new role and set it to user from hive CLI, will i be able to see that in the Ranger Policy Manager UI -&gt;HiveService-&gt;SERVICE_NAME</p><p>3) Is HDFS plugin mandatory for the above set-up? i.e can I set policies from UI and hive CLI without using HDFS policy and HDFS plugin setup? </p><p>4) is Solr installation mandatory for setting up the ranger-hive-plugin?(in my current set-up, i dont have the solr setup) </p>","tags":["hive-metastore","ranger-hive-plugin"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-28 14:04:29.0","id":68707,"title":"HIVE exception while reading JSON UTF8 emoji,HIVE exception - Invalid UTF-8 start byte - while querying JSON","body":"<p>Hi,</p><p>After creating an external table on top of a JSON file that contains some UTF8 emojis, I try to query data and get exception:</p><p>Caused by: org.apache.hadoop.hive.serde2.SerDeException: org.codehaus.jackson.JsonParseException: Invalid UTF-8 start byte 0x8d at [Source: java.io.ByteArrayInputStream@210112ff; line: 1, column: 1271] at org.apache.hive.hcatalog.data.JsonSerDe.deserialize(JsonSerDe.java:169) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:136) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:100) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:492) ... 17 more Caused by: org.codehaus.jackson.JsonParseException: Invalid UTF-8 start byte 0x8d at [Source: java.io.ByteArrayInputStream@210112ff; line: 1, column: 1271] at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1432) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:385) at org.codehaus.jackson.impl.Utf8StreamParser._reportInvalidInitial(Utf8StreamParser.java:2796) at org.codehaus.jackson.impl.Utf8StreamParser._reportInvalidChar(Utf8StreamParser.java:2790) at org.codehaus.jackson.impl.Utf8StreamParser._skipString(Utf8StreamParser.java:2046) at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:435) at org.apache.hive.hcatalog.data.JsonSerDe.extractCurrentField(JsonSerDe.java:365) at org.apache.hive.hcatalog.data.JsonSerDe.populateRecord(JsonSerDe.java:208) at org.apache.hive.hcatalog.data.JsonSerDe.deserialize(JsonSerDe.java:164) ... 20 more</p><p>Does it seem thar HIVE cannot read 4 octet UTF8 emojis?</p><p>They seem to be perfectly valid emojis. Example of one can be fond here: http://apps.timwhitlock.info/unicode/inspect?s=%F0%9F%91%AA</p><p>Some table details:</p><p>ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs://location' TBLPROPERTIES ( 'COLUMN_STATS_ACCURATE'='false', 'numFiles'='0', 'numRows'='-1', 'rawDataSize'='-1', 'totalSize'='0', 'transient_lastDdlTime'='1480340392')</p><p>Help most appreciated</p>,<p>Hi,</p><p>After creating external table on top of a text JSON file, I try to query it then HIVE throws exception: Invalid UTF-8 start byte...Looks like HIVE cannot read 4 octet emojis for example, http://apps.timwhitlock.info/unicode/inspect?s=%F0%9F%91%AA, which seem to be valid UTF-8 code points.</p><p>Some details:</p><p>ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION '/source' TBLPROPERTIES ( 'COLUMN_STATS_ACCURATE'='false', 'numFiles'='0', 'numRows'='-1', 'rawDataSize'='-1', 'totalSize'='0', 'transient_lastDdlTime'='1480340392')</p><p>Help appreciated</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-28 11:22:19.0","id":68685,"title":"​Ambari showing old hardware specs for agent nodes","body":"<p>Ambari showing old hardware specs for agent nodes\n\nA customer has HDP 2.5 and Ambari 2.4.0.1. They have a number of slave virtual machines and each one of them initially had 6 CPUs and 32GB of RAM. At some point they changed the hardware specs of the VMs to increase bot CPU and RAM for each node.\n\nThe problem is that Ambari still shows me the old specs for each node. I don't know how exactly they went about changing the specs, and whether the machines got shutdown in the process or not. My first thought was to restart ambari-agent on each node, but this is a production environment, so I wanted to first ask and get a confirmation that (1) this will indeed refresh the hardware specs seen in Ambari and (2) will not interfere with the various Sqoop jobs that are currently importing data.\n\nI can't seem to find a definitive answer in the documentation, so any comments will be appreciated.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-28 13:10:53.0","id":68627,"title":"Hana data Preview not working due to HANAESSQLCONTEXT","body":"<p>Hi,</p><p>We have a requirement to view Hive Tables from Hana Studio. We have integrated Hadoop and Hana using Spark Controller.</p><p></p><p>Hana SPS10</p><p>Spark 1.4.1.2.3</p><p>Spark Controller 1.5 Patch 0</p><p>Hive 1.2.1.2.3</p><p>Error log:</p><p></p><p>2016-11-28 01:48:56,328 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:56,328 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13269829910526; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000037; Session Id=&gt; ;\n2016-11-28 01:48:56,328 [DEBUG] OutBound Message\n2016-11-28 01:48:56,328 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13269833755725; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000037; Session Id=&gt; ;\n2016-11-28 01:48:56,332 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:56,332 [DEBUG] Message Type=&gt; OPEN_SESSION; Message Id=&gt; 13269836743241; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000039; Session Id=&gt; 1214684264082440213-9984082768356704312;\n2016-11-28 01:48:56,333 [INFO] All is well\n2016-11-28 01:48:56,334 [DEBUG] Received Request for OPEN_SESSION\n2016-11-28 01:48:56,399 [DEBUG] Relaying Message to Client SESSION_OPENED\n2016-11-28 01:48:56,399 [DEBUG] ChannelManager: New Session is added\n2016-11-28 01:48:56,399 [DEBUG] OutBound Message\n2016-11-28 01:48:56,399 [DEBUG] Message Type=&gt; SESSION_OPENED; Message Id=&gt; 13269901373970; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000039; Session Id=&gt; 1214684264082440213-9984082768356704312;\n2016-11-28 01:48:56,401 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:56,401 [DEBUG] Message Type=&gt; Check Capability; Message Id=&gt; 13269905966756; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003a; Session Id=&gt; 1214684264082440213-9984082768356704312;\n2016-11-28 01:48:56,402 [DEBUG] OutBound Message\n2016-11-28 01:48:56,402 [DEBUG] Message Type=&gt; Check Capability  Reply; Message Id=&gt; 13269907137259; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003a; Session Id=&gt; 1214684264082440213-9984082768356704312;\n2016-11-28 01:48:56,403 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:56,404 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13269908639555; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003b; Session Id=&gt; ;\n2016-11-28 01:48:56,404 [DEBUG] OutBound Message\n2016-11-28 01:48:56,404 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13269909405868; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003b; Session Id=&gt; ;\n2016-11-28 01:48:56,407 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:56,407 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13269911548737; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003c; Session Id=&gt; ;\n2016-11-28 01:48:56,407 [DEBUG] OutBound Message\n2016-11-28 01:48:56,407 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13269912736072; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003c; Session Id=&gt; ;\n2016-11-28 01:48:56,409 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:56,409 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13269913840816; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003d; Session Id=&gt; ;\n2016-11-28 01:48:56,410 [DEBUG] OutBound Message\n2016-11-28 01:48:56,410 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13269915159089; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003d; Session Id=&gt; ;\n2016-11-28 01:48:56,411 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:56,411 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13269916112645; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003e; Session Id=&gt; ;\n2016-11-28 01:48:56,412 [DEBUG] OutBound Message\n2016-11-28 01:48:56,412 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13269917095965; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003e; Session Id=&gt; ;\n2016-11-28 01:48:57,980 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,980 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271485251605; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003f; Session Id=&gt; ;\n2016-11-28 01:48:57,981 [DEBUG] OutBound Message\n2016-11-28 01:48:57,981 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271486180329; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-96614000003f; Session Id=&gt; ;\n2016-11-28 01:48:57,983 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,983 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271487569948; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000040; Session Id=&gt; ;\n2016-11-28 01:48:57,983 [DEBUG] OutBound Message\n2016-11-28 01:48:57,983 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271488718645; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000040; Session Id=&gt; ;\n2016-11-28 01:48:57,985 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,986 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271490651541; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000041; Session Id=&gt; ;\n2016-11-28 01:48:57,986 [DEBUG] OutBound Message\n2016-11-28 01:48:57,986 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271491432434; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000041; Session Id=&gt; ;\n2016-11-28 01:48:57,987 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,987 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271492317204; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000042; Session Id=&gt; ;\n2016-11-28 01:48:57,987 [DEBUG] OutBound Message\n2016-11-28 01:48:57,987 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271492984265; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000042; Session Id=&gt; ;\n2016-11-28 01:48:57,988 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,989 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271493693186; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000043; Session Id=&gt; ;\n2016-11-28 01:48:57,989 [DEBUG] OutBound Message\n2016-11-28 01:48:57,989 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271494431455; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000043; Session Id=&gt; ;\n2016-11-28 01:48:57,992 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,992 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271497010296; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000044; Session Id=&gt; ;\n2016-11-28 01:48:57,992 [DEBUG] OutBound Message\n2016-11-28 01:48:57,992 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271497687212; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000044; Session Id=&gt; ;\n2016-11-28 01:48:57,993 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,993 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271498436671; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000045; Session Id=&gt; ;\n2016-11-28 01:48:57,993 [DEBUG] OutBound Message\n2016-11-28 01:48:57,994 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271499104972; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000045; Session Id=&gt; ;\n2016-11-28 01:48:57,994 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,995 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271499655166; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000046; Session Id=&gt; ;\n2016-11-28 01:48:57,995 [DEBUG] OutBound Message\n2016-11-28 01:48:57,995 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271500417615; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000046; Session Id=&gt; ;\n2016-11-28 01:48:57,996 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,996 [DEBUG] Message Type=&gt; Ping; Message Id=&gt; 13271501020191; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000047; Session Id=&gt; ;\n2016-11-28 01:48:57,996 [DEBUG] OutBound Message\n2016-11-28 01:48:57,996 [DEBUG] Message Type=&gt; Pong; Message Id=&gt; 13271501766202; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000047; Session Id=&gt; ;\n2016-11-28 01:48:57,998 [DEBUG] Inbound Message=&gt;\n2016-11-28 01:48:57,998 [DEBUG] Message Type=&gt; QUERY_EXECUTE; Message Id=&gt; 13271502700475; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000048; Session Id=&gt; 1214684264082440213-9984082768356704312;\n2016-11-28 01:48:57,999 [INFO] All is well\n2016-11-28 01:48:57,999 [DEBUG] Received Request for QUERY_EXECUTE\n2016-11-28 01:48:58,000 [DEBUG] Setting Driver Host as 10.39.20.216</p><p>2016-11-28 01:48:58,000 [DEBUG] &lt;--xml plan for query&gt;\n2016-11-28 01:49:00,069 [DEBUG] Limit 200\n HiveTableScan [name#0,dept#1,level#2], (MetastoreRelation default, emp, Some(def_emp)), None</p><p>2016-11-28 01:49:00,070 [DEBUG] Relaying Message to Client QUERY_ACCEPTED\n2016-11-28 01:49:00,071 [DEBUG] OutBound Message\n2016-11-28 01:49:00,071 [DEBUG] Message Type=&gt; QUERY_ACCEPTED; Message Id=&gt; 13273575178560; Request Id=&gt;10db6ce7-0ae4-0015-8a8e-966140000048; Session Id=&gt; 1214684264082440213-9984082768356704312;\n2016-11-28 01:49:00,331 [DEBUG] Limit 200\n Project [name#0,dept#1,level#2]\n  MetastoreRelation default, emp, Some(def_emp)</p><p>[Stage 0:&gt;  (0 + 0) / 2]\n2016-11-28 01:49:01,410 [DEBUG] Terminating Orchestrator Instance for OPEN_SESSION1444506341\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]\n[Stage 0:&gt;  (0 + 0) / 2]</p><p>A successful connection would show :</p><p></p><p>INFO HanaESSQLContext: SELECT\n`spark_demo_employee`.`name`, `spark_demo_employee`.`dept`,\n`spark_demo_employee`.`numb` FROM `default`.`employee`\n`spark_demo_employee`  LIMIT 200</p><p>I guess the issue is with HanaESSQLContext. can anyone help out here.</p>","tags":["Spark","Hive","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-28 09:05:38.0","id":68647,"title":"uploadtable tab not found in hive view","body":"<p>In hive view , i am not able to see the \"uploadtable\" tab</p>","tags":["Tez","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-28 12:29:59.0","id":68686,"title":"easiest way to remove hadoop logs","body":"<p>Hi,</p><p>Can I just delete rm -rf  * from some of the log folders such as /var/log/hive</p><p>16G     ./hive</p><p>Thanks,</p><p>Avijeet</p>","tags":["logs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-27 07:06:02.0","id":68525,"title":"HDP 2.4 Kafka error in publishing - Kerberized Cluster","body":"<p>hi, i've a kerberized HDP 2.4 cluster .. & running into issues in publishing to Kafka topic.</p><p>1) i've create topic - kafka1_topic1, </p><p>2) ran kinit for user kafka -&gt;</p><p>kinit kafka/sandbox.hortonworks.com@EXAMPLE.COM -kt /etc/security/keytabs/kafka.service.keytab</p><p>and trying to publish to the topic, </p><p>3) created topic</p><p>$KAFKA_HOME/bin/kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic kafka1_topic1 --partitions 1 --replication-factor 1\nWARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\nCreated topic \"kafka1_topic1\".</p><p>4) Publishing to the topic, and get error as shown below</p><p>-------------------------------------</p><p>[root@sandbox ~]# $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list sandbox.hortonworks.com:6667 --topic kafka1_topic1 --security-protocol SASL_PLAINTEXT\nhi there\n[2016-11-27 06:47:28,085] WARN Error while fetching metadata [{TopicMetadata for topic kafka1_topic1 -&gt; \nNo partition metadata for topic kafka1_topic1 due to kafka.common.LeaderNotAvailableException}] for topic [kafka1_topic1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\n[2016-11-27 06:47:28,127] WARN Error while fetching metadata [{TopicMetadata for topic kafka1_topic1 -&gt; \nNo partition metadata for topic kafka1_topic1 due to kafka.common.LeaderNotAvailableException}] for topic [kafka1_topic1]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\n[2016-11-27 06:47:28,128] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: kafka1_topic1 (kafka.producer.async.DefaultEventHandler)</p><p>----------------------------------------------</p><p>Any ideas on this ?</p><p>Thanks for your help in advance. </p>","tags":["Kafka"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-28 03:45:09.0","id":68607,"title":"CORS header 'Access-Control-Allow-Origin' missing exception invoking NiFi flow rest endpoint.","body":"<p>I have NiFi flow exposed as REST endpoint using ListenHTTP processor. I am able to access the REST endpoint using java client with no issues but when I tried accessing the same endpoint using web application (html/javascript) I am getting following exception.</p><p>Cross-Origin\n Request Blocked: The Same Origin Policy disallows reading the remote \nresource at &lt;REST end point&gt;. \n(Reason: CORS header 'Access-Control-Allow-Origin' missing).</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 15:28:08.0","id":68706,"title":"Yarn containers get KILLED automatically. MapReduce/Hive jobs fail","body":"<p>We have a 39-node HDP cluster. We have been observing a peculiar phenomena. Some MapReduce/Hive jobs are allotted containers - they transition to RUNNING state and then get automatically killed. Here are the logs from a nodemanager -</p><pre>2016-11-28 16:26:02,521 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000002 transitioned from LOCALIZED to RUNNING\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000004\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000002\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000003\n2016-11-28 16:26:02,613 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(375)) - Starting resource-monitoring for container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:02,646 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19599 for container-id container_e40_1479381018014_95355_01_000004: 2\n4.6 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:02,681 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19597 for container-id container_e40_1479381018014_95355_01_000005: 30.0 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:02,717 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19600 for container-id container_e40_1479381018014_95355_01_000002: 39.3 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:02,760 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 19598 for container-id container_e40_1479381018014_95355_01_000003: 50.3 MB of 4.5 GB physical memory used; 4.4 GB of 9.4 GB virtual memory used\n2016-11-28 16:26:03,698 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\n2016-11-28 16:26:03,699 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\net:s016-11-28 16:26:03,700 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,700 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:03,701 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:03,701 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000005 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,701 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(371)) - Cleaning up container container_e40_1479381018014_95355_01_000005\n2016-11-28 16:26:03,702 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\n2016-11-28 16:26:03,702 INFO  ipc.Server (Server.java:saslProcess(1441)) - Auth successful for appattempt_1479381018014_95355_000001 (auth:SIMPLE)\n2016-11-28 16:26:03,702 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,703 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000004\n2016-11-28 16:26:03,703 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000004\n2016-11-28 16:26:03,703 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,704 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000002\n2016-11-28 16:26:03,704 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000002\n2016-11-28 16:26:03,704 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(135)) - Authorization successful for appattempt_1479381018014_95355_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB\n2016-11-28 16:26:03,704 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(966)) - Stopping container with container Id: container_e40_1479381018014_95355_01_000003\n2016-11-28 16:26:03,704 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=bdsa_ingest  IP=172.23.35.43 OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1479381018014_95355   CONTAINERID=container_e40_1479381018014_95355_01_000003\n2016-11-28 16:26:03,706 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:launchContainer(381)) - Exit code from container container_e40_1479381018014_95355_01_000005 is : 143\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000004 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000002 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000003 transitioned from RUNNING to KILLING\n2016-11-28 16:26:03,716 INFO  container.ContainerImpl (ContainerImpl.java:handle(1136)) - Container container_e40_1479381018014_95355_01_000005 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL\n2016-11-28 16:26:03,716 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(371)) - Cleaning up container container_e40_1479381018014_95355_01_000004</pre>","tags":["yarn-container"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-27 11:12:40.0","id":68555,"title":"Cloudbreak error [Error occurred on 49 stack during the setup: Connection reset] when creating cluster on google cloud platform","body":"<p>In Cloudbreak version 1.61, I cannot create a cluster in GCP</p><p> I get the following error:</p><p>Failed to create cluster : stack creation failed, cannot create cluster</p><p>Infrastructure creation failed. Reason  error occurred on 48 stack during the setup: Connection reset</p><p>This is the corresponding stack trace in the Cloudbreak log:</p><p>cloudbreak_1   | 2016-11-27 10:52:31,985 [reactorDispatcher-20] prepareImage:84 ERROR c.s.c.c.g.GcpProvisionSetup - [owner:8fd6cda0-f2fa-4a4d-9793-3315af97f646] [type:springLog] [id:49] [name:slb-dev-v17] Error occurred on 49 stack during the setup: Connection reset\ncloudbreak_1   | java.net.SocketException: Connection reset\ncloudbreak_1   |        at java.net.SocketInputStream.read(SocketInputStream.java:209)\ncloudbreak_1   |        at java.net.SocketInputStream.read(SocketInputStream.java:141)\ncloudbreak_1   |        at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)\ncloudbreak_1   |        at sun.security.ssl.InputRecord.read(InputRecord.java:503)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387)\ncloudbreak_1   |        at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)\ncloudbreak_1   |        at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)\ncloudbreak_1   |        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1283)\ncloudbreak_1   |        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1258)\ncloudbreak_1   |        at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250)\ncloudbreak_1   |        at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77)\ncloudbreak_1   |        at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:965)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.TokenRequest.executeUnparsed(TokenRequest.java:283)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.TokenRequest.execute(TokenRequest.java:307)\ncloudbreak_1   |        at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.executeRefreshToken(GoogleCredential.java:384)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.Credential.intercept(Credential.java:217)\ncloudbreak_1   |        at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:859)\ncloudbreak_1   |        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419)\ncloudbreak_1   |        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352)\ncloudbreak_1   |        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469)\ncloudbreak_1   |        at com.sequenceiq.cloudbreak.cloud.gcp.GcpProvisionSetup.prepareImage(GcpProvisionSetup.java:56)\ncloudbreak_1   |        at com.sequenceiq.cloudbreak.cloud.handler.PrepareImageHandler.accept(PrepareImageHandler.java:46)\ncloudbreak_1   |        at com.sequenceiq.cloudbreak.cloud.handler.PrepareImageHandler.accept(PrepareImageHandler.java:21)\ncloudbreak_1   |        at sun.reflect.GeneratedMethodAccessor252.invoke(Unknown Source)\ncloudbreak_1   |        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\ncloudbreak_1   |        at java.lang.reflect.Method.invoke(Method.java:498)\ncloudbreak_1   |        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\ncloudbreak_1   |        at org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor.invoke(MethodBeforeAdviceInterceptor.java:52)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\ncloudbreak_1   |        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\ncloudbreak_1   |        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:208)\ncloudbreak_1   |        at com.sun.proxy.$Proxy207.accept(Unknown Source)\ncloudbreak_1   |        at reactor.bus.EventBus$3.accept(EventBus.java:317)\ncloudbreak_1   |        at reactor.bus.EventBus$3.accept(EventBus.java:310)\ncloudbreak_1   |        at reactor.bus.routing.ConsumerFilteringRouter.route(ConsumerFilteringRouter.java:72)\ncloudbreak_1   |        at reactor.bus.routing.TraceableDelegatingRouter.route(TraceableDelegatingRouter.java:51)\ncloudbreak_1   |        at reactor.bus.EventBus.accept(EventBus.java:591)\ncloudbreak_1   |        at reactor.bus.EventBus.accept(EventBus.java:63)\ncloudbreak_1   |        at reactor.core.dispatch.AbstractLifecycleDispatcher.route(AbstractLifecycleDispatcher.java:160)\ncloudbreak_1   |        at reactor.core.dispatch.MultiThreadDispatcher$MultiThreadTask.run(MultiThreadDispatcher.java:74)\ncloudbreak_1   |        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\ncloudbreak_1   |        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\ncloudbreak_1   |        at java.lang.Thread.run(Thread.java:745)\ncloudbreak_1   | 2016-11-27 10:52:31,985 [reactorDispatcher-20] accept:70 DEBUG c.s.c.c.f.Flow2Handler - [owner:8fd6cda0-f2fa-4a4d-9793-3315af97f646] [type:springLog] [id:49] [name:slb-dev-v17] flow control event arrived: key: PREPAREIMAGERESULT_ERROR, flowid: ac692d6c-20d5-4576-ba60-1053ec54cf53, payload: CloudPlatformResult{status=FAILED, statusReason='Error occurred on 49 stack during the setup: Connection reset', errorDetails=com.sequenceiq.cloudbreak.cloud.exception.CloudConnectorException: Error occurred on 49 stack during the setup: Connection reset, request=CloudPlatformRequest{cloudContext=CloudContext{id=49, name='slb-dev-v17', platform='StringType{value='GCP'}', owner='8fd6cda0-f2fa-4a4d-9793-3315af97f646'}, cloudCredential=com.sequenceiq.cloudbreak.cloud.model.CloudCredential@68a59f6}}\ncloudbreak_1   | 2016-11-27 10:52:32,019 [reactorDispatcher-20] execute:64 INFO  c.s.c.c.f.AbstractAction - [owner:8fd6cda0-f2fa-4a4d-9793-3315af97f646] [type:STACK] [id:49] [name:slb-dev-v17] Stack: 49, flow state: IMAGESETUP_STATE, phase: service, execution time 0 sec\ncloudbreak_1   | 2016-11-27 10:52:32,019 [reactorDispatcher-20] handleStackCreationFailure:176 ERROR c.s.c.c.f.s.p.a.StackCreationService - [owner:8fd6cda0-f2fa-4a4d-9793-3315af97f646] [type:STACK] [id:49] [name:slb-dev-v17] Error during stack creation flow:\ncloudbreak_1   | com.sequenceiq.cloudbreak.cloud.exception.CloudConnectorException: Error occurred on 49 stack during the setup: Connection reset\ncloudbreak_1   |        at com.sequenceiq.cloudbreak.cloud.gcp.GcpProvisionSetup.prepareImage(GcpProvisionSetup.java:85)\ncloudbreak_1   |        at com.sequenceiq.cloudbreak.cloud.handler.PrepareImageHandler.accept(PrepareImageHandler.java:46)\ncloudbreak_1   |        at com.sequenceiq.cloudbreak.cloud.handler.PrepareImageHandler.accept(PrepareImageHandler.java:21)\ncloudbreak_1   |        at sun.reflect.GeneratedMethodAccessor252.invoke(Unknown Source)\ncloudbreak_1   |        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\ncloudbreak_1   |        at java.lang.reflect.Method.invoke(Method.java:498)\ncloudbreak_1   |        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\ncloudbreak_1   |        at org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor.invoke(MethodBeforeAdviceInterceptor.java:52)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\ncloudbreak_1   |        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)\ncloudbreak_1   |        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\ncloudbreak_1   |        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:208)\ncloudbreak_1   |        at com.sun.proxy.$Proxy207.accept(Unknown Source)\ncloudbreak_1   |        at reactor.bus.EventBus$3.accept(EventBus.java:317)\ncloudbreak_1   |        at reactor.bus.EventBus$3.accept(EventBus.java:310)\ncloudbreak_1   |        at reactor.bus.routing.ConsumerFilteringRouter.route(ConsumerFilteringRouter.java:72)\ncloudbreak_1   |        at reactor.bus.routing.TraceableDelegatingRouter.route(TraceableDelegatingRouter.java:51)\ncloudbreak_1   |        at reactor.bus.EventBus.accept(EventBus.java:591)\ncloudbreak_1   |        at reactor.bus.EventBus.accept(EventBus.java:63)\ncloudbreak_1   |        at reactor.core.dispatch.AbstractLifecycleDispatcher.route(AbstractLifecycleDispatcher.java:160)\ncloudbreak_1   |        at reactor.core.dispatch.MultiThreadDispatcher$MultiThreadTask.run(MultiThreadDispatcher.java:74)\ncloudbreak_1   |        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\ncloudbreak_1   |        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\ncloudbreak_1   |        at java.lang.Thread.run(Thread.java:745)\ncloudbreak_1   | Caused by: java.net.SocketException: Connection reset\ncloudbreak_1   |        at java.net.SocketInputStream.read(SocketInputStream.java:209)\ncloudbreak_1   |        at java.net.SocketInputStream.read(SocketInputStream.java:141)\ncloudbreak_1   |        at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)\ncloudbreak_1   |        at sun.security.ssl.InputRecord.read(InputRecord.java:503)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403)\ncloudbreak_1   |        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387)\ncloudbreak_1   |        at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)\ncloudbreak_1   |        at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)\ncloudbreak_1   |        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1283)\ncloudbreak_1   |        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1258)\ncloudbreak_1   |        at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250)\ncloudbreak_1   |        at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77)\ncloudbreak_1   |        at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:965)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.TokenRequest.executeUnparsed(TokenRequest.java:283)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.TokenRequest.execute(TokenRequest.java:307)\ncloudbreak_1   |        at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.executeRefreshToken(GoogleCredential.java:384)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489)\ncloudbreak_1   |        at com.google.api.client.auth.oauth2.Credential.intercept(Credential.java:217)\ncloudbreak_1   |        at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:859)\ncloudbreak_1   |        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419)\ncloudbreak_1   |        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352)\ncloudbreak_1   |        at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469)\ncloudbreak_1   |        at com.sequenceiq.cloudbreak.cloud.gcp.GcpProvisionSetup.prepareImage(GcpProvisionSetup.java:56)\ncloudbreak_1   |        ... 25 common frames omitted\ncloudbreak_1   | 2016-11-27 10:52:32,020 [reactorDispatcher-20] fireEventAndLog:24 DEBUG c.s.c.c.f.s.FlowMessageService - [owner:8fd6cda0-f2fa-4a4d-9793-3315af97f646] [type:STACK] [id:49] [name:slb-dev-v17] STACK_INFRASTRUCTURE_CREATE_FAILED [STACK_FLOW_STEP].\ncloudbreak_1   | 2016-11-27 10:52:32,020 [reactorDispatcher-20] fireCloudbreakEvent:51 INFO  c.s.c.s.e.DefaultCloudbreakEventService - [owner:8fd6cda0-f2fa-4a4d-9793-3315af97f646] [type:STACK] [id:49] [name:slb-dev-v17] Firing Cloudbreak event: CloudbreakEventData{entityId=49, eventType='UPDATE_IN_PROGRESS', eventMessage='Infrastructure creation failed. Reason: Error occurred on 49 stack during the setup: Connection reset'}\ncloudbreak_1   | 2016-11-27 10:52:32,020 [reactorDispatcher-20] handleFailure:212 DEBUG c.s.c.c.f.s.p.a.StackCreationService - [owner:8fd6cda0-f2fa-4a4d-9793-3315af97f646] [type:STACK] [id:49] [name:slb-dev-v17] Nothing to do. OnFailureAction DO_NOTHING\ncloudbreak_1   | 2016-11-27 10:52:32,020 [reactorDispatcher-18] accept:33 INFO  c.s.c.s.e.CloudbreakEventHandler - [owner:undefined] [type:CLOUDBREAKEVENTDATA] [id:undefined] [name:undefined] Handling cloudbreak event: Event{id=null, headers=null, replyTo=null, key=CLOUDBREAK_EVENT, data=CloudbreakEventData{entityId=49, eventType='UPDATE_IN_PROGRESS', eventMessage='Infrastructure creation failed. Reason: Error occurred on 49 stack during the setup: Connection reset'}}\ncloudbreak_1   | 2016-11-27 10:52:32,021 [reactorDispatcher-18] accept:36 INFO  c.s.c.s.e.CloudbreakEventHandler - [owner:undefined] [type:CLOUDBREAKEVENTDATA] [id:undefined] [name:undefined] Persisting data: CloudbreakEventData{entityId=49, eventType='UPDATE_IN_PROGRESS', eventMessage='Infrastructure creation failed. Reason: Error occurred on 49 stack during the setup: Connection reset'}\ncloudbreak_1   | 2016-11-27 10:52:32,021 [reactorDispatcher-18] createStackEvent:66 DEBUG c.s.c.s.e.DefaultCloudbreakEventService - [owner:undefined] [type:CLOUDBREAKEVENTDATA] [id:undefined] [name:undefined] Creating stack event from: CloudbreakEventData{entityId=49, eventType='UPDATE_IN_PROGRESS', eventMessage='Infrastructure creation failed. Reason: Error occurred on 49 stack during the setup: Connection reset'}\ncloudbreak_1   | 2016-11-27 10:52:32,060 [reactorDispatcher-18] populateClusterData:133 DEBUG c.s.c.s.e.DefaultCloudbreakEventService - [owner:undefined] [type:CLOUDBREAKEVENTDATA] [id:undefined] [name:undefined] No cluster data available for the stack: 49</p>","tags":["Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-27 21:06:16.0","id":68599,"title":"Hive Query Problem","body":"<p><strong></strong><strong>select * from tweets_clean;</strong></p><p>this  query is taking too long than usual and showing logs only as following</p><p><strong>INFO : Session is already open\nINFO : Dag name: select * from tweets_clean(Stage-1)</strong></p><p><strong>while select * from tweets_text/tweets_simple is working fine. </strong></p>","tags":["hdp-2.5.0","tutorial-210"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-28 04:12:08.0","id":68629,"title":"NIFI Secure Site-Site transfer","body":"<p>Hi,</p><p>I have two secure nifi clusters. I want to transfer data from one cluster to another cluster using site-site protocol. </p><p>I used RPG at source and Port at destination. i changed fallowing properties at destination.</p><p>nifi.remote.input.socket.host=</p><p>nifi.remote.input.socket.port=</p><p>nifi.remote.input.secure=true</p><p>can some one point out documentation or example configuration ..</p><p>Thanks,</p><p>V_A</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 07:12:40.0","id":68625,"title":"Caching of spark dataframe","body":"<p>Why caching of a spark dataframe results in less space on the memory compared to a RDD cache? </p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-27 17:19:35.0","id":68575,"title":"Solr Password - When trying to create a collection in SSH","body":"<p>Hi</p><p>I'm trying to create a tweet collection and need to chance the owner for opt/lucidworks-hdpsearch/solr. </p><p>However, when I attempt to do this i get 'Operation not permitted'. So then I attempted to run the command as sudo, however now I am being asked for the solr password. As far as I know i've not set one and i can't seem to locate a default username/password for solr. please can someone help?</p><p>Thank you</p><p>[solr@sandbox dashboards]$ chown -R solr:solr /opt/lucidworks-hdpsearch/solr</p><p>chown: changing ownership of `/opt/lucidworks-hdpsearch/solr/server/solr-webapp/webapp/banana/app/dashboards/default.json': Operation not permitted</p><p>[solr@sandbox dashboards]$ sudo -u hdfs hadoop fs chown -R solr:solr /opt/lucidworks-hdpsearch/solr</p><p>We trust you have received the usual lecture from the local System</p><p>Administrator. It usually boils down to these three things:</p><p>    #1) Respect the privacy of others.</p><p>    #2) Think before you type.</p><p>    #3) With great power comes great responsibility.</p><p>[sudo] password for solr: </p><p>Sorry, try again.</p>","tags":["tweets","hdfs-permissions"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-28 06:49:39.0","id":68641,"title":"Kerberized HDP 2.4 - Kafka giving error - kafka.common.LeaderNotAvailableException","body":"<p>Hi All,</p><p>I've a Kerberised Kafka cluster, and running into issues wen i publish messages on Kafka producer.</p><p>Here is the error -</p><p>-------------------------------------------------------------</p><p>[root@sandbox libs]# $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list sandbox.hortonworks.com:6667 --topic kafka1_topic2 --security-protocol SASL_PLAINTEXT\nhello\n[2016-11-28 06:44:03,499] WARN Error while fetching metadata [{TopicMetadata for topic kafka1_topic2 -&gt; \nNo partition metadata for topic kafka1_topic2 due to kafka.common.LeaderNotAvailableException}] for topic [kafka1_topic2]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)</p><p>---------------------------------------------------------------</p><p>Any ideas on this ? Pls. let me know.</p><p>Thanks !</p><p>Also, Here is the kafka server.properties file :</p><p>[root@sandbox conf]# cat server.properties \n# Generated by Apache Ambari. Mon Nov 28 06:42:53 2016\n   \nadvertised.host.name=sandbox.hortonworks.com\nallow.everyone.if.no.acl.found=true\nauthorizer.class.name=org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer\nauto.create.topics.enable=true\nauto.leader.rebalance.enable=true\ncompression.type=producer\ncontrolled.shutdown.enable=true\ncontrolled.shutdown.max.retries=3\ncontrolled.shutdown.retry.backoff.ms=5000\ncontroller.message.queue.size=10\ncontroller.socket.timeout.ms=30000\ndefault.replication.factor=1\ndelete.topic.enable=false\nexternal.kafka.metrics.exclude.prefix=kafka.network.RequestMetrics,kafka.server.DelayedOperationPurgatory,kafka.server.BrokerTopicMetrics.BytesRejectedPerSec\nexternal.kafka.metrics.include.prefix=kafka.network.RequestMetrics.ResponseQueueTimeMs.request.OffsetCommit.98percentile,kafka.network.RequestMetrics.ResponseQueueTimeMs.request.Offsets.95percentile,kafka.network.RequestMetrics.ResponseSendTimeMs.request.Fetch.95percentile,kafka.network.RequestMetrics.RequestsPerSec.request\nfetch.purgatory.purge.interval.requests=10000\nhost.name=sandbox.hortonworks.com\nkafka.ganglia.metrics.group=kafka\nkafka.ganglia.metrics.host=sandbox.hortonworks.com\nkafka.ganglia.metrics.port=8671\nkafka.ganglia.metrics.reporter.enabled=true\nkafka.metrics.reporters=org.apache.hadoop.metrics2.sink.kafka.KafkaTimelineMetricsReporter\nkafka.timeline.metrics.host=sandbox.hortonworks.com\nkafka.timeline.metrics.maxRowCacheSize=10000\nkafka.timeline.metrics.port=6188\nkafka.timeline.metrics.reporter.enabled=true\nkafka.timeline.metrics.reporter.sendInterval=5900\nleader.imbalance.check.interval.seconds=300\nleader.imbalance.per.broker.percentage=10\nlisteners=SASL_PLAINTEXT://sandbox.hortonworks.com:6667\nlog.cleanup.interval.mins=10\nlog.dirs=/kafka-logs\nlog.index.interval.bytes=4096\nlog.index.size.max.bytes=10485760\nlog.retention.bytes=-1\nlog.retention.hours=168\nlog.roll.hours=168\nlog.segment.bytes=1073741824\nmessage.max.bytes=1000000\nmin.insync.replicas=1\nnum.io.threads=8\nnum.network.threads=3\nnum.partitions=1\nnum.recovery.threads.per.data.dir=1\nnum.replica.fetchers=1\noffset.metadata.max.bytes=4096\noffsets.commit.required.acks=-1\noffsets.commit.timeout.ms=5000\noffsets.load.buffer.size=5242880\noffsets.retention.check.interval.ms=600000\noffsets.retention.minutes=86400000\noffsets.topic.compression.codec=0\noffsets.topic.num.partitions=50\noffsets.topic.replication.factor=3\noffsets.topic.segment.bytes=104857600\nprincipal.to.local.class=kafka.security.auth.KerberosPrincipalToLocal\nproducer.purgatory.purge.interval.requests=10000\nqueued.max.requests=500\nreplica.fetch.max.bytes=1048576\nreplica.fetch.min.bytes=1\nreplica.fetch.wait.max.ms=500\nreplica.high.watermark.checkpoint.interval.ms=5000\nreplica.lag.max.messages=4000\nreplica.lag.time.max.ms=10000\nreplica.socket.receive.buffer.bytes=65536\nreplica.socket.timeout.ms=30000\nsasl.enabled.mechanisms=GSSAPI\nsecurity.inter.broker.protocol=SASL_PLAINTEXT\nsecurity.protocol=SASL_PLAINTEXT\nsocket.receive.buffer.bytes=102400\nsocket.request.max.bytes=104857600\nsocket.send.buffer.bytes=102400\nsuper.users=user:kafka;user:kafka1;user:kafka2\nzookeeper.connect=sandbox.hortonworks.com:2181\nzookeeper.connection.timeout.ms=25000\nzookeeper.session.timeout.ms=30000\nzookeeper.sync.time.ms=2000</p>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-27 06:56:24.0","id":68546,"title":"Why Oozie uses DAG and not just any Directed graph ?","body":"<p>Hi Gurus,</p><p>Sorry for this silly question. However, can someone explain why Oozie enforces DAG ?</p><p>Regards,</p><p>Soumya</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-26 21:46:47.0","id":68511,"title":"Hive Query error","body":"<p>create table IF NOT EXISTS tweets_sentiment stored as orc as select\n  tweet_id, </p><p>  case\n    when sum( polarity ) &gt; 0 then 'positive' when sum( polarity ) &lt; 0 then 'negative' else 'neutral' end as sentiment from l3 group by tweet_id;</p><p><strong>Tihs query gives following error :-</strong></p><p><img src=\"/storage/attachments/9796-screenshot-3.png\"></p>","tags":["hdp-2.5.0","tutorial-210"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-27 21:07:15.0","id":68600,"title":"hive query problem","body":"<p><strong>select * from tweets_clean;</strong></p><p><strong></strong></p><p>this  query is taking too long than usual and showing logs only as following</p><p><strong>INFO : Session is already open\nINFO : Dag name: select * from tweets_clean(Stage-1)</strong></p><p><strong>And Status is running only</strong></p><p><strong>while select * from tweets_text/tweets_simple is working fine. </strong></p>","tags":["hdp-2.5.0","tutorial-210"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-28 06:58:27.0","id":68623,"title":"XML file to Hive Table Using NIFI","body":"<p>Hi guys,</p><p>Can you give me any clue on how will I save my data from XML fil to Hive Table?</p><p>This is my sample data in xml file:</p><p>&lt;Customer&gt;</p><p style=\"margin-left: 20px;\">&lt;CustomerID&gt;testId&lt;/CustomerID&gt;</p><p style=\"margin-left: 20px;\">&lt;CustomerName&gt;testName&lt;/CustomerName&gt;</p><p style=\"margin-left: 20px;\">&lt;GroupAdminPhoneNumber&gt;123-4567&lt;/GroupAdminPhoneNumber&gt;</p><p style=\"margin-left: 20px;\">&lt;ContactEmail&gt;testEmail&lt;/ContactEmail&gt;</p><p style=\"margin-left: 20px;\">&lt;Address&gt;</p><p style=\"margin-left: 40px;\">&lt;StreetName&gt;testStreet&lt;/StreetName&gt;</p><p style=\"margin-left: 40px;\">&lt;UnitFloorSuite&gt;1234&lt;/UnitFloorSuite&gt;</p><p style=\"margin-left: 40px;\">&lt;City&gt;testCity&lt;/City&gt;</p><p style=\"margin-left: 40px;\">&lt;Province&gt;testProvince&lt;/Province&gt;</p><p style=\"margin-left: 40px;\">&lt;PostalCode&gt;1234&lt;/PostalCode&gt;</p><p style=\"margin-left: 20px;\">&lt;/Address&gt;</p><p style=\"margin-left: 20px;\">&lt;CustomerStatus&gt;Active&lt;/CustomerStatus&gt;</p><p>&lt;/Customer&gt;</p><p>Thank you.</p><p>Regie</p>","tags":["Nifi"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-28 09:37:45.0","id":68667,"title":"Best tool to deploy Spark","body":"<p>I have some spark-streaming applications that ingest data from Kafka and insert into ElasticSearch and Cassandra.</p><p>Currently I need to deploy those applications on YARN so they are running 24/7.</p><p>What is the best tool for this task? I have been pondering Oozie, but it doesn't feel like a tool designed to deploy jobs which will be running 24/7, is there any other thing I could use? Are spark jobs the best option to move data from Kafka to other system or should I consider using Flume?</p>","tags":["Kafka","Spark"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-27 14:30:26.0","id":68570,"title":"Cannot connect to Phoenix Query Server using JDBC thin client","body":"<p>Hi,</p><p>I could not connect to Phoenix Query Server in HDP2.5 in my Java program using the thin JDBC client. Any advice?</p><p>My connection string is: jdbc:phoenix:thin:url=http://localhost:8765. I am using the thin driver: org.apache.phoenix.queryserver.client.Driver.</p><pre>try {\n    Class.forName(driver);\n    conn = DriverManager.getConnection(connectionString);\n} catch(Exception e) {\n    logger.error(\"Failed connecting to database.\", e);\n}</pre><p>I got the below error:</p><pre>java.lang.RuntimeException: org.apache.phoenix.shaded.com.fasterxml.jackson.core.JsonParseException: Unexpected character ('o' (code 111)): Expected space separating root-level values\n at [Source:\n8org.apache.calcite.avatica.proto.Responses$ErrorResponse�\n�org.apache.calcite.avatica.com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.\n\tat org.apache.calcite.avatica.com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:70)\n\tat org.apache.calcite.avatica.com.google.protobuf.CodedInputStream.skipRawBytesSlowPath(CodedInputStream.java:1293)\n\tat org.apache.calcite.avatica.com.google.protobuf.CodedInputStream.skipRawBytes(CodedInputStream.java:1276)\n\tat org.apache.calcite.avatica.com.google.protobuf.CodedInputStream.skipField(CodedInputStream.java:197)\n\tat org.apache.calcite.avatica.com.google.protobuf.CodedInputStream.skipMessage(CodedInputStream.java:273)\n\tat org.apache.calcite.avatica.com.google.protobuf.CodedInputStream.skipField(CodedInputStream.java:200)\n\tat org.apache.calcite.avatica.proto.Common$WireMessage.&lt;init&gt;(Common.java:11627)\n\tat org.apache.calcite.avatica.proto.Common$WireMessage.&lt;init&gt;(Common.java:11595)\n\tat org.apache.calcite.avatica.proto.Common$WireMessage$1.parsePartialFrom(Common.java:12061)\n\tat org.apache.calcite.avatica.proto.Common$WireMessage$1.parsePartialFrom(Common.java:12055)\n\tat org.apache.calcite.avatica.com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:89)\n\tat org.apache.calcite.avatica.com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:95)\n\tat org.apache.calcite.avatica.com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)\n\tat org.apache.calcite.avatica.proto.Common$WireMessage.parseFrom(Common.java:11791)\n\tat org.apache.calcite.avatica.remote.ProtobufTranslationImpl.parseRequest(ProtobufTranslationImpl.java:354)\n\tat org.apache.calcite.avatica.remote.ProtobufHandler.decode(ProtobufHandler.java:51)\n\tat org.apache.calcite.avatica.remote.ProtobufHandler.decode(ProtobufHandler.java:31)\n\tat org.apache.calcite.avatica.remote.AbstractHandler.apply(AbstractHandler.java:94)\n\tat org.apache.calcite.avatica.remote.ProtobufHandler.apply(ProtobufHandler.java:46)\n\tat org.apache.calcite.avatica.server.AvaticaProtobufHandler.handle(AvaticaProtobufHandler.java:124)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.server.Server.handle(Server.java:499)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n\tat org.apache.phoenix.shaded.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n\tat java.lang.Thread.run(Thread.java:745)\n</pre>","tags":["queryserver"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-27 18:58:13.0","id":68590,"title":"Kerberized HDP 2.4 - can i use users kafka1 for publish/subscribe to Kafka topic","body":"<p>Hi Kafka, HDP experts,</p><p>I've a Kerberized HDP 2.4 sandbox..</p><p>i'm able to do kinit for the principal user (kafka), and publish & subscribe to topic.</p><p>How do i do the same with additional users (e.g. kafka1) ?</p><p>I've a Unix user - kafka1 setup.. How do i obtain a kinit for kafka1 user, so i can publish/subscribe messages to Kafka topic ?</p>","tags":["kerberos","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-28 09:33:08.0","id":68669,"title":"CSV Query to run from hive.default.fileformat is ORC","body":"<p>Hi, </p><p>i would like to run the query LOAD DATA INPATH '/user/maria_dev/drivers.csv' OVERWRITE INTO TABLE temp_drivers;</p><p>But my hive default fileformat is ORC, Data is in .csv format.<a href=\"/storage/attachments/9805-help.png\">help.png</a></p>","tags":["orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-27 03:29:29.0","id":68542,"title":"Does Zeppelin Sparkcontext will sleep automatically after some time inactivity?","body":"<p>Hi, all</p><p>     I am using Zeppelin 0.5.6 with a self-configured Spark 1.6.1 cluster, which means I export SPARK_HOME and set spark master url for it to use, and it works well when I submit some sparkcontext jobs. However, I am seeking answer for whether Zeppelin has some autosleep mechanism. Like, I submitted some job to Spark through Zeppelin and got the expected result, then I left Zeppelin started but without job submission for a very long time, will Zeppelin's spark application still be alive?  which means from spark UI, we can still see the Zeppelin application be in RUNNING state or instead Finish State. If so, how much time we need to wait ?</p><p>Thanks all in advance!</p>","tags":["zeppelin-notebook"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-27 18:02:19.0","id":68576,"title":"HDP 2.4 - not starting up, stuck at Starting Zeppelin","body":"<p>Hi, i've been using the HDP 2.4 sandbox, and for the 2nd time - it is not starting up.</p><p>It seems to be stuck at - Starting Zeppelin</p><p>The first time, i wanted to move ahead - so re-downloaded the sandbox, and it started up. I get the feeling it happened after i kerberized the environment.</p><p>Any ideas on what needs to be done ?</p><p>Pls note - I've allocated 8Gb (of total 16 GB) to the sandbox </p><p>thanks !</p>","tags":["zeppelin","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-27 20:55:50.0","id":68596,"title":"Can NiFi help other technologies perform better?","body":"<p>Hi, i am new to HDF but am very interested in NiFi. There are a few questions i would like to ask about NiFi.</p><p>1; How does NiFi get the data from IoT devices? My understanding is that they send the data to a folder on the machine running NiFi and it uses a getFile processor to retrieve it. Is this incorrect.</p><p>2: Can you perform enough processing on NiFi that would ease the workload on other technologies such as spark etc, enabling them to perform tasks quicker?</p><p>3: Where does MiniFi fit into the NFi scenario?</p><p>I have read many documents and watched videos but still need a litlle help in understanding certain aspects of NiFi. I would appreciate any input on these questions, thank you.</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 01:11:45.0","id":68611,"title":"HBase doesn't start when Sandbox is started under Docker on Mac","body":"<p>Started Sandbox under Docker using instructions given at: http://hortonworks.com/hadoop-tutorial/hortonworks-sandbox-guide/#section_4</p><p>\nI am using Mac OS. Everything worked fine - except...</p><ol><li>http://sandbox.hortonworks.com:8888/ didn't work, but http://localhost:8888/ worked.</li><li>http://localhost:8080/ also worked.</li><li>Under 'HBase' -&gt; 'Service Actions', clicked on 'Start'. After long time, it showed 100% completed in green. But in Ambari, HBase Master is still in 'Stopped' state & under 'RegionServers', I see 0/1 alive.</li></ol><p>What am I doing wrong? Please help. Thanks.</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-26 17:18:09.0","id":68486,"title":"Ambari 2.1.2 REST API very slow","body":"<p>Hi, I am executing the following REST API call to start all the services on a given node.  I see two different behaviors.  When I run it from the command line, it runs almost instantaneously and I see that all the services are started.  However when I call the very same API from a shell script which is in turn called from our framework, it takes nearly 10m to start all the services.  I see a few of the services saying \"INSTALLING\" for quite some time before they start up.  </p><p>http://10.9.100.227:8080/api/v1/clusters/hdp/hosts/vnode-109-149.robinsystems.com/host_components</p>","tags":["hadoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-29 19:45:21.0","id":69055,"title":"[Hive] Do the .db files' permissions affect who can access the Hive databases?","body":"<p>For example, say /apps/hive/warehouse/my_database.db has changed from 777 permissions to 750 for user my_user and group my_group. Assume too that no Hive roles were set. Would another_user, who is outside my_group, still be able to query the Hive database?</p>","tags":["Hive","user-groups"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-01 10:46:48.0","id":69458,"title":"nifi template versioning","body":"<p>Hi,</p><p>Is it possible to save a nifi dataflow to an existing template, instead of creating new templates everytime.</p><p>Thanks,</p><p>Avijeet</p>","tags":["nifi-templates","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 13:43:09.0","id":68702,"title":"How can I use resource manager api calls in python script","body":"<p>I am able to get the required result by running the below command on the shell\n           <em>curl --negotiate -u : \"http://rm_host:8088/ws/v1/cluster\"</em></p><p>I need to generate a report of jobs running/failing on a every day basis. Also need to add other information like CPU utilization, memory usage, categorizing different jobs etc. I am thinking of using resource manager api calls for this purpose.</p><p>The response received by the api call is in json format. </p><p>I am thinking of parsing the json object to get the desired result </p><p>example:</p><p><em>info=json.loads(curl_command_response) </em></p><p><em>print(info['clusterInfo']['state']) </em></p><p>When I am executing the below python code, I am getting authorization required error</p><p><em>import urllib2, base64 </em></p><p><em>request = urllib2.Request(\"http://rm_host:8088/ws/v1/cluster\")</em></p><p><em>base64string = base64.b64encode('%s:%s' % ('username','password'))</em></p><p><em> request.add_header(\"Authorization\", \"Basic %s\" % base64string)</em></p><p><em>result = urllib2.urlopen(request)</em></p><p><em>===================================================</em></p><p><em>Error:</em></p><p><strong>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/lib64/python2.6/urllib2.py\", line 126, in urlopen\n    return _opener.open(url, data, timeout)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 397, in open\n    response = meth(req, response)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 510, in http_response\n    'http', request, response, code, msg, hdrs)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 435, in error\n    return self._call_chain(*args)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 369, in _call_chain\n    result = func(*args)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 518, in http_error_default\n    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)\nurllib2.HTTPError: HTTP Error 401: Authentication required</strong></p><p>=====================================================</p>","tags":["api","resource-manager"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-27 07:44:38.0","id":68548,"title":"Settings to include to Enable ranger plugin for hive metastore?","body":"<p>I am currently trying to integrate ranger plugin for hive metastore. I see from the installation page that the \"the hive.metastore.pre.event.listeners and hive.metastore.event.listeners need to be configured to use Ranger implementations.\"  What values are requried to be added in the hive.metastore.event.listeners and hive.metastore.pre.event.listeners in the hiveserver2-site.xml?</p><p>I am trying to implement using : hive-version : 0.14, HDP 2.5, ranger - 0.6</p>","tags":["ranger-admin","Ranger"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-27 15:21:46.0","id":68573,"title":"How to show Zeppelin View in Ambari VMWare HDP 2.5","body":"<p>\n\tHello everbody,</p><p>\n\tI would like to try the tutorial on Zeppelin (https://github.com/hortonworks/tutorials/blob/hdp-2.5/tutorials/hortonworks/getting-started-with-apache-zeppelin/tutorial.md) which is a prerequesite to HANDS-ON TOUR OF APACHE SPARK IN 5 MINUTES</p><p>But when I connect with maria_dev account, I can't show the Zeppelin View in Ambari DashBoard.</p><p>Zeppelin Service is running and when I open the interface, I sign in as 'Anonymous'.</p><p>How can I configure Zeppelin View in Ambari? </p><p>Thank you for your reply!</p>","tags":["view","vmware"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-27 11:18:27.0","id":68556,"title":"knox over HS2 HA is not working as expected?","body":"<p>I tried to setup KNOX over HS2 HA using Zookeeper discovery (using https://gist.github.com/rajkrrsingh/e79c615d23562b606559ec65e5651e77) but noticed that it is not doing load balancing, I observed that all the client session are landing on the single instance of HS2. once the HS2 is down or unresponsive then only I can see the client connecting to the other HS2 instances.wanted to know, is this expected behavior using knox or I am missing something in configuration?</p>","tags":["Knox"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-28 10:00:26.0","id":68681,"title":"NiFi some basics","body":"<p>Hi,</p><p>I am using a InvokeHTTP to call a external rest api and then Extract a few values from the JSON response using EvaluateJSONPath</p><p>What I am not able to understand is, why are we doing the extract from flowfile-attributes and not flowfile-content</p><p>Comparing the terminology with flume (which I have used before), flowfile-attributes are the Event headers and flowfile-content is Event body</p><p>Then how is EvaluateJSONPath is extracting from header using EL</p><p>Thanks,</p><p>Avijeet</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 13:08:34.0","id":68698,"title":"When I run a SELECT on a ACID/Bucketed Table In Zeppelin it returns no data. When run in beeline the query returns the data Why?","body":"<p>Anyone have any idea why when selecting data from a hive table that is transactional (ACID) from Zeppelin it does not return any data.</p><p>I've tried this using %sql %pyspark and %spark interpreters.</p><p>I can query and receive data from non-transactional ORC tables just fine.</p><p>Cheers,</p><p>Mike</p>","tags":["zeppelin","Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-28 13:50:48.0","id":68705,"title":"Need to transform Spark RDD","body":"<p>I have data like this</p><p>1, A B C,xxx</p><p>2, A B C,yyy</p><p>i want my output will be look like </p><p>1,A,xxx</p><p>1,B,xxx</p><p>1,C,xxx</p><p>2,A,xxx</p><p>2,B,xxx</p><p>2,C,xxx</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-28 07:08:33.0","id":68624,"title":"Install HDF 2.0/NiFi on RHEL 6.X instance on AWS","body":"<p>Hi,</p><p>I'm trying to install NiFi on the server and so I've done the following options but both fails :</p><p>1-  Installing HDF 2.0 on a separate instance than HDP 2.5 but on the deploy step i get \"<a href=\"http://ec2-54-186-152-153.us-west-2.compute.amazonaws.com:8080/#\">Infra Solr Instance Install</a> error \"resource_management.core.exceptions.Fail: Execution of '/usr/bin/yum -d 0 -e 0 -y install ambari-infra-solr-client' returned 1. Error: Nothing to do\"</p><p>2- Installing and extracting NIFI on the same server as HDP2.5, but although  JDK 1.8 is installed on the server i get \"Exception in thread \"main\" java.lang.UnsupportedClassVersionError: org/apache/nifi/bootstrap/RunNiFi : Unsupported major.minor version 52.0\" error.</p><p>Can you please help me to fix my issue.</p><p>Thanks,</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-28 05:51:15.0","id":68635,"title":"spark stream Flume","body":"<p>Hi</p><p>I am getting error while trying spark streaming .</p><p>I am importing </p><p>import org.apache.spark.streaming.flume._</p><p>Error :</p><p>:19: error: object flume is not a member of package org.apache.spark.streaming</p><p>Regards</p><p>Mangesh C Berde</p>","tags":["Flume"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-28 09:44:04.0","id":68678,"title":"Reducer is running very slow in hbase bulk load","body":"<p>Please find all details here </p><p>http://stackoverflow.com/questions/40821032/last-reducer-is-running-from-last-24-hour-for-200-gb-of-data-set</p>","tags":["Hbase","bulk"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-28 10:09:31.0","id":68682,"title":"PIG: Loading a file with embedded newlines in the fields.","body":"<p>Hi,</p><p>I have a file with embedded newlines in the fields (and \\u0001 as field delimiter, and \\u0002 as row delimiter). How can I read this file so PIG ignores the newline with either PigStorage or TextLoader?</p><p>Thx! /W</p>","tags":["Pig"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 00:26:59.0","id":69086,"title":"Zeppelin Exception while selecting data from Hive table.","body":"<p>I am getting following exception in Zeppelin while I am trying to get data from hive.</p><p>org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)\nclass java.sql.SQLException\norg.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:255)\norg.apache.zeppelin.jdbc.JDBCInterpreter.executeSql(JDBCInterpreter.java:356)\norg.apache.zeppelin.jdbc.JDBCInterpreter.interpret(JDBCInterpreter.java:442)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\norg.apache.zeppelin.scheduler.Job.run(Job.java:176)\norg.apache.zeppelin.scheduler.ParallelScheduler$JobRunner.run(ParallelScheduler.java:162)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745) </p>","tags":["zeppelin","Hive","hdp-2.5.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-30 04:57:35.0","id":69122,"title":"put command in distributed environment in hadoop","body":"<p>I have this pig command executed through oozie:</p><pre>fs -put -f /home/test/finalreports/accountReport.csv /user/hue/intermediateBingReports </pre><p>/home/test/finalreports/accountReport.csv is created on local filesystem of only one of the hdfs nodes. I recently added a new HDFS node and this command fails on that hdfs node since /home/test/finalreports/accountReport.csv doesn't exist there.</p><p>What is the way to go for this?</p>","tags":["HDFS","Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 06:06:20.0","id":69131,"title":"How to upload hundred's of tag at one shot in Apache Atlas??,How to upload hundred's of tag in Apache Atlas ?","body":"<p>I have a requirement where i need to create 100 of tags in Apache Atlas. Is there any provision which can be used to upload  csv file in Apache Atlas ??  </p><p>Thanks in Advance,</p><p>Subash</p>,<p>I have a requirement where i need to create 100 of tags, Are there any provision in Atlas to upload csv file ??</p><p>Thanks,</p><p>Subash</p>","tags":["atlas-api","Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 14:08:48.0","id":69227,"title":"Data Node Not Starting and Not showing any error logs","body":"<p>Data node is not starting and it is not giving any error logs in logs file.</p><p>error logs:-</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py\", line 167, in &lt;module&gt;\n    DataNode().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py\", line 62, in start\n    datanode(action=\"start\")\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py\", line 89, in thunk\n    return fn(*args, **kwargs)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_datanode.py\", line 72, in datanode\n    create_log_dir=True\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/utils.py\", line 267, in service\n    Execute(daemon_cmd, not_if=process_id_exists_command, environment=hadoop_env_exports)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 238, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg) \n\n\nresource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh --config /usr/hdp/current/hadoop-client/conf start datanode'' returned 1. starting datanode, logging to /data/log/hadoop/hdfs/hadoop-hdfs-datanode-hostname-out</pre><p>in <strong>/var/log/hadoop/hdfs/hadoop-hdfs-datanode.log</strong></p><pre>at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2411)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2345)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2526)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2550)\n2016-05-04 17:42:04,139 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1\n2016-05-04 17:42:04,140 INFO  datanode.DataNode (LogAdapter.java:info(45)) - SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at FQDN/IP\n\n\n\n</pre><p>When i start the datanode through ambari i dont see any logs in datanode log file.</p><p>In <strong>/data/log/hadoop/hdfs/hadoop-hdfs-datanode-hostname-out</strong></p><pre>core file size          (blocks, -c) 0 data seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0 file size               (blocks, -f) unlimited\npending signals                 (-i) 63785\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 1024\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 63785\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n/data/log/hadoop/hdfs/hadoop-hdfs-datanode-D-9539.out: line 2: syntax error near unexpected token `('\n/data/log/hadoop/hdfs/hadoop-hdfs-datanode-D-9539.out: line 2: `core file size          (blocks, -c) unlimited'\n\n\n</pre><p>Please suggest me.</p><p>Mohan.V</p>","tags":["data-nodes"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-01 09:14:16.0","id":69442,"title":"Is there a tool or a set of tools I could use to subscribe to a broadcast channel and ingest data into HDFS? The tool would need to support the Bayeux messaging protocol.","body":"","tags":["ingestion"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 06:03:38.0","id":69130,"title":"How to load the Logs data from Remote  System to my HDFS using FLUME?","body":"<p>Hi All,</p><p>I  have the logs which has been created continuously in the <strong>remote system connected through the network</strong>. Now i need to <strong>load the logs to my HDFS</strong> using <strong>Flume</strong>. What is the <strong>source configuration</strong> for this flume source to pull the Real Time Logs to my HDFS.</p><p>I tried this link,</p><p>https://github.com/keedio/flume-ftp-source</p><p>How to proceed further?</p><p>I am struck in the middle.</p>","tags":["Flume","ftp"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-01 06:35:20.0","id":69422,"title":"Nifi InferAvroSchema Error","body":"<p>Am trying to load CSV files into Hive using Nifi processors. but my nifi InferAvroSchema processor giving fallowing exception:</p><p>please find attachements.<a href=\"/storage/attachments/9970-capture.png\">capture.png</a><a href=\"/storage/attachments/9991-capture1.png\">capture1.png</a></p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-29 17:11:14.0","id":69018,"title":"Hive not running after HDP 2.5 ambari installment","body":"<p><a href=\"/storage/attachments/9866-hive-error.txt\">hive-error.txt</a> </p><p>I installed HDP 2.5 and tried to run hive but i'm getting the error attached. Then after that I did run the following command to give it permission but still not working... what should I do?</p><p>sudo -u hdfs hdfs dfs -mkdir /user/ec2-user </p><p>sudo -u hdfs hdfs dfs -chown -R ec2-user:hdfs /user/ec2-user</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-29 17:21:00.0","id":69029,"title":"Performance Management in Nifi","body":"<p>Hi Team,</p><p>I am working on Nifi flows for my team and facing issues on managing the performance of flow while using SplitText/ReplaceText/ExtractText.</p><p>One of the workaround i got is to use 3-4 SplitText Processors, but not sure how can we do the same.</p><p>Please let me know what changes i required in my flows to ingest and transform 1 GB of data records.</p><p>PS. Please refer this link to know all of my use cases - </p><p><a href=\"https://community.hortonworks.com/articles/66861/nifi-etl-removing-columns-filtering-rows-changing.html\">https://community.hortonworks.com/articles/66861/nifi-etl-removing-columns-filtering-rows-changing.html</a></p><p>Thanks in advance!</p><p><strong>\n</strong></p><p><strong>Regards,</strong></p><p>Garima.</p>","tags":["nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 14:53:26.0","id":69240,"title":"Ambar-server start failes with \"DB configs consistency check failed\"","body":"<p>Recently we have been dealing with some server issues I believe we may have lost power at some point.  Upon starting the ambari-server I am met with the \"DB configs consistency check failed. Run \"ambari-server start --skip-database-check\" to skip\" message.  The message from /var/log/ambari-server/ambari-server-check-database.log is:</p><p>2016-11-30 09:25:53,306  INFO - ******************************* Check database started ******************************* </p><p>2016-11-30 09:25:57,570  INFO - Checking for configs not mapped to any cluster </p><p>2016-11-30 09:25:57,593  INFO - Checking for configs selected more than once </p><p>2016-11-30 09:25:57,595  INFO - Checking for hosts without state </p><p>2016-11-30 09:25:57,596  INFO - Checking host component states count equals host component desired states count </p><p>2016-11-30 09:25:57,598  INFO - Checking services and their configs </p><p>2016-11-30 09:26:02,972 ERROR - You have non selected configs: ams-hbase-log4j,ams-hbase-security-site,ams-hbase-policy,ams-log4j for service AMBARI_METRICS from cluster slush! </p><p>2016-11-30 09:26:02,973 ERROR - You have non selected configs: zeppelin-config,zeppelin-ambari-config for service ZEPPELIN from cluster slush! </p><p>2016-11-30 09:26:02,973  INFO - ******************************* Check database completed ******************************* </p><p>2016-11-30 09:26:12,880  INFO - Checking DB store version </p><p>2016-11-30 09:26:13,360  INFO - DB store version is compatible</p><p>I have started ambari-server with the \"--skip-database-check\" option however upon start up no services are show to be in the cluster.  I have tried re-adding the services but end up with a server error and stopping at \"Preparing to deploy: 18 of 18 tasks completed.</p><p>Any advice or path forward would be appreciated.</p>","tags":["ambari-server"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-11-30 03:51:01.0","id":69115,"title":"not able to remove nifi","body":"<p>Hi,</p><p>Due to space issues I wanted to remove nifi installed folder, but some reason it stays and the logs come back</p><p>[root@Master nifi-1.0.0]# ls </p><p>database_repository </p><p>[root@Master nifi-1.0.0]# cd database_repository/ </p><p>[root@Master database_repository]# ls </p><p>nifi-flow-audit.lock.db  nifi-user-keys.lock.db</p><p>These files are not getting removed. pls suggest.</p><p>Thanks,</p><p>Avijeet</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 19:14:24.0","id":69340,"title":"Can we query the metadata to query the sources in Atlas","body":"<p>Suppose metadata is already captured from sources and stored in Atlas, is it possible to query the sources by querying the metadata in Atlas?</p><p>It may sound vague but I need some idea so that I can design the system accordingly.</p><p>Any help will be highly appreciated. Thanking you in anticipation.</p>","tags":["metadata"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 10:46:51.0","id":69198,"title":"Get IoT data with HDF/NiFi","body":"<p>Hello,</p><p>I don't know so much about IoT and data ingestion solutions and I have this use case:</p><p>- Several hubs accross the world</p><p>- Several sensors are located in each hub</p><p>- I'd like to collect the data given by these sensors and process it later</p><p>What is the best solution to get this data ?</p><p>Is the MiNiFi a good solution and how it works ?</p><p>Thanks a lot !</p>","tags":["minifi","IOT","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 17:47:05.0","id":69302,"title":"[Nifi] Ingest same configuration file using GetFile, ListFile/FetchFile or other processor","body":"<p>I have a scenario where I am looking to ingest the same configuration file every time a flow runs. </p><p>Here is a simple example:</p><p>I have flow #1 watching for a data file to be dropped on s3. When a file is found it moves the file around and then ultimately drops a message on a kafka queue specific to that data source. Flow #2 is consuming from that kafka queue and runs a series of Hive SQL commands to load the data to necessary tables. <strong>I am looking to have the Hive SQL itself sourced from files on the filesystem/git so that the SQL is controlled in our configurations</strong>. Now I know I can store the SQL statement(s) in custom properties files that Nifi picks up based on the nifi.variable.registry.properties property (I have that working), however I don't really want to store large SQL commands in Java property files unless I have to. Also those property files appear to <strong>only</strong> be picked up on Nifi service restart. Therefore the goal is to ingest these SQL configuration files every time the flow runs. This is what I have looked into...</p><p>GetFile - My first thought was to use GetFile right after the ConsumeKafka process gets a new message BUT GetFile appears to not allow input connections into it.</p><p>ListFile/FetchFile - My second thought was to use ListFile/FetchFile but it appears that ListFile only picks up newly modified files and therefore the flow could pull in the configuration files the first run but then all subsequent runs would fail as the file would not be modified. Also similarly to the GetFile processor it appears you can not have an input link to the ListFile Processor.</p><p>ReplaceTextWithMapping - This was my last thought which was to replace everything in the flowfile with the contents of the \"mapping\" file but this processor appears to work on newline record delimitation and tab delimited columns. This doesn't sound like it will work nicely with multi-line SQL.</p><p>So....any thoughts on how I could accomplish this task?</p>","tags":["Nifi","data-ingestion","ingestion"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 23:13:43.0","id":69388,"title":"cant create a kerberos ticket","body":"<p>I cant create  a ticket  I think I need one because beeline is failing to connect to hive2 server .</p><pre>[root@hadoop1 ~]# klist\nTicket cache: FILE:/tmp/krb5cc_0\nDefault principal: admin/admin@TOLLS.DOT.STATE.FL.US\nValid starting     Expires            Service principal\n11/30/16 17:00:42  12/01/16 17:00:42  krbtgt/TOLLS.DOT.STATE.FL.US@TOLLS.DOT.STATE.FL.US\n        renew until 11/30/16 17:00:42\n[root@hadoop1 ~]# kinit hive\nkinit: Client not found in Kerberos database while getting initial credentials\n[root@hadoop1 ~]#\n\n</pre><p>beeline error  </p><pre>beeline&gt; !connect jdbc:hive2://hadoop2:10000/default;principal=hive/hadoop2@TOLLS.DOT.STATE.FL.US\nConnecting to jdbc:hive2://hadoop2:10000/default;principal=hive/hadoop2@TOLLS.DOT.STATE.FL.US\nEnter username for jdbc:hive2://hadoop2:10000/default;principal=hive/hadoop2@TOLLS.DOT.STATE.FL.US: hive\nEnter password for jdbc:hive2://hadoop2:10000/default;principal=hive/hadoop2@TOLLS.DOT.STATE.FL.US: *******\n16/11/30 18:09:19 [main]: ERROR transport.TSaslTransport: SASL negotiation failure\njavax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n</pre>","tags":["hdp-2.5.0","beeline","kerberos"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-29 21:10:44.0","id":69072,"title":"NiFi doesnt start on red-hat 6.8","body":"<p>Hi,</p><p>I am trying to install NiFi on redhat 6.8 withing following steps</p><pre>wget http://public-repo-1.hortonworks.com/HDF/2.0.1.0/HDF-2.0.1.0-12.tar.gz\ntar -zxf HDF-2.0.1.0-12.tar.gz\ncd HDF-2.0.1.0-12/nifi\nbin/nifi.sh start\n</pre><p>After this I see following messages in logs</p><pre>[root@abc000732 nifi]# tailf logs/nifi-bootstrap.log\n2016-11-29 20:06:03,579 INFO [main] org.apache.nifi.bootstrap.RunNiFi NiFi never started. Will not restart NiFi\n2016-11-29 20:07:11,636 INFO [main] o.a.n.b.NotificationServiceManager Successfully loaded the following 0 services: []\n2016-11-29 20:07:11,639 INFO [main] org.apache.nifi.bootstrap.RunNiFi Registered no Notification Services for Notification Type NIFI_STARTED\n2016-11-29 20:07:11,639 INFO [main] org.apache.nifi.bootstrap.RunNiFi Registered no Notification Services for Notification Type NIFI_STOPPED\n2016-11-29 20:07:11,639 INFO [main] org.apache.nifi.bootstrap.RunNiFi Registered no Notification Services for Notification Type NIFI_DIED\n2016-11-29 20:07:11,657 INFO [main] org.apache.nifi.bootstrap.Command Starting Apache NiFi...\n2016-11-29 20:07:11,657 INFO [main] org.apache.nifi.bootstrap.Command Working Directory: /opt/HDF-2.0.1.0/nifi\n2016-11-29 20:07:11,657 INFO [main] org.apache.nifi.bootstrap.Command Command: java -classpath /opt/HDF-2.0.1.0/nifi/./conf:/opt/HDF-2.0.1.0/nifi/./lib/nifi-documentation-1.0.0.2.0.1.0-12.jar:/opt/HDF-2.0.1.0/nifi/./lib/logback-classic-1.1.3.jar:/opt/HDF-2.0.1.0/nifi/./lib/nifi-api-1.0.0.2.0.1.0-12.jar:/opt/HDF-2.0.1.0/nifi/./lib/nifi-nar-utils-1.0.0.2.0.1.0-12.jar:/opt/HDF-2.0.1.0/nifi/./lib/slf4j-api-1.7.12.jar:/opt/HDF-2.0.1.0/nifi/./lib/nifi-properties-1.0.0.2.0.1.0-12.jar:/opt/HDF-2.0.1.0/nifi/./lib/commons-lang3-3.4.jar:/opt/HDF-2.0.1.0/nifi/./lib/nifi-properties-loader-1.0.0.2.0.1.0-12.jar:/opt/HDF-2.0.1.0/nifi/./lib/nifi-framework-api-1.0.0.2.0.1.0-12.jar:/opt/HDF-2.0.1.0/nifi/./lib/logback-core-1.1.3.jar:/opt/HDF-2.0.1.0/nifi/./lib/jul-to-slf4j-1.7.12.jar:/opt/HDF-2.0.1.0/nifi/./lib/log4j-over-slf4j-1.7.12.jar:/opt/HDF-2.0.1.0/nifi/./lib/jcl-over-slf4j-1.7.12.jar:/opt/HDF-2.0.1.0/nifi/./lib/bcprov-jdk15on-1.54.jar:/opt/HDF-2.0.1.0/nifi/./lib/nifi-runtime-1.0.0.2.0.1.0-12.jar -Dorg.apache.jasper.compiler.disablejsr199=true -Xmx512m -Xms512m -Dsun.net.http.allowRestrictedHeaders=true -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true -XX:+UseG1GC -Djava.protocol.handler.pkgs=sun.net.www.protocol -Dnifi.properties.file.path=/opt/HDF-2.0.1.0/nifi/./conf/nifi.properties -Dnifi.bootstrap.listen.port=19169 -Dapp=NiFi -Dorg.apache.nifi.bootstrap.config.log.dir=/opt/HDF-2.0.1.0/nifi/logs org.apache.nifi.NiFi\n2016-11-29 20:07:12,102 INFO [NiFi Bootstrap Command Listener] org.apache.nifi.bootstrap.RunNiFi Apache NiFi now running and listening for Bootstrap requests on port 52062\n2016-11-29 20:07:14,687 INFO [main] org.apache.nifi.bootstrap.RunNiFi NiFi never started. Will not restart NiFi\n</pre><pre>[root@abc000732 nifi]# tailf logs/nifi-app.log\n2016-11-29 20:07:13,939 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-standard-services-api-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-standard-services-api-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,941 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-enrich-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-enrich-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,944 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-elasticsearch-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-elasticsearch-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,949 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-standard-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-standard-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,950 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-avro-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-avro-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,951 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-amqp-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-amqp-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,961 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-hive-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-hive-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,962 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-riemann-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-riemann-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,963 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/./work/nar/extensions/nifi-scripting-nar-1.0.0.2.0.1.0-12.nar-unpacked as class loader org.apache.nifi.nar.NarClassLoader[./work/nar/extensions/nifi-scripting-nar-1.0.0.2.0.1.0-12.nar-unpacked]\n2016-11-29 20:07:13,965 INFO [main] org.apache.nifi.nar.NarClassLoaders Loaded NAR file: /opt/HDF-2.0.1.0/nifi/\n\n</pre><p>I have oracle java (1.8.0_60) installed on this VM and iptables is off and selinux is disabled.</p><p>Anyone know whats wrong with this. Same steps works on Centos 6.8</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 07:18:29.0","id":69154,"title":"Unable to load  CSV data in to Hive table using Puthivestreaming using Nifi ?","body":"<p><strong>Below are the errors i'm getting while loading CSV file to HIVE table using NIFI.</strong></p><p>1.Hive streaming connect/write error, flowfile will be penalized and routed to retry. </p><p>2.Error connecting to Hive endpoint:table routes at thrift.</p><p>3.Failed to create hive writer for endpoint.</p><p>Any solutions for the above much appreciated& Thanks a lot in advance.</p>","tags":["csv"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 19:17:24.0","id":69342,"title":"Can we query the sources by querying the metadata through Atlas?","body":"<p>Suppose metadata is already captured from sources and stored in Atlas, is it possible to query the sources by querying the metadata in Atlas?</p><p>It may sound vague but I need some idea so that I can design the system accordingly.</p><p>Any help will be highly appreciated. Thanking you in anticipation.</p>","tags":["metadata"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 10:18:16.0","id":69184,"title":"Ranger audit to HDFS , path issue","body":"<p>Hello,</p><p>I am wondering how the final HDFS path for storing Ranger audits is being generated, because it doesn't match the configured property via Ambari.</p><p>Configuration looks like:</p><p><img src=\"/storage/attachments/9934-ambari-config-ranger-audit-hdfs-path.png\"></p><p>checked via /etc/hadoop/conf/ranger-hdfs-audit.xml :</p><p><img src=\"/storage/attachments/9935-ambari-config-ranger-audit-hdfs-path-in-xml-file.png\"></p><p>But the created folder in HDFS is:</p><p><img src=\"/storage/attachments/9936-ambari-config-ranger-audit-hdfs-path-created-in-hd.png\"></p><p>I expected having the audit-log-files under \"hdfs://.../ranger/audit/20161130\" directly.</p><p>Why is there the subfolder \"hdfs\" and an additional subfolder with the yyyyMMdd again ?!?!</p><p>Maybe a known bug/issue in the version in use: HDP 2.3.4.7, Ambari 2.2.1.1  ?!?!</p><p>Thanks for any feedback...</p>","tags":["Ranger"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-30 15:08:16.0","id":69243,"title":"Adding a hdp installed node to existing cluster","body":"<p>Hi, \nI have a small production cluster running and would like to add a node to it. But the node has hdp installed already(this is a test node which was installed before even starting the cluster). All the versions, jdk, ambari, hdp and hadoop versions are exactly same. Will i be able to add this node to the cluster, if yes then what checks i have to make? is it recommended? or do i just do a format or reimage of the node and start over confguring again? Answers are highly appreciated.</p>","tags":["hadoop","add"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-30 17:04:28.0","id":69294,"title":"How do you delete old Ambari Operations History","body":"<p style=\"margin-left: 20px;\">In Ambari 2.2.1 how can I delete old Ambari Server Ops History? </p><p style=\"margin-left: 20px;\">I think that the history of tasks is stored in /var/lib/ambari-agent/data. </p><p style=\"margin-left: 20px;\">Can I simply delete the folder contents?  </p>","tags":["Ambari","ambari-agent"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-12-01 16:34:01.0","id":69496,"title":"Using single SSL Certificate on multiple hosts?","body":"<p>Hi all,</p><p>Currently in the cluster we have different host certs for each host of the cluster. Is it possible to configure a single SSL cert for all the hosts?  (This is to avoid generating multiple CSR and getting them signed)</p><p>1) Is it possible? </p><p>2) How if possible.</p><p>3) Possible security concerns?</p><p>Regards,</p><p>Arpan</p>","tags":["ssl"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-30 07:11:20.0","id":69153,"title":"Nifi  in production","body":"<p>Hi,</p><p>It seems the nifi canvas can be used by only one person for one dataflow</p><p>How do we start/stop/schedule multiple data flows? We can't keep all on the canvas..right?</p><p>Thanks,</p><p>Avijeet</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 14:46:07.0","id":69239,"title":"Has anyone used Kafka Streams with YARN?","body":"<p>Has anyone deployed a Kafka Streams application via YARN, perhaps with Slider?  If so, what was your experience?</p>","tags":["Kafka","slider"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 17:17:45.0","id":69296,"title":"Adding new dashboard in grafana","body":"<p>Hi,</p><p>I am using ambari 2.4.1. i wanted to add a custom dashboard in grafana to see the hbase replication status. I logged in as admin. When i tried, i simple get add row but could not do anything after that. Please advise if it is supported.</p><p>Thanks,</p><p>Pradheep</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-29 21:01:09.0","id":69061,"title":"CONFIGURE KDC CLIENT FAILING","body":"<p><a href=\"/storage/attachments/9896-config-kerb.jpg\">config-kerb.jpg</a> <a href=\"/storage/attachments/9895-kdc-error.txt\">kdc-error.txt</a> <a href=\"/storage/attachments/9897-ambari-error.jpg\">ambari-error.jpg</a></p><p>I have installed the the KDC server and created principals . The configure Kerberos part goes fine from the ambari console and so does the install client Kerberos part , but the test client part is failing with some internal exception , please see the upload ambari log file and and the screen shots for the configuration screen .</p>","tags":["hdp-2.5.0","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 05:44:56.0","id":69128,"title":"Nifi: MERGE (or UPSERT) for MS SQL Server","body":"<p>I am using MS SQL Server as my data store.</p><p>The PutSQL processor is great if we are clear about the kind of DB operation we want to perform ie. INSERT or UPDATE.</p><p>But what if we want to do something like UPSERT or MERGE operation? </p><p>eg. https://myadventuresincoding.wordpress.com/2016/01/05/sql-server-how-to-write-an-upsert-using-merge/</p><p>I can't find any suitable set of processors that can achieve this.</p><p>Anyone done this before?</p>","tags":["Nifi","putsql"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 14:53:15.0","id":69251,"title":"Zepellin sandbox 5.2 error","body":"<p>Hi, </p><p>Using HDP Sandbox 5.2 in Windows 7 with Oracle VirtualBox, all is fine except I cannot get Zepellin UI working (http://sandbox.hortonworks.com:9995/)..Zepellin log :</p><p>WARN [2016-11-29 20:37:27,593] ({qtp784960760-13-acceptor-0@207ebd8f-ServerConnector@56f1a478{HTTP/1.1}{0.0.0.0:9995}} AbstractConnector.java[run]:505) - \njava.lang.NoClassDefFoundError: Could not initialize class sun.nio.ch.SocketChannelImpl$DefaultOptionsHolder\nat sun.nio.ch.SocketChannelImpl.supportedOptions(SocketChannelImpl.java:251)</p><p>Maybe issue with JRE version? Weird because I get sandbox out of box...</p><p>Please advice</p><p>Thanks</p>","tags":["hdp-2.5.0","zeppelin-notebook"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-30 15:51:04.0","id":69269,"title":"Is there a permanent solution to: \"H100 Unable to submit statement show databases like '*': org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe\"?","body":"<p>I have seen numerous posts about this and everyone says it should be fixed in later versions or to restart Ambari-Server and Hive.  That temporarily fixes the issue, but the error comes back every couple days.  My cluster does not have Kerberos enabled.  It does seem that the issue occurs when trying to connect to Hive using JDBC whether through beeline or an external studio software.  Those connections fail and then Hive View fails as well.</p>","tags":["error","ambari-server","hive-views"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 18:40:23.0","id":69334,"title":"Prior to deleting oozie you must delete the following dependent services","body":"<p>1) When I am deleting the oozie service from one node on HDP 2.4 it is throwing an error. Saying that Prior to deleting oozie you must delete the following dependent services (Service is falcon). </p>","tags":["services"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 22:48:52.0","id":69377,"title":"Zeppelin Web UI SSL Enabled","body":"<p>Hi </p><p>I am trying to make the Zeppelin Web UI SSL Enabled like it s done in the doc : https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_zeppelin-component-guide/content/zepp-ssl.html . I created a csr file , got a cer file back then created my keystore that I pass in the zeppelin-configs , When I try to access https </p><p>through Zeppelin , the browser just sits there not giving me anything. Tried disabling the iptables , adding the </p><p>port 9995to the iptables ... Nothing. You guys have any idea what s going on ?</p><p>Loic Ambamany.</p>","tags":["zeppelin"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-12-01 00:53:52.0","id":69398,"title":"Java Spark Program Does Not Run Good When df.show() is called for a Hive table backed by HBase table","body":"<p>I have a Hive table that is integrated with HBase table. It works fine on Hive command line to see data; however, when I try to do the same in Spark Java code where create a dataframe object by select statement and call show method, I see the following message forever:</p><p>16/11/30 19:40:31 INFO ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x15802d56675006a, negotiated timeout = 90000 </p><p>16/11/30 19:40:31 INFO RegionSizeCalculator: Calculating region sizes for table \"st_tbl_1\". </p><p>16/11/30 19:41:19 INFO RpcRetryingCaller: Call exception, tries=10, retries=35, started=48332 ms ago, cancelled=false, msg= </p><p>16/11/30 19:41:40 INFO RpcRetryingCaller: Call exception, tries=11, retries=35, started=68473 ms ago, cancelled=false, msg= </p><p>16/11/30 19:42:00 INFO RpcRetryingCaller: Call exception, tries=12, retries=35, started=88545 ms ago, cancelled=false, msg= </p><p>16/11/30 19:42:20 INFO RpcRetryingCaller: Call exception, tries=13, retries=35, started=108742 ms ago, cancelled=false, msg=</p>","tags":["spark-sql","Hive","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-01 06:51:44.0","id":69427,"title":"DataNode give syntax error: unexpected end of file and failes to start","body":"<p><a href=\"/storage/attachments/9994-datanode-failed.jpg\">datanode-failed.jpg</a></p>","tags":["ambari-server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 05:34:36.0","id":69125,"title":"copy a file from local file system of a remote server","body":"<p>I have server A which also a node ,infact master node in hadoop environment.</p><p>This node's local FS has some files that need to be copied to HDFS</p><p>fs -put command is executed through oozie so it can be executed on any of the worker nodes.</p><p>How do I specify the server, local FS of  which has the files that need to be copied.</p><p>I came across this but it doesn't seem to work for me: I get:</p><pre>put: `/home/test/finalreports/accountReport.csv': No such file or directory</pre><pre>hadoop fs -fs masternode:8020 -put /home/test/finalreports/accountReport.csv hadoopFolderName/</pre>","tags":["node"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 17:24:38.0","id":69297,"title":"Does PDflush have any impact to NiFI?","body":"<p>Since NiFi uses JVM heap memory not sure if pdflush has any impact to nifi  performance.  Your insights appreciated.  </p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 23:20:04.0","id":69381,"title":"How to define Ranger policies in UI for non-leaf resource types ?","body":"<p>We are creating a new Ranger plugin for Apache HAWQ(incubating) service, which is similar to Postgres DB. We have defined the following resources: DATABASE, SCHEMA, TABLE as a resource hierarchy (schema is a parent of table and database is a parent of schema). All of them are marked as mandatory in service definition JSON, as the same table might exist in different schemas / databases so the parent resource types also serve as a namespace for the leaf types.</p><p>This allows us to create a policy such that a user can create any table in a given db/schema (X/Y) by specifying \"X\" for DATABASE and \"Y\" for SCHEMA and \"*\" for TABLE and assign CREATE access-type to the user.</p><p>However, how can we define a policy such that a user can create any (or specific) database only ? The Ranger Admin UI requires entries for resource sub-levels (schema and table in this case), so it is not possible to just specify \"*\" for DATABASE and nothing for schema and table. Removing mandatory designation from SCHEMA / TABLE is not an option either as they are required when working with tables.</p><p>The same question applies to SCHEMA resource, we want to be able to specify how users are allowed to interact with schemas. It seems that the policies can only be specified for leaf resources in the resource type hierarchy.</p><p><img src=\"/storage/attachments/9964-screen-shot-2016-11-30-at-31835-pm.png\"></p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-12-01 13:38:52.0","id":69474,"title":"[HDP 2.5] HDFS is not chekcpoint-ing","body":"<p>Hello,</p><p>We a running a POC with a small cluster. The stach is HDP 2.5, 8 Woker nodes and 4 Masters.</p><p>NameNode is running in HA mode. There was an infrastructure issue which rendered all the hosts inaccessible and all services went down. We were able to recover all services. HDFS is reported as HEALTHY with 0 corrupt/missing blocks  but doing any snapshots. There are two alarms about time since last snapshot and they seem in harmony with reality:</p><p>Here is a JMX snippet:</p><p> \"SavingCheckpointCount\" : 0,</p><p>\n    \"SavingCheckpointElapsedTime\" : 0, </p><p>    \"SavingCheckpointTotal\" : 0, </p><p>    \"SavingCheckpointPercentComplete\" : 1.0,</p><p>.</p><p>.</p><p>\"TransactionsSinceLastCheckpoint\" : 17752,\n    \"TransactionsSinceLastLogRoll\" : 0,</p><p>\n    \"LastWrittenTransactionId\" : 4375273,\n    \"LastCheckpointTime\" : 1480462943000,</p><p>Was wondering how to recover from this... Perhaps a manual snapshot? Putting HDFS into safe mode will be an effort as it is used (not heavily but...) </p><p>Thanks.</p>","tags":["hdp-2.5.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-29 21:05:00.0","id":69070,"title":"Sandbox 2.5 Cannot Install Service via Add Service Wizard","body":"<p><a href=\"/storage/attachments/9887-screen-shot-2016-11-29-at-203939-2.png\">screen-shot-2016-11-29-at-203939-2.png</a><a href=\"/storage/attachments/9888-screen-shot-2016-11-29-at-203753-2.png\">screen-shot-2016-11-29-at-203753-2.png</a><a href=\"/storage/attachments/9889-ambari-log.txt\">ambari-log.txt</a>I had a working instance of sandbox 2.5 installed, however due to some log issues with 2.5, I decided to delete the VM image and re install afresh. However, now that I have a 'fresh' install of sandbox 2.5 I am no longer able to add any services. When I attempt to add a service, I'm not able to progress from the '<strong>Customize Services</strong>' screen. I have attached some screen shots and a error log. Please can someone advise?</p>","tags":["ambari-service","installation"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-29 21:38:45.0","id":69075,"title":"Nifi - Beeline connection over  knox","body":"<p>Hi,</p><p>I have a nifi instance is running outside the cluster and my cluster is kerberos authenticated.</p><p>I am trying to see if Nifi putHQL processor works for beeline connection over knox authentication instead of Kerberos.</p><p>I can not use Kerberos authentication because of the firewall issues.</p><p>I use knox authentication to connect to hive from other data tools like Aqua data studio. I tried using the same with Nifi hive connection pool creation but it didn't work for me. </p><p>Any idea on how I can get this working in Nifi ?</p><p>Thanks</p><p>Aruna</p>","tags":["beeline","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 23:00:44.0","id":69380,"title":"Smartsense recommendations format","body":"<p>When we retrieve a report from Smartsense, I find myself manipulating the reports into a more format more usable to organize, prioritize and operationally execute changes. </p><p>In the future, can we expect to see either of these features?</p><p>Export recommendations to CSV in the support tools dashboard.</p><p>An API that I can interact with to retrieve recommendations once a bundle has been process.</p>","tags":["smartsense"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-29 19:09:26.0","id":69050,"title":"HDP 2.5 wiring architecture diagram","body":"<p>Is there any document that provides HDP 2.5 architecture diagrams?</p><p>I am looking for HDP 2.5 wiring architecture diagram</p>","tags":["architecture"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-11-30 08:05:20.0","id":69159,"title":"How to install S2Graph in HDP 2.5 ?","body":"<p>Hi, I need some help on how to install Apache s2graph in HDP 2.5. Is it possible ?</p>","tags":["graph-database"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-30 08:44:20.0","id":69167,"title":"What are the different methods to create 100 of policies in Apache Ranger in one go ??","body":"<p>Hi Guys,</p><p>I am trying to explore different ways of creating multiple policies in Apache Ranger. So far i haven't found any related documentation.Any help will be appreciated.</p><p>Thanks in Advance,</p><p>Subash</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-30 06:21:41.0","id":69135,"title":"Resource Manager Heap usage when cluster is idle","body":"<p>I notice when a cluster is idle (no jobs running) that the Resource Manager heap usage fluctuates between 6% up to 32%. This is when no jobs are running on the cluster. Is it normal for heap usage to get this high? I start the cluster and all services via Ambari, heap is at 6%, goes up to 32%, drops, and then repeats.</p>","tags":["resource-manager"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 18:27:18.0","id":69333,"title":"ambari-infra-solr-client rpm file","body":"<p>Ambari is unable to download ambari-infra-solr-client on HDP 2.5. This is needed for Ranger and Atlas. Does any one know where I can find the rpm file for this to install it manually.</p><p>I am having the same issue as here - https://community.hortonworks.com/questions/64893/when-using-hdp2500-i-installed-ambari-infra-or-ran.html</p><p>Thanks,</p><p>Raffi</p>","tags":["Ranger","Atlas"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-29 19:25:15.0","id":69045,"title":"Spark SQL join fails parse on temporary variable, works on real (orc) table","body":"<p><a href=\"/storage/attachments/9890-hdpjoinscalatempvariable.zip\">hdpjoinscalatempvariable.zip</a>  </p><p>Hello, can anyone see what I am doing incorrectly?  Here's the SQL against the real  table:</p><p>scala&gt; val sq110m_Q601 = sqlContext.sql(\"SELECT COUNT(*) FROM gendb2bg T1, gendb </p><p>2bg T2 WHERE T1.K100K = 49 AND T1.K250K = T2.k500k\")                             </p><p>16/11/29 17:48:38 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM gendb2bg T1, gendb2bg T2 WHERE T1.K100K = 49 AND T1.K250K = T2.k500k</p><p>16/11/29 17:48:38 INFO ParseDriver: Parse Completed</p><p>16/11/29 17:48:38 INFO OrcRelation: Listing hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/gendb2bg on driver</p><p>16/11/29 17:48:38 INFO OrcRelation: Listing hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/gendb2bg on driver</p><p>sq110m_Q601: org.apache.spark.sql.DataFrame = [_c0: bigint]</p><p>scala&gt;</p><p>Against temp variable, I've included a prior SQL to show the variable is assigned properly, notice the spurious column names:</p><p>16/11/29 17:49:28 INFO ParseDriver: Parsing command: SELECT K10, K25, COUNT(*) FROM db_temp1 GROUP BY K10, K25</p><p>16/11/29 17:49:28 INFO ParseDriver: Parse Completed</p><p>sq110m_Q503: org.apache.spark.sql.DataFrame = [K10: string, K25: string, _c2: bigint]</p><p>scala&gt; val sq110m_Q601 = sqlContext.sql(\"SELECT COUNT(*) FROM db_temp1 T1, db_te </p><p>mp1 T2 WHERE T1.K100K = 49 AND T1.K250K = T2.k500k\")                             </p><p>16/11/29 17:49:29 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM db_temp1 T1, db_temp1 T2 WHERE T1.K100K = 49 AND T1.K250K = T2.k500k</p><p>16/11/29 17:49:29 INFO ParseDriver: Parse Completed</p><p>org.apache.spark.sql.AnalysisException: cannot resolve 'T2.k500k' given input columns: [k10, k5m, _col13, _col1, _col7, _col14, s4, _col8, _col9, k1m, k10m, k100m, k2m, k25, _col23, kseq, sortchar, _col2, k500m, sortbin, s6, _col24, _col20, _col12, k100, k4, k1b, _col0, k5, _col21, sortpack, _col28, k100k, k1k, s7, _col17, s5, _col11, k500k, _col4, _col10, _col18, s8, _col15, _col16, _col25, k10k, _col6, _col26, kpart, _col19, k40k, k250k, _col3, _col22, _col5, _col27, k2]; line 1 pos 81</p><p>\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)</p><p>Thanks for taking the time to consider this issue.</p><p>Here's the DDL from a similar setup on Google Cloud Dataproc, works ok there:</p><p>CREATE TABLE SQ4</p><p>( KPART    int,</p><p>  KSEQ     int   ,</p><p>  K1B      int   ,</p><p>  K500M    int   ,</p><p>  K100M    int   ,</p><p>  K10M     int   ,</p><p>  K5M      int   ,</p><p>  K2M      int   ,</p><p>  K1M      int   ,</p><p>  K500K    int   ,</p><p>  K250K    int   ,</p><p>  K100K    int   ,</p><p>  K40K     int   ,</p><p>  K10K     int   ,</p><p>  K1K      int   ,</p><p>  K100     int   ,</p><p>  K25      int   ,</p><p>  K10      int   ,</p><p>  K5       int   ,</p><p>  K4       int   ,</p><p>  K2       int   ,</p><p>  SORTBIN  int   ,</p><p>  SORTPACK int   ,</p><p>  SORTCHAR string  ,</p><p>  S4       string  ,</p><p>  S5       string  ,</p><p>  S6       string  ,</p><p>  S7       string  ,</p><p>  S8       string)</p><p>row format delimited</p><p>fields terminated by ','</p><p>stored as orc</p><p>tblproperties (\"orc.compress\"=\"SNAPPY\",</p><p>'orc.block.size'='268435456');</p><p>Can anyone see what I am doing wrong?</p><p>Thank you.</p>","tags":["spark-sql"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-30 10:29:32.0","id":69187,"title":"How to programmatically change Ambari admin password?","body":"<p>I am working on automating the installation and provisioning of clusters, which includes installing Ambari and using a Blueprint via the API.  This is all working fine, but I have one more thing which I'm having trouble with, and that is changing the Ambari password during the overall provisioning process (I'm referring to my process here, not the Ambari cluster creation process).  What I do NOT want is to leave newly provisioned Ambari installs sitting around with the default \"admin/admin\" credentials, and I also can't have a human going in to change this every time.  </p><p>I've tried the REST API, but even though the call appears to execute correctly (eg, no errors are returned), the new password never takes effect.  </p><p>I thought about using the ambari-admin-password-reset command via ssh, but my Ambari install (using 2.1.0) doesn't appear to have that command. I've double checked that the agent is running, and I've tried it as the root user, and in every case, it yields a \"command not found\" error.</p><p>I also thought about using JDBC, connecting directly to Postgres, and munging the password directly, but I'm not sure which hash algorithm Ambari uses, and/or where it stores the salt (if one is used).  </p><p>If anyone can provide any advice on how to make any or all of these approaches work, it would be greatly appreciated.  Right now I'm about to tear my last hair out fighting with this. </p>","tags":["ambari-server","password"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 11:05:31.0","id":69200,"title":"Oozie - Scheduling - Pig and Hive Scripts and Spark","body":"<p>Hi all, </p><p>Is possible to create an workflow on Oozie that automatically execute some Hive, Pig and Spark scripts in order to automate my analytics process?</p><p>Many thanks!</p>","tags":["Oozie","Spark"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 08:59:02.0","id":69175,"title":"How to use wff:callback()??","body":"<p>Hi everybody,</p><p>For monitoring reasons I would like to use the wf:callback() function but I cannot get it running in Oozie.</p><p>I tried to send it with an email action using:</p><pre>    &lt;action name=\"email-test\"&gt;\n        &lt;email xmlns=\"uri:oozie:email-action:0.2\"&gt;\n            &lt;to&gt;${email}&lt;/to&gt;\n            &lt;subject&gt;${wf:name()} with ${wf:id()} &lt;/subject&gt;\n            &lt;body&gt;\n                Log Message:\n                Workflow Callback: ${wf:callback('ERROR')}\n            &lt;/body&gt;\n        &lt;/email&gt;\n        &lt;ok to=\"kill\"/&gt;\n        &lt;error to=\"kill\"/&gt;\n    &lt;/action&gt;\n</pre><p>I receive the following error:</p><p><img src=\"/storage/attachments/9933-01.png\"></p><p>How do I use the function properly?</p><p>Documentation just says</p><p>\"<strong>String wf:callback(String stateVar)</strong></p><p>It returns the callback URL for the current workflow action node, stateVar\n can be a valid exit state (=OK= or\n ERROR\n) for the action or a token to be replaced with the exit state by the remote system executing the task.\"</p><p>Thanks in advance!</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 07:43:31.0","id":69157,"title":"HIVE query takes very long time to fetch 20 gb records","body":"<p>Hi I have a hive table on HBASE that has 200gb of records .\nI am running simple hive query to fetch 20 gb records .\nBut this takes around 4 hours of time .\nI can not create partition on HIVE table cause it is integrated on HBASE.\n\nPlease suggest any idea to improve performance\n \nThis is my HIVE query \n\n  INSERT OVERWRITE LOCAL DIRECTORY '/hadoop/user/m6034690/FSDI/FundamentalAnalytic/FundamentalAnalytic_2014.txt'\n  ROW FORMAT DELIMITED\n  FIELDS TERMINATED BY '\\t'\n  STORED AS TEXTFILE\n  select * from hbase_table_FundamentalAnalytic  where FilePartition='ThirdPartyPrivate' and FilePartitionDate='2014'; </p>","tags":["Hbase","hive-hbase","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 08:51:17.0","id":69171,"title":"Is this Phoenix's bug","body":"<p>hi,I find a problem when  use phoenix'index  to query ?</p><p>\n First I Execute sql</p><p>:\n  0: jdbc:phoenix:zookeeper1&gt; <strong>select /*+ INDEX(\"SysActionLog\" \"SYSACTIONLOG_IDX\")*/ \"UserID\",\"RowKey\" from \"SysActionLog\"\n. . . . . . . . . . . . . &gt; where \"CreateTime\" &gt;= '20161130150108000' and \"CreateTime\" &lt;= '20161130160108000' order by \"CreateTime\" desc limit 10;\n</strong>+-----------------\n</p><p><img src=\"/storage/attachments/9930-1qa.png\"></p><p>sql\n0: jdbc:phoenix:zookeeper1&gt; <strong>select /*+ INDEX(\"SysActionLog\" \"SYSACTIONLOG_IDX\")*/ \"UserID\",\"RowKey\" from \"SysActionLog\"\n. . . . . . . . . . . . . &gt; where \"CreateTime\" &gt;= '20161130150108000' and \"CreateTime\" &lt;= '20161130160108000' order by \"CreateTime\" desc limit 20; </strong></p><p><strong></strong></p><p><img src=\"/storage/attachments/9931-ws.png\"></p><p>so  i Execute sql,\n0: </p><p>jdbc:phoenix:zookeeper<strong>1&gt; select /*+ INDEX(\"SysActionLog\" \"SYSACTIONLOG_IDX\")*/ \"UserID\",\"RowKey\" from \"SysActionLog\"</strong></p><p><strong>\n. . . . . . . . . . . . . &gt; where </strong><em><strong>\"RowKey\" &lt; '20161130150108010#Open#WebApi#POST##'</strong></em><strong> and \"CreateTime\" &gt;= '20161130150108000' and \"CreateTime\" &lt;= '20161130160108000'  order by \"CreateTime\" desc limit 10; </strong></p><p><img src=\"/storage/attachments/9932-w3333.png\"></p><p><strong>Problem is\nlast sql should be have  result.but can't find result .</strong></p><p><strong>\nIn sql I remove createtime filter only use RowKey  can query result </strong></p><p><strong>why? is this phoenix bug in phoenix 4.2 </strong></p><p><img src=\"/storage/attachments/9929-erer.png\"></p>","tags":["Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-01 14:22:16.0","id":69475,"title":"Evaluate Association Rules using FP-Gworth","body":"<p>Hi all,</p><p>Probably is a dummy question :) </p><p>How can I evaluate association rules from FP-Growth using Spark from:</p><p>http://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html</p><p>Since I don't have way to calculate the Lift...How can I detect that this is a strongly rule or not</p><p>Thanks!</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-12-01 04:58:15.0","id":69416,"title":"Kerberos hive authentication question","body":"<p>I have created a principal for myself called sami  just like hive principal created by ambari (see below) .  I also get a ticket  and I have added my username 'sami' to the Ranger HIVE policy in ambari . </p><p>what I am not understanding are two things: </p><p>a)  I can login into hive using hive principal ? I should not be allowed to use other principals </p><pre>!connect jdbc:hive2://hadoop2:10000/default;principal=hive/hadoop2@MY.COM\nDriver: Hive JDBC (version 1.2.1000.2.5.0.0-1245)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n\n</pre><p>b) If I try to use my principal i.e sami   I don't get connection  ,what I am missing ? </p><pre>!connect jdbc:hive2://hadoop2:10000/default;principal=hive/hadoop2@MY.COM\nError: Could not open client transport with JDBC Uri: jdbc:hive2://hadoop2:10000/default;principal=sami/hadoop2@MY.COM: Peer indicated failure: GSS initiate failed (state=08S01,code=0)\n\n\n</pre>\n<pre>sami/hadoop2.my.com@MY.COM\nsami/hadoop2@MY.COM\nsami@MY.COM\nkadmin.local:\nkadmin.local:  listprincs hive*\nhive/admin@MY.COM\nhive/hadoop1.my.com@MY.COM\nhive/hadoop2.my.com@MY.COM\nhive/hadoop3.my.com@MY.COM\nhive/hadoop4.my.com@MY.COM\nhive/hadoop5.my.com@MY.COM\nhive@MY.COM\n</pre>","tags":["kerberos","Ranger","hdp-2.5.0"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-30 08:23:16.0","id":69164,"title":"Registration with the server failed without any error!","body":"<p>Hello Everyone,</p><p>We setup ambari server on a main node and agents on 3 server.</p><p>We are trying to install hortonworks using ambari but host can not be regeistered. There is no error on status on confirm host phase.</p><p>We set iptables as off. Reverse IP/hostname settings are done. Hostname is correct. We can do passwordless ssh. We tried to register only 1 server but still same things happened. And all the other minimum system requirements, which is in the ambari installation document, are succesfully done. Hostname is correct in ambari-agent.ini files. There is no error or something relevant with this issue in ambari logs.</p><p> Do you have any suggestions?</p><p>Here is the status details:</p><pre>==========================\nCreating target directory...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nConnection to xxxx.thynet.thy.com closed.\nSSH command execution finished\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nCopying ambari sudo script...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nscp /var/lib/ambari-server/ambari-sudo.sh\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nCopying common functions script...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nscp /usr/lib/python2.6/site-packages/ambari_commons\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nCopying OS type check script...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nscp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.py\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nRunning OS type check...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\nCluster primary/cluster OS family is redhat6 and local/current OS family is redhat6\n\n\nConnection to xxxx.thynet.thy.com closed.\nSSH command execution finished\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nChecking 'sudo' package on remote host...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nConnection to xxxx.thynet.thy.com closed.\nSSH command execution finished\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nCopying repo file to 'tmp' folder...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nscp /etc/yum.repos.d/ambari.repo\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nMoving file to repo dir...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nConnection to xxxx.thynet.thy.com closed.\nSSH command execution finished\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nChanging permissions for ambari.repo...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nConnection to xxxx.thynet.thy.com closed.\nSSH command execution finished\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nCopying setup script file...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n\n\nscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:53:33\n\n\n==========================\nRunning setup agent script...\n==========================\n\n\nCommand start time 2016-11-30 10:53:33\n('INFO 2016-11-30 10:45:52,794 DataCleaner.py:120 - Data cleanup started\nINFO 2016-11-30 10:45:52,794 DataCleaner.py:122 - Data cleanup finished\nINFO 2016-11-30 10:45:52,910 PingPortListener.py:50 - Ping port listener started on port: 8670\nINFO 2016-11-30 10:45:52,912 main.py:349 - Connecting to Ambari server at https://xxxx.thynet.thy.com:8440 (192.168.246.144)\nINFO 2016-11-30 10:45:52,913 NetUtil.py:62 - Connecting to https://xxxx.thynet.thy.com:8440/ca\nINFO 2016-11-30 10:53:36,546 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:53:36,546 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:53:36,546 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:53:36,551 HeartbeatHandlers.py:83 - Ambari-agent received 15 signal, stopping...\nINFO 2016-11-30 10:54:06,649 main.py:226 - Agent not going to die gracefully, going to execute kill -9\nINFO 2016-11-30 10:54:06,657 ExitHelper.py:53 - Performing cleanup before exiting...\nINFO 2016-11-30 10:54:07,067 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:54:07,067 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:54:07,067 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:54:07,068 DataCleaner.py:39 - Data cleanup thread started\nINFO 2016-11-30 10:54:07,069 DataCleaner.py:120 - Data cleanup started\nINFO 2016-11-30 10:54:07,070 DataCleaner.py:122 - Data cleanup finished\nINFO 2016-11-30 10:54:07,189 PingPortListener.py:50 - Ping port listener started on port: 8670\nINFO 2016-11-30 10:54:07,191 main.py:349 - Connecting to Ambari server at https://xxxx.thynet.thy.com:8440 (192.168.246.144)\nINFO 2016-11-30 10:54:07,191 NetUtil.py:62 - Connecting to https://xxxx.thynet.thy.com:8440/ca\n', None)\n('INFO 2016-11-30 10:45:52,794 DataCleaner.py:120 - Data cleanup started\nINFO 2016-11-30 10:45:52,794 DataCleaner.py:122 - Data cleanup finished\nINFO 2016-11-30 10:45:52,910 PingPortListener.py:50 - Ping port listener started on port: 8670\nINFO 2016-11-30 10:45:52,912 main.py:349 - Connecting to Ambari server at https://xxxx.thynet.thy.com:8440 (192.168.246.144)\nINFO 2016-11-30 10:45:52,913 NetUtil.py:62 - Connecting to https://xxxx.thynet.thy.com:8440/ca\nINFO 2016-11-30 10:53:36,546 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:53:36,546 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:53:36,546 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:53:36,551 HeartbeatHandlers.py:83 - Ambari-agent received 15 signal, stopping...\nINFO 2016-11-30 10:54:06,649 main.py:226 - Agent not going to die gracefully, going to execute kill -9\nINFO 2016-11-30 10:54:06,657 ExitHelper.py:53 - Performing cleanup before exiting...\nINFO 2016-11-30 10:54:07,067 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:54:07,067 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:54:07,067 main.py:90 - loglevel=logging.INFO\nINFO 2016-11-30 10:54:07,068 DataCleaner.py:39 - Data cleanup thread started\nINFO 2016-11-30 10:54:07,069 DataCleaner.py:120 - Data cleanup started\nINFO 2016-11-30 10:54:07,070 DataCleaner.py:122 - Data cleanup finished\nINFO 2016-11-30 10:54:07,189 PingPortListener.py:50 - Ping port listener started on port: 8670\nINFO 2016-11-30 10:54:07,191 main.py:349 - Connecting to Ambari server at https://xxxx.thynet.thy.com:8440 (192.168.246.144)\nINFO 2016-11-30 10:54:07,191 NetUtil.py:62 - Connecting to https://xxxx.thynet.thy.com:8440/ca\n', None)\n\n\nConnection to xxxx.thynet.thy.com closed.\nSSH command execution finished\nhost=xxxx.thynet.thy.com, exitcode=0\nCommand end time 2016-11-30 10:54:09\n\n\nRegistering with the server...\nRegistration with the server failed.\n\n</pre>","tags":["ambari-server","Ambari","hortonwork"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 09:10:43.0","id":69177,"title":"How to  update/Import Hive metadata  into Apache Atlas in automated way ??","body":"<p>hey Guys,</p><p>Does Apache Atlas refreshes its metadata repository whenever any changes occurs in hive Database?? </p><p>Let us consider a scenario where user A created a table in hive. Changes will reflect in Hive Database, but it seems that metadata in Apache Atlas doesn't refresh.After running import-hive.sh shell script,Hive metadata of Apache Atlas refreshes. </p><p>Are there any properties or configuration which needs to be changed to automate the update of metadata repository.Any help would be appreciated.</p><p>I am using  Apache Atlas 0.5.0.2.3</p><p>Thanks in Advance,</p><p>Subash</p>","tags":["atlas-api","Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-30 15:20:41.0","id":69247,"title":"NiFi thread node per processor control","body":"<p>I'm seeing behavior in my environment were the work between processors is being handled by more than one node/thread resulting in multiples of the payload being created.   For instance, the ExecuteSQL processor uses three nodes (I'm not sure I'm using the correct terminology here, but the number shows in the upper right of the processor box when active) which each generates a separate copy of the results set.   Another potential issue outside of being inefficient is that the results aren't always exactly the same, but I use xpath to evaluate the results downstream.   I'm assuming there are configuration properties that will globally influence this behavior, but I'm also interested in more per processor tuning options.  Can you offer any suggestion regarding how to control this behavior?</p>","tags":["nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-29 17:21:30.0","id":69030,"title":"How to run a spark Job from Eclipse IDE to my HDP spark cluster ?","body":"<p>Dear Team,</p><p>I am developing Spark streaming application . For my development I am using Eclipse IDE (PyDev) tool, I can able to test my cods in standalone local machine. I have a test 6-Node Hadoop cluster (HDP 2.4) also available. To continue my development I want to run my code directly in my test cluster from Eclipse which is in my laptop. Did any one follow this practise ?</p><p>Thanks in advance for your support.</p><p>Br,</p><p>Ganesan</p>","tags":["Spark","spark-streaming"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-30 17:31:36.0","id":69311,"title":"Ambari View - Pig View - Files view","body":"<p>Hello everyone,</p><p>I recently install HDP 2.5 and in Ambari UI i cant to acces to Hive view (it's blank). It is the same with pig and Tez and i don't have all directory with files view.</p><p>I don't know what's the problem and how can i solve that? </p><p>Can you help me please?</p><p>Thanks</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 02:07:41.0","id":69118,"title":"java.io.IOException: Previous writer likely failed to write hdfs://sandbox.hortonworks.com:8020/tmp/hive/hive/_tez_session_dir/cfcd5d27-c46c-440e-b6e2-51bf35bcbf43/hive-hcatalog-core.jar. Failing because I am unlikely to write too","body":"<p>Hi All,</p><p>I've a Kerberized HDP 2.4 cluster, and i'm trying to access Hive on command line.</p><p>I'm logged in as - root, got the kerberos token for user - hive</p><p>On hive command, i get the following error -</p><p>Any ideas ?</p><p>------------------------------------------------------------------</p><p>[root@sandbox ~]# hive\nWARNING: Use \"yarn jar\" to launch YARN applications.\n\nLogging initialized using configuration in file:/etc/hive/2.4.0.0-169/0/hive-log4j.properties\nException in thread \"main\" java.lang.RuntimeException: java.io.IOException: Previous writer likely failed to write hdfs://sandbox.hortonworks.com:8020/tmp/hive/hive/_tez_session_dir/cfcd5d27-c46c-440e-b6e2-51bf35bcbf43/hive-hcatalog-core.jar. Failing because I am unlikely to write too.\n   at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:507)\n   at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:680)\n   at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)\n   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   at java.lang.reflect.Method.invoke(Method.java:606)\n   at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n   at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.io.IOException: Previous writer likely failed to write hdfs://sandbox.hortonworks.com:8020/tmp/hive/hive/_tez_session_dir/cfcd5d27-c46c-440e-b6e2-51bf35bcbf43/hive-hcatalog-core.jar. Failing because I am unlikely to write too.\n   at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982)\n   at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862)\n   at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeTempFilesFromConf(DagUtils.java:805)\n   at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.refreshLocalResourcesFromConf(TezSessionState.java:233)\n   at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:158)\n   at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:117)\n   at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:504)\n   ... 8 more\n[root@sandbox ~]# klist\nTicket cache: FILE:/tmp/krb5cc_0\nDefault principal: hive/sandbox.hortonworks.com@EXAMPLE.COM\n\nValid starting  Expires  Service principal\n11/30/16 02:03:26  12/01/16 02:03:26  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n   renew until 11/30/16 02:03:26</p>","tags":["security","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-01 01:41:37.0","id":69401,"title":"Error sending metrics to server. 'NoneType' object has no attribute 'encode'","body":"<p>when i tail -f /var/log/ambari-metrics-monitor/ambari-metrics-monitor.out, i see some error like  \"Error sending metrics to server. 'NoneType' object has no attribute 'encode'\";</p><p>and when i tail -f /var/log/hbase/hbase-hbase-regionserver-ambari-agent.hugedata.com.log, i see some erro like \"[HBase-Metrics2-1] impl.MetricsSystemImpl: Error creating sink 'timeline'\norg.apache.hadoop.metrics2.impl.MetricsConfigException: Error creating plugin: org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink\nat org.apache.hadoop.metrics2.impl.MetricsConfig.getPlugin(MetricsConfig.java:203)\nat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.newSink(MetricsSystemImpl.java:529)\nat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configureSinks(MetricsSystemImpl.java:501)\nat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configure(MetricsSystemImpl.java:480)\nat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:189)\nat org.apache.hadoop.metrics2.impl.JmxCacheBuster$JmxCacheBusterRunnable.run(JmxCacheBuster.java:109)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\nat java.util.concurrent.FutureTask.run(FutureTask.java:166)\nat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\nat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:722)\nCaused by: java.lang.NullPointerException\nat org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink.init(HadoopTimelineMetricsSink.java:108)\nat org.apache.hadoop.metrics2.impl.MetricsConfig.getPlugin(MetricsConfig.java:199)\n... 13 more\"</p>","tags":["ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-29 19:13:04.0","id":69041,"title":"KDC test failing","body":"<p>I am in the Kerberos install menu and I choose the options \"existing MIT KDC\"  but on the next screen (see attached picture) the KDC test fails . </p><p>this is my first time installing Ranger so I am not even sure if I should be choosing this option so please guide me on how to install/fix the Kerberos issue on HDP2.5. </p><p><a href=\"/storage/attachments/9869-capture.jpg\">capture.jpg</a></p>","tags":["hdp-2.5.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-12-01 14:59:57.0","id":69488,"title":"HUE not running, requires python 2.6,HUE service fails due to python 2.6","body":"<p>I have a AWS cluster and I have installed HDP and other services through ambari using the below doc.</p><p><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_installing_manually_book/content/ch_installing_hue_chapter.html\">https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_installing_manually_book/content/ch_installing_hue_chapter.html</a></p><p>I checked and it says that hue has been installed properly</p><p>sudo yum install hue\nLoaded plugins: amazon-id, rhui-lb, search-disabled-repos\nPackage hue-2.6.1.2.3.4.0-3485.el6.x86_64 already installed and latest version\nNothing to do</p><p>The problem is that I am not able to start the HUE server using the command</p><p>/etc/init.d/hue start</p><p>Job for hue.service failed because the control pr ocess exited with error code. See \"systemctl status hue.service\" and \"journalctl -xe\" for details.</p><p>and when I run the above command \"systemctl status hue.service\"</p><p>It shows the error as follows:</p><p>/usr/bin/env: python2.6: No such file or directory\npam_unix(runuser:session): session closed for user hue\npam_unix(runuser:session): session opened for user hue by (uid=0)\nStarting hue: /usr/bin/env: python2.6: No such file or directory\npam_unix(runuser:session): session closed for user hue\n[FAILED]\nhue.service: control process exited, code=exited status=127\nFailed to start SYSV: Hue web server.\nUnit hue.service entered failed state.\nhue.service failed.</p><p>I already have the python 2.7 installed by default by ambari 2.2.0. Can someone put some light on this?</p>","tags":["hue","python"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 06:55:13.0","id":69150,"title":"Connection Refused - namenode on 8020","body":"<p>Hi,</p><p>I cannot start NameNode during the installation of HDP2.5 on AWS. It gives me the following error, </p><p>2016-11-30 01:23:51,115 WARN  datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(222)) - Problem connecting to server: ip-172-31-15-40.us-west-2.compute.internal/54.191.198.14:8020</p><p>I do the installation on my local and so there is not applied firewall ;however, i ran iptables commands to disable firewall rules.</p><p>When i do telnet to the 8020, it gives me the connection refused.</p><p>Can you please help me? Should i change the namenode parameters and choose different port?</p><p>SJ</p>","tags":["aws"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-30 01:51:56.0","id":69082,"title":"max value of previous 30 days for every record of a table in hive","body":"<p>hi, i have a table in which I need to calculate the max among the previous 30 days for every record , is there a way in hive with sub queries?, see below for sample data</p>\n<p><table><tbody><tr><td>FIELD1</td><td>FIELD2</td><td>FIELD3</td><td>DATE</td><td>VAR</td><td>MAX VAR</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20160930</td><td>1</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161001</td><td>1</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161002</td><td>1</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161003</td><td>1</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161004</td><td>1</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161005</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161006</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161007</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161008</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161009</td><td>1</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161010</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161011</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161012</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161013</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161014</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161015</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161016</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161017</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161018</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161019</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161020</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161021</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161022</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161023</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161024</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161025</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161026</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161027</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161028</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161029</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161030</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161031</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161101</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161102</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161103</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161104</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161105</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161106</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161107</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161108</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161109</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161110</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161111</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161112</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161113</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161114</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161115</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161116</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161117</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161118</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161119</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161120</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161121</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161122</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161123</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161124</td><td>2</td><td>3</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161125</td><td>2</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161126</td><td>2</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161127</td><td>2</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161128</td><td>2</td><td>2</td></tr><tr><td>USA</td><td>TX</td><td>RTB</td><td>20161129</td><td>2</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20160930</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161001</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161002</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161003</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161004</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161005</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161006</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161007</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161008</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161009</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161010</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161011</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161012</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161013</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161014</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161015</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161016</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161017</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161018</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161019</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161020</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161021</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161022</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161023</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161024</td><td>1</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161025</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161026</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161027</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161028</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161029</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161030</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161031</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161101</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161102</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161103</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161104</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161105</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161106</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161107</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161108</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161109</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161110</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161111</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161112</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161113</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161114</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161115</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161116</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161117</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161118</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161119</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161120</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161121</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161122</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161123</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161124</td><td>2</td><td>4</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161125</td><td>2</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161126</td><td>2</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161127</td><td>2</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161128</td><td>2</td><td>2</td></tr><tr><td>UK</td><td>LONDON</td><td>RTB</td><td>20161129</td><td>2</td><td>2</td></tr></tbody></table></p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-29 17:41:55.0","id":69024,"title":"HDF 2.5 - Ambari server and agent are started but fail to launch on browser","body":"<p>HDF 2.5 - Ambari server and agent are started but fail to launch on browser.</p><p>I am using Virtual Box on Windows 10. The <a href=\"http://127.0.0.1:8888/\">http://127.0.0.1:8888/</a> is able to lunch but when I try to ambari url 127.0.0.1:8080 it doesn't work.</p><p>I tried looking at eth0 inet url which is <a href=\"http://172.17.0.2:8080/\">http://172.17.0.2:8080/</a> as mentioned in few of the questions in community site..but then also it  doesn't work.</p><p>Kindly suggest what need to be done here?</p>","tags":["ambari-server"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-29 19:14:45.0","id":69042,"title":"Need to confirm use of MergeContent is appropriate","body":"<p>My flow is such</p><p>Get accounts from database</p><p>Get child records (type codes) for each account - one account can have multiple type codes</p><p>Derive attribute based on multiple child records</p><p>so if Acct A has one row with typecd = ABC set attribute ABC_IND =1 </p><p>If Acct A has one row with typecd = XYZ set attribute XYZ_IND = 1</p><p>Merge all these attributes back to the original account record</p><p>Write the original and derived attributes to file</p><p>I am using the mergecontent to merge all the derived attributes flowfile back to the account flowfile based on attribute - acctNum</p><p>It worked on couple of records but when I ran for 8 records where 4 had child records and 4 did not I should have gotten 4 merged records written to file but I only got 2. My Merge Content is using bin-packing algorithm</p><p>Can merge be used for merging child records back to parent when I have not used any split processor?</p>","tags":["Nifi","nifi-processor","mergecontent"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-29 16:59:33.0","id":69015,"title":"Hive with HBase integration: how to load dates","body":"<p>Dear all,</p><p>\nI created the following table in HBase (using Phoenix): </p><pre>CREATE TABLE test_table (username VARCHAR PRIMARY KEY, firstlogin DATE, lastlogin DATE);\nUPSERT INTO test_table VALUES ('abc', '2016-03-05', '2016-11-20'); </pre><p>When I query tha data, I get the expected result:</p><pre> \n USERNAME  |        FIRSTLOGIN        |        LASTLOGIN         |\n+-----------+--------------------------+--------------------------+\n| abc       | 2016-03-05 00:00:00.000  | 2016-11-20 00:00:00.000 </pre><p>\nAfter this, I want to access this data from Hive, creating the following table: </p><pre>CREATE EXTERNAL TABLE hive_test(\nUSERNAME STRING,\nFIRSTLOGIN INT,\nLASTLOGIN INT\n) \nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,0:FIRSTLOGIN#b,0:LASTLOGIN#b\")\nTBLPROPERTIES (\"hbase.table.name\" = \"TEST_TABLE\"); </pre><p>However, when querying the data, I do not get the expected results for FIRSTLOGIN and LASTLOGIN columns, that are supposed to be unix timestamps: </p><pre> hive_test.username  | hive_test.firstlogin  | hive_test.lastlogin  |\n+---------------------+-----------------------+----------------------+--+\n| abc                 | -2147483309           | -2147483304    \n</pre><p>What am I doing wrong?</p><p>Thanks in advance!</p>","tags":["Hbase","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 16:35:31.0","id":69275,"title":"Nifi putHDFS Error","body":"<p>I am trying to send data into HDFS from my local mac. I have Nifi installed on my Mac and I downloaded the conf files from ambari.. Then I created a getFile and putHDFS process too do the streaming of files. Now I'm getting the error attached... Can anyone point out what i'm doing wrong?</p><p><img src=\"/storage/attachments/9945-screen-shot-2016-11-30-at-111811-am.png\"></p><p><a href=\"/storage/attachments/9941-screen-shot-2016-11-30-at-111811-am.png\">screen-shot-2016-11-30-at-111811-</a></p><p><img src=\"/storage/attachments/9943-screen-shot-2016-11-30-at-111837-am.png\"></p><p><img src=\"/storage/attachments/9944-screen-shot-2016-11-30-at-111844-am.png\"></p><p><a href=\"/storage/attachments/9941-screen-shot-2016-11-30-at-111811-am.png\">am.png</a></p><p><img src=\"/storage/attachments/9942-screen-shot-2016-11-30-at-111828-am.png\"></p>","tags":["nifi-streaming","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-30 09:19:23.0","id":69189,"title":"Configure Email group for alerts","body":"<p>Hi,</p><p>Are we able to configure email groups in the PutEmail processor for alerts? I have configured an email group (#usergroup@domain.com) with the PutEmail but it doesn't work. Any clue?</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-01 09:13:22.0","id":69441,"title":"Does Ranger provide any method to create access Permissions and policy for Linux directories (Edge Node)? Currently we are using Ranger to create policies for HDFS and other hadoop services.","body":"","tags":["ranger-admin","Ranger"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-30 15:45:26.0","id":69278,"title":"Not able to read Event Hub data using Spark Streaming API","body":"<p>Hi All</p><p>I have created a Event Hub and sending data to it .I have a Spark HDInsight cluster created . I am trying the code from below link to read the data from the event hub <a href=\"https://github.com/hdinsight/spark-streaming-data-persistence-examples\">https://github.com/hdinsight/spark-streaming-data-persistence-examples</a> by following the steps mentioned in this link https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-eventhub-streaming.I am not able get the data in Azure Blob Storage .</p><p>As per Spark Logs .I am facing these below exceptions</p><p>Failed to connect to\nserver: spark master node :/yarn resource manager scheduler address :8030</p><p>and </p><pre>ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error handling message, restarting receiver - com.microsoft.azure.servicebus.CommunicationException: java.nio.channels.UnresolvedAddressException. This is usually caused by incorrect hostname or network configuration. Please check to see if namespace information is correct.\n\tat com.microsoft.azure.servicebus.MessagingFactory$RunReactor.run(MessagingFactory.java:363)</pre><p> I have changed Event hub namespace to the proper one with *.servicebus.windows.net.</p><p>But I think their is an issue in resource manager and node manager connection .</p><p>Regards</p><p>Ashish P.B </p><p><a href=\"https://github.com/hdinsight/spark-streaming-data-persistence-examples\"></a></p>","tags":["spark-streaming"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-12-01 01:45:24.0","id":69386,"title":"NiFi secure site to site(RPG) with HAproxy","body":"<p>Hi all, </p><p>Our team create five nodes NiFi cluster by HDF. </p><p>And I have configured NiFi to run securely for Multi-Tenant Authorization purpose. </p><p>I have tested Site-to-Site with RAW and HTTP Protocol. And no problem. </p><p>Just for test, so I configure the RPG to the NiFi cluster itself. </p><p>But after I set <strong>HTTP Proxy Server properties in RPG(HAProxy with SSL Pass-Through). </strong></p><p>The Remote Process Group always show WARN. </p><p><img src=\"/storage/attachments/9966-rpg.jpg\"></p><p>Here is some logs & configs</p><p>Thanks for your help. </p><p>(一)NiFi config </p><p>nifi.remote.input.host=hdf-1.persp.net </p><p>\nnifi.remote.input.http.enabled=true </p><p>\nnifi.remote.input.secure=True </p><p>nifi.web.https.host=hdf-1.persp.net </p><p>\nnifi.web.https.port=9091 </p><p>(二)RPG config</p><p><img src=\"/storage/attachments/9967-rpg-config.jpg\"></p><p>\n(三)HAproxy config </p><p><img src=\"/storage/attachments/9968-haproxy.jpg\"></p><p>\n(四)nifi-app.log </p><p>1.access RPG through Proxy</p><p>org.apache.http.NoHttpResponseException</p><p><img src=\"/storage/attachments/9969-nohttpresponseexception.jpg\"></p><p>2.try to put data to RPG input port</p><p>2016-12-01 09:18:44,734 ERROR [I/O dispatcher 4297] o.a.n.r.util.SiteToSiteRestApiClient Failed to create transaction for https://hdf-1.persp.net:9091/nifi-api/data-transfer/input-ports/52353928-15b3-1976-bdf2-3e23779a1727/transactions</p><p>(五)HAproxy log </p><p>Nov 30 18:32:20 localhost haproxy[15318]: 172.18.0.191:43956 [30/Nov/2016:18:32:20.777] NiFi nifi/nifi 1/0/2 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:20 localhost haproxy[15318]: 172.18.0.191:43958 [30/Nov/2016:18:32:20.779] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:20 localhost haproxy[15318]: 172.18.0.191:43960 [30/Nov/2016:18:32:20.781] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:20 localhost haproxy[15318]: 172.18.0.191:43962 [30/Nov/2016:18:32:20.782] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:26 localhost haproxy[15318]: 172.18.0.193:50792 [30/Nov/2016:18:32:26.369] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:26 localhost haproxy[15318]: 172.18.0.193:50794 [30/Nov/2016:18:32:26.371] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:26 localhost haproxy[15318]: 172.18.0.193:50796 [30/Nov/2016:18:32:26.373] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:26 localhost haproxy[15318]: 172.18.0.193:50798 [30/Nov/2016:18:32:26.375] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:28 localhost haproxy[15318]: 172.18.0.194:49186 [30/Nov/2016:18:32:28.096] NiFi nifi/nifi 1/0/2 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:28 localhost haproxy[15318]: 172.18.0.194:49188 [30/Nov/2016:18:32:28.099] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:28 localhost haproxy[15318]: 172.18.0.194:49190 [30/Nov/2016:18:32:28.100] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0 Nov 30 18:32:28 localhost haproxy[15318]: 172.18.0.194:49192 [30/Nov/2016:18:32:28.102] NiFi nifi/nifi 1/0/1 0 -- 0/0/0/0/0 0/0</p>","tags":["Nifi","apache-nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-01 15:00:16.0","id":69477,"title":"Ambari Hive View - User's Saved Queries Location","body":"<p>In the Ambari Hive view, there is a \"Saved Queries\" tab, where are these queries saved? Are they in the Ambari DB? A local file on the Ambari node? HDFS?</p>","tags":["hive-views","Ambari"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-29 21:28:09.0","id":69063,"title":"Nifi processor for file search and manipulation and executing shell script","body":"<p>hi Matt,</p><p>I am just starting to learn and use Nifi, I have project for automatic data loading in to database. I am working Well LAS 2.0 Files. ( <a href=\"http://www.cwls.org/wp-content/uploads/2014/09/LAS_20_Update_Jan2014.pdf\">http://www.cwls.org/wp-content/uploads/2014/09/LAS_20_Update_Jan2014.pdf</a> )</p><p>1)- I will pickup the files from a dropbox  </p><p>2)- Read the LAS files for metadata and create a file to put into HDFS, process it at spark/hive to get the unique values from all this thousand files insert/update target database tables </p><p>3)- Also capture the UWI, NULL, SRVC and DATE , $HDFS/FILENAME  to pass this a c-shell script for execution one at a time </p><p>Any guidance ? </p><p>Regards,</p><p>Biswa</p>","tags":["nifi-processor"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-11-30 08:02:58.0","id":69158,"title":"NIFI splittext to split the single file into multiple","body":"<p>I split the file successfully using splitText into three file but when I try store using putfile its throwing error as 'file with same name'. Please suggest which processor do I need to use to save the files in same directory. I used update attribute to update file name but it still throwing the same error</p>","tags":["nifi-processor","Nifi"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 06:24:14.0","id":69141,"title":"SOLR Cloud & HDFS - problems with write.lock","body":"<p>I am trying to create a multi-node SOLR Cloud configuration using HDFS for index storage.</p><p>I keep running into problems with a 'write.lock' when I try to create a multi-shard index</p><p>For example, I have 2 SOLR nodes running on ports 34010 and 34011.  If I try to create a 2-shard collection; one shard gets created successfully but the other fails to create because of an existing write.lock.  It appears that the 2 nodes are getting in each other's way.  </p><p>The following CREATE command:</p><p>http://xxxxxx:34010/solr/admin/collections?action=CREATE&name=car_cloud&numShards=2&replicationFactor=1&maxShardsPerNode=2&collection.configName=car_cloud</p><p>Returns the following error.  It shows that one shard is successfully created but the other fails because of an existing write.lock.  (Note, before running this command I deleted all the relevant directories from hdfs (hdfs dfs -rm -r /solr/car_cloud) so the lock file did not exist prior to running the command.</p><pre>&lt;response&gt;\n&lt;lst name=\"responseHeader\"&gt; \n&lt;int name=\"status\"&gt;0&lt;/int&gt; \n&lt;int name=\"QTime\"&gt;34958&lt;/int&gt;\n&lt;/lst&gt; \n&lt;lst name=\"failure\"&gt; \n&lt;str name=\"99.9.99.9:34010_solr\"&gt;\norg.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException:Error from server at http://99.9.99.9:34010/solr: Error CREATEing SolrCore 'car_cloud_shard2_replica1': Unable to create core [car_cloud_shard2_replica1] Caused by: /solr/car_cloud/core_node1/data/index/write.lock for client 99.9.99.9 already exists at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2606) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)\n&lt;/str&gt; \n&lt;/lst&gt;\n&lt;lst name=\"success\"&gt; \n&lt;lst name=\"99.9.99.9:34011_solr\"&gt; \n&lt;lst name=\"responseHeader\"&gt; \n&lt;int name=\"status\"&gt;0&lt;/int&gt; \n&lt;int name=\"QTime\"&gt;4031&lt;/int&gt;\n&lt;/lst&gt; \n&lt;str name=\"core\"&gt;car_cloud_shard1_replica1&lt;/str&gt;\n&lt;/lst&gt;\n&lt;/lst&gt;\n&lt;/response&gt;</pre><p>Any suggestions?</p>","tags":["SOLR","cloud"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-30 13:18:55.0","id":69220,"title":"How to create a dataframe for HBase table access with SHC in Python?","body":"","tags":["Hbase","spark-sql"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 17:29:45.0","id":69301,"title":"Why Co Processor taking more time to process the request.?","body":"<p>Dear All,</p><p>We are using the Coprocessor to fetch the records from the HBase .There are 3 Region Servers present each having 60 regions currently ,  But when we call the coprocessor it making making a call to all the regions(I think it is obvious with coprocessor). But as the each Region Server is getting the 60 RPC calls the time taken to process the last regions is increasing always . Result of it the over all time taken by coprocessor is more few seconds .   Example :   Rpc Request. Time Taken in Seconds  [B.defaultRpcServer.handler=46,queue=4,port=16020]  20 [B.defaultRpcServer.handler=24,queue=0,port=16020]  10  [B.defaultRpcServer.handler=40,queue=4,port=16020]   4 [B.defaultRpcServer.handler=33,queue=3,port=16020]   4 [B.defaultRpcServer.handler=38,queue=2,port=16020]   3 [B.defaultRpcServer.handler=38,queue=2,port=16020]   3  [B.defaultRpcServer.handler=49,queue=1,port=16020]   2  [B.defaultRpcServer.handler=7,queue=1,port=16020]    1   Could some please explain . Why it taking more time for the last RPCs . What does B.defaultRpcServer.handler=46,queue=4,port=16020 mean in the Region Server log. </p><p>Thanks in Advance, </p><p>Param.</p>","tags":["Hbase","coprocessors"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-30 13:28:55.0","id":69221,"title":"Could anyone send me some documents about the detail of advantages and release notes of Atlas?","body":"<p>I want to write some introduction about\nAtlas, but the information, which official webside atlas.apache.org provides, is not\nenough.</p><p>So I need some documents such as .pdf or\n.ppt file about the following aspects:</p><ul>\n<li>The advantages of Atlas</li><li>The release note or history of Atlas</li><li>The introduction of architecture of Atlas</li><li>Other overall information</li></ul><p>Thank you very much.</p>","tags":["Atlas","documents","hdp-2.4.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-23 22:32:16.0","id":7901,"title":"How to set a timeout on the oozie launcher?","body":"<p>customer wants to set a timeout value for oozie launcher -- I did not see any oozie specific property that allowed to set a timeout on the launcher. We also tried oozie.launcher.dfs.socket.timeout and oozie.launcher.mapreduce.task.timeout  unsuccessfully -- ideas?</p>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-25 06:07:31.0","id":8044,"title":"express upgrade ambari 2.2 stuck","body":"<p>I started an express upgrade on ambari 2.2 and now I keep getting this error in the log and no upgrade is stuck.</p><p>I am upgrading from 2.2.4 to 2.3.2</p><p>25 Dec 2015 00:03:12,810  WARN [ambari-action-scheduler] ActionScheduler:200 - Exception received\njava.lang.NullPointerException\nat org.apache.ambari.server.orm.entities.StageEntity.isAutoSkipOnFailureSupported(StageEntity.java:260)\nat org.apache.ambari.server.actionmanager.Stage.&lt;init&gt;(Stage.java:132)\nat org.apache.ambari.server.actionmanager.StageFactoryImpl.createExisting(StageFactoryImpl.java:77)\nat org.apache.ambari.server.actionmanager.ActionDBAccessorImpl.getStagesInProgress(ActionDBAccessorImpl.java:256)\nat org.apache.ambari.server.actionmanager.ActionScheduler.doWork(ActionScheduler.java:230)\nat org.apache.ambari.server.actionmanager.ActionScheduler.run(ActionScheduler.java:195)\nat java.lang.Thread.run(Thread.java:745)</p><p>thanks </p>","tags":["Ambari","upgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-28 08:57:08.0","id":8124,"title":"returns empty result set when using TimestampType and NullType as StructType","body":"<p>\n\tBelow code returns empty resullt set as I used TimeStamp as one of the StructField</p>\n<pre>15/12/28 03:34:27 INFO SparkILoop: Created sql context (with Hive support)..\nSQL context available as sqlContext.\n\nscala&gt; import org.apache.spark.sql.hive.HiveContext\nimport org.apache.spark.sql.hive.HiveContext\n\nscala&gt; import org.apache.spark.sql.hive.orc._\nimport org.apache.spark.sql.hive.orc._\n\nscala&gt; val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n15/12/28 03:34:57 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n15/12/28 03:34:57 INFO HiveContext: Initializing execution hive, version 0.13.1\nhiveContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@3413fbe\n\nscala&gt; import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,FloatType ,LongType ,TimestampType,NullType };\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType, LongType, TimestampType, NullType}\n\nscala&gt; val loandepoSchema = StructType(Seq(\n     | StructField(\"COLUMN1\", StringType, true),\n     | StructField(\"COLUMN2\", StringType  , true),\n     | StructField(\"COLUMN3\", TimestampType , true),\n     | StructField(\"COLUMN4\", TimestampType , true),\n     | StructField(\"COLUMN5\", StringType , true),\n     | StructField(\"COLUMN6\", StringType, true),\n     | StructField(\"COLUMN7\", IntegerType, true),\n     | StructField(\"COLUMN8\", IntegerType, true),\n     | StructField(\"COLUMN9\", StringType, true),\n     | StructField(\"COLUMN10\", IntegerType, true),\n     | StructField(\"COLUMN11\", IntegerType, true),\n     | StructField(\"COLUMN12\", IntegerType, true),\n     | StructField(\"COLUMN13\", StringType, true),\n     | StructField(\"COLUMN14\", StringType, true),\n     | StructField(\"COLUMN15\", StringType, true),\n     | StructField(\"COLUMN16\", StringType, true),\n     | StructField(\"COLUMN17\", StringType, true),\n     | StructField(\"COLUMN18\", StringType, true),\n     | StructField(\"COLUMN19\", StringType, true),\n     | StructField(\"COLUMN20\", StringType, true),\n     | StructField(\"COLUMN21\", StringType, true),\n     | StructField(\"COLUMN22\", StringType, true)))\nloandepoSchema: org.apache.spark.sql.types.StructType = StructType(StructField(COLUMN1,StringType,true), StructField(COLUMN2,StringType,true), StructField(COLUMN3,TimestampType,true), StructField(COLUMN4,TimestampType,true), StructField(COLUMN5,StringType,true), StructField(COLUMN6,StringType,true), StructField(COLUMN7,IntegerType,true), StructField(COLUMN8,IntegerType,true), StructField(COLUMN9,StringType,true), StructField(COLUMN10,IntegerType,true), StructField(COLUMN11,IntegerType,true), StructField(COLUMN12,IntegerType,true), StructField(COLUMN13,StringType,true), StructField(COLUMN14,StringType,true), StructField(COLUMN15,StringType,true), StructField(COLUMN16,StringType,true), StructField(COLUMN17,StringType,true), StructField(COLUMN18,StringType,true), StructField(COLUMN19,Strin...\nscala&gt; val lonadepodf = hiveContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").schema(loandepoSchema).load(\"/tmp/TestDivya/loandepo_10K.csv\")\n15/12/28 03:37:52 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.\nlonadepodf: org.apache.spark.sql.DataFrame = [COLUMN1: string, COLUMN2: string, COLUMN3: timestamp, COLUMN4: timestamp, COLUMN5: string, COLUMN6: string, COLUMN7: int, COLUMN8: int, COLUMN9: string, COLUMN10: int, COLUMN11: int, COLUMN12: int, COLUMN13: string, COLUMN14: string, COLUMN15: string, COLUMN16: string, COLUMN17: string, COLUMN18: string, COLUMN19: string, COLUMN20: string, COLUMN21: string, COLUMN22: string]\n\nscala&gt; lonadepodf.select(\"COLUMN1\").show(10)\n15/12/28 03:38:01 INFO MemoryStore: ensureFreeSpace(216384) called with curMem=0, maxMem=278302556\n15/12/28 03:38:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 211.3 KB, free 265.2 MB)\n...............................................................................\n15/12/28 03:38:07 INFO DAGScheduler: ResultStage 2 (show at &lt;console&gt;:33) finished in 0.653 s\n15/12/28 03:38:07 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool\n15/12/28 03:38:07 INFO DAGScheduler: Job 2 finished: show at &lt;console&gt;:33, took 0.669388 s\n+-------+\n|COLUMN1|\n+-------+\n+-------+\n</pre><p>\nOnce Timestamp StructField is removed . Result set is returned </p>\n<pre>\nscala&gt; val loandepoSchema = StructType(Seq(\n     | StructField(\"COLUMN1\", StringType, true),\n     | StructField(\"COLUMN2\", StringType  , true),\n     | StructField(\"COLUMN3\", StringType , true),\n     | StructField(\"COLUMN4\", StringType , true),\n     | StructField(\"COLUMN5\", StringType , true),\n     | StructField(\"COLUMN6\", StringType, true),\n     | StructField(\"COLUMN7\", IntegerType, true),\n     | StructField(\"COLUMN8\", IntegerType, true),\n     | StructField(\"COLUMN9\", StringType, true),\n     | StructField(\"COLUMN10\", IntegerType, true),\n     | StructField(\"COLUMN11\", IntegerType, true),\n     | StructField(\"COLUMN12\", IntegerType, true),\n     | StructField(\"COLUMN13\", StringType, true),\n     | StructField(\"COLUMN14\", StringType, true),\n     | StructField(\"COLUMN15\", StringType, true),\n     | StructField(\"COLUMN16\", StringType, true),\n     | StructField(\"COLUMN17\", StringType, true),\n     | StructField(\"COLUMN18\", StringType, true),\n     | StructField(\"COLUMN19\", StringType, true),\n     | StructField(\"COLUMN20\", StringType, true),\n     | StructField(\"COLUMN21\", StringType, true),\n     | StructField(\"COLUMN22\", StringType, true)))\nloandepoSchema: org.apache.spark.sql.types.StructType = StructType(StructField(COLUMN1,StringType,true), StructField(COLUMN2,StringType,true), StructField(COLUMN3,StringType,true), StructField(COLUMN4,StringType,true), StructField(COLUMN5,StringType,true), StructField(COLUMN6,StringType,true), StructField(COLUMN7,IntegerType,true), StructField(COLUMN8,IntegerType,true), StructField(COLUMN9,StringType,true), StructField(COLUMN10,IntegerType,true), StructField(COLUMN11,IntegerType,true), StructField(COLUMN12,IntegerType,true), StructField(COLUMN13,StringType,true), StructField(COLUMN14,StringType,true), StructField(COLUMN15,StringType,true), StructField(COLUMN16,StringType,true), StructField(COLUMN17,StringType,true), StructField(COLUMN18,StringType,true), StructField(COLUMN19,StringType,...\nscala&gt; val lonadepodf = hiveContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").schema(loandepoSchema).load(\"/tmp/TestDivya/loandepo_10K.csv\")\nlonadepodf: org.apache.spark.sql.DataFrame = [COLUMN1: string, COLUMN2: string, COLUMN3: string, COLUMN4: string, COLUMN5: string, COLUMN6: string, COLUMN7: int, COLUMN8: int, COLUMN9: string, COLUMN10: int, COLUMN11: int, COLUMN12: int, COLUMN13: string, COLUMN14: string, COLUMN15: string, COLUMN16: string, COLUMN17: string, COLUMN18: string, COLUMN19: string, COLUMN20: string, COLUMN21: string, COLUMN22: string]\n\nscala&gt; lonadepodf.select(\"COLUMN1\").show(10)\n15/12/28 03:39:48 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.31.20.85:40013 in memory (size: 4.2 KB, free: 265.3 MB)\n\n15/12/28 03:39:49 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool\n15/12/28 03:39:49 INFO DAGScheduler: Job 6 finished: show at &lt;console&gt;:33, took 0.223277 s\n+-------+\n|COLUMN1|\n+-------+\n|   CTR0|\n|   CTR1|\n|   CTR2|\n|   CTR3|\n|   CTR4|\n|   CTR5|\n|   CTR6|\n|   CTR7|\n|   CTR8|\n|   CTR9|\n+-------+\n</pre>","tags":["hdp-2.3.2","spark-csv","dataframe","spark-1.4.1"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-28 14:31:46.0","id":8143,"title":"sqoop sqlserver with windows authentication","body":"<p>i'm trying to sqoop (Linux server) data from Sql server...i was able to do it with local account on sql server, but i want to try with Windows authentication</p>","tags":["windows","Sqoop"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-25 23:53:40.0","id":8069,"title":"Ambari 2.2 Kafka broker.id missing?","body":"<p>Installing Ambari 2.2, it seem that the kafka properties broker.id is missing, causing kafka broker not to start.  Is this a bug? I had to add it to the config as a custom broker.</p>","tags":["Kafka","Ambari","ambari-2.2.0"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-28 14:50:18.0","id":8140,"title":"how to see metrics per region server","body":"<p>currently in ambari 2.1.2</p><p>i can see metrics for ALL region servers when i create a widget.</p><p>can i create metrics per region server.</p><p>objective: Find out  whether hot spotting is occurring or not. </p><p>Currently i am at a loss,because all metrics for hbase rs are AGGREGATED  </p>","tags":["ambari-metrics","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-30 08:29:59.0","id":8353,"title":"which directory ambari is installed at?","body":"<p>I tried to install HDP using ambari. </p><p>At my computer, <a href=\"http://node1:8080/#/main\">http://node1:8080/#/main</a> works.</p><p>Actually I want to see the real page that the site address works.</p><p>at which directory, I can see the pages.</p><p>If anybody knows that, please tell me. Thanks.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-23 09:56:09.0","id":7807,"title":"Storm supervisor is not writing pid in /var/run/storm on a worker node even access rights are given.","body":"<p>Ambari 1.2.1, HDP 2.3.0, Storm 0.10.0</p><p>Supervisor get started via Ambari, but the service check via jps fails. When I touch the PID file manually as user storm and run a service check status goes to green.</p>","tags":["ambari-service","ambari-2.1.2","supervisors","pid","Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-05 03:30:36.0","id":8719,"title":"Decommission of datanode","body":"<p><a href=\"https://cwiki.apache.org/confluence/display/AMBARI/API+to+decommission+DataNodes+and+NodeManagers\">https://cwiki.apache.org/confluence/display/AMBARI...</a></p><p>When the REST API is used to trigger the decommission of the datanode a request ID is returned since its an async operation (see <a href=\"https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md#asynchronous-response\">https://github.com/apache/ambari/blob/trunk/ambari...</a>)</p><p><a href=\"https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md#asynchronous-response\"></a>After polling on the status of the returned request ID for sometime, the request status shows \"COMPLETED\". However the decommission is actually still progressing (incomplete!)</p><p>Is this a bug ? Shouldn't the Ambari asynchronous request be marked COMPLETED only after the decommission has finished completely ?</p>","tags":["hadoop","datanode","decommission"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-03 08:30:36.0","id":8574,"title":"why fail to regist ambari2.2.0  agent on centos 6,because of timeout to down repomd.xml,but wget *.repmd.xml is OK","body":"<pre>==========================\nRunning setup agent script...\n==========================\n\nCommand start time 2016-01-03 15:48:09\n<a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> [Errno 12] Timeout on <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> (28, 'connect() timed out!')\nTrying other mirror.\n<a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> [Errno 12] Timeout on <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> (28, 'connect() timed out!')\nTrying other mirror.\n<a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> [Errno 12] Timeout on <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> (28, 'connect() timed out!')\nTrying other mirror.\nError Downloading Packages:\n  ambari-agent-2.2.0.0-1310.x86_64: failure: ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm from Updates-ambari-2.2.0.0: [Errno 256] No more mirrors to try.http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml: [Errno 12] Timeout on <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> (28, 'connect() timed out!')\nTrying other mirror.\n<a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> [Errno 12] Timeout on <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> (28, 'connect() timed out!')\nTrying other mirror.\nError Downloading Packages:\n  ambari-agent-2.2.0.0-1310.x86_64: failure: ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm from Updates-ambari-2.2.0.0: [Errno 256] No more mirrors to try.http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml: [Errno 12] Timeout on <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> (28, 'connect() timed out!')\nTrying other mirror.\n<a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> [Errno 12] Timeout on <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm:\">http://public-repo-1.hortonworks.com/ambari/cento...</a> (28, 'connect() timed out!')\nTrying other mirror.\nError Downloading Packages:\n  ambari-agent-2.2.0.0-1310.x86_64: failure: ambari/ambari-agent-2.2.0.0-1310.x86_64.rpm from Updates-ambari-2.2.0.0: [Errno 256] No more mirrors to try.but, manual wget is OK:wget <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml\">http://public-repo-1.hortonworks.com/ambari/cento...</a>\n--2016-01-03 15:57:28--  <a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/repodata/repomd.xml\">http://public-repo-1.hortonworks.com/ambari/cento...</a>\n正在连接 *.*.*:9999... 已连接。\n已发出 Proxy 请求，正在等待回应... 200 OK\n长度：2985 (2.9K) [text/xml]\n正在保存至: “repomd.xml”100%[===================================================================================================================================================&gt;] 2,985  --.-K/s  in 0s  2016-01-03 15:57:28 (95.3 MB/s) - 已保存 “repomd.xml” [2985/2985])</pre>","tags":["help","timeout","ambari-2.2.0","centos","repomd.xml"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-05 17:03:26.0","id":8822,"title":"Possible to Utilize CBO in the Hive view?","body":"<p>I'm following a tutorial on the Hortonworks site, <a href=\"http://hortonworks.com/hadoop-tutorial/supercharging-interactive-queries-hive-tez/.\">http://hortonworks.com/hadoop-tutorial/superchargi...</a></p><p>I'm wondering if it is possible to get the CBO working. I've tried setting the 4 params and getting the table stats, but for some reason I'm getting an error which looks like: </p><pre>   org.apache.ambari.view.hive.client.HiveErrorStatusException: H110 Unable to submit statement. Error while processing statement: Cannot modify set hive.cbo.enable at runtime. It is not in list of params that are allowed to be modified at runtime [ERROR_STATUS]</pre><p>I get the error across 3 different params. How do I get the CBO working?</p>","tags":["ambari-views","Hive","Ambari"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-05 16:00:57.0","id":8798,"title":"Ambari Views - Server Side Development Best Practice for Iterative Development","body":"<p>When customers are developing server side resources for their View, working with <a href=\"https://cwiki.apache.org/confluence/display/AMBARI/Framework+Services\">Framework Services</a> (ViewContext, InstanceData, Config Properties, events, etc.) - what is the best practice recommendation for rapid iterative development and unit test cycle? </p><p>E.g. if I want to quickly develop and unit test a servlet which reads data from ViewContext - what is the best way to execute and unit test this servlet? Do I need to first deploy it to a live Ambari Server instance for the initial Unit Test? Or are there other ways to more quickly unit test my servlet?</p><p>Thanks!</p>","tags":["development","Ambari","ambari-views"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-02 04:07:21.0","id":8527,"title":"Error executing DISTINCT Function in Pig","body":"<p>I am trying to execute the following Pig Script.  DISTINCT is not working.  Am I missing anything.  Please help.</p><p>A = LOAD '/tmp/admin/data/gpa.txt' using PigStorage(',') AS (name, age, gpa);\nB = group A by age;\nC = foreach B generate ABS(SUM(A.gpa)), DISTINCT(A.name), MIN(A.gpa)+MAX(A.gpa)/2, group;  \ndump C;</p><pre>2016-01-02 04:03:21,049 [main] ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. Could not resolve DISTINCT using imports: [, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.]\nFailed to parse: Pig script failed to parse: \n&lt;file script.pig, line 6, column 40&gt; Failed to generate logical plan. Nested exception: org.apache.pig.backend.executionengine.ExecException: ERROR 1070: Could not resolve DISTINCT using imports: [, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.]</pre>","tags":["Pig","hive-udf"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-06 18:27:37.0","id":8985,"title":"If we are using slider to start the package and run that application into yarn container, do we need to mention node label as well in the command while starting the application through slider,in order to run on particular node?","body":"","tags":["slider","yarn-node-labels","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-06 19:01:46.0","id":8977,"title":"how to use Sqoop to export data from Hive to a flat file to be picked up by Hive again?","body":"<p>Can anyone let me know how to use Sqoop to export data from Hive to a flat file to be picked up by Hive again?</p><p>I understand that Sqoop can export data to tables, however I have a scenario where I want to export data into flat files</p>","tags":["Sqoop","Hive","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-06 21:12:43.0","id":8995,"title":"Access to Hive from Oozie java action with Kerberos","body":"<p>Hi,</p><p>I am trying to execute  Hive query from java action that is part of Oozie workflow. My preferred way is to use the Beeline or JBDC rather than old Hive CLI. However I am struggling with this a little bit, since the connection is failing due to authentication errors. When the java action code is executed on some data node in a cluster, Oozie does not do kinit for the user and thus the connection fails. Both Beeline and JDBC connection string seem to support delegation tokens, but those token can only be obtained when user is logged in by Kerberos (kinited).</p><p>We are currently using hive-0.14.0 and oozie-4.1. </p><p>I have found out that  new hive2 action introduced in oozie-4.2 seems to first create jdbc connection under Oozie Kerberos login, obtain delegation token from this connection and finally pass this token to the Beeline. Maybe the same approach could be used here as well. It would require a new custom oozie action (e.g. java-jdbc). . Seems possible but it is quite complicated; is there some easier way?</p><p>Thanks for any comments,</p><p>Pavel</p>","tags":["Oozie","java","jdbc","Hive","kerberos"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 09:56:11.0","id":6316,"title":"slave VM removed from list of slaves and still being accessed by Yarn/Tez","body":"<p>So I removed the vm4 from the list of slave VMs and when I run the following command it doesn't access it </p><pre>hdfs dfsadmin -report</pre>\n<p>result is:</p><pre>ubuntu@anmol-vm1-new:~$ hdfs dfsadmin -report\n15/12/1406:56:12 WARN util.NativeCodeLoader:Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nConfiguredCapacity:1268169326592(1.15 TB)PresentCapacity:1199270457337(1.09 TB)\nDFS Remaining:1199213064192(1.09 TB)\nDFS Used:57393145(54.73 MB)\nDFS Used%:0.00%Under replicated blocks:27Blocks with corrupt replicas:0Missing blocks:0-------------------------------------------------Datanodes available:3(3 total,0 dead)Live datanodes:Name:10.0.1.191:50010(anmol-vm2-new)Hostname: anmol-vm2-newDecommissionStatus:NormalConfiguredCapacity:422723108864(393.69 GB)\nDFS Used:19005440(18.13 MB)Non DFS Used:21501829120(20.03 GB)\nDFS Remaining:401202274304(373.65 GB)\nDFS Used%:0.00%\nDFS Remaining%:94.91%ConfiguredCacheCapacity:0(0 B)CacheUsed:0(0 B)CacheRemaining:0(0 B)CacheUsed%:100.00%CacheRemaining%:0.00%Last contact:MonDec1406:56:12 UTC 2015Name:10.0.1.190:50010(anmol-vm1-new)Hostname: anmol-vm1-newDecommissionStatus:NormalConfiguredCapacity:422723108864(393.69 GB)\nDFS Used:19369984(18.47 MB)Non DFS Used:25831350272(24.06 GB)\nDFS Remaining:396872388608(369.62 GB)\nDFS Used%:0.00%\nDFS Remaining%:93.88%ConfiguredCacheCapacity:0(0 B)CacheUsed:0(0 B)CacheRemaining:0(0 B)CacheUsed%:100.00%CacheRemaining%:0.00%Last contact:MonDec1406:56:13 UTC 2015Name:10.0.1.192:50010(anmol-vm3-new)Hostname: anmol-vm3-newDecommissionStatus:NormalConfiguredCapacity:422723108864(393.69 GB)\nDFS Used:19017721(18.14 MB)Non DFS Used:21565689863(20.08 GB)\nDFS Remaining:401138401280(373.59 GB)\nDFS Used%:0.00%\nDFS Remaining%:94.89%ConfiguredCacheCapacity:0(0 B)CacheUsed:0(0 B)CacheRemaining:0(0 B)CacheUsed%:100.00%CacheRemaining%:0.00%Last contact:MonDec1406:56:11 UTC 2015</pre>\n<p>however at some point Yarn tries to access it. Here's the log I received:</p><pre>yarn logs -applicationId application_1450050523156_0009</pre>\n<p><a href=\"http://pastebin.com/UVHnkRRp\">http://pastebin.com/UVHnkRRp</a></p><pre>Service org.apache.tez.dag.app.rm.TaskScheduler failed in state STARTED; cause: java.lang.IllegalArgumentException: java.net.UnknownHostException: anmol-vm4-new\n        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:377)\n        at org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager.newInstance(BaseNMTokenSecretManager.java:145)\n        at org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager.createNMToken(BaseNMTokenSecretManager.java:136)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM.createAndGetOptimisticNMToken(NMTokenSecretManagerInRM.java:325)\n        at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:297)\n        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)\n        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2014)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2010)\n        at java.security.AccessController.doPrivileged(NativeMethod)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1561)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2008)Caused by: java.net.UnknownHostException: anmol-vm4-new...15 more</pre>\n<p>Any idea why is it trying to access VM4 which is not in slaves list and how that could be fixed?</p><p>UPDATE:\nI did the following but still I receive an error because it tries to access <code>vm4</code>:</p><p>1)add the files <code>exclude</code> and <code>mapred.exclude</code> in <code>conf</code> directory of yarnpp including the private IP address of vm4.</p><p>2)add this in <code>mapred-site.xml</code>:</p><pre>&lt;property&gt;&lt;name&gt;mapred.hosts.exclude&lt;/name&gt;&lt;value&gt;/home/hadoop/yarnpp/conf/mapred.exclude&lt;/value&gt;&lt;description&gt;Names a file that contains the list of hosts that\n      should be excluded by the jobtracker.If the value is empty, no\n      hosts are excluded.&lt;/description&gt;&lt;/property&gt;</pre>\n<p>3)add this to <code>hdfs-site.xml</code>:</p><pre>&lt;property&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt;&lt;value&gt;/home/hadoop/yarnpp/conf/exclude&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;/property&gt;</pre>\n<p>3.5) added this to <code>yarn-site.xml</code>:</p><pre>&lt;property&gt;&lt;name&gt;yarn.resourcemanager.nodes.exclude-path&lt;/name&gt;&lt;value&gt;/home/hadoop/yarnpp/conf/exclude&lt;/value&gt;&lt;description&gt;Path to file with nodes to exclude.&lt;/description&gt;&lt;/property&gt;</pre>\n<p>4)run cp_host.sh to copy the conf directory to all the slaves!</p><p>5)run reboot_everything script (which does <code>stop-all.sh</code>, formatting and <code>start-all.sh</code>)</p><p>6) <code>hadoop dfsadmin -refreshNodes</code></p><p>7) run this command in master VM:</p><pre> yarn rmadmin -refreshNodes</pre>\n<p>And here's the new log: <a href=\"http://pastebin.com/cKPY9gmB\">http://pastebin.com/cKPY9gmB</a></p><p><img src=\"/storage/attachments/800-screen-shot-2015-12-14-at-23915-am.png\"></p>","tags":["ui","Tez","hadoop","HDFS","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-09 19:55:51.0","id":5665,"title":"Getting HConnection as null , Failed to create local dir /data0/hadoop/hbase/local/jars, DynamicClassLoader failed to init","body":"<p>I am trying to connect to HBase server running on different server \nfrom a linux client and i get below error . The code works fine from my \nwindows laptop and I am able to connect to Hbase server and get results.\n I think I am missing some dependencies jar for my linux server  ibecause \nwhen i added hbase-client jar it worked from my laptop, which indicates \nmy code logic is correct. All of configuration is being picked up \ncorrectly as I have verified it from  my laptop. Please provide some \nsuggestion. I am passing hbase-site.xml,core-site.xml,hdfs-site.xml in \nmy resources . My port and zookeeper qurom is correct. My kerberose code\n works fine. I don't understand if this  can be permission issue too. i don't understand why this happening and when does it happen.</p><p>Any help or suggestion is much appreciated</p><p>Code : connection is returned as null :-( </p><pre>this.conf =HBaseConfiguration.create();\nthis.conf.set(\"hbase.zookeeper.quorum\", zookeeperQuorum);\nthis.conf.set(\"hbase.zookeeper.property.clientPort\", port);\nthis.conf.set(\"zookeeper.znode.parent\",\"/hbase-secure\");\n//  this.conf.set(\"hbase.client.retries.number\", Integer.toString(35));\n//  this.conf.set(\"zookeeper.session.timeout\", Integer.toString(20000));\n//this.conf.set(\"zookeeper.recovery.retry\", Integer.toString(1));\nthis.conf.set(\"hadoop.security.authentication\",\"kerberos\");\nthis.conf.set(\"hbase.security.authentication\",\"kerberos\");\nthis.conf.set(\"hbase.master.kerberos.principal\", userName);this.conf.set(\"user.name\",userName)\n\n;try{this.connection =HConnectionManager.createConnection(conf);}catch(IOException e){// TODO Auto-generated catch block\n            e.printStackTrace();}\n\npom.xml :&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 &lt;a href=\"http://maven.apache.org\"&gt;http://maven.apache.org&lt;/a&gt; /xsd/maven-4.0.0.xsd\"&gt;\n&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n&lt;groupId&gt;org.msoa.hbase.client&lt;/groupId&gt;\n&lt;artifactId&gt;simpleHBase&lt;/artifactId&gt;\n&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n&lt;packaging&gt;jar&lt;/packaging&gt;\n&lt;name&gt;HbaseWrite&lt;/name&gt;\n&lt;url&gt;http://maven.apache.org&lt;/url&gt;\n\n&lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n&lt;configuration&gt;&lt;archive&gt;\n&lt;manifest&gt;\n&lt;mainClass&gt;simpleHBase.actionClass&lt;/mainClass&gt;\n&lt;/manifest&gt;\n&lt;/archive&gt;\n&lt;descriptorRefs&gt;\n&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n&lt;/descriptorRefs&gt;&lt;/configuration&gt;\n&lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt;\n\n&lt;!--  added for dev box  --&gt;\n&lt;repositories&gt;&lt;repository&gt;&lt;id&gt;repo.hortonworks.com&lt;/id&gt;&lt;name&gt;Hortonworks HDP MavenRepository&lt;/name&gt;&lt;url&gt;http://repo.hortonworks.com/content/repositories/releases/&lt;/url&gt;&lt;/repository&gt;&lt;/repositories&gt;&lt;!--  end dev box --&gt;\n\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;junit&lt;/groupId&gt;\n&lt;artifactId&gt;junit&lt;/artifactId&gt;\n&lt;version&gt;3.8.1&lt;/version&gt;\n&lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n&lt;groupId&gt;jdk.tools&lt;/groupId&gt;&lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;&lt;scope&gt;system&lt;/scope&gt;&lt;version&gt;1.7.0_60&lt;/version&gt;\n&lt;systemPath&gt;C:\\Program Files\\Java\\jdk1.7.0_60\\lib\\tools.jar&lt;/systemPath&gt;&lt;/dependency&gt;\n\n&lt;!--   adding to test on beam --&gt;\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;&lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;!--  add protocol for beam test--&gt;\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&lt;artifactId&gt;hbase-protocol&lt;/artifactId&gt;&lt;version&gt;0.98.0-hadoop2&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&lt;artifactId&gt;hbase-client&lt;/artifactId&gt;&lt;version&gt;0.98.0-hadoop2&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&lt;artifactId&gt;hbase-common&lt;/artifactId&gt;&lt;version&gt;0.98.0-hadoop2&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&lt;artifactId&gt;hbase-protocol&lt;/artifactId&gt;&lt;version&gt;0.98.0-hadoop2&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&lt;artifactId&gt;hbase-server&lt;/artifactId&gt;&lt;version&gt;0.98.0-hadoop2&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.springframework&lt;/groupId&gt;&lt;artifactId&gt;spring-core&lt;/artifactId&gt;&lt;version&gt;4.2.3.RELEASE&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.springframework&lt;/groupId&gt;&lt;artifactId&gt;spring-context&lt;/artifactId&gt;&lt;version&gt;4.2.3.RELEASE&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;dependency&gt;&lt;groupId&gt;org.springframework&lt;/groupId&gt;&lt;artifactId&gt;spring-beans&lt;/artifactId&gt;&lt;version&gt;4.2.3.RELEASE&lt;/version&gt;&lt;/dependency&gt;\n\n&lt;/dependencies&gt;\n\n\n\nError : \n\n\n\n  java.io.IOException: java.lang.reflect.InvocationTargetException\n          at \norg.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:416)\n\n          at \norg.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:309)\n\n          at \nsimpleHBase.HBaseConnectionFactory.(HBaseConnectionFactory.java:99)\n          at simpleHBase.HBaseClient.(HBaseClient.java:26)\n          at simpleHBase.actionClass.main(actionClass.java:118) Caused \nby: java.lang.reflect.InvocationTargetException\n          at \nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n          at \nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\n          at \nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\n          at \njava.lang.reflect.Constructor.newInstance(Constructor.java:408)\n          at \norg.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:414)\n\n          ... 4 more Caused by: java.lang.ExceptionInInitializerError\n          at \norg.apache.hadoop.hbase.ClusterId.parseFrom(ClusterId.java:64)\n          at \norg.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:69)\n\n          at \norg.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:83)\n\n          at \norg.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.retrieveClusterId(HConnectionManager.java:857)\n\n          at \norg.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.(HConnectionManager.java:662)\n\n          ... 9 more Caused by: java.lang.RuntimeException: Failed to \ncreate local dir /data0/hadoop/hbase/local/jars, DynamicClassLoader\n  failed to init\n          at \norg.apache.hadoop.hbase.util.DynamicClassLoader.(DynamicClassLoader.java:94)\n\n          at \norg.apache.hadoop.hbase.protobuf.ProtobufUtil.(ProtobufUtil.java:201)\n          ... 14 more\n\n\n</pre>","tags":["Hbase","hadoop","java"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-25 18:49:42.0","id":309,"title":"Issue in connecting Kafka from outside Sandbox","body":"<p>I am using hortonwork Sandbox for kafka server trying to connect kafka from eclipse with java code . Use this configuration to connect to producer to send the message</p><pre>metadata.broker.list=sandbox.hortonworks.com:45000\nserializer.class=kafka.serializer.DefaultEncoder\nzk.connect=sandbox.hortonworks.com:2181\nrequest.required.acks=0\nproducer.type=sync\n</pre>\n<p>where sandbox.hortonworks.com is sandboxname to whom i connect</p><p>in kafka server.properties I changed this configuration</p><pre>host.name=sandbox.hortonworks.com\n\nadvertised.host.name=System IP(on which my eclipse is running)\nadvertised.port=45000\n</pre>\n<p>did the port forwarding also , </p><p>I am able to connect to kafka server from eclipse but while sending the message get the exception Exception\"Failed to send messages after 3 tries.\"</p>","tags":["Kafka","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-09 15:47:47.0","id":5566,"title":"/bin/bash: /bin/java: No such file or directory Error on Mac EL Captain 10.11","body":"<p>0<a href=\"http://stackoverflow.com/questions/33968422/bin-bash-bin-java-no-such-file-or-directory/33969096#\">favorite</a></p><p>I was trying to run a simple wordcount MapReduce Program using Java 1.7 SDK and Hadoop2.7.1 on Mac OS X EL Captain 10.11 and I am getting the following error message in my container log \"stderr\" /bin/bash: /bin/java: No such file or directory</p><p>Application Log-</p><pre>5/11/2702:52:33 WARN util.NativeCodeLoader:Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n15/11/2702:52:33 INFO client.RMProxy:Connecting to ResourceManager at /192.168.200.96:803215/11/2702:52:34 INFO input.FileInputFormat:Total input paths to process :015/11/2702:52:34 INFO mapreduce.JobSubmitter: number of splits:015/11/2702:52:34 INFO mapreduce.JobSubmitter:Submitting tokens for job: job_1448608832342_0003\n15/11/2702:52:34 INFO impl.YarnClientImpl:Submitted application application_1448608832342_0003\n15/11/2702:52:34 INFO mapreduce.Job:The url to track the job: &lt;a href=\"http://mymac.local:8088/proxy/application_1448608832342_0003/15/11/2702:52:34\"&gt;http://mymac.local:8088/proxy/application_1448608...&lt;/a&gt; INFO mapreduce.Job:Running job: job_1448608832342_0003\n15/11/2702:52:38 INFO mapreduce.Job:Job job_1448608832342_0003 running in uber mode :false15/11/2702:52:38 INFO mapreduce.Job:  map 0% reduce 0%15/11/2702:52:38 INFO mapreduce.Job:Job job_1448608832342_0003 failed with state FAILED due to:Application application_1448608832342_0003 failed 2 times due to AM Containerfor appattempt_1448608832342_0003_000002 exited with  exitCode:127For more detailed output, check application tracking page:http://mymac.local:8088/cluster/app/application_1448608832342_0003Then, click on links to logs of each attempt.Diagnostics:Exception from container-launch.Container id: container_1448608832342_0003_02_000001\nExit code:127Stack trace:ExitCodeException exitCode=127:\n    at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)\n    at org.apache.hadoop.util.Shell.run(Shell.java:456)\n    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)\n    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)Container exited with a non-zero exit code 127Failingthis attempt.Failing the application.15/11/2702:52:38 INFO mapreduce.Job:Counters:0</pre>\n<pre>The command I am running-</pre>\n<pre>hadoop jar wordcount.jar org.myorg.WordCount /user/gangadharkadam/input/ /user/gangadharkadam/output/</pre>\n<pre>My ENV Variable are as below-</pre>\n<pre>export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home\nexport HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.1\nexport HADOOP_MAPRED_HOME=/usr/local/hadoop/hadoop-2.7.1\nexport HADOOP_COMMON_HOME=/usr/local/hadoop/hadoop-2.7.1\nexport HADOOP_HDFS_HOME=/usr/local/hadoop/hadoop-2.7.1\nexport YARN_HOME=/usr/local/hadoop/hadoop-2.7.1\nexport HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-2.7.1/etc/hadoop\nexport HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar\n\nexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:HADOOP_HOME/sbin:$M2_HOME/bin:$ANT_HOME/bin:$IVY_HOME/bin:$FB_HOME/bin:$MYSQL_HOME/bin:$MYSQL_HOME/lib:$SQOOP_HOME/bin\n</pre>\n<pre>The problem seems to be because YARN is using different path for JAVA executable different then you have in your OS. the local logs for the failed task in “stderr” shows- /bin/bash: /bin/java: No such file or directory\nI tried to create a soft link from $JAVA_HOM/bin/java to /bin/java but in El Captian OS X but its not allowing to create a softlink. The New OS X EL Captian has a rootless login and user can not create anything on certain restricted folders like /bin/. Any work around on this issue is highly appreciated.Thanks in advance.\n\n</pre>","tags":["MapReduce","hadoop","HDFS","osx"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-28 14:06:54.0","id":356,"title":"One JDBC connection to HiveServer2 shared with multiple threads cause Exceptions","body":"<p>I have a main thread that opens a JDBC connection to HiveServer2, this connection object is shared by multiple threads. The thread has a prepared statement and executes a select query(not CRUD) and does some processing with the resultset.</p><p>I am trying this with Hive as I have some legacy code from the product I work on which I dont want to change and I know that this works with Oracle.</p><p>Below is the stack trace of the exception.</p><pre>org.apache.thrift.transport.TTransportException\nat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\nat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\nat org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)\nat org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)\nat org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)\nat org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)\nat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\nat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\nat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\nat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\nat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\nat org.apache.hive.service.cli.thrift.TCLIService$Client.recv_FetchResults(TCLIService.java:501)\nat org.apache.hive.service.cli.thrift.TCLIService$Client.FetchResults(TCLIService.java:488)\nat org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:360)\nat hivetrial.RSIterator2.run(ConcurrentRSIteration2.java:60)\nat java.lang.Thread.run(Unknown Source)</pre>\n<p>Trying to understand if this is a limitation.</p>","tags":["Hive","jdbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-07 17:26:04.0","id":9117,"title":"FAILED: SemanticException FAILED: SemanticException Unable to fetch table <-->","body":"<p></p><p>1) What is the cause of this?</p><p>2) What is the work around?</p>","tags":["Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-07 19:49:33.0","id":9168,"title":"Using HDInsight as a production Hadoop cluster?","body":"<p>Curious to know what do folks think about using HDInsight as a production cluster.</p><p>Requirements: </p><p>- daily ETL from MSSQL server from customer site into HDInsight </p><p>- pulling data into Hive </p><p>- transformations on data -- so far possible though HiveQL and UDF </p><p>- connection to Tableau</p>","tags":["azure","use-cases","hdinsight"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-01-07 15:25:37.0","id":9119,"title":"KiteSDK FileNotFoundException hdfs://sandbox.hortonworks.com:8020/tmp/crunch-...","body":"<p>I try to insert data from csv file with the kitesdk in the hortonworks sandbox 2.3.2.</p><p>After fixing the missing ojdbc6.jar issue and missing mapreduce.tar.gz, I'm run in following error:</p><pre>1 job failure(s) occurred:\norg.kitesdk.tools.CopyTask: Kite(dataset:file:/tmp/f00edc6d-4862-40d0-ad48-d6973b3261... ID=1 (1/1)(1): java.io.FileNotFoundException: File does not exist: hdfs://sandbox.hortonworks.com:8020/tmp/crunch-615301065/p1/REDUCE\nat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)\nat \norg.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\nat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\nat org.apache.hadoop.fs.FileSystem.resolvePath(FileSystem.java:751)\nat org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(MRApps.java:571)\nat org.apache.hadoop.mapreduce.v2.util.MRApps.setupDistributedCache(MRApps.java:463)\nat org.apache.hadoop.mapred.LocalDistributedCacheManager.setup(LocalDistributedCacheManager.java:93)\nat org.apache.hadoop.mapred.LocalJobRunner$Job.&lt;init&gt;(LocalJobRunner.java:163)\nat org.apache.hadoop.mapred.LocalJobRunner.submitJob(LocalJobRunner.java:731)\nat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:240)\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\nat org.apache.crunch.hadoop.mapreduce.lib.jobcontrol.CrunchControlledJob.submit(CrunchControlledJob.java:329)\nat org.apache.crunch.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.startReadyJobs(CrunchJobControl.java:204)\nat org.apache.crunch.hadoop.mapreduce.lib.jobcontrol.CrunchJobControl.pollJobStatusAndStartNewOnes(CrunchJobControl.java:238)\nat org.apache.crunch.impl.mr.exec.MRExecutor.monitorLoop(MRExecutor.java:112)\nat org.apache.crunch.impl.mr.exec.MRExecutor.access$000(MRExecutor.java:55)\nat org.apache.crunch.impl.mr.exec.MRExecutor$1.run(MRExecutor.java:83)\nat java.lang.Thread.run(Thread.java:745)</pre><p>Any idea whats going wrong here or what I'm missed?</p><p>Thanks in advanced.</p>","tags":["Sandbox","kitesdk"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-08 18:05:51.0","id":9214,"title":"3rd jackson-core-*.jar on HDP install","body":"<p>I am finding many libraries part of HDP base install are added to class path like jackson-core-2.2.3.jar.  However these libraries do not come with the vanilla (non hdp) install.  Does anyone know why and how these libraries are used?  If new version of the library exist, HDP may force to specify classpath for each application running with different jar.  Is there a possible work around where HDP separate core hadoop libraries classpth from non core libraries (like jackson..*)?</p>","tags":["Ambari","MapReduce","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-08 18:19:35.0","id":9216,"title":"If a MapReduce job has exitValue != 0 and state == SUCCEEDED does that mean it failed or had an error","body":"<p>We have a Hadoop agent that can run MapReduce jobs using WebHCat, one of our customers has a MapReduce job that has an exitValue of 6 and a state of SUCCEEDED. Because of the non-zero exitValue our agent throws an Exception indicating job failure. They think SUCCEEDED means everything ran OK and the agent should not throw an Exception, but I think the exitValue of 6 indicates a problem with their job. Who is correct?</p>","tags":["MapReduce","hadoop","java"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-07 04:21:46.0","id":9046,"title":"Best practices for running HDP on docker toolbox OS X(No cloudbreak). If possible provide an optimized HDP docker image","body":"","tags":["hdp-2.3.4","Ambari","docker"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-07 18:22:28.0","id":9142,"title":"Getting VirtualBox Error while importing Virtual Appliance","body":"<p>I downloaded the HDP 2.3.2 sandbox. Following that I am trying to import the Virual Appliance into the Oracle VM VirtualBox Manager. I keep getting the following errors. The site says the sandbox file is supposed to be 8.5G but the downloaded file is only 2G. Can someone please help? Thanks a lot.</p><pre><strong>Failed to import appliance C:\\Users\\xxxxxxx\\Downloads\\HDP_2.3.2_virtualbox.ova.\n</strong><strong>Could not create the imported medium 'C:\\Users\\xxxxxxx\\VirtualBox VMs\\Hortonworks Sandbox with HDP 2.3.2\\Hortonworks Sandbox with HDP 2.3.2-disk1.vmdk'.\n</strong><strong>VMDK: Compressed image is corrupted 'C:\\Windows\\system32\\Hortonworks Sandbox with HDP 2.3.2-disk1.vmdk' (VERR_ZIP_CORRUPTED).\n</strong><strong>Result Code: </strong>\n<strong>VBOX_E_FILE_ERROR (0x80BB0004)\n</strong><strong>Component: </strong>\n<strong>ApplianceWrap\n</strong><strong>Interface: </strong>\n<strong>IAppliance {8398f026-4add-4474-5bc3-2f9f2140b23e}</strong></pre>","tags":["Sandbox","virtualbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-15 20:00:34.0","id":6585,"title":"What's the best practice to get data from hbase and form dataframe for Python/R?","body":"<p>What's the best practice to get data from hbase and form dataframe for Python/R? If we want to use our Panda/R libraries, how to get data from hbase and form dataframe automatically?</p>","tags":["Spark","best-practices","dataframe","Hbase"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-08 18:49:09.0","id":9228,"title":"Roles and Flume agent setup","body":"<p>Ambari supports users with roles like cluster level cluster user, service operator, ambari level administrator etc (see https://issues.apache.org/jira/browse/AMBARI-11350) </p><p>It should be possible for a cluster user to setup flume agents since it seems more like a \"data worker\" task.</p><p>I'm assuming that a typical data worker (submits MR jobs, sets up data ingestion etc) would belong to the \"Cluster User\" category.</p><p>Currently to use flume, the flume agent needs to be setup which is a configuration change (flume.conf) kind of operation.</p><p>Cluster users and Service Operators cannot perform configuration changes and hence cannot setup flume agents.</p><p>Can someone clarify the following -</p><p>1) What role is typically assigned to the data workers ?</p><p>2) Currently flume agent setup can only be done by operators or administrator. Is there a plan to change this?</p><p>3) How is flume typically setup in the field across different users and roles ?</p>","tags":["roles","Ambari","Flume"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-08 20:15:18.0","id":9248,"title":"HTTPS access to Ranger via Knox ?","body":"<p>Hi,</p><p>due to security concerns I need to provide Ranger WebUI via Https, and I thought accessing it through Knox would be a simple approach. But I can also imagine some wired conflicts while e.g. configuring Knox policies for Knox, in Ranger and thereby creating some Kind of 'deadlock'....</p><p>What do you think about that approach, is it possible at all and how would a topology in Knox look like?!?!</p><p>Thanks for any thoughts and Hints!</p>","tags":["wire-encryption","Knox","Ranger"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-10 16:16:00.0","id":9315,"title":"access RM Web-UI through Knox","body":"<p>Hi,</p><p>I am currently looking for a service/topology example to access the RM WebUI through Knox (also NN and HMaster WebUI is on the list).</p><p>According to thread <a target=\"_blank\" href=\"https://community.hortonworks.com/questions/3259/recommendation-for-proxying-hadoop-services-withou.html\">proxying Hadoop services without built-in support in Knox</a> it looks like others are interested as well, but it would be great if someone can provide an example configuration (the referenced openweather example is great and good to get the basic points done...)</p><p>Anybody out there who made this exercise yet, to access RM-/NN-WebUI through Knox ?!?</p><p>Any hint highly appreciated...</p>","tags":["ui","proxy","namenode","knox-gateway","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-11 17:56:49.0","id":9394,"title":"Kerberos ticket isn't being renewed by Solr when storing indexes on HDFS","body":"<p>Hello,</p><p>I am running HDP Search's Solr Cloud on HDP 2.3 with it configured to store its index files on a Kerberized HDFS. When Solr is started it is able to write index files correctly to HDFS, however, after 24 hours have elapsed Solr becomes unable to connect to HDFS as it says it doesn't have a valid Kerberos tgt anymore (my default Kerberos ticket lifetime is 24 hours).  </p><p>A restart of Solr Cloud resolves the issue again for another 24 hours, so it appears that Solr is not renewing its Kerberos ticket itself when it expires. Could this be an issue with Solr? Or is there configuration I can add to get Solr to automatically renew its ticket?</p><p>This is stack trace I'm getting in the Solr log:</p><pre>java.io.IOException: Failed on local exception: java.io.IOException: Couldn't setup connection for solr/sandbox.hortonworks.com@HORTONWORKS.COM to sandbox.hortonworks.com/10.0.2.15:8020; Host Details : local host is: \"sandbox.hortonworks.com/10.0.2.15\"; destination host is: \"sandbox.hortonworks.com\":8020;\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1472)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy10.renewLease(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:571)\n        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy11.renewLease(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:879)\n        at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)\n        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)\n        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)\n        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Couldn't setup connection for solr/sandbox.hortonworks.com@HORTONWORKS.COM to sandbox.hortonworks.com/10.0.2.15:8020\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:672)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:643)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:730)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1438)\n        ... 16 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:553)\n        at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:368)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:722)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:718)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:717)\n        ... 19 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)\n        ... 28 more\n</pre><p>This is the configuration I'm using for my collection:</p><pre>&lt;directoryFactory name=\"DirectoryFactory\" class=\"solr.HdfsDirectoryFactory\"&gt;\n    &lt;str name=\"solr.hdfs.home\"&gt;hdfs://sandbox.hortonworks.com/user/solr&lt;/str&gt;\n    &lt;str name=\"solr.hdfs.confdir\"&gt;/usr/hdp/current/hadoop-client/conf&lt;/str&gt;\n    &lt;bool name=\"solr.hdfs.blockcache.enabled\"&gt;true&lt;/bool&gt;\n    &lt;int name=\"solr.hdfs.blockcache.slab.count\"&gt;1&lt;/int&gt;\n    &lt;bool name=\"solr.hdfs.blockcache.direct.memory.allocation\"&gt;false&lt;/bool&gt;\n    &lt;int name=\"solr.hdfs.blockcache.blocksperbank\"&gt;16384&lt;/int&gt;\n    &lt;bool name=\"solr.hdfs.blockcache.read.enabled\"&gt;true&lt;/bool&gt;\n    &lt;bool name=\"solr.hdfs.blockcache.write.enabled\"&gt;false&lt;/bool&gt;\n    &lt;bool name=\"solr.hdfs.nrtcachingdirectory.enable\"&gt;true&lt;/bool&gt;\n    &lt;int name=\"solr.hdfs.nrtcachingdirectory.maxmergesizemb\"&gt;16&lt;/int&gt;\n    &lt;int name=\"solr.hdfs.nrtcachingdirectory.maxcachedmb\"&gt;192&lt;/int&gt;\n    &lt;bool name=\"solr.hdfs.security.kerberos.enabled\"&gt;true&lt;/bool&gt;\n    &lt;str name=\"solr.hdfs.security.kerberos.keytabfile\"&gt;/etc/solr/conf/solr.keytab&lt;/str&gt;\n    &lt;str name=\"solr.hdfs.security.kerberos.principal\"&gt;solr/sandbox.hortonworks.com@HORTONWORKS.COM&lt;/str&gt;\n&lt;/directoryFactory&gt;\n</pre>","tags":["SOLR","kerberos","solrcloud","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-11 22:37:38.0","id":9475,"title":"Why does Ambari use headless keytab for Spark history server","body":"<p>It looks like Ambari uses headless keytab for Spark history server:</p><pre>Execute['/usr/bin/kinit -kt /dsap/etc/security/keytabs/spark.headless.keytab spark-abc@EXAMPLE.COM; '] {'user': 'spark'}</pre><p>Does anyone know why? </p><p>Also current documentation suggests that we need to create keytabs on host as described here:</p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_spark-guide/content/ch_installing-kerb-spark.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_spark-guide/content/ch_installing-kerb-spark.html</a></p><p>This is somehow confusing.</p>","tags":["kerberos","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-11 13:43:50.0","id":9389,"title":"NiFi and GeoLocalization","body":"<p>Hi all,</p><p>Does someone know if there is a way to enrich a FlowFile with geolocalization infos by querying an external GIS server like ArcGIS? I saw that in NiFi there is the GeoEnrichIP processor, but I need to get latitude and longitude from an address that is a FlowFile attribute.</p><p>Thank you,</p><p>D.</p>","tags":["Nifi","gis"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-11 10:51:05.0","id":9384,"title":"Unauthorized connection for super-user","body":"<p>Hi,</p><p>I want to run start a coordinator job, but I received the following error:</p><pre>Error: E0501 : E0501: Could not perform authorization operation, Unauthorized connection for super-user: oozie from IP X.X.X.X</pre><p>then, i added the properties in core-site.xml file, and restart the cluster, but didn't accomplish anything.</p><pre>&lt;property\n    &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt;                                  \n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hadoop.proxyuser.oozie.groups&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n</pre><p>How can i solve it ?</p>","tags":["help","authorization"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-11 19:12:43.0","id":9438,"title":"spark access old version of Hadoop 2.1.0 and Hive version 0.11","body":"<p>Hi, All:</p><p>I’m trying to read and write from the hdfs cluster using SparkSQL hive context. My current build of spark is 1.5.2.  The problem is that currently our company has very old version of hdfs (hadoop 2.1.0) and hive metastore (0.11) using Hortonworks bundle.  </p><p>One of the possible solution is to establish a separate cluster running hadoop 2.6.0 and hive &gt;0.12. But is it still possible I can read data from old hdfs cluster and write to new cluster ? Or the only solution is to upgrade my Hortonworks bundle ? Will that impact current Hive tables ? </p><p>If the only solution is to upgrade, will that be a risky process? If I just upgrade the hdfs and hive will that affect other components in the current cluster?</p><p>Here is the versions on the current hdfs cluster:</p><p>Hue</p><p>2.2.0:67</p><p>HDP</p><p>2.0.5</p><p>Hadoop</p><p>2.1.0</p><p>HCatalog</p><p>0.11.0</p><p>Pig</p><p>0.11.2</p><p>Hive</p><p>0.11.0</p><p>Oozie</p><p>4.0.0</p><p>Thanks a lot,</p><p>Jade</p>","tags":["Spark","hivecontext"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-11 20:38:37.0","id":9458,"title":"Spark 1.4.1 and HBase 1.1.2 Integration","body":"<p>I am running the following:</p><pre>$ run-example HBase-table &lt;tablename&gt;\n</pre><p>But I keep getting the following error message:</p><pre>16/01/11 20:36:00 INFO BlockManagerMaster: Registered BlockManager\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration\n\tat org.apache.spark.examples.HBaseTest$.main(HBaseTest.scala:31)\n\tat org.apache.spark.examples.HBaseTest.main(HBaseTest.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.HBaseConfiguration\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\t... 11 more\n</pre><p>I have tried with adding the hbase-&lt;version&gt;-client.jar to both Hadoop & Spark classpath but to no avail. </p>","tags":["Hbase","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-12 10:32:20.0","id":9503,"title":"Ambari Heartbeat Lost during Installation","body":"<p>We are trying to install , HDP via Ambari 2.1, but we during the install process, the ambari server (ambari-server.log) reports that it has lost an heart beat of the agent. </p><p>Error Message :</p><pre>Heartbeat lost from host amabri.agent.com</pre><p>The ambari-agent log reports:</p><pre>Failed to connect to https://amabri-server.com:8440/connection_info due to [Errno 111] Connection refused</pre><p>We are using openjdk 1.7 on RHEL 6.6 64 bit. </p><p>Any pointer to the issue would help immensely ?</p>","tags":["installation","heartbeat","ambari-2.1.0","Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-11 19:01:35.0","id":9439,"title":"Accessing Mysql DB on my Mac via Sqoop","body":"<p>Hi, </p><p>I am trying to perform the following task: 'Import data from a table in a relational database into HDFS'. </p><p>To get a working example of this - I have created a mySQL database (hostname=127.0.0.1 and port=xxxx) on my Mac. </p><p>I have then logged in to ssh (http://127.0.0.1:4200/) root within a terminal and type:</p><pre>sqoop list-databases --connect jdbc:mysql://127.0.0.1:xxxx --username root --password abcabcabc</pre><p>\nInstead of a list of databases, I get the below output. Can anyone advise?  </p><pre>Warning: /usr/hdp/2.3.2.0-2950/accumulo does not exist! Accumulo imports will fail.        \nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.                      \n16/01/11 18:44:42 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6.2.3.2.0-2950              \n16/01/11 18:44:42 WARN tool.BaseSqoopTool: Setting your password on the command-line is ins\necure. Consider using -P instead.                                                          \n16/01/11 18:44:43 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. \nSLF4J: Class path contains multiple SLF4J bindings.                                        \nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar\n!/org/slf4j/impl/StaticLoggerBinder.class]                                                \nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/zookeeper/lib/slf4j-log4j12-1.6.1.j\nar!/org/slf4j/impl/StaticLoggerBinder.class]                                              \nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.          \nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]                      \n16/01/11 18:44:43 ERROR manager.CatalogQueryManager: Failed to list databases              \njava.sql.SQLException: Access denied for user 'root'@'localhost' (using password: YES)    \n        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)                  \n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)                      \n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)                      \n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:935)                      \n        at com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:4101)                        \n        at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1300)                          \n        at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2337)            \n        at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2370)      \n        at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2154)            \n        at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:792)                  \n        at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:49)                  \n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)          \n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorI\nmpl.java:57)                                                                              \n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorA\nccessorImpl.java:45)                                                                      \n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)                \n        at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)                            \n        at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:381)              \n        at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:305)      \n        at java.sql.DriverManager.getConnection(DriverManager.java:571)                    \n        at java.sql.DriverManager.getConnection(DriverManager.java:215)                    \n        at org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:885)        \n        at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.jav\na:52)                                                                                      \n        at org.apache.sqoop.manager.CatalogQueryManager.listDatabases(CatalogQueryManager.j\nava:57)                                                                                    \n        at org.apache.sqoop.tool.ListDatabasesTool.run(ListDatabasesTool.java:49)          \n        at org.apache.sqoop.Sqoop.run(Sqoop.java:148)                                      \n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)                      \n        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:184)                                \n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:226)                                  \n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:235)                                  \n        at org.apache.sqoop.Sqoop.main(Sqoop.java:244)                                    \n16/01/11 18:44:43 ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.RuntimeExceptio\nn: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: YES)  \njava.lang.RuntimeException: java.sql.SQLException: Access denied for user 'root'@'localhost\n' (using password: YES)                                                                    \n        at org.apache.sqoop.manager.CatalogQueryManager.listDatabases(CatalogQueryManager.j\nava:73)                                                                                    \n        at org.apache.sqoop.tool.ListDatabasesTool.run(ListDatabasesTool.java:49)          \n        at org.apache.sqoop.Sqoop.run(Sqoop.java:148)                                      \n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)                      \n        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:184)                                \n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:226)                                  \n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:235)                                  \n        at org.apache.sqoop.Sqoop.main(Sqoop.java:244)                                    \nCaused by: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password\n: YES)                                                                                    \n        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)                  \n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)                      \n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)                      \n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:935)                      \n        at com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:4101)                        \n        at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1300)                          \n        at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2337)            \n        at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2370)      \n        at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2154)            \n        at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:792)                  \n        at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:49)                  \n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)          \n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorI\nmpl.java:57)                                                                              \n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorA\nccessorImpl.java:45)                                                                      \n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)                \n        at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)                            \n        at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:381)              \n        at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:305)      \n        at java.sql.DriverManager.getConnection(DriverManager.java:571)                    \n        at java.sql.DriverManager.getConnection(DriverManager.java:215)                    \n        at org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:885)        \n        at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.jav\na:52)                                                                                      \n        at org.apache.sqoop.manager.CatalogQueryManager.listDatabases(CatalogQueryManager.j\nava:57)                                                                                    \n        ... 7 more                            </pre>","tags":["Sqoop","mysql","jdbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-10 23:55:15.0","id":9368,"title":"ambari-server install requires python26","body":"<p>i am trying to install ambari-server, but getting \"requires python26\" error. my environment is centos 6.6 with python 2.6 installed by default. I am sure the python 26 is available. anything wrong?</p>","tags":["installation","Ambari"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-15 13:03:54.0","id":10042,"title":"Can I create Flume config group with stopped agents?","body":"<p>I am using ambri rest API. I can create config group and add there desired properties. But after that flume agents start immediately. Can I create this with stopped agents and than start manualy?</p>","tags":["Ambari","api","Flume"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-13 19:49:09.0","id":9793,"title":"Can you Run ambari-server setup After Deploying Cluster without Data Loss","body":"<p>can we run ambari-server setup even after cluster is setup and all is well? will running setup loose existing data in Mysql .? i guess not...</p><p>we have an issue with java home....old version of ambari is pointing to correct java and after upgrade new version is pointing to wrong home.... </p>","tags":["Ambari","ambari-server","configuration","java"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-14 17:41:16.0","id":9949,"title":"Pig ParquetStorer is not working","body":"<p>Hi There,</p><p>We are getting the following error when using ParquetStorer in Pig</p><pre>ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. Cannot instantiate class org.apache.pig.builtin.ParquetStorer (parquet.pig.ParquetStorer) </pre><p>We are using HDP-2.3.4.0-3485 version. </p><p>Appreciate if any one have any pointers on this.</p><p>Thank you,</p><p>Ibrahim</p>","tags":["error","Pig","parquet"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-15 01:51:59.0","id":10008,"title":"ubuntu14 local repository is wrong","body":"<p>I want to build a ubuntu14 local repository, but when i download and untar the tar.gz file.</p><p>* the content of public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.4.0/HDP-2.3.4.0-ubuntu14-deb.tar.gz is repo for ubuntu12</p><p>* the content of public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.2.0/HDP-2.3.2.0-ubuntu14-deb.tar.gz is for debian7</p><p>would someone fix this bug? </p>","tags":["hdp-2.3.2","ubuntu","installation","hdp-2.3.4"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-14 21:52:47.0","id":9999,"title":"Anyone familiar with accumulo word count example? I'm trying to hive accumulo integration example,","body":"<p>Trying to run hive Accumulo Integration examples, but it is failing while creating the table. </p><p>hive -hiveconf accumulo.instance.name=accumulo -hiveconf accumulo.zookeepers=localhost -hiveconf accumulo.user.name=hive -hiveconf accumulo.user.pass=hive\nWARNING: Use \"yarn jar\" to launch YARN applications.\n2016-01-14 13:53:15,365 WARN  - [main:] ~ HiveConf of name hive.hook.dgi.url does not exist (HiveConf:2753)\n2016-01-14 13:53:15,368 WARN  - [main:] ~ HiveConf of name hive.cluster.name does not exist (HiveConf:2753)</p><p>hive&gt; CREATE TABLE accumulo_table(rowid STRING, name STRING, age INT, weight DOUBLE, height INT)\n    &gt; STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\n    &gt; WITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,person:name,person:age,person:weight,person:height');\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Instance name accumulo does not exist in zookeeper.  Run \"accumulo org.apache.accumulo.server.util.ListInstances\" to see a list.</p><p><strong>ERROR:</strong></p><p><strong>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Instance name accumulo does not exist in zookeeper.  Run \"accumulo org.apache.accumulo.server.util.ListInstances\" to see a list.</strong></p>","tags":["Accumulo","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-15 17:55:31.0","id":10081,"title":"LimitExceededException in counters","body":"<p>I have been reviewing the list of unresolved bugs in 2.3.4. I have found in the tour list HIVE-11303, but I have not found in this list of improvements the TEZ-2629 that is necessary in this case. TEZ-2629 is included in version 2.3.4?</p>","tags":["Pig","Tez","hdp-2.3.4"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-15 16:45:38.0","id":10074,"title":"Need to install Oozie client on the RHEL  outside the SUSE cluster and should be managed by ambari.","body":"<p>Need to install Oozie client on the RHEL  (red hat server outside the cluster) and have it talk with the SUSE Linux HDP cluster. RHEL server should be managed by Ambari. Is this possible? if so any directions please</p>","tags":["rhel","Oozie","Ambari","suse","client"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-15 21:33:02.0","id":10120,"title":"Can Hive avro tables support changing schemas?","body":"<p>Avro, in general, supports the idea of evolving schemas and I'm trying to support that with an external Hive table.  In other words, declare an external hive table and define its schema through the table properties to be read from an HDFS file:</p><pre>TBLPROPERTIES ('avro.schema.url'='hdfs://namenode/common/schemas/schema_v1.avsc')</pre><p>This works fine when all of the files in the directory for the external table are create with schema version 1.  However, if i add avro files of version 2 and update the tblproperties accordingly the table becomes unusable.  I see errors like this on a select count(*) statement</p><pre>Caused by: org.apache.avro.AvroTypeException: Found com.target.category_data, expecting com.target.category_data\nat org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:231)\nat org.apache.avro.io.parsing.Parser.advance(Parser.java:88)\nat org.apache.avro.io.ResolvingDecoder.readFieldOrder(ResolvingDecoder.java:127)\nat org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:176)\nat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)\nat org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:193)\nat org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:183)\nat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)\nat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:142)\nat org.apache.avro.file.DataFileStream.next(DataFileStream.java:233)\nat org.apache.avro.file.DataFileStream.next(DataFileStream.java:220)\nat org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.next(AvroGenericRecordReader.java:153)\nat org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.next(AvroGenericRecordReader.java:52)\nat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:347)</pre><p>It appears that the Hive Avro serde is using the schema in tblproperties to decode the individual avro files (or more accurately the individual file splits, I supose) instead of the schema in the header of each avro file.  I would like for binary avro files created with different avro schemas to be read by the same hive table with a potentially different avro schema.  Could anyone suggest ways to do that?  I am not opposed to making code changes on the read side (i.e. the hive serde) or the write side (I'm using Storm's avro bolt to write my files).</p><p>Thanks! -Aaron</p>","tags":["Hive","avro"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-23 22:03:12.0","id":7887,"title":"Phoenix throws \"malformed url \" error when keytab path contains \"C:\\User\\abc\\principal.keytab\"","body":"<p>Hi,</p><p>My code works fine and I get results when my keytab path do not contain \":\" . I understand its a token used by Phoenix internally but if my path contains this token and i try to escape it using \"\\\" , I still get malformed url error . Can you please guide here ? Is it a bug in phoenix connection string code or I am missing something here .</p><p>my keytab path in my property file : </p><pre>keytab.path=C\\:\\\\Users\\\\VBorhad\\\\krbProperties\\\\hdpsrvc.keytab</pre><p>Error :</p><pre>\njava.sql.SQLException: ERROR 102 (08001): Malformed connection url. jdbc:phoenix:p006.unix.gsm1900.org,p001.unix.gsm1900.org,p002.unix.gsm1900.org,p003.unix.gsm1900.org:2181:/hbase-secure:srvc@HDP_EIT_DEV.com:C:\\Users\\VBorhad\\krbProperties\\hdpsrvc.keytab\nat org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newException(SQLExceptionCode.java:337)\n   at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQLExceptionInfo.java:133)</pre>","tags":["kerberos","Phoenix"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-16 19:49:40.0","id":10186,"title":"Adding HBase and Phoenix in Ambari","body":"<p>I ve decided to add HBase to my multinode cluster (RegionServers and Phoenix Query Servers on all 3 datanodes). I added the service through Ambari and the installation was abrupted after 12%. The following error message appeared:</p><p>resource_management.core.exceptions.Fail: Execution of '/usr/bin/yum -d 0 -e 0 -y install 'phoenix_2_3_*'' returned 1. Error downloading packages:</p><pre>  phoenix_2_3_2_0_2950-4.4.0.2.3.2.0-2950.el6.noarch: [Errno 256] No more mirrors to try.</pre><p>I waited for the installation to timeout, ssh-ed to the datanode where the error was reporting (only on one datanode) - I ran sudo yum install phoenix, went back to ambari and retried the installation.</p><p>Worked well the second time.</p><p>Hope this helps someone in any way.</p>","tags":["ambari-2.1.2","installation","Hbase","Phoenix"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-17 22:14:41.0","id":10250,"title":"Ambari View - Zeppelin - \"Unable to resolve the server's DNS Address\"","body":"<p>I just downloaded the most recent release of the sandbox and tried to load the built in Zeppelin view.  I got an error(see attached picture) that said  \"Unable to resolve the server's DNS Address\" when you hovered over it.  The rest of the install went cleanly but I am wondering how to fix this default view.  Also how would I raise this as a ticket?<a href=\"/storage/attachments/1420-screenshot-from-2016-01-17-171207.png\">screenshot-from-2016-01-17-171207.png</a></p>","tags":["ambari-views","zeppelin","Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-17 21:31:12.0","id":10176,"title":"What are various design considerations to be made while creating a Flume Topology for both textual and non textual events?","body":"<p>Trying to understand various considerations to be made while designing a Flume Topology for text based and non-text based events?</p><p>Please advise with some examples or references, if available.</p>","tags":["Flume","design"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-17 19:25:11.0","id":10242,"title":"decrease log verbosity of container log output ?","body":"<p>Hi,</p><p>I am facing lots of \"bad dir status\" issues on Nodemanagers while running huge job. This is caused by flooding the container log files with [DEBUG] messages (in detail the 'syslog' file is getting very large).</p><p>How can I modify the loglevel of the container specific log to reduce the size of the syslog file ?</p><p>E.g.:</p><pre>$ ls -alh \n./container_e12_1453036276967_0004_01_000004:\ntotal 7.4G\ndrwxr-s---  2 siad hadoop 4.0K Jan 17 18:39 .\ndrwxr-s--- 10 siad hadoop 4.0K Jan 17 20:21 ..\n-rw-r-----  1 siad hadoop  222 Jan 17 18:47 stderr\n-rw-r-----  1 siad hadoop    0 Jan 17 18:39 stdout\n-rw-r-----  1 siad hadoop 7.4G Jan 17 18:47 syslog</pre><p>In /etc/hadoop I cannot find any config setting including the value 'debug', so where does this DEBUG output come from ?</p><p>Thanks, Gerd</p>","tags":["logging","YARN","yarn-container","debug","nodemanager"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-16 18:41:37.0","id":10169,"title":"Is it intended that a manually created rack topology script and data file is overwritten by Ambari after an HDFS restart?","body":"<p>I played around with rack awareness in HDP 2.3.0 and Ambari 2.1.0</p><p>Configuring it solely in Ambari in the Hosts view works fine.</p><p>Then I wanted to deploy my own script and my own data file. So I put the script and the data file in in /etc/hadoop/conf/ on all nodes and pointed HDFS to the script via the property net.topology.script.file.name. -&gt; Saved -&gt; restarted.</p><p>What happened: The file I deployed was overwritten by the ambari default rack topology script, which points to the default rack data file, which is overwritten after each restart as well.</p><p>In terms of consistency, it makes sense to centrally manage this file. On the other hand, there is no option to configure the script and the data file in Ambari.</p><p>How would one deploy self-made topology scirpts and data files?</p>","tags":["HDFS","rack-awareness","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-15 19:16:45.0","id":10108,"title":"Oozie db to PGSQL in Ambari install.","body":"<p>While installing Ambari on CentOS7, i was asked about the Oozie db target. I opted to use PGSQL, but that failed, since i did not have any idea what the UserName, Pwd or DBName defaulted to or where to set them. The instructions page gives not idea what to use for them.</p>","tags":["postgres","Ambari","Oozie"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-18 18:44:14.0","id":10404,"title":"Hadoop Eclipse plugin","body":"<pre>I am getting below error while executing Simple mapreduce programme using eclipse 'mars'\nHadoop 2.6\nubuntu: 15.05 (hacked it and made it 14.04 to avaoid any compatibility issues with hadoop)\nusing hadoop­eclipse pluggin 2.6\nNote that i am able to connect to HDFS via eclipse plugging since in the DFS location on right hand side i can\nsee the directory structure of HDFS.\nPlease find below the settings in eclipse and help to overcome the error while executing Mapreduce programme.\n(Please note that if i make a jar out of the project via eclipse and run it in traditional command prompt way, i am\ngetting no issues(as the output and results are clearly visible in DFS location in eclipse, but i want to run it wia\neclipse as a whole))\nError while exeuting the programme :\nPicked up JAVA_TOOL_OPTIONS: ­javaagent:/usr/share/java/jayatanaag.jar\n2016­01­08 07:32:46,255 INFO [main] datanode.DataNode\n(StringUtils.java:startupShutdownMessage(633)) ­ STARTUP_MSG:\n/************************************************************\n.....\n...\n..\nSTARTUP_MSG: build = https://git­wip­us.apache.org/repos/asf/hadoop.git ­r\ne3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014­11­13T21:10Z\nSTARTUP_MSG: java = 1.8.0_66\n************************************************************/\n2016­01­08 07:32:46,271 INFO [main] datanode.DataNode (SignalLogger.java:register(91)) ­\nregistered UNIX signal handlers for [TERM, HUP, INT]\nUsage: java DataNode [­regular | ­rollback]\n­regular : Normal DataNode startup (default).\n­rollback : Rollback a standard or rolling upgrade.\nRefer to HDFS documentation for the difference between standard\nand rolling upgrades.\n2016­01­08 07:32:46,770 WARN [main] datanode.DataNode (DataNode.java:secureMain(2392)) ­\nExiting Datanode\n2016­01­08 07:32:46,773 INFO [main] util.ExitUtil (ExitUtil.java:terminate(124)) ­ Exiting\nwith status 1\n2016­01­08 07:32:46,778 INFO [Thread­1] datanode.DataNode (StringUtils.java:run(659)) ­\nSHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/127.0.1.1\n************************************************************/</pre>","tags":["hadoop","MapReduce"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-18 16:16:01.0","id":10391,"title":"IBM Omni-channel (formerly Unica Software) hive integration","body":"<p>Does anyone have any experience with IBM unica integration with hive?  If so I would appreciate any feedback </p><ul>\n<li>Best practices integrating hive with IBM unica.  I am already aware of the IBM knowledge center.</li><li>Your experience during and post integration</li><li>Any other insights you can share which would help prior, during and post integration.</li></ul>","tags":["hdp-2.3.0","Hive","integration","ibm"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-18 23:12:06.0","id":10473,"title":"Can not deploy to hosts","body":"<p>Hi All,</p><p>I'm having below error while deploying to hosts. Hosts are reachable using ssh without password. I have removed and re downloaded repo and updated apt. still having the same error. have any idea?</p><pre>==========================\nCreating target directory...\n==========================\nCommand start time 2016-01-19 01:07:53\nchmod: cannot access ‘/var/lib/ambari-agent/data’: No such file or directory\nConnection to hadoop01 closed.\nSSH command execution finished\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:07:54\n==========================\nCopying common functions script...\n==========================\nCommand start time 2016-01-19 01:07:54\nscp /usr/lib/python2.6/site-packages/ambari_commons\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:07:55\n==========================\nCopying OS type check script...\n==========================\nCommand start time 2016-01-19 01:07:55\nscp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.py\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:05\n==========================\nRunning OS type check...\n==========================\nCommand start time 2016-01-19 01:08:05\nCluster primary/cluster OS family is ubuntu14 and local/current OS family is ubuntu14\nConnection to hadoop01 closed.\nSSH command execution finished\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:07\n==========================\nChecking 'sudo' package on remote host...\n==========================\nCommand start time 2016-01-19 01:08:07\nsudo             install\nConnection to hadoop01 closed.\nSSH command execution finished\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:09\n==========================\nCopying repo file to 'tmp' folder...\n==========================\nCommand start time 2016-01-19 01:08:09\nscp /etc/apt/sources.list.d/ambari.list\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:10\n==========================\nMoving file to repo dir...\n==========================\nCommand start time 2016-01-19 01:08:10\nConnection to hadoop01 closed.\nSSH command execution finished\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:10\n==========================\nChanging permissions for ambari.repo...\n==========================\nCommand start time 2016-01-19 01:08:10\nConnection to hadoop01 closed.\nSSH command execution finished\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:10\n==========================\nUpdate apt cache of repository...\n==========================\nCommand start time 2016-01-19 01:08:10\n0% [Working]\n   \nHit http://public-repo-1.hortonworks.com Ambari InRelease\n0% [Working]\n   \n0% [InRelease gpgv 3.187 B]\n   \n33% [Working]\n   \nHit http://public-repo-1.hortonworks.com Ambari/main i386 Packages\n33% [Working]\n   \n33% [Packages 0 B] [Waiting for headers]\n   \n50% [Waiting for headers]\n50% [Waiting for headers]\n50% [Waiting for headers]\n   \nIgn http://public-repo-1.hortonworks.com Ambari/main Translation-en_US\n   \n67% [Working]\n   \nIgn http://public-repo-1.hortonworks.com Ambari/main Translation-en\n83% [Working]\n   \nReading package lists... 0%\nReading package lists... 0%\nReading package lists... 0%\nReading package lists... 0%\nReading package lists... 6%\nReading package lists... Done\nW: Duplicate sources.list entry http://public-repo-1.hortonworks.com/ambari/ubuntu14/2.x/updates/2.2.0.0/ Ambari/main i386 Packages (/var/lib/apt/lists/public-repo-1.hortonworks.com_ambari_ubuntu14_2.x_updates_2.2.0.0_dists_Ambari_main_binary-i386_Packages)\nW: You may want to run apt-get update to correct these problems\nConnection to hadoop01 closed.\nSSH command execution finished\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:13\n==========================\nCopying setup script file...\n==========================\nCommand start time 2016-01-19 01:08:13\nscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py\nhost=hadoop01, exitcode=0\nCommand end time 2016-01-19 01:08:13\n==========================\nRunning setup agent script...\n==========================\nCommand start time 2016-01-19 01:08:13\nE: No packages found\nE: No packages found\n('', None)\nConnection to hadoop01 closed.\nSSH command execution finished\nhost=hadoop01, exitcode=1\nCommand end time 2016-01-19 01:08:18\nERROR: Bootstrap of host hadoop01 fails because previous action finished with non-zero exit code (1)\nERROR MESSAGE: Connection to hadoop01 closed.\nSTDOUT: E: No packages found\nE: No packages found\n('', None)\nConnection to hadoop01 closed.</pre>","tags":["Ambari","deoloyment","repository","error"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-31 09:22:43.0","id":8460,"title":"Does HDP support 3rd party KMS?","body":"","tags":["security","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-17 20:37:55.0","id":10246,"title":"ERROR Ambari Metrics Monitor \"role in invalid state\"","body":"<p>Hi Team,</p><p>I am getting below error when starting ambari-metrics-monitor from ambari v2.1.1 and HDP 2.2.4.2.</p><p><em>\"On host &lt;HOST&gt; role METRICS_MONITOR in invalid state. Invalid transition. Invalid event: HOST_SVCCOMP_OP_IN_PROGRESS at INIT</em>\"</p><p>Thanks,</p><p>Karthik</p>","tags":["Ambari","ambari-2.1.1","monitoring","ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-19 12:01:53.0","id":10537,"title":"java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration","body":"<p>Hi,</p><p>I am trying to access a HBase-backed Hive table via 'select * from tbl_name', but seems like some HBase jar's are not in place.</p><p>Any help highly appreciated ;)</p><p>Details:</p><p>0: jdbc:hive2://deala.corp:1&gt; select * from tbl_name;\nError: java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration (state=,code=0)</p><p>Regards, Gerd</p><p>PS: HDP2.2.4.2, Ambari 2.1.2</p>","tags":["help","Hbase","java","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-19 15:40:51.0","id":10580,"title":"How can the Ambari Ranger Admin internal user (amb_ranger_admin) be force recreated in Ranger?","body":"<p>When installing an existing Ranger 0.4 into Ambari 2.2 to facilitate management in Ambari, the amb_ranger_admin user was not created in Ranger.  </p><p>What is the simplest way to force this account to be generated? </p>","tags":["installation","Ambari","migration","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-17 02:42:02.0","id":6933,"title":"Tutorial Issue: Can't connect to HDFS through the Knox Gateway","body":"<p>I'm running though \"<a target=\"_blank\" href=\"http://hortonworks.com/hadoop-tutorial/securing-hadoop-infrastructure-apache-knox/\">Securing your Hadoop Infrastructure with Apache Knox</a>.\" I've installed HDP 2.3 and have been following the steps described. I can run step 6, accessing the Hadoop Cluster via WebHDFS. But step 7, accessing the Hadopp Cluster via the Apache Know Gateway fails. </p><p>curl -iku guest:guest-password -X GET 'https://localhost:8443/gateway/sandbox/webhdfs/v1/?op=LISTSTATUS'</p><p><strong>HTTP/1.1 404 Not Found</strong>\nCache-Control: must-revalidate,no-cache,no-store\nContent-Type: text/html;charset=ISO-8859-1\nContent-Length: 1294\nServer: Jetty(8.1.14.v20131031)\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html;charset=ISO-8859-1\"/&gt;\n&lt;title&gt;Error 404 Not Found&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h2&gt;HTTP ERROR: 404&lt;/h2&gt;\n&lt;p&gt;<strong>Problem accessing /gateway/sandbox/webhdfs/v1/. Reason:\n&lt;pre&gt;    Not Found&lt;/pre&gt;</strong>&lt;/p&gt;\n&lt;hr /&gt;&lt;i&gt;&lt;small&gt;Powered by Jetty://&lt;/small&gt;&lt;/i&gt;\n&lt;/body&gt;\n&lt;/html&gt;</p><p>All the requisite processes seem to be running. I have no clue what to look at to get this working.</p>","tags":["how-to-tutorial","curl","Sandbox","Knox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-20 06:19:21.0","id":10717,"title":"how to fetch hive data from html/jquery web pages?","body":"<p>Hi,</p><p>I'm working on a project, in that i have to fetch data from hive table to my web pages which was developed in html & jquery.</p><p>Can u please suggest me any idea to do so..or shall we use obdc connectivity to retrieve those hive records.</p><p>Thanks in advance.</p><p>Regards,</p><p>Priya</p>","tags":["api","development","webhcat","Hive","webhdfs"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-25 08:12:38.0","id":11506,"title":"HUE is not supported on RHEL 7/CentOS7. Please provide recommended RHEL/CentOS versions which support HUE on HDP 2.3.4 platfrom. Is it recommended to use RHEL 6.5/CentOS 6.5?","body":"","tags":["hue"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-05 19:41:50.0","id":8851,"title":"Nifi GetHttp processor for facebook api error","body":"<p>Iam using facebook api <a href=\"http://www.programmableweb.com/api/facebook\">http://www.programmableweb.com/api/facebook</a> and gave fb endpoint   <a href=\"http://api.facebook.com/restserver.php\">http://api.facebook.com/restserver.php</a></p><p>and got this error.</p><p>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;error_response xmlns=\"http://api.facebook.com/1.0/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://api.facebook.com/1.0/ <a href=\"http://api.facebook.com/1.0/facebook.xsd\">&gt;\n  &lt;error_code&gt;3&lt;/error_code&gt;\n  &lt;error_msg&gt;Unknown method (3)&lt;/error_msg&gt;\n  &lt;request_args list=\"true\"/&gt;\n&lt;/error_response&gt;\n</a></p><p>I used BBC api and it gave valid json. DIdnt use any authentication not sure how it is different from GetTwitter processor.</p><p>Iam trying to use Nifi to stream data from any of the following API.. not twitter..  Any help is appreciated</p><p><a href=\"http://www.programmableweb.com/apis/directory\">http://www.programmableweb.com/apis/directory</a></p>","tags":["nifi-streaming","streaming","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-25 20:51:12.0","id":11652,"title":"How to update Hive row with JOIN","body":"<p>\n\tI want to port the following SQL statement to Hive from Sybase.  What is the best approach to get the below to work in Hive?  Hive <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LanguageManualDML-Update\">DML</a> UPDATE syntax doesn't mention support for JOINs or nested SELECT statements. </p>\n<pre>UPDATE table1 \n SET table1.column1 = table2.column1,\n     table1.column10 = table2.column10\n FROM table1, table2\n WHERE table1.columnID = table2.columnID\n</pre><p>After enabling <em>transactional=true</em> on <em>table1</em> the above produces an error:</p><pre>Error while compiling statement: FAILED: ParseException ... missing EOF at 'FROM' ...</pre>","tags":["Hive","hdp-2.3.2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-26 16:42:14.0","id":11790,"title":"HDP and Ambari patches","body":"<p>I have seen repos and binaries for major releases but where can we find binaries/repo for the patches</p><p>For example, say we find an issue with the ambari-metrics group of rpm’s. Where would I find an example of a patch to update the software contained in ambari-metrics.</p><p>Another example: Say there is a bug in HDFS. Where would I find an example patch for the HDP 2.3.4.0 version of HDFS (one that doesn’t include the entire distribution of HDP, but only updates that specific version of HDFS)?</p>","tags":["code","hdp-2.3.0","bug"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-26 12:47:05.0","id":11779,"title":"HBase Master and Region Server shutting down with ZooKeeper delete failed after 4 attempts","body":"<p>We installed HDP 2.3.4 cluster with Ambari 2.2..</p><p>HBase Master and Region servers starts but after some time the HBase Master shuts down.</p><p>The log file says:</p><pre>2016-01-25 14:46:47,340 WARN  [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/master\n2016-01-25 14:46:47,340 ERROR [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: ZooKeeper getData failed after 4 attempts\n2016-01-25 14:46:47,340 WARN  [master/node03.test.com/x.x.x.x:16000] zookeeper.ZKUtil: master:16000-0x3527a1898200012, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/master\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/master\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n    at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)\n    at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:359)\n    at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:745)\n    at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:148)\n    at org.apache.hadoop.hbase.master.ActiveMasterManager.stop(ActiveMasterManager.java:267)\n    at org.apache.hadoop.hbase.master.HMaster.stopServiceThreads(HMaster.java:1164)\n    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1071)\n    at java.lang.Thread.run(Thread.java:745)\n2016-01-25 14:46:47,340 ERROR [master/node03.test.com/x.x.x.x:16000] zookeeper.ZooKeeperWatcher: master:16000-0x3527a1898200012, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, baseZNode=/hbase-unsecure Received unexpected KeeperException, re-throwing exception\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/master\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n    at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)\n    at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:359)\n    at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:745)\n    at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:148)\n    at org.apache.hadoop.hbase.master.ActiveMasterManager.stop(ActiveMasterManager.java:267)\n    at org.apache.hadoop.hbase.master.HMaster.stopServiceThreads(HMaster.java:1164)\n    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1071)\n    at java.lang.Thread.run(Thread.java:745)\n2016-01-25 14:46:47,340 ERROR [master/node03.test.com/x.x.x.x:16000] master.ActiveMasterManager: master:16000-0x3527a1898200012, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, baseZNode=/hbase-unsecure Error deleting our own master address node\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/master\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n    at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)\n    at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:359)\n    at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:745)\n    at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:148)\n    at org.apache.hadoop.hbase.master.ActiveMasterManager.stop(ActiveMasterManager.java:267)\n    at org.apache.hadoop.hbase.master.HMaster.stopServiceThreads(HMaster.java:1164)\n    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1071)\n    at java.lang.Thread.run(Thread.java:745)\n2016-01-25 14:46:47,341 INFO  [master/node03.test.com/x.x.x.x:16000] hbase.ChoreService: Chore service for: node03.test.com,16000,1453750627948_splitLogManager_ had [] on shutdown\n2016-01-25 14:46:47,341 INFO  [master/node03.test.com/x.x.x.x:16000] flush.MasterFlushTableProcedureManager: stop: server shutting down.\n2016-01-25 14:46:47,342 INFO  [master/node03.test.com/x.x.x.x:16000] ipc.RpcServer: Stopping server on 16000\n2016-01-25 14:46:47,342 INFO  [RpcServer.listener,port=16000] ipc.RpcServer: RpcServer.listener,port=16000: stopping\n2016-01-25 14:46:47,343 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped\n2016-01-25 14:46:47,343 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping\n2016-01-25 14:46:47,345 WARN  [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/node03.test.com,16000,1453750627948\n2016-01-25 14:46:48,345 WARN  [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/node03.test.com,16000,1453750627948\n2016-01-25 14:46:50,345 WARN  [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/node03.test.com,16000,1453750627948\n2016-01-25 14:46:54,346 WARN  [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/node03.test.com,16000,1453750627948\n2016-01-25 14:47:02,346 WARN  [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=node03.test.com:2181,node02.test.com:2181,node01.test.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/node03.test.com,16000,1453750627948\n2016-01-25 14:47:02,346 ERROR [master/node03.test.com/x.x.x.x:16000] zookeeper.RecoverableZooKeeper: ZooKeeper delete failed after 4 attempts\n2016-01-25 14:47:02,347 WARN  [master/node03.test.com/x.x.x.x:16000] regionserver.HRegionServer: Failed deleting my ephemeral node\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/node03.test.com,16000,1453750627948\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n    at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\n    at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:178)\n    at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1345)\n    at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1334)\n    at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteMyEphemeralNode(HRegionServer.java:1403)\n    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1079)\n    at java.lang.Thread.run(Thread.java:745)\n2016-01-25 14:47:02,350 INFO  [master/node03.test.com/x.x.x.x:16000] regionserver.HRegionServer: stopping server node03.test.com,16000,1453750627948; zookeeper connection closed.\n2016-01-25 14:47:02,351 INFO  [master/node03.test.com/x.x.x.x:16000] regionserver.HRegionServer: master/node03.test.com/x.x.x.x:16000 exiting\n\n</pre><p>What steps do I take to solve this?</p>","tags":["zookeeper","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-28 10:49:01.0","id":12161,"title":"Ranger UI Login - LDAP integration","body":"<p>Need to integrate Ranger UI login with LDAP. Does below look correct?</p><p>1. Login to Ambari UI </p><p>2. Go to Ranger Service --&gt; Configs \n3. Expand \"Ranger Settings\" \n4. Under \"Authentication method\" select the appropriate. \n5. Upon selection of LDAP/ACTIVE_DIRECTORY the settings for the same will appear below in the next section. Please fill in the details accordingly.\n6. Save the changes and restart the Ranger service. </p>","tags":["Ranger","security","ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-28 19:36:42.0","id":12261,"title":"TestDFSIO in latest HDP","body":"<p>Hi,</p><p>I am trying to run TestDFSIO performance tests on my cluster and trying to do as below. </p><p>Logged into the name node machine</p><p>Logged in as hdfs user</p><pre>/usr/hdp/2.3.4.0-3485/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-*tests.jar TestDFSIO -write -nrFiles 100 -fileSize 100 TestDFSIO Read Test hadoop jar</pre><p>After running the above test, I am seeing a permission denied error. Not sure what could be the reason for it as I am running as hdfs user, but still it doesnt run.</p><pre>bash: /usr/hdp/2.3.4.0-3485/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.2.3.4.0-3485-tests.jar: Permission denied</pre><p>Any idea what could be the reason?</p><p>Thanks in advance.</p>","tags":["HDFS","benchmark","hdfs-permissions","performance"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-01-28 23:38:17.0","id":12311,"title":"Not able to install Zeppelin on an Ambari Managed Cluster","body":"<p><a rel=\"user\" href=\"/users/132/abajwa.html\" nodeid=\"132\">@Ali Bajwa</a> : We were following your github Post :  https://github.com/hortonworks-gallery/ambari-zeppelin-service#option-1-deploy-zeppelin-on-existing-cluster </p><p>We got an error when Ambari was installing Zeppelin . OS : CentOS 6.5 . Ambari 2.2 , HDP 2.3.4 .  We were installing zeppelin on a node other than the Ambari Server node. </p><p>and were getting the following error in our Zeppelin logs </p><pre>Compiling Zeppelin view...\n\nInitialized empty Git repository in /home/zeppelin/iframe-view/.git/\n[INFO] Scanning for projects...[INFO]\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Zeppelin View 1.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\nDownloading: https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-clean-plugin/2.5/maven-clean-plugin-2.5.pom\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1.039 s\n[INFO] Finished at: 2016-01-28T16:57:49-05:00\n[INFO] Final Memory: 9M/102M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Plugin org.apache.maven.plugins:maven-clean-plugin:2.5 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-clean-plugin:jar:2.5: Could not transfer artifact org.apache.maven.plugins:maven-clean-plugin:pom:2.5 from/to central (https://repo.maven.apache.org/maven2): java.security.ProviderException: java.security.KeyException -&gt; [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException</pre><p>And the Ambari Stack Trace is : </p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.3/services/ZEPPELIN/package/scripts/master.py\", line 295, in &lt;module&gt;\n    Master().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.3/services/ZEPPELIN/package/scripts/master.py\", line 119, in install\n    Execute(format(\"{service_packagedir}/scripts/setup_snapshot.sh {zeppelin_dir} {hive_metastore_host} {hive_metastore_port} {zeppelin_host} {zeppelin_port} {setup_view}  &gt;&gt; {zeppelin_log_file}\"), user=params.zeppelin_user)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 238, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of '/var/lib/ambari-agent/cache/stacks/HDP/2.3/services/ZEPPELIN/package/scripts/setup_snapshot.sh /opt/incubator-zeppelin 0.0.0.0 None  172.16.6.142 9995 True  &gt;&gt; /var/log/zeppelin/zeppelin-setup.log' returned 1. --2016-01-28 16:57:41--  &lt;a href=\"https://github.com/hortonworks-gallery/zeppelin-notebooks/archive/master.zip\"&gt;https://github.com/hortonworks-gallery/zeppelin-notebooks/archive/master.zip&lt;/a&gt;\nResolving &lt;a href=\"http://github.com/\"&gt;github.com&lt;/a&gt;... 192.30.252.128\nConnecting to &lt;a href=\"http://github.com/\"&gt;github.com&lt;/a&gt;|192.30.252.128|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: &lt;a href=\"https://codeload.github.com/hortonworks-gallery/zeppelin-notebooks/zip/master\"&gt;https://codeload.github.com/hortonworks-gallery/zeppelin-notebooks/zip/master&lt;/a&gt; [following]\n--2016-01-28 16:57:42--  &lt;a href=\"https://codeload.github.com/hortonworks-gallery/zeppelin-notebooks/zip/master\"&gt;https://codeload.github.com/hortonworks-gallery/zeppelin-notebooks/zip/master&lt;/a&gt;\nResolving &lt;a href=\"http://codeload.github.com/\"&gt;codeload.github.com&lt;/a&gt;... 192.30.252.162\nConnecting to &lt;a href=\"http://codeload.github.com/\"&gt;codeload.github.com&lt;/a&gt;|192.30.252.162|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 448310 (438K) [application/zip]\nSaving to: ?notebooks.zip?\n\n     0K .......... .......... .......... .......... .......... 11%  324K 1s\n    50K .......... .......... .......... .......... .......... 22%  651K 1s\n   100K .......... .......... .......... .......... .......... 34% 1.15M 1s\n   150K .......... .......... .......... .......... .......... 45% 1.36M 0s\n   200K .......... .......... .......... .......... .......... 57%  665K 0s\n   250K .......... .......... .......... .......... .......... 68% 1.17M 0s\n   300K .......... .......... .......... .......... .......... 79% 1.40M 0s\n   350K .......... .......... .......... .......... .......... 91% 71.9M 0s\n   400K .......... .......... .......... .......              100% 78.8M=0.5s\n\n2016-01-28 16:57:43 (946 KB/s) - ?notebooks.zip? saved [448310/448310]</pre><p>Let us know if we are missing something. Let us know if we can provide more info about this in order to debug this.  </p><p>Mangesh</p>","tags":["Spark","zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-29 11:43:42.0","id":12377,"title":"Best practice to load multiple client data into Hadoop","body":"<p>We are creating POC on Hadoop framework .We want to load data of multiple client into Hive tables.</p><p>As of now, we have separate database for each client on SQL Server. This infrastructure will remain same for OLTP. Hadoop will be used for OLAP. We have some primary dimension tables which are same for each client. All client database has schema. These tables have same primary key value. Till now, this was fine as we have separate database for client. Now we are trying to load multiple client data into same data container (Hive tables). Now we will have multiple row with same primary key value if we load data directly into Hive from multiple SQL Server databases through Sqoop job. I am thinking to use the surrogate key in Hive tables but Hive does not support auto increment but can be achieved with UDF.</p><p>We don't want to modify the SQL Server data as it's running production data.</p><p>a. What are the standard/generic way/solution to load multiple client data into Hadoop ecosystem ? We never want that data of different clients gets mixed. Referential constraints are also missing on Hive. </p><p>b. How primary key of sql server database table can be mapped easily to Hadoop Hive table so data we can pick data by client name ?</p><p>c. How we can ensure that one client is never able to see the data of other client?</p><p>Thanks</p>","tags":["rdbms","hadoop-ecosystem","Hive"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-29 13:24:48.0","id":12411,"title":"HBase Master Failed to start ??","body":"<p>Please see the log below:-</p><pre>2016-01-29 13:00:41,580 FATAL [eu-lamp-dev-xl-0019-hadoop-sec-master:16000.activeMasterManager] master.HMaster: Failed to become active master\norg.apache.hadoop.ipc.RemoteException(org.apache.ranger.authorization.hadoop.exceptions.RangerAccessControlException): Permission denied: principal{user=hbase,groups: [hadoop]}, access=null, /apps/hbase/data\n        at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:327)\n        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1698)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:108)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3856)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1011)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:843)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1427)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n        at com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)\n        at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1424)\n        at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:424)\n        at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:146)\n        at org.apache.hadoop.hbase.master.MasterFileSystem.&lt;init&gt;(MasterFileSystem.java:126)\n        at org.apache.hadoop.hbase.master.HMaster.finishActiveMasterInitialization(HMaster.java:649)\n        at org.apache.hadoop.hbase.master.HMaster.access$500(HMaster.java:182)\n        at org.apache.hadoop.hbase.master.HMaster$1.run(HMaster.java:1646)\n        at java.lang.Thread.run(Thread.java:745)\n2016-01-29 13:00:41,583 FATAL [eu-lamp-dev-xl-0019-hadoop-sec-master:16000.activeMasterManager] master.HMaster: Unhandled exception. Starting shutdown.\norg.apache.hadoop.ipc.RemoteException(org.apache.ranger.authorization.hadoop.exceptions.RangerAccessControlException): Permission denied: principal{user=hbase,groups: [hadoop]}, access=null, /apps/hbase/data\n        at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:327)\n        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1698)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:108)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3856)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1011)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:843)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1427)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n        at com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)\n        at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1424)\n        at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:424)\n        at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:146)\n        at org.apache.hadoop.hbase.master.MasterFileSystem.&lt;init&gt;(MasterFileSystem.java:126)\n        at org.apache.hadoop.hbase.master.HMaster.finishActiveMasterInitialization(HMaster.java:649)\n        at org.apache.hadoop.hbase.master.HMaster.access$500(HMaster.java:182)\n        at org.apache.hadoop.hbase.master.HMaster$1.run(HMaster.java:1646)\n        at java.lang.Thread.run(Thread.java:745)\n2016-01-29 13:00:41,584 INFO  [eu-lamp-dev-xl-0019-hadoop-sec-master:16000.activeMasterManager] regionserver.HRegionServer: STOPPED: Unhandled exception. Starting shutdown.\n2016-01-29 13:00:41,585 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] regionserver.HRegionServer: Stopping infoServer\n2016-01-29 13:00:41,610 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] mortbay.log: Stopped SelectChannelConnector@0.0.0.0:16010\n2016-01-29 13:00:41,612 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] regionserver.HRegionServer: stopping server eu-lamp-dev-xl-0019-hadoop-sec-master,16000,1454072438556\n2016-01-29 13:00:41,612 DEBUG [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] zookeeper.MetaTableLocator: Stopping MetaTableLocator\n2016-01-29 13:00:41,613 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x24fb78762b2015b\n2016-01-29 13:00:41,621 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] zookeeper.ZooKeeper: Session: 0x24fb78762b2015b closed\n2016-01-29 13:00:41,621 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000-EventThread] zookeeper.ClientCnxn: EventThread shut down\n2016-01-29 13:00:41,621 DEBUG [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] ipc.AbstractRpcClient: Stopping rpc client\n2016-01-29 13:00:41,631 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] regionserver.HRegionServer: stopping server eu-lamp-dev-xl-0019-hadoop-sec-master,16000,1454072438556; all regions closed.\n2016-01-29 13:00:41,632 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] hbase.ChoreService: Chore service for: eu-lamp-dev-xl-0019-hadoop-sec-master,16000,1454072438556 had [] on shutdown\n2016-01-29 13:00:41,632 DEBUG [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] master.HMaster: Stopping service threads\n2016-01-29 13:00:41,646 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] ipc.RpcServer: Stopping server on 16000\n2016-01-29 13:00:41,646 INFO  [RpcServer.listener,port=16000] ipc.RpcServer: RpcServer.listener,port=16000: stopping\n2016-01-29 13:00:41,649 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped\n2016-01-29 13:00:41,649 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping\n2016-01-29 13:00:41,680 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] zookeeper.RecoverableZooKeeper: Node /hbase-unsecure/rs/eu-lamp-dev-xl-0019-hadoop-sec-master,16000,1454072438556 already deleted, retry=false\n2016-01-29 13:00:41,694 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] zookeeper.ZooKeeper: Session: 0x24fb78762b2015a closed\n2016-01-29 13:00:41,694 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down\n2016-01-29 13:00:41,694 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] regionserver.HRegionServer: stopping server eu-lamp-dev-xl-0019-hadoop-sec-master,16000,1454072438556; zookeeper connection closed.\n2016-01-29 13:00:41,694 INFO  [master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000] regionserver.HRegionServer: master/eu-lamp-dev-xl-0019-hadoop-sec-master/10.8.7.62:16000 exiting\n[hdfs@eu-lamp-dev-xl-0019-hadoop-sec-master hbase]$ hdfs dfs -ls /apps/hbase\nFound 2 items\ndrwxr-xr-x   - hbase hdfs          0 2016-01-11 12:18 /apps/hbase/data\ndrwx--x--x   - hbase hdfs          0 2015-07-22 16:26 /apps/hbase/staging\n[hdfs@eu-lamp-dev-xl-0019-hadoop-sec-master hbase]$ hdfs dfs -ls /apps/hbase/data\nFound 10 items\ndrwxr-xr-x   - hbase hdfs          0 2015-09-10 10:48 /apps/hbase/data/.hbase-snapshot\ndrwxr-xr-x   - hbase hdfs          0 2016-01-11 12:18 /apps/hbase/data/.tmp\ndrwxr-xr-x   - hbase hdfs          0 2016-01-11 12:18 /apps/hbase/data/MasterProcWALs\ndrwxr-xr-x   - hbase hdfs          0 2016-01-12 11:31 /apps/hbase/data/WALs\ndrwxr-xr-x   - hbase hdfs          0 2015-09-10 10:48 /apps/hbase/data/archive\ndrwxr-xr-x   - hbase hdfs          0 2015-07-27 14:47 /apps/hbase/data/corrupt\ndrwxr-xr-x   - hbase hdfs          0 2015-07-22 16:32 /apps/hbase/data/data\n-rwxr-xr-x   3 hbase hdfs         42 2015-07-22 16:32 /apps/hbase/data/hbase.id\n-rwxr-xr-x   3 hbase hdfs          7 2015-07-22 16:32 /apps/hbase/data/hbase.version\ndrwxr-xr-x   - hbase hdfs          0 2016-01-28 13:32 /apps/hbase/data/oldWALs\n[hdfs@eu-lamp-dev-xl-0019-hadoop-sec-master hbase]$ hdfs dfs -ls /\nFound 10 items\ndrwxrwxrwx   - yarn   hadoop          0 2016-01-04 16:35 /app-logs\ndrwxrwxrwx   - hdfs   hdfs            0 2015-07-22 16:28 /apps\ndrwxr-xr-x   - hdfs   hdfs            0 2015-07-22 16:26 /hdp\ndrwxr-xr-x   - mapred hdfs            0 2015-07-22 16:23 /mapred\ndrwxrwxrwx   - hdfs   hdfs            0 2015-07-22 16:23 /mr-history\ndrwxr-xr-x   - hdfs   hdfs            0 2016-01-26 09:23 /sources\ndrwxr-xr-x   - hdfs   hdfs            0 2015-07-22 16:22 /system\ndrwxr-xr-x   - hdfs   hdfs            0 2015-10-27 11:22 /test\ndrwxrwxrwx   - hdfs   hdfs            0 2016-01-14 10:19 /tmp\ndrwxr-xr-x   - hdfs   hdfs            0 2016-01-07 10:21 /user\n[hdfs@eu-lamp-dev-xl-0019-hadoop-sec-master hbase]$ hdfs dfs -ls /apps\nFound 2 items\ndrwxrwxrwx   - hdfs hdfs          0 2015-07-22 16:26 /apps/hbase\ndrwxr-xr-x   - hdfs hdfs          0 2015-07-22 16:28 /apps/hive\n[hdfs@eu-lamp-dev-xl-0019-hadoop-sec-master hbase]$ hdfs dfs -ls /apps/hbase\nFound 2 items\ndrwxr-xr-x   - hbase hdfs          0 2016-01-11 12:18 /apps/hbase/data\ndrwx--x--x   - hbase hdfs          0 2015-07-22 16:26 /apps/hbase/staging\n[hdfs@eu-lamp-dev-xl-0019-hadoop-sec-master hbase]$</pre>","tags":["Hbase","Ranger"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-01 00:56:46.0","id":12746,"title":"How to load schema file for NiFi ConvertCSVtoAvro?","body":"<p>I want to configure the ConvertCSVtoAvro processor to read an Avro schema from a local text file.  When I set the \"Record schema\" parameter to the file's path, C:\\Avro\\schema1.avsc, I receive an error that reads \"Failed to parse schema: C:\\Avro\\schema1.avsc\".  </p><p>How can I get the Record schema parameter to point to the contents of my schema file?</p>","tags":["nifi-processor","Nifi"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-01 09:47:15.0","id":12799,"title":"Trouble with classes while using s3a filesystem in oozie coordinator input dataset","body":"<p>For our hadoop clusters ( versions 2.6.0 and 2.7.1) we are using s3 buckets as the source of a number of input files. \nWe have been successful at creating an oozie workflow with a distcp action, putting the data from the bucket to where we want it on the cluster. The filesystem we chose to do this is s3a (we expect large files).</p><p>Now we want to schedule this workflow, using a location on the bucket as the uri for the input dataset. Below is an example of the dataset. </p><pre>\t\t&lt;dataset name=\"s3bucket\" initial-instance=\"${coorStartTime}\" timezone=\"Europe/Amsterdam\" frequency=\"15\"&gt;\n \t\t\t&lt;uri-template&gt;s3a://our-s3-bucket/${YEAR}${MONTH}${DAY}&lt;/uri-template&gt;\n \t\t\t&lt;done-flag&gt;&lt;/done-flag&gt;\t\n\t\t&lt;/dataset&gt;\n</pre><p>Since this is a coordinator dataset and no oozie workflow-action has been started yet, the hadoop-aws&lt;version&gt;.jar in the share lib is not picked up. As a result we get an error saying that the class org.apache.hadoop.fs.s3a.S3AFileSystem is missing.</p><p>I tried adding the hadoop-aws.jar to &lt;path-to-oozie&gt;/libext and then do</p><pre>./oozie-setup.sh prepare-war</pre><p>That gave us an error when starting the oozie service back up. (the same on both clusters)</p><p>(directly placing the jar in /libserver got me the same error)</p><p><em>Note: without the added hadoop-aws.jar this particular part of the oozie.log shows all the share lib jars it can find, instead of the error.</em></p><pre>2016-01-27 12:33:29,948 ERROR ShareLibService:540 - SERVER[host] USER[-] GROUP[-] org.apache.oozie.service.ServiceException: E0104: Could not fully initialize service [org.apache.oozie.service.ShareLibService], Not able to cache sharelib. An Admin needs to install the sharelib with oozie-setup.sh and issue the 'oozie admin' CLI command to update the sharelib\norg.apache.oozie.service.ServiceException: E0104: Could not fully initialize service [org.apache.oozie.service.ShareLibService], Not able to cache sharelib. An Admin needs to install the sharelib with oozie-setup.sh and issue the 'oozie admin' CLI command to update the sharelib\n\tat org.apache.oozie.service.ShareLibService.init(ShareLibService.java:123)\n\tat org.apache.oozie.service.Services.setServiceInternal(Services.java:383)\n\tat org.apache.oozie.service.Services.setService(Services.java:369)\n\tat org.apache.oozie.service.Services.loadServices(Services.java:302)\n\tat org.apache.oozie.service.Services.init(Services.java:210)\n\tat org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:45)\n\tat org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4210)\n\tat org.apache.catalina.core.StandardContext.start(StandardContext.java:4709)\n\tat org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:799)\n\tat org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:779)\n\tat org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)\n\tat org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:675)\n\tat org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:601)\n\tat org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:502)\n\tat org.apache.catalina.startup.HostConfig.start(HostConfig.java:1317)\n\tat org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:324)\n\tat org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1065)\n\tat org.apache.catalina.core.StandardHost.start(StandardHost.java:822)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1057)\n\tat org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)\n\tat org.apache.catalina.core.StandardService.start(StandardService.java:525)\n\tat org.apache.catalina.core.StandardServer.start(StandardServer.java:754)\n\tat org.apache.catalina.startup.Catalina.start(Catalina.java:595)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)\n\tat org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)\nCaused by: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.s3a.S3AFileSystem could not be instantiated\n\tat java.util.ServiceLoader.fail(ServiceLoader.java:224)\n\tat java.util.ServiceLoader.access$100(ServiceLoader.java:181)\n\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:377)\n\tat java.util.ServiceLoader$1.next(ServiceLoader.java:445)\n\tat org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2586)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2597)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2614)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2653)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2635)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169)\n\tat org.apache.oozie.service.ShareLibService.init(ShareLibService.java:112)\n\t... 29 more\nCaused by: java.lang.NoClassDefFoundError: com/amazonaws/services/s3/AmazonS3\n\tat java.lang.Class.getDeclaredConstructors0(Native Method)\n\tat java.lang.Class.privateGetDeclaredConstructors(Class.java:2532)\n\tat java.lang.Class.getConstructor0(Class.java:2842)\n\tat java.lang.Class.newInstance(Class.java:345)\n\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)\n\t... 39 more\nCaused by: java.lang.ClassNotFoundException: com.amazonaws.services.s3.AmazonS3\n\tat org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680)\n\tat org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526)\n\t... 44 more</pre><p>Other classes seems to be missing.But... The error talks about the share lib, but I changed nothing there and it works fine with the workflow\nMy question is: why did my adding a library to oozie /libext upset the share lib? \n And above all how can I get the coordinator to make use of the s3a filesystem class while trying to reach the bucket?</p><p><strong>Edited after comment by Artem Ervits</strong></p><p><strong>\n</strong></p><p>The workflow I am talking about is a regular workflow-app, using a distcp action to move .txt files from s3 to a folder on the cluster.</p><pre>&lt;workflow-app xmlns=\"uri:oozie:workflow:0.5\" name=\"100-load-full\"&gt;\n\n\t&lt;global&gt;\n\t\t&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n\t\t&lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n\t\t&lt;job-xml&gt;${jobXML}&lt;/job-xml&gt;\n\t\t&lt;configuration&gt;\n\t\t\t&lt;property&gt;\n\t\t\t\t&lt;name&gt;mapred.job.queue.name&lt;/name&gt;\n\t\t\t\t&lt;value&gt;${queueName}&lt;/value&gt;\n\t\t\t&lt;/property&gt;\n\t\t&lt;/configuration&gt;\n\t&lt;/global&gt;\n\n\t&lt;start to=\"start-distcp\"/&gt;\n\n\t&lt;action name=\"start-distcp\"&gt;\n         &lt;distcp xmlns=\"uri:oozie:distcp-action:0.2\"&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n            &lt;prepare&gt;\n            \t&lt;mkdir path=\"${nameNode}${dropzoneMgm}/${loadDate}\"/&gt;\n            &lt;/prepare&gt;\n\t    &lt;arg&gt;-Dmapred.job.queue.name=${queueName}&lt;/arg&gt;\n            &lt;arg&gt;s3a://my-favourite-s3-bucket/*&lt;/arg&gt;\n            &lt;arg&gt;${nameNode}${dropzoneMgm}/${loadDate}&lt;/arg&gt;\n         &lt;/distcp&gt; \n       \n        &lt;ok to=\"create-success\"/&gt;\n        &lt;error to=\"fail\"/&gt;\n    &lt;/action&gt;\n\t&lt;action name=\"fail\"&gt;\n        &lt;email xmlns=\"uri:oozie:email-action:0.1\"&gt;\n            &lt;to&gt;${mailgroep}&lt;/to&gt;   \n            &lt;subject&gt;050-workflow-mgm-bvn.xml&lt;/subject&gt;\n            &lt;body&gt;De workflow is met fout(en) beeindigd bij tabel ${tableName}&lt;/body&gt;\n        &lt;/email&gt;\n        &lt;ok to=\"fail-end\"/&gt;\n        &lt;error to=\"fail-end\"/&gt;\n\t&lt;/action&gt;\n\n\t&lt;action name=\"create-success\"&gt;\n\t\t&lt;fs&gt;\n\t\t\t&lt;touchz path=\"${nameNode}${dropzoneMgm}/${loadDate}/_SUCCESS\"/&gt;\n\t\t&lt;/fs&gt;\n\t\t&lt;ok to=\"end\"/&gt;\n\t\t&lt;error to=\"fail\"/&gt;\n\t&lt;/action&gt;\n\n\t&lt;kill name=\"fail-end\"&gt;\n\t\t&lt;message&gt;job failed&lt;/message&gt;\n\t&lt;/kill&gt;\n\t&lt;end name=\"end\"/&gt;\n\t\n&lt;/workflow-app&gt;\n</pre><p>Starting this workflow by using a coordinator which polls for a dataset works when this uri is on the cluster itself (hdfs uri).</p><p>For example using the dataset:</p><pre>&lt;dataset name=\"s3bucket\" initial-instance=\"${coorStartTime}\" timezone=\"Europe/Amsterdam\" frequency=\"15\"&gt;\n \t\t&lt;uri-template&gt;${nameNode}/dropzone&lt;/uri-template&gt;\n \t\t&lt;done-flag&gt;_TEST&lt;/done-flag&gt;\n&lt;/dataset&gt;\n</pre><p>This coordinator - workflow combination does what we expect it to. It copies everything in s3a://my-favourite-s3-bucket/ to ${nameNode}${dropzoneMgm}/${loadDate} if _TEST exists in ${nameNode}/dropzone</p><p>When I change the filesystem of the uri to a path on the bucket, then it goes wrong.</p><p>By the way we did set the property oozie.service.HadoopAccessorService.supported.filesystems to hdfs,s3a in oozie-site (because the standard was just hdfs)</p><p><strong>\n</strong></p>","tags":["s3","filesystem","Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-01 14:06:20.0","id":12846,"title":"How to solve \"Connection refused \" errors in HDP","body":"<p>I have been working on Hortonworks Sandbox. The Sandbox is deployed on CentOS 6.7 server. I was, at some point in time, able to access the Web UIs on my browser from remote PC. But most of the services are not accessible except the welcome page on port 8888. When I try to start the services with command line in PuTTY, no success! That is, the services are not running. When  I run this command, netstat -nltu , I can't see the service ports in the list of listening ports. How can I make the ports listen for TCP connection. Any walk through on configuration of iptables and port forwarding would also be highly appreciated!</p>","tags":["port","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-01 18:15:38.0","id":12921,"title":"Datanodes start and then stop ... only one datanode process working at a time","body":"<p>hi</p><p><strong>The Env details:</strong></p><p>I installed a hadoop 2.7.2 (not HW but pure Hadoop) multinode cluster on AWS (1 Namenode/1 2nd NN/ 3 datanodes - ubuntu 14.04). </p><p>The cluster was based on the following tutorial(http://mfaizmzaki.com/2015/12/17/how-to-install-hadoop-2-7-1-multi-node-cluster-on-amazon-aws-ec2-instance-improved-part-1/) --&gt; this means the first install (master) is copied and tuned across</p><p><strong>The Issue:</strong></p><p>The 3 data nodes individually work correctly if I configure the cluster with 1 Datanode (I specifically excluded the 2 others).</p><p>As soon as I add another data node the data node booting first log a FATAL error (see extract of the log file hereafter and snapshot of the VERSION file) and stop. The data node booting second work then fine...</p><ol><li><b>Any idea-recommendation ?</b></li><li><b>Am I doing something wrong cloning the AMI of the master on other machine?</b></li></ol><p>Thanks Folks!</p><p><strong>Log File</strong></p><p><strong>INFO</strong> org.apache.hadoop.hdfs.server.datanode.DataNode: Unsuccessfully sent block report 0x1858458671b,  containing 1 storage report(s), of which we sent 0. The reports had 0 total blocks and used 0 RPC(s). This took 5 msec to generate and 35 msecs for RPC and NN processing. Got back no commands. </p><p><strong>WARN </strong>org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1251070591-172.Y.Y.Y-1454167071207 (Datanode Uuid 54bc8b80-b84f-4893-8b96-36568acc5d4b) service to master/172.Y.Y.Y:9000 is shutting down\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.UnregisteredNodeException): Data node DatanodeRegistration(172.X.X.X:50010, datanodeUuid=54bc8b80-b84f-4893-8b96-36568acc5d4b, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-8e09ff25-80fb-4834-878b-f23b3deb62d0;nsid=278157295;c=0) is attempting to report storage ID 54bc8b80-b84f-4893-8b96-36568acc5d4b. Node 172.Z.Z.Z:50010 is expected to serve this storage. </p><p><strong>WARN </strong>org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-1251070591-172.31.34.94-1454167071207 (Datanode Uuid 54bc8b80-b84f-4893-8b96-36568acc5d4b) service to master/172.Y.Y.Y:9000 </p><p><strong>INFO</strong> org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool BP-1251070591-172.Y.Y.Y-1454167071207 (Datanode Uuid 54bc8b80-b84f-4893-8b96-36568acc5d4b)\nINFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing block pool BP-1251070591-172.31.34.94-1454167071207 </p><p><strong>WARN </strong>org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode\nINFO org.apache.hadoop.util.ExitUtil: Exiting with status 0 </p><p>INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at HNDATA2/172.X.X.x ************************************************************/</p>","tags":["hadoop","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-01 22:06:12.0","id":12945,"title":"Index XML documents (in HDFS) using Solr","body":"","tags":["xml","SOLR"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-01 22:33:33.0","id":12962,"title":"Used Sqoop to import Mysql table (Employees) onto HDP. Now, I would like to create a Hive ORC Employees Table based on it with the as is schema and data on HDP. How can I do it?","body":"<p>My first step was to create an ORC table on Hive with the same schema as the one imported on hdp :</p><pre>CREATE TABLE IF NOT EXISTS orcemployees( emp_no int, birth_date date,\n first_name string, last_name string, gender string, hire_date date) \nSTORED AS ORC;</pre><p>My second step was to copy the data from employees into orcemployees:</p><pre>insert into table orcemployees select * from employees;</pre><p>The problem is when I execute select * from orcemployees; the schema is displayed and not the associated data.</p>","tags":["Hive","Sqoop","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-02 10:43:44.0","id":13070,"title":"Ranger policy is not applied","body":"<p>Hi,</p><p>I created a policy for HDFS in Ranger and gave a dedicated user full permissions to the corresponding folder.\nBut that user is not able to list the contents of that folder due to HDFS-access denied-error. It seems like that the Ranger policy is not really in effect / not applied, see screenshots below for the details.</p><p>What I want to do (normally pretty simple ;) ) grant user w999711 full permissions to HDFS folder /data/raw. Authorization shall be handled completely by Ranger, therefore HDFS permissions on that folder are restrictive (700).</p><p><strong>The error</strong></p><p><img src=\"/storage/attachments/1714-dfs-command-line-error.png\"></p><p><strong>Policy config</strong></p><p><img src=\"/storage/attachments/1715-hdfs-policy-rawlayer.png\"></p><p><strong>Audit log</strong></p><p><img src=\"/storage/attachments/1716-hdfs-audit-w999711-access-denied.png\"></p><p>Why isn't the defined policy applied while accessing /data/raw ?!?! It also confuses me that in the Audit log there is enforcer \"hadoop-acl\", whereas I'd expected \"xa-secure-acl\" for accessing /data/raw</p><p>Any hints highly appreciated.....thanks, Gerd</p><p>PS: HDP2.2.4.2, Ambari 2.1.2.1</p>","tags":["HDFS","Ranger","hdfs-permissions"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-02 07:56:32.0","id":13045,"title":"Hortonworks Certified Developer Exam Compatibility Issues","body":"<p>Hi,</p><p>I ran compatibility test before scheduling the exam and it said a minimum requirement of 2 GHz frequency and a dual core processor. My system has dual core but the frequency is only about 1.8 GHz. How much of impact will this be?</p><p>Can we take up this test from prometric centre because of the restriction with  my hardware?</p><p>Lastly, test can be conducted on windows or we'll have to take it up through LINUX platform only?</p><p>Please assist me on these.</p><p>Regards</p><p>Saurabh</p>","tags":["hortonworks-university","hdpcd"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-02 17:24:45.0","id":13367,"title":"beeline connect","body":"<p>Hi:</p><p>After type this, i cant connect to hive metastore, why the connection is closed??</p><pre>5: jdbc:hive2://lnxbig05.cajarural.gcr:10000 (closed)&gt; !connect jdbc:hive2://lnxbig05.cajarural.gcr:10000 **** **** org.apache.hive.jdbc.HiveDriver;\nConnecting to jdbc:hive2://lnxbig05.cajarural.gcr:10000\norg/apache/hive/jdbc/HiveDriver;\n6: jdbc:hive2://lnxbig05.cajarural.gcr:10000 (closed)&gt;</pre>","tags":["Hive","beeline"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-02 23:19:42.0","id":13633,"title":"Hive import errors \"had to be cast to a less precise type in Hive\"","body":"<p>I have a SQL DB that I'm importing using Sqoop and --hive-import. Naturally there are many datatypes that don't exist in Hive.</p><p>The datatypes from my SQLSERVER DB that are getting the \"had to be cast to a less precise type in Hive\" error in Hive are as follows:</p><p>[datetime]</p><p>[float] (turning into scientific number once imported into Hive)</p><p>[money]</p><p>[decimal](18, 2)</p><p>etc.</p><p>I'm not sure what the use of importing them into Hive is if there isn't a correct Data Type for them to be imported to.</p><p>HDP 2.3\nHive Version = Hive 1.2.1.2.3.4.0-3485</p><p>Sqoop Version = Sqoop 1.4.6.2.3.4.0-3485</p><p>Am I relegated to just using String or Double --map-column-hive mappings for everything? Are there any Hive plugins from the community that handle realistic Data Types like money?</p>","tags":["Sqoop","Hive","hdp-2.3.0"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-02 20:47:13.0","id":13561,"title":"How to discp a partitioned multi-level directory while preserving structure","body":"<p>I need to take a list of HDFS directories and copy the contents of those directories to another HDFS using discp. The problem is recursively creating the directories automatically.  These are large partitioned files, and the available means seem to preserved structure only one level deep. Can anyone provide an example?</p>","tags":["HDFS","discp"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-03 03:03:15.0","id":13744,"title":"Ambari Web start zookeeper successfully, but Using\" zookeeper-server status\" show zookeeper not running","body":"<p>Ambari Web start zookeeper successfully, but Using\" zookeeper-server status\" cmd does not show \"follower or leader\",it shows zookeeper failed to startt</p>","tags":["zookeeper","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-03 13:11:18.0","id":13825,"title":"Unable to start derby server","body":"<p>i have installed hdp 2.3.4 on windows 2012 server.  when i run the file start_local_hdp_services.cmd, everything starts fine except derbyserver. to debug the issue, i executed derbyserver.cmd that i sitting under hdp\\hive directory and got the following error:</p><p></p><p>Error: Could not find or load main class org.apache.derby.drda.NetworkServerControl</p><p>i used @echo to print out the classpath and got the following output:</p><p></p><p>c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\etc\\hadoop;c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\share\\hadoop\\common\\lib\\*;c:\\hdp\\hadoop-2.7\n.1.2.3.4.0-3485\\share\\hadoop\\common\\*;c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\share\\hadoop\\hdfs;c:\\hdp\\hadoop-2.7.1.2.3.4.0-348\n5\\share\\hadoop\\hdfs\\lib\\*;c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\share\\hadoop\\hdfs\\*;c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\share\\ha\ndoop\\yarn\\lib\\*;c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\share\\hadoop\\yarn\\*;c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\share\\hadoop\\mapre\nduce\\lib\\*;c:\\hdp\\hadoop-2.7.1.2.3.4.0-3485\\share\\hadoop\\mapreduce\\*;c:\\hdp\\tez-0.7.0.2.3.4.0-3485\\conf\\;c:\\hdp\\tez-0.7.\n0.2.3.4.0-3485\\*;c:\\hdp\\tez-0.7.0.2.3.4.0-3485\\lib\\*;;c:\\hdp\\hive-1.2.1.2.3.4.0-3485\\lib\\derby-10.10.2.0.jar</p><p>then i did jar -tf derby-10.10.2.0.jar to see if it contains the NetworkServerControl and it does not; this is the class that its trying to start and it does not exist, at least in the derby-10.10.20.0.jar file. </p><p>on searching online, the above class sits in derbynet.jar file. maybe its named as derby-10.10.20.0.jar file, but issue is that the NetworkServerControl is missing and derbyserver is not starting.  </p><p>i'll really appreciate any help with this.</p>","tags":["hdp-2.3.4"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-03 12:20:12.0","id":13858,"title":"Hadoop daemons not running. (Ambari installation)","body":"<p>I have successfully installed Ambari Service manually. I have installed most of the services which were listed in Ambari. However I do not see the hadoop deamons running. I don't even find the hadoop directory under /bin directory. I have the following questions:</p><p>1) When Ambari server is setup on a Centos machine, where does Ambari install Hadoop?</p><p>2) Under which folder Hadoop is installed?</p><p>3) Why is hadoop deamons not started automatically?</p><p>4) If hadoop is not installed, what are the next steps?</p><p>Please can someone help me, because I do not find anything documentation that helps me understand this?</p>","tags":["ambari-2.1.2","hadoop"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-03 06:01:28.0","id":13764,"title":"Any Idea whether spanshot also consume space in hdfs ?","body":"<p>Actually we have setup a spacequota for one of our dir and limit is 200TB, also we have enabled snapshots. \nProblem is we just have 50TB data and after replication it become 150 TB but we are getting space issue . So my question is does snapshot also consume some data space or not. </p>","tags":["snapshot"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-03 15:56:23.0","id":13986,"title":"Work putHDFS on HDFS in HA?","body":"<p>I have a NiFi job but putHDFS processor return an error:</p><pre>failed to invoke @OnScheduled method due to java.lang.IllegalArgumentException: java.net.UnknownHostException: provaha; processor will not be scheduled to run for 30 sec: java.lang.IllegalArgumentException: java.net.UnknownHostException: provaha</pre><p>provaha is the correct reference for HDFS High Availability.</p><p>is there any particular configuration for puthdfs that it work with HDFS in HA?</p>","tags":["HDFS","Nifi","high-availability"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-04 07:35:56.0","id":14328,"title":"What is the difference between Partitioner, Combiner, Shuffle and sort phase in Map Reduce. What is the order of execution","body":"<p>What is the difference between Partitioner, Combiner, Shuffle and sort phase in Map Reduce. What is the order of execution of these phases.\n\nMy understanding of the process flow is as follows:</p><p>1) Each Map Task output is Partitioned and sorted in memory and Combiner functions runs on it. This output is written to local disk called as Intermediate Data.</p><p>2) All the intermediate data from all the DataNodes go through a phase called Shuffle and sort and which is taken care by Hadoop Framework.</p><p>3) Sorted output is given as input to Reducers.</p><p>Please verify if the process flow is correct and provide your valuable inputs.</p>","tags":["MapReduce"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-04 13:35:41.0","id":14402,"title":"Yarn preemption and Oozie","body":"<p>I wanted to know if people around here have experience with oozie and yarn preemption.</p><p>I think I remember that the two do not work well with each other.</p><p>I.e. lets assume we have the launcher Application Master, the launcher map task and the Task ( perhaps pig ) Application Master and the Pig tasks</p><p>So there are 4 possibilities:</p><p>A) Pig Container is killed</p><p>Should be fine, pig will reschedule it through the application master</p><p>B) Pig Application Master is killed</p><p>Should be rare since preemption kills Application Masters only as a last resort. I assume the oozie launcher would fail but that there is a retry parameter in oozie?</p><p>C) Oozie launcher Map is killed</p><p>Suddenly the pig task is orphaned. Will oozie application master restart the map? Will the map reconnect to the pig task? Or will it start a second one? </p><p>D) Oozie launcher AM is killed</p><p>Similar to C) but will oozie server restart the task or will it be shown as killed</p><p>I also remember an engagement where they had orphaned tasks because of oozie and preemption anybody seen someting like that?</p><p>Thanks a lot. </p>","tags":["YARN","Oozie","preemption"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-05 00:13:18.0","id":14589,"title":"PigStorage in mapreduce mode","body":"<p>Hi,</p><p>I am trying to execute pig script in mapreduce mode, script is simple:</p><pre>grunt&gt; sourceData = load 'hdfs://sandbox.hortonworks.com:8020/src/CustomerData.csv' using PigStorage(';') as (nullname: chararray,customerId: chararray,VIN: chararray,Birthdate: chararray,Mileage: chararray,Fuel_Consumption: chararray);</pre><p>File is stored in HDFS:</p><pre>hadoop fs -ls hdfs://sandbox.hortonworks.com:8020/src/CustomerData.csv\n\n-rw-r--r--  3 hdfs hdfs  6828 2016-02-04 23:55 hdfs://sandbox.hortonworks.com:8020/src/CustomerData.csv</pre><p>Error that i got:</p><p>Failed Jobs:\nJobId  Alias  Feature Message Outputs\njob_1454609613558_0003  sourceData  MAP_ONLY  Message: Job failed!  hdfs://sandbox.hortonworks.com:8020/tmp/temp-710368608/tmp-1611282262,</p><p>Input(s):\nFailed to read data from \"hdfs://sandbox.hortonworks.com:8020/src/CustomerData.csv\"</p><p>Output(s):\nFailed to produce result in \"hdfs://sandbox.hortonworks.com:8020/tmp/temp-710368608/tmp-1611282262\"</p><pre>Pig Stack Trace---------------ERROR 1066: Unable to open iterator for alias sourceDataorg.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias sourceData  at org.apache.pig.PigServer.openIterator(PigServer.java:935)  at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:754)  at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:376)  at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)  at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)  at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)  at org.apache.pig.Main.run(Main.java:565)  at org.apache.pig.Main.main(Main.java:177)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.apache.hadoop.util.RunJar.run(RunJar.java:221)  at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: java.io.IOException: Job terminated with anomalous status FAILED  at org.apache.pig.PigServer.openIterator(PigServer.java:927)  ... 13 more</pre>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-05 04:39:09.0","id":14605,"title":"Is there an option to update a column in a table using values from another table in hive?","body":"<p>I want to update the joining date column in table x which is currently messed up, by using the start date column in table y. The schema for both the tables are different however the common column is the id field. I'm unable to use update statement since I can't mention a sub query in the where clause. So I tried using the following and I'm unable to update the column : </p><p>insert into table x (joining_date) select min(start_date) from table y group by id;</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-06 04:34:11.0","id":14914,"title":"Expand Storage on hdfs disk - virtual","body":"<p>I need best practice or gotchas anyone can share about expanding storage (not adding) for hdfs.  Example: my node is a VM and I have 100gig of storage..I will to expand (not adding another disk) it to 200gig.  The disk is the same but I am able to virtually expand the storage (SAN).  Would this any any impact?  Any gotcha or things to watch for.  </p>","tags":["YARN","storage","HDFS","replication"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-08 08:18:41.0","id":15134,"title":"How to know the version of Ambari Server and Ambari Agent?","body":"<p>Can anybody help me understand how I can know the version of Ambari server and Ambari agent?</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-05 16:28:19.0","id":14779,"title":"HDF apache nifi best practice","body":"<p>Hi all,</p><p>can anybody share what is the best practice to install HDF</p><p>it is install inside HDP or on standalone server ?</p><p>if HDF install on standalone server, how to connect to hdfs on the HDP cluster?</p>","tags":["Nifi","hdf","best-practices"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-08 10:34:48.0","id":15160,"title":"Adding a new user to the cluster","body":"<p>Say users are allowed to access a cluster from the edge node of a cluster. If the user wants to run jobs on the cluster, does the user should have his account on all the nodes of the cluster or just having an account on the edge node is enough?</p>","tags":["edge"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-08 14:01:45.0","id":15219,"title":"HDFS Service Check Timing Out","body":"<p>I'm receiving the following error when running the HDFS service check.  It seems to be timing out on a WebHDFS PUT statement...</p><p>Ambari Version 2.1.1</p><pre>Python script has been killed due to timeout after waiting 300 secs\n</pre><pre>2016-02-08 08:37:18,268 - ExecuteHadoop['dfsadmin -fs hdfs://vxkid-phdpdv05.lmig.com:8020 -safemode get | grep OFF'] {'bin_dir': '/usr/hdp/current/hadoop-client/bin', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'logoutput': True, 'try_sleep': 3, 'tries': 20, 'user': 'hdfs'}\n2016-02-08 08:37:18,270 - Execute['hadoop --config /usr/hdp/current/hadoop-client/conf dfsadmin -fs hdfs://vxkid-phdpdv05.lmig.com:8020 -safemode get | grep OFF'] {'logoutput': True, 'try_sleep': 3, 'environment': {}, 'tries': 20, 'user': 'hdfs', 'path': ['/usr/hdp/current/hadoop-client/bin']}\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\nSafe mode is OFF\n2016-02-08 08:37:21,067 - HdfsResource['/tmp'] {'security_enabled': False, 'hadoop_bin_dir': '/usr/hdp/current/hadoop-client/bin', 'keytab': [EMPTY], 'default_fs': 'hdfs://vxkid-phdpdv05.lmig.com:8020', 'hdfs_site': ..., 'kinit_path_local': 'kinit', 'principal_name': None, 'user': 'hdfs', 'action': ['create_on_execute'], 'hadoop_conf_dir': '/usr/hdp/current/hadoop-client/conf', 'type': 'directory', 'mode': 0777}\n2016-02-08 08:37:21,071 - checked_call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X GET '\"'\"'http://vxkid-phdpdv05.lmig.com:50070/webhdfs/v1/tmp?op=GETFILESTATUS&user.name=hdfs'\"'\"' 1&gt;/tmp/tmp5cvrI_ 2&gt;/tmp/tmpcKO2SB''] {'logoutput': None, 'quiet': False}\n2016-02-08 08:37:21,259 - checked_call returned (0, '')\n2016-02-08 08:37:21,260 - HdfsResource['/tmp/idb60a9aca_date370816'] {'security_enabled': False, 'hadoop_bin_dir': '/usr/hdp/current/hadoop-client/bin', 'keytab': [EMPTY], 'default_fs': 'hdfs://vxkid-phdpdv05.lmig.com:8020', 'hdfs_site': ..., 'kinit_path_local': 'kinit', 'principal_name': None, 'user': 'hdfs', 'action': ['delete_on_execute'], 'hadoop_conf_dir': '/usr/hdp/current/hadoop-client/conf', 'type': 'file'}\n2016-02-08 08:37:21,261 - checked_call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X GET '\"'\"'http://vxkid-phdpdv05.lmig.com:50070/webhdfs/v1/tmp/idb60a9aca_date370816?op=GETFILESTATUS&user.name=hdfs'\"'\"' 1&gt;/tmp/tmpggYpl7 2&gt;/tmp/tmpW_lYIG''] {'logoutput': None, 'quiet': False}\n2016-02-08 08:37:21,426 - checked_call returned (0, '')\n2016-02-08 08:37:21,427 - HdfsResource['/tmp/idb60a9aca_date370816'] {'security_enabled': False, 'hadoop_bin_dir': '/usr/hdp/current/hadoop-client/bin', 'keytab': [EMPTY], 'source': '/etc/passwd', 'default_fs': 'hdfs://vxkid-phdpdv05.lmig.com:8020', 'hdfs_site': ..., 'kinit_path_local': 'kinit', 'principal_name': None, 'user': 'hdfs', 'action': ['create_on_execute'], 'hadoop_conf_dir': '/usr/hdp/current/hadoop-client/conf', 'type': 'file'}\n2016-02-08 08:37:21,428 - checked_call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X GET '\"'\"'http://vxkid-phdpdv05.lmig.com:50070/webhdfs/v1/tmp/idb60a9aca_date370816?op=GETFILESTATUS&user.name=hdfs'\"'\"' 1&gt;/tmp/tmp1n4fuh 2&gt;/tmp/tmpwXsc1P''] {'logoutput': None, 'quiet': False}\n2016-02-08 08:37:21,593 - checked_call returned (0, '')\n2016-02-08 08:37:21,594 - Creating new file /tmp/idb60a9aca_date370816 in DFS\n2016-02-08 08:37:21,595 - checked_call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X PUT -T /etc/passwd '\"'\"'http://vxkid-phdpdv05.lmig.com:50070/webhdfs/v1/tmp/idb60a9aca_date370816?op=CREATE&user.name=hdfs&overwrite=True'\"'\"' 1&gt;/tmp/tmp9ChlXL 2&gt;/tmp/tmpCmlsMf''] {'logoutput': None, 'quiet': False}</pre>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-10 21:07:32.0","id":16158,"title":"Hive and spagobi BI","body":"<p>Hi:</p><p>Which jars i need for any client or BI that want to connect to the HIVE??? Y enought with hive driver??? Or i need somenthing morr like hadoop jars</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-14 03:38:07.0","id":16837,"title":"Cannot copy from local machine to VM datanode via Java","body":"<p>Hello,</p><p>I have an application that copies data to HDFS, but is failing due to the datanode being excluded. See snippet:</p><pre>private void copyFileToHdfs(FileSystem hdfs, Path localFilePath, Path hdfsFilePath) throws IOException, InterruptedException {\n    log.info(\"Copying \" + localFilePath + \" to \" + hdfsFilePath);\n    hdfs.copyFromLocalFile(localFilePath, hdfsFilePath);\n}</pre><p>However, when I try to execute, I get:</p><pre>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/dev/workflows/test.jar could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1583)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3109)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3033)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:725)\n</pre><p>HDFS commands work fine. I can create, modify, delete. Copying data is the only problem. I've also attempted to check the ports by telnet'ing. I can telnet 8020, but not 50010. I assume this is the root of the issue and why the single datanode is being excluded. I attempted to add an iptable firewall rule but I still am running into the same issue.</p><p>Any help is appreciated.</p>","tags":["Sandbox","datanode","HDFS"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-14 07:37:17.0","id":45131,"title":"Not Able to start hive metastore","body":"<pre>Resource_management.core.exceptions.Fail: Execution of 'export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-metastore/bin/schematool -initSchema -dbType postgres -userName hive -passWord [PROTECTED]' returned 254. /etc/profile: fork: retry: Resource temporarily unavailable\n/etc/profile: fork: retry: Resource temporarily unavailable\n/etc/profile: fork: retry: Resource temporarily unavailable\n/etc/profile: fork: retry: Resource temporarily unavailable\n/etc/profile: fork: Resource temporarily unavailable\n/etc/profile: fork: retry: Resource temporarily unavailable </pre><p>/etc/profile: fork: retry: Resource temporarily unavailable</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-18 12:49:28.0","id":45641,"title":"List out the Metadata attributes in Hadoop","body":"<p>1. List out the Metadata attributes in Hadoop</p><p>2. Can we see the block level metadata file? if Yes, how can we see that file</p>","tags":["hadoop","metadata"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-19 11:42:48.0","id":45854,"title":"Unable get the final result after processing pig script","body":"<p>Hi <a rel=\"user\" href=\"/users/11295/slachterman.html\" nodeid=\"11295\">@slachterman</a></p><p>I am executing the below pig scripts and not getting the desired results.</p><p><img src=\"/storage/attachments/5860-pig-script.png\"></p><p>I added <strong>-useHCatlog </strong>argument also and did syntax check before executing this query.</p><p>Below are the schema information about hive tables i used in pig scripts</p><p><img src=\"/storage/attachments/5861-gelocation.png\"></p><p><img src=\"/storage/attachments/5862-driver-mileage.png\"></p><p><img src=\"/storage/attachments/5863-risk-factor.png\"></p><p>and when i run the <strong>select * from riskfacto</strong>r i dont get any data in riskfactor table in hive.</p><p>Please find the attached result and log file details.</p><p><a href=\"/storage/attachments/5864-result.txt\">result.txt</a></p><p>In the log file it says below message:</p><p><strong>WARNING: Use \"yarn jar\" to launch YARN applications.\n16/07/19 10:59:47 INFO pig.Main: Pig script completed in 4 seconds and 983 milliseconds (4983 ms)</strong>\n</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-20 00:16:43.0","id":45996,"title":"unresolved packages installing HDP 2.4.2 on Suse?","body":"<p>I'm following the 20160509 instructions for manually installing HDP. and getting a failure when I try to install the hadoop packages.</p><p>I'm running openSuse Leap42.1</p><p>When I do</p><p> $ zypper install hadoop </p><p>I get the following error output:</p><p>Loading repository data... </p><p>Reading installed packages... </p><p>Resolving package dependencies...</p><p>Problem: nothing provides hadoop_2_4_2_0_258 needed by hadoop-2.7.1.2.4.2.0-258.noarch</p><p> Solution 1: do not install hadoop-2.7.1.2.4.2.0-258.noarch </p><p>Solution 2: break hadoop-2.7.1.2.4.2.0-258.noarch by ignoring some of its dependencies</p><p>My repo list is:</p><p># zypper repos\n# | Alias  | Name  | Enabled | GPG Check | Refresh </p><p>--+-----------------------------------+----------------------------------------+---------+-----------+-------- </p><p>1 | HDP-2.4.2.0  | HDP Version - HDP-2.4.2.0  | Yes  | (  ) No  | No </p><p>2 | HDP-UTILS-1.1.0.20  | HDP Utils Version - HDP-UTILS-1.1.0.20 | Yes  | (  ) No  | No </p><p>3 | openSUSE-Ports-Leap-42.1-repo-oss | openSUSE-Ports-Leap-42.1-repo-oss  | Yes  | (r ) Yes  | Yes   </p><p>Why is this happening?</p><p>thanks,</p><p>-david</p><p style=\"margin-left: 40px;\"></p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-19 19:08:14.0","id":45958,"title":"Update policy REST request fails","body":"<p>I have created a policy object in Ranger with </p><pre>curl -iv -u admin:admin  -H \"Content-Type: application/json\" -d @hdfs-create-policy.payload -X POST http://192.168.26.111:6080/service/public/v2/api/policy</pre><p>and I'm trying to modify that entry (adding permissions) using the update policy by service-name and policy-name which fails.</p><pre> # curl -iv -u admin:admin  -H \"Content-Type: application/json\" -d @hdfs-update-policy-by-name.payload -X PUT http://192.168.26.111:6080/service/public/v2/api/Sandbox_hdfs/policy/appaccess\n* About to connect() to 192.168.26.111 port 6080 (#0)\n*   Trying 192.168.26.111... connected\n* Connected to 192.168.26.111 (192.168.26.111) port 6080 (#0)\n* Server auth using Basic with user 'admin'\n&gt; PUT /service/public/v2/api/Sandbox_hdfs/policy/appaccess HTTP/1.1\n&gt; Authorization: Basic YWRtaW46YWRtaW4=\n&gt; User-Agent: curl/7.19.7 (x86_64-suse-linux-gnu) libcurl/7.19.7 OpenSSL/0.9.8j zlib/1.2.7 libidn/1.10\n&gt; Host: 192.168.26.111:6080\n&gt; Accept: */*\n&gt; Content-Type: application/json\n&gt; Content-Length: 760\n&gt;\n&lt; HTTP/1.1 404 Not Found\nHTTP/1.1 404 Not Found\n&lt; Server: Apache-Coyote/1.1\nServer: Apache-Coyote/1.1\n&lt; Set-Cookie: JSESSIONID=125E35576B917EC0F85ED9BEAC80DF72; Path=/; HttpOnly\nSet-Cookie: JSESSIONID=125E35576B917EC0F85ED9BEAC80DF72; Path=/; HttpOnly\n&lt; X-Frame-Options: DENY\nX-Frame-Options: DENY\n&lt; Content-Length: 0\nContent-Length: 0\n&lt; Date: Tue, 19 Jul 2016 18:47:53 GMT\nDate: Tue, 19 Jul 2016 18:47:53 GMT\n&lt;\n* Connection #0 to host 192.168.26.111 left intact\n* Closing connection #0</pre><p>There is nothing at all helpful in the log. Can someone suggest what is the issue here? I'm at total loss :( TIA.</p><p>The payload for policy creation is:</p><pre>{\n    \"isEnabled\":true,\n    \"version\":1,\n    \"service\":\"Sandbox_hdfs\",\n    \"name\":\"appaccess\",\n    \"description\":\"This policy to test Apache Ranger API\",\n    \"isAuditEnabled\":true,\n    \"resources\":{\n         \"path\":{\n            \"isRecursive\":true,\n            \"values\":[\"/app/*\"],\n            \"isExcludes\":false\n         }\n    },\n    \"policyItems\":[\n       {\n         \"users\":[],\n         \"groups\":[\"public\",\"hadoop\"],\n         \"delegateAdmin\":true,\n         \"accesses\":[\n             {\"isAllowed\":true,\"type\":\"read\"},\n             {\"isAllowed\":true,\"type\":\"write\"},\n             {\"isAllowed\":true,\"type\":\"execute\"}\n         ],\n         \"conditions\":[]\n       }\n    ]\n}</pre><p>The payload for modify request is:</p><pre>{\n    \"isEnabled\":true,\n    \"version\":1,\n    \"service\":\"Sandbox_hdfs\",\n    \"name\":\"appaccess\",\n    \"description\":\"This policy to test Apache Ranger API\",\n    \"isAuditEnabled\":true,\n    \"resources\":{\n         \"path\":{\n            \"isRecursive\":true,\n            \"values\":[\"/app/*\"],\n            \"isExcludes\":false\n         }\n    },\n    \"policyItems\":[\n        {\n            \"users\":[],\n            \"groups\":[\"hadoop\"],\n            \"delegateAdmin\":true,\n            \"accesses\":[\n                {\"isAllowed\":true,\"type\":\"read\"},\n                {\"isAllowed\":true,\"type\":\"write\"},\n                {\"isAllowed\":true,\"type\":\"execute\"}\n                ]\n            ,\"conditions\":[]\n        }\n        {\n            \"users\":[],\n            \"groups\":[\"users\"],\n            \"delegateAdmin\":true,\n            \"accesses\":[\n                {\"isAllowed\":true,\"type\":\"read\"}\n                ]\n            ,\"conditions\":[]\n        }\n    ]\n}</pre>","tags":["ranger-0.5.0","ranger-migration"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-07-20 12:54:17.0","id":46099,"title":"Java. Lang. OutOfMemoryError","body":"","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-21 05:56:01.0","id":46347,"title":"My sand box VM is mounted successfully on windows but i login successfully but my web sandbox is not working and advance option button is not working","body":"<p><img src=\"/storage/attachments/5946-sandboxurl.png\"></p><p><img src=\"/storage/attachments/5947-vm-screen-shot.png\"></p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-21 10:26:14.0","id":46399,"title":"Nifi PutHDFS","body":"<p>Hello</p><p>I am trying to use the PutHDFS processor to push files into my hadoop cluster. I have a folder called CF in the root of my HDFS to which i want to push these files through the nifi processor.</p><p>What should i put in the Directory property of the nifi processor? I was advised i should put the fully qualified path of my folder. If the CF folder is in the root of HDFS, what would my fully qualified path be? </p><p>Thanks</p>","tags":["nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-21 10:13:37.0","id":46384,"title":"bin/solr status throwing an error \"clusterstatus the collection time out:180s\"","body":"<p>I have setup my cluster in cloud mode with 3 nodes and it was running fine. I have created two collections as well. But when I was enabling ranger plugin then I had to restart all the nodes. I stopped all the solr instance and started once again then following error occurred. I can see solr UI and my collection but I can't see cluster status in command line as getting below error. </p><p>solr@m1 solr]$ bin/solr stop -all</p><p>Sending stop command to Solr running on port 8983 ... waiting 5 seconds to allow Jetty process 15060 to stop gracefully.</p><p>[solr@m1 solr]$ bin/solr start -c -z m1.hdp22:2181,m2.hdp22:2181,w1.hdp22:2181 -Dsolr.directoryFactory=HdfsDirectoryFactory -Dsolr.lock.type=hdfs</p><p>Started Solr server on port 8983 (pid=24214). Happy searching!</p><p>[solr@m1 solr]$ bin/solr status</p><p>Found 1 Solr nodes: </p><p>Solr process 24214 running on port 8983</p><p><strong>Failed to get system information from http://localhost:8983/solr/ due to: org.apache.solr.client.solrj.SolrServerException: clusterstatus the collection time out:180s</strong></p><p><strong>at org.apache.solr.util.SolrCLI.getJson(SolrCLI.java:537)</strong></p><p><strong>at org.apache.solr.util.SolrCLI.getJson(SolrCLI.java:471)</strong></p><p><strong>at org.apache.solr.util.SolrCLI$StatusTool.getCloudStatus(SolrCLI.java:721)</strong></p><p><strong>at org.apache.solr.util.SolrCLI$StatusTool.reportStatus(SolrCLI.java:704)</strong></p><p><strong>at org.apache.solr.util.SolrCLI$StatusTool.runTool(SolrCLI.java:662)</strong></p><p><strong>at org.apache.solr.util.SolrCLI.main(SolrCLI.java:215)</strong></p><p><strong>\n</strong></p><p>I went through following  jira which tell it is bug but I am not sure how to solve this issue,so can someone please help me to get it resolved. </p><p>https://issues.apache.org/jira/browse/SOLR-7018</p><p>Note : I am using solr-spec-version\":\"5.2.1 version. </p>","tags":["SOLR"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-21 13:45:28.0","id":46460,"title":"Ambari home page","body":"<p>I have successfully installed in my server.However while opening the url through the browser the login page is not coming.Please see the atatchment.<a href=\"/storage/attachments/5960-ambari-home.png\">ambari-home.png</a>.Am I  missing something here.</p>","tags":["ambari-server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-24 06:13:02.0","id":7926,"title":"Zeppelin starting error in HDP 2.3.2 on Amazon Ec2","body":"<p>Hi,</p><p>I am getting error while starting the Zeppelin service through Ambari </p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.3/services/ZEPPELIN/package/scripts/master.py\", line 295, in &lt;module&gt;\n    Master().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 216, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.3/services/ZEPPELIN/package/scripts/master.py\", line 230, in start\n    Execute (params.zeppelin_dir+'/bin/zeppelin-daemon.sh start &gt;&gt; ' + params.zeppelin_log_file, user=params.zeppelin_user)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 152, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 118, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 260, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 290, in _call\n    err_msg = Logger.filter_text((\"Execution of '%s' returned %d. %s\") % (command_alias, code, all_output))\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 31: ordinal not in range(128)</pre><p>Any pointer/guidance would be really appreaciated.</p><p>Thanks</p>","tags":["hdp-2.3.2","ec2","zeppelin","aws"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-12-28 05:25:37.0","id":8130,"title":"Some Hive unit tests doesn't work in intellij","body":"<p>Hi</p><p>Is there someone who develop Hive with intellij? </p><p>I want to test with\nMiniHS2 whose MiniClusterType is MR. But it doesn't work in intellij. </p><p>For example, I can't run TestJdbcWithMiniMR in intellij. (I can run it\nwith maven.)</p><p> It seems to fail when MapReduce is running.</p><pre>2015-12-24 03:37:54,388 Stage-1 map = 0%,  reduce = 0%\nEnded Job = job_1450957034598_0002 with errors\nError during job, obtaining debugging information...\nJob Tracking URL: http://&lt;local machine name&gt;:50259/cluster/app/application_1450957034598_0002\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\nMapReduce Jobs Launched:\nStage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL\nTotal MapReduce CPU Time Spent: 0 msec\njava.sql.SQLException: Error while processing statement: FAILED: Execution\nError, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:282)\n\tat org.apache.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:378)\n\tat org.apache.hive.jdbc.TestJdbcWithMiniMr.verifyResult(TestJdbcWithMiniMr.java:339)\n\tat org.apache.hive.jdbc.TestJdbcWithMiniMr.testKvQuery(TestJdbcWithMiniMr.java:319)\n\tat org.apache.hive.jdbc.TestJdbcWithMiniMr.testMrQuery(TestJdbcWithMiniMr.java:152)\n</pre><p>I'd like to hear anything you happen to know about the situation.</p>","tags":["development","Hive","intellij"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-01-07 04:31:58.0","id":9060,"title":"How to move service ambari  from node 1 to another node","body":"<p>Hai guys i have some question about ambari, how i can move service which was install on the server, i will plan to add 2 new node to the existing cluster and i want to move some service to that new nodethanks</p>","tags":["ambari-server","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-05 20:35:51.0","id":8868,"title":"Resourcemanager spams krb5kdc.log","body":"<p>Hello,</p><p>I'm wondering if anyone has any advice regarding this problem:</p><p>Our krb5kdc.log is filling up with lines like these:</p><pre>Jan 01 03:18:02 censored-hostname2 krb5kdc[66490](info): TGS_REQ (6 etypes {18 17 16 23 1 3}) 10.204.167.8: ISSUE: authtime 1451550638, etypes {rep=18 tkt=18 ses=18}, rm/censored-hostname@OUR.REALM for HTTP/censored-hostname@OUR.REALM</pre><p>The current file contains log messages from 4.5 days and I have about 4 million of these lines in that file. These are filling up the /var partition for us.</p><p>I can't find anything in the resourcemanager logs that indicates that it would spam the kdc with ticket requests.</p><p>We are running HDP-2.2.6.0 and MIT Kerberos. The cluster is used in production and is at times heavily utilized.</p><p>Grateful for any input.</p><p>/Thomas</p>","tags":["security","YARN","kerberos","resource-manager"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-07 10:10:52.0","id":9079,"title":"Remote debug HiveServer2","body":"<p>Hello,</p><p>I'm running HDP-2.2.6.0-2800 and am trying to remote debug the HiveServer2 process.</p><p>My problem is that I don't get the line numbers to match when setting breakpoints in the code and stepping through the code.</p><p>This usually happens when the source code on my local machine from which I am running the debugger does not match the compiled code running on the server I'm debugging.</p><p>Has anyone got this to work? In that case how?</p><p>Here are my instructions to reproduce this problem using the HDP-2.2.4 sandbox (there is no 2.2.6 sandbox afaik):</p><p>======================================</p><p>1. Create a new HDP-2.2.4 sandbox instance.&lt;br/&gt;</p><p>2. Add the following snippet to Advanced hive-env -&gt; hive-env template </p><p>&lt;br/&gt;</p><pre># Enable remote debugging of hiveserver2.\nif [ \"$SERVICE\" = \"hiveserver2\" ]; then\n  export HADOOP_OPTS=\"$HADOOP_OPTS -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\"\nfi</pre><p>3. Modify the /usr/hdp/2.2.4.2-2/hive/bin/hive.distro file by replacing the following section &lt;br/&gt;</p><pre># Make sure we're using a compatible version of Hadoop\nif [ \"x$HADOOP_VERSION\" == \"x\" ]; then\n  HADOOP_VERSION=$($HADOOP version | awk '{if (NR == 1) {print $2;}}');\nfi</pre><p>With this: (When you start the hiveserver with the agent flags, it \nprint an additional line to stdout which confuses this awk script)</p><pre># Make sure we're using a compatible version of Hadoop\nif [ \"$SERVICE\" == 'hiveserver2' ]; then\n  if [ \"x$HADOOP_VERSION\" == \"x\" ]; then\n    HADOOP_VERSION=$($HADOOP version | awk '{if (NR == 2) {print $2;}}');\n  fi\nelse\n  if [ \"x$HADOOP_VERSION\" == \"x\" ]; then\n    HADOOP_VERSION=$($HADOOP version | awk '{if (NR == 1) {print $2;}}');\n  fi\nfi\n</pre><p>4. Clone the hive git repo and change to the branch-0.14 branch.&lt;br/&gt;</p><p>5. Create a remote debug connection to the hiveserver2 java process&lt;br/&gt;</p><p>(i'm using IntelliJ IDEA to setup the project)</p><p>6. Set a breakpoint in org.apache.hive.service.cli.session.SessionManager, line 268, i.e.</p><pre>if (withImpersonation) {\n\tHiveSessionImplwithUGI sessionWithUGI = new HiveSessionImplwithUGI(protocol, username, password,hiveConf, ipAddress, delegationToken);\n\tsession = HiveSessionProxy.getProxy(sessionWithUGI, sessionWithUGI.getSessionUgi());\n\tsessionWithUGI.setProxySession(session);\n} else {\n      session = new HiveSessionImpl(protocol, username, password, hiveConf, ipAddress);\n}\n\nsession.setSessionManager(this);\nsession.setOperationManager(operationManager); // &lt;--- Set breakpoint here for example\ntry {\n      session.initialize(sessionConf);\n      if (isOperationLogEnabled) {\n      \tsession.setOperationLogSessionDir(operationLogRootDir);\n      }\n      session.open();\n} catch (Exception e) {\n      throw new HiveSQLException(\"Failed to open new session\", e);\n}</pre>\n<p>7. Start a new hive session, I'm using beeline &lt;br/&gt;</p><p>8. See that the hiveserver2 execution is halted at the breakpoint. &lt;br/&gt;</p><p>9. Try to \"Step into\" the session.setOperationManager method, and you actually end up in </p><pre>org.apache.hive.service.cli.session.HiveSessionImpl.getSessionHandle()</pre><p> An obvious line mismatch here as you can see.</p><p>Perhaps I am missing something here.</p><p>Grateful for any tips.</p><p>/Thomas</p>","tags":["hiveserver2","remote-debug"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-17 03:23:20.0","id":6971,"title":"can not start zookeeper","body":"<p>Hi, </p><p>We were able to run zookeeper on our cluster, but it crushed and can not start any more. The error msg now is like this: </p><p>zookeeper cannot open channel to X at election address</p><p>and Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMain</p><p>I checked our zookeeper-3.4.6.jar is under kafka/libs and it is of the same version. </p><p>I also tried this but did not work:</p><p><a href=\"http://stackoverflow.com/questions/30940981/zookeeper-error-cannot-open-channel-to-x-at-election-address\">http://stackoverflow.com/questions/30940981/zookee...</a></p><pre>server.1=0.0.0.0:2888:3888\nserver.2=192.168.10.10:2888:3888\nserver.3=192.168.2.1:2888:3888</pre>\n<p>Do you have any idea how to fix this problem? </p><p>Thank you very much for your help,</p>","tags":["HDFS","cluster","zookeeper","hadoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-05 15:17:27.0","id":8784,"title":"How to setup high availability for lily Hbase indexer","body":"<p>I'm looking for a solution to make the Hbase lily indexer high Availabile.</p><p>Thanks =)</p>","tags":["high-availability","indexer","SOLR","Hbase","search","lily"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-28 19:58:26.0","id":8195,"title":"Does the  TaskTracker spawns a new Mapper for each input split or for each key-value pair?","body":"<p>As per the The Definitive Guide-</p><ul><li><strong>Mapper</strong> as in the Map task spawned by the Tasktracker in a separate JVM to process an input split. ( all of it ). For TextInputFormat , this would be a specific number of lines from your input file.</li><li><strong>Map method</strong> that is called for every record(key-value pair) in the split. Mapper.map(...) . In case of TextInputFormat, each map method (invocation)will process a line in your input split</li></ul><p>With the above consideration the TaskTracker spawns a new Mapper for each input split.</p><p>But if you look at the Mapper class code-</p><pre> public class MaxTemperatureMapper\n     extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {</pre>\n<p>It means the Mapper class/object will take one key/value pair each time, when this k/v pair is been processed, the class/object is done, it is finished. Next k/v pair will be processed by another Mapper, a new class/object.</p><p>For Example, Think of 64MB block size contains 1000 records(key-value pairs). does the framework creates 1000 mapper here or just a single mapper.</p><p>This is little confusing. Can any one highlight more on whats exactly happens in this case.</p><p>Thanks in advance.</p>","tags":["YARN","HDFS","MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-07 17:03:35.0","id":9132,"title":"Ambari MIB for alerts via SNMP","body":"<p>Our customer would like to see Ambari alerts integrated with their fault management system via SNMP. So they have the following questions. Could someone please take a look at these questions and let me know how we can address them?</p><p>What is the Ambari MIB for alerts via SNMP? </p><p>Any recommendations on OIDs? </p><p>Do we have a list of custom properties that can be configured for the MIB via Ambari?</p>","tags":["monitoring","alerts","Ambari","ambari-metrics","snmptrap"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-08 23:02:46.0","id":9236,"title":"How-to Bypass Unsupported OS Ambari Agent Install Failures","body":"<p>I am trying to test Hadoop using Ubuntu 15 and also wanted to use Ambari for the deployment.  When I try to auto-install the agent in the initial server install wizard, it gets all the way to the end then fails saying \"Unsupported OS.\"  The agent appears to have installed correctly though, it just fails simply because the OS is identified as Ubuntu 15.</p><p>One workaround I read was to modify the OS identifier on each host to read Ubuntu 14 instead of 15.  Since the Ambari server is also running on Ubuntu 15 though, this generates a new error saying that the host is not in the same OS family as the server.</p><p>Is there a file where I can add Ubuntu 15 to the list of supported OS's?  Or can I disable this check?  Or is this enabled because there is no chance Ambari will run on Ubuntu 15?</p>","tags":["Ambari","ubuntu","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-09 05:28:29.0","id":9279,"title":"Submitted Storm topology fails due to class serialization error","body":"<p>\n\tI am running HDP 2.2 cluster. Trying to submit Storm topology which has been tested to run in local development environment. Topology gets deployed to the cluster but fails to initialize due to some class serializaiton issue.</p><p>\n\tI am getting the following error:</p>\n<pre>2016-01-09 00:18:51 b.s.d.worker [ERROR] Error on initialization of server mk-worker\njava.lang.RuntimeException: java.io.InvalidClassException: backtype.storm.task.ShellBolt; local class incompatible: stream classdesc serialVersionUID = -141067271633232418, local class serialVersionUID = 1217679306363381476\n        at backtype.storm.serialization.DefaultSerializationDelegate.deserialize(DefaultSerializationDelegate.java:56) ~[storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n...\n</pre><p>To avoid library incompatibility, I have localy copied the</p><pre>storm-core-0.9.3.2.2.9.0-3393.jar</pre><p>from the cluster running Storm instance. The library is used to build the topology and to submit it. Still no luck. </p><p>Anybody experienced similar issue? How do you submit your topologies to Storm?</p>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-09 23:26:10.0","id":9301,"title":"Apache Nifi Image metadata extraction like created_date etc","body":"<p>Trying to implement a data flow using nifi to get a jpeg or png and pdf file from a dir and also extract image and pdf metadata like created_date, filepath, date_modified etc and store the metadat in hdfs. I can go with PutHdfs. We have an processor ExtractImageMetadata but it doesnt have many properties to fulfill my requirement. Please let me know if i need a custom processor here? thank you</p>","tags":["Nifi","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-09 17:52:49.0","id":9284,"title":"ORC Stripe size","body":"<p>I have orc.stripe.size=67108864</p><p>What size of stripe would be for small orc file (for example 2 Mb, 350K records)? </p><p>I thought that in this case file contains 1 stripe with size 2Mb. However in orcfiledump I see 426 stripes! Why?</p>","tags":["stripe","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-10 08:04:26.0","id":9306,"title":"beeline and kerberos","body":"<p>I am trying to use beeline with hive + kerberos (Hortonworks sandbox 2.3) </p><p>The problem is that I can use hdfs but not beeline and I do not know \nwhat is wrong.</p><p>Console output:</p><p>[margusja@sandbox ~]$ kdestroy</p><p>[margusja@sandbox ~]$ hdfs dfs -ls <em>/user/</em></p><p>16/01/09 15:45:32 WARN ipc.Client: Exception encountered while \nconnecting to the server : javax.security.sasl.SaslException: GSS \ninitiate failed [Caused by GSSException: No valid credentials provided \n(Mechanism level: Failed to find any Kerberos tgt)]</p><p>ls: Failed on local exception: java.io.IOException: \njavax.security.sasl.SaslException: GSS initiate failed [Caused by \nGSSException: No valid credentials provided (Mechanism level: Failed to \nfind any Kerberos tgt)]; Host Details : local host is: \n\"sandbox.hortonworks.com/10.0.2.15\"; destination host is: \n\"sandbox.hortonworks.com\":8020; </p><p>[margusja@sandbox ~]$ kinit margusja</p><p>Password for <a href=\"mailto:margusja@EXAMPLE.COM\">margusja@EXAMPLE.COM</a>:</p><p>[margusja@sandbox ~]$ hdfs dfs -ls <em>/user/</em></p><p>Found 11 items</p><p>drwxrwx---  - ambari-qa hdfs  0 2015-10-27 12:39 \n/user/ambari-qa</p><p>drwxr-xr-x  - guest  guest  0 2015-10-27 12:55 /user/guest</p><p>drwxr-xr-x  - hcat  hdfs  0 2015-10-27 12:43 /user/hcat</p><p>drwx------  - hdfs  hdfs  0 2015-10-27 13:22 /user/hdfs</p><p>drwx------  - hive  hdfs  0 2016-01-08 19:44 /user/hive</p><p>drwxrwxrwx  - hue  hdfs  0 2015-10-27 12:55 /user/hue</p><p>drwxrwxr-x  - oozie  hdfs  0 2015-10-27 12:44 /user/oozie</p><p>drwxr-xr-x  - solr  hdfs  0 2015-10-27 12:48 /user/solr</p><p>drwxrwxr-x  - spark  hdfs  0 2015-10-27 12:41 /user/spark</p><p>drwxr-xr-x  - unit  hdfs  0 2015-10-27 12:46 /user/unit</p><p>So I think margusja's credential is ok</p><p>[margusja@sandbox ~]$ klist -f\nTicket cache: FILE:/tmp/krb5cc_1024\nDefault principal: margusja@EXAMPLE.COM\nValid starting     Expires            Service principal\n01/10/16 07:54:34  01/11/16 07:54:34  krbtgt/EXAMPLE.COM@EXAMPLE.COM\nrenew until 01/17/16 07:54:34, Flags: FRI</p><p>Now I try to use beeline:</p><p>[margusja@sandbox ~]$ beeline -u \n<a href=\"mailto:jdbc:hive2://127.0.0.1:10000/default;principal=hive/sandbox.hortonworks.com@EXAMPLE.COM\">\"jdbc:hive2://127.0.0.1:10000/default;principal=hive/sandbox.hortonworks.com@EXAMPLE.COM\"</a></p><p>SLF4J: Class path contains multiple SLF4J bindings.</p><p>SLF4J: Found binding in \n[jar:file:/usr/hdp/2.3.2.0-2950/spark/lib/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar!/org/slf4j/impl/StaticLoggerBinder.class]</p><p>SLF4J: Found binding in \n[jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</p><p>SLF4J: See <a href=\"http://www.slf4j.org/codes.html#multiple_bindings\">http://www.slf4j.org/codes.html#multiple_bindings</a> for an \nexplanation.</p><p>SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</p><p>WARNING: Use \"yarn jar\" to launch YARN applications.</p><p>SLF4J: Class path contains multiple SLF4J bindings.</p><p>SLF4J: Found binding in \n[jar:file:/usr/hdp/2.3.2.0-2950/spark/lib/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar!/org/slf4j/impl/StaticLoggerBinder.class]</p><p>SLF4J: Found binding in \n[jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</p><p>SLF4J: See <a href=\"http://www.slf4j.org/codes.html#multiple_bindings\">http://www.slf4j.org/codes.html#multiple_bindings</a> for an \nexplanation.</p><p>SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</p><p>Connecting to \n<a href=\"mailto:jdbc:hive2://127.0.0.1:10000/default;principal=hive/sandbox.hortonworks.com@EXAMPLE.COM\">jdbc:hive2://127.0.0.1:10000/default;principal=hive/sandbox.hortonworks.com@EXAMPLE.COM</a></p><p>16/01/09 15:46:59 [main]: ERROR transport.TSaslTransport: SASL \nnegotiation failure</p><p>javax.security.sasl.SaslException: GSS initiate failed [Caused by \nGSSException: No valid credentials provided (Mechanism level: Failed to \nfind any Kerberos tgt)]</p><p>  at \ncom.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)</p><p>  at \norg.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)</p><p>  at \norg.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)</p><p>  at \norg.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)</p><p>  at \norg.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)</p><p>  at \norg.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)</p><p>  at java.security.AccessController.doPrivileged(Native Method)</p><p>  at javax.security.auth.Subject.doAs(Subject.java:415)</p><p>  at \norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)</p><p>  at \norg.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)</p><p>  at \norg.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:210)</p><p>  at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:180)</p><p>  at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)</p><p>  at java.sql.DriverManager.getConnection(DriverManager.java:571)</p><p>  at java.sql.DriverManager.getConnection(DriverManager.java:187)</p><p>  at \norg.apache.hive.beeline.DatabaseConnection.connect(DatabaseConnection.java:142)</p><p>  at \norg.apache.hive.beeline.DatabaseConnection.getConnection(DatabaseConnection.java:207)</p><p>  at org.apache.hive.beeline.Commands.connect(Commands.java:1149)</p><p>  at org.apache.hive.beeline.Commands.connect(Commands.java:1070)</p><p>  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>  at \nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</p><p>  at \nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>  at java.lang.reflect.Method.invoke(Method.java:606)</p><p>  at \norg.apache.hive.beeline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:52)</p><p>  at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:970)</p><p>  at org.apache.hive.beeline.BeeLine.initArgs(BeeLine.java:707)</p><p>  at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:757)</p><p>  at \norg.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)</p><p>  at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)</p><p>  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>  at \nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</p><p>  at \nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>  at java.lang.reflect.Method.invoke(Method.java:606)</p><p>  at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</p><p>  at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p><p>Caused by: GSSException: No valid credentials provided (Mechanism level: \nFailed to find any Kerberos tgt)</p><p>  at \nsun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)</p><p>  at \nsun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)</p><p>  at \nsun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)</p><p>  at \nsun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)</p><p>  at \nsun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)</p><p>  at \nsun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)</p><p>  at \ncom.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)</p><p>  ... 34 more</p><p>Error: Could not open client transport with JDBC Uri: \n<a href=\"mailto:jdbc:hive2://127.0.0.1:10000/default;principal=hive/sandbox.hortonworks.com@EXAMPLE.COM\">jdbc:hive2://127.0.0.1:10000/default;principal=hive/sandbox.hortonworks.com@EXAMPLE.COM</a>: \nGSS initiate failed (state=08S01,code=0)</p><p>Beeline version 1.2.1.2.3.2.0-2950 by Apache Hive</p><p>0: jdbc:hive2://127.0.0.1:10000/default (closed)&gt;</p><p>Hive is configured as documentation requires:</p><p>  &lt;property&gt;</p><p>  &lt;name&gt;hive.server2.authentication&lt;/name&gt;</p><p>  &lt;value&gt;KERBEROS&lt;/value&gt;</p><p>  &lt;/property&gt;</p><p>  &lt;property&gt;</p><p>&lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</p><p>&lt;value&gt;/etc/security/keytabs/hive.service.keytab&lt;/value&gt;</p><p>  &lt;/property&gt;</p><p>  &lt;property&gt;</p><p>&lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</p><p>  &lt;value&gt;<a href=\"mailto:hive/_HOST@EXAMPLE.COM\">hive/_HOST@EXAMPLE.COM</a>&lt;/value&gt;</p><p>  &lt;/property&gt;</p><p>One more notice</p><p>When I do:</p><p>[margusja@sandbox ~]$ hdfs dfs -ls /</p><p>I see in krb5kdc log:</p><p>Jan 09 21:36:53 sandbox.hortonworks.com krb5kdc[8565](info): TGS_REQ (6 \netypes {18 17 16 23 1 3}) 10.0.2.15: ISSUE: authtime 1452375310, etypes \n{rep=18 tkt=18 ses=18}, <a href=\"mailto:margusja@EXAMPLE.COM\">margusja@EXAMPLE.COM</a> for \n<a href=\"mailto:nn/sandbox.hortonworks.com@EXAMPLE.COM\">nn/sandbox.hortonworks.com@EXAMPLE.COM</a></p><p>but when I use beeline I see there no lines in krb5kdc log.</p><p>When I do</p><p>[margusja@sandbox ~]$ kdestroy</p><p>and hdfs dfs -ls / - I see there no lines also in krb5kdc log.</p><p>I am so confused - What beeline expecting? I do kinit and I am getting \nticket before using beeline.</p><p>\nAny hints, because I am out of ideas. </p>","tags":["Hive","kerberos"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-11 01:54:46.0","id":9372,"title":"Adding Hadoop Client to existing nodes","body":"<p>How can hadoop client libraries be added to a node that already exists in the cluster? Can this be done through Ambari?</p>","tags":["client","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-09 22:27:42.0","id":9266,"title":"Range Audit Page - No Data for Report","body":"<p>We have HDP 2.3 with Ranger. Audit data store is MySQL DB. The audit transactions are getting stored in ranger_audit db. But the Ranger Audit page on ranger admin portal did not showing any record. We are also trying to store log to HDFS/ranger/audit folder, but these also not getting recorded. </p>","tags":["ranger-0.5.0","Ranger","audit","ranger-admin"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-11 03:24:04.0","id":9360,"title":"Timeout parameter for Ambari Blueprint installation","body":"<p>Can we set timeout parameter to create cluster API command using Ambari Blueprints ?</p>","tags":["installation","ambari-blueprint","timeout","api"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-11 03:31:58.0","id":9361,"title":"How to track \"HDP installation status using Ambari Blueprints\"","body":"<p>I know we can track installation status using below API call or from directly using Ambari UI.</p><pre>curl -H \"X-Requested-By: ambari\" -X GET -u admin:admin http://&lt;ambari-hostname&gt;:8080/api/v1/clusters/&lt;cluster-name&gt;/requests/</pre><p>Is there any other efficient way to track the status of installation in an automated way ? </p>","tags":["installation","ambari-blueprint","automation","api","Ambari","curl"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-11 18:56:20.0","id":9425,"title":"Is Flume to Kafka Sink supported in HDP 2.3.2?","body":"<p>Hi,</p><p>the following link says Kafka Source & Kafka Sink are supported in the Flume that comes with HDP2.3.</p><p>http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/new-features-230.html</p><p>But the JIRA (https://issues.apache.org/jira/browse/FLUME-2242) mentioned in the hortonworks docs says that it is available only from Flume 1.6 (while HDP 2.3.2 has only Flume 1.5.2).</p><p>Can someone confirm that Kafka Source & kafka Sink is indeep available in the Flume that comes with HDP 2.3.2 ?</p><p>thanks,</p><p>Raga</p>","tags":["Flume","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-11 17:29:40.0","id":9416,"title":"Falcon with HA Resource Manager","body":"<p>I have seen the question for HA Namenodes however HA Resource Managers still confuse me. In Hue you are for example told to add a second resource manager entry with the same logical hue name. I.e. Hue supports adding two resource manager urls and he will manually try both.</p><p>How does that work in Falcon, how can I enter an HA Resource Manager entry into the interfaces of the cluster Entity document. For Namenode HA I would use the logical name and the program would then read the hdfs-site.xml</p><p>I have seen the other similar questions for oozie but I am not sure it was answered or I didn't really understand it.</p><p><a href=\"https://community.hortonworks.com/questions/2740/what-value-should-i-use-for-jobtracker-for-resourc.html\">https://community.hortonworks.com/questions/2740/what-value-should-i-use-for-jobtracker-for-resourc.html</a></p><p>so assuming my active resource manager is </p><pre>mycluster1.com:8050</pre><p>and standby is</p><pre>mycluster2,com:8050</pre>","tags":["namenode-ha","Falcon","resource-manager"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-12 18:53:55.0","id":9567,"title":"Ambari 2.1.2 : cannot start newly added datanode because /usr/hdp/current link is missing","body":"<p>Hi,</p><p>I have an issue at installation time of datanode (Ambari 2.1.2, HDP 2.2.4) , it fails because at step \"Datanode start\" with the error:</p><p>\"\"</p><pre>  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 87, in action_create\n    raise Fail(\"Applying %s failed, parent directory %s doesn't exist\" % (self.resource, dirname))\nresource_management.core.exceptions.Fail: Applying File['/usr/hdp/current/hadoop-client/conf/hadoop-policy.xml'] failed, parent directory /usr/hdp/current/hadoop-client/conf doesn't exist</pre><p>\"\"</p><p>Obviously the cause of this issue already happens at step \"Datanode install\", because the last log message is:</p><p>\"\"</p><pre>...\n\n2016-01-12 19:44:17,233 - Skipping XmlConfig['core-site.xml'] due to only_if\n2016-01-12 19:44:17,233 - Can only link configs for HDP-2.3 and higher.</pre><p>\"\"</p><p>Hmmm, strange, HDP2.2 should be supported by recent versions of Ambari...</p><p>Is this a known issue? any workarounds, hints what to do ?</p><p>Thanks, Gerd</p>","tags":["Ambari","installation","datanode","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-10 18:18:50.0","id":9341,"title":"History server and hive server2 is not coming up in HDP 2.3 using ambari 2.2.","body":"<p>Please find the error while bringing up history server:--</p><p>Traceback (most recent call last): </p><pre>  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/historyserver.py\", line 182, in &lt;module&gt;\n    HistoryServer().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/historyserver.py\", line 100, in start\n    host_sys_prepped=params.host_sys_prepped)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/copy_tarball.py\", line 201, in copy_to_hdfs\n    replace_existing_files=replace_existing_files,\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py\", line 402, in action_create_on_execute\n    self.action_delayed(\"create\")\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py\", line 399, in action_delayed\n    self.get_hdfs_resource_executor().action_delayed(action_name, self)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py\", line 255, in action_delayed\n    self._create_resource()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py\", line 269, in _create_resource\n    self._create_file(self.main_resource.resource.target, source=self.main_resource.resource.source, mode=self.mode)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py\", line 322, in _create_file\n    self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/providers/hdfs_resource.py\", line 210, in run_command\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'curl -sS -L -w '%{http_code}' -X PUT -T /usr/hdp/2.3.4.0-3485/hadoop/mapreduce.tar.gz 'http://namenode:50070/webhdfs/v1/hdp/apps/2.3.4.0-3485/mapreduce/mapreduce.tar.gz?op=CREATE&user.name=hdfs&overwrite=True&permission=444'' returned status_code=403. \n{\n  \"RemoteException\": {\n    \"exception\": \"IOException\", \n    \"javaClassName\": \"java.io.IOException\", \n    \"message\": \"Failed to find datanode, suggest to check cluster health.\"\n  }</pre>","tags":["hdp-2.3.0","hiveserver2","Hive","ambari-2.1.2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-13 03:39:43.0","id":9634,"title":"Not able to change log directories for Ranger Admin and Ranger usersync","body":"<p>While Ambari UI provides feature to change log directories from UI, but it still is not being taken by Ranger. Looks like Ranger is overriding what I am setting in Ambari.</p>","tags":["hdp-2.3.2","ambari-2.1.2","ranger-usersync","ranger-admin"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-13 00:17:26.0","id":9618,"title":"Flume - question","body":"<p>Can anybody help me to test the flume on sandbox.  I did the following 3 steps.  Please help on next steps to test this. Like: telnet and input the data.</p><p><strong>A.</strong>  I have installed flume on HDP sandbox the Hortonworks documentation (yum install flume; yum install flume-agent).</p><p><strong>B.</strong>  I have used the sample agent given; shown below.</p><p>=============================</p><p>Configuration File</p><p>==============================</p><p># example.conf: A single-node Flume configuration </p><p># Name the components on this agent </p><p>a1.sources = r1 </p><p>a1.sinks = k1 </p><p>a1.channels = c1</p><p># Describe/configure the source </p><p> a1.sources.r1.type = netcat </p><p> a1.sources.r1.bind = localhost </p><p>a1.sources.r1.port = 44444</p><p># Describe the </p><p>sink \na1.sinks.k1.type = logger</p><p># Use a channel that buffers events in memory </p><p>a1.channels.c1.type = memory </p><p>a1.channels.c1.capacity = 1000 </p><p>a1.channels.c1.transactionCapacity = 100</p><p># Bind the source and sink to the channel </p><p>a1.sources.r1.channels = c1 </p><p>a1.sinks.k1.channel = c1</p><p><strong>C.</strong>  Started the agent: flume-ng agent -n &lt;agent-name&gt; -f &lt;configuration file name&gt;</p>","tags":["Sandbox","Flume"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-13 01:16:20.0","id":9624,"title":"Reading Input Stream using Spark Custom receiver","body":"<p>\n\tI have written custom receiver to receive the stream that is being generated by one of our application. The receiver starts the process gets the stream and then cals store. However, the receive method gets called multiple times, I have written proper loop break condition, but, could not do it. How to ensure it only reads once and does not read the already processed data.?\nHere is my custom receiver code: </p><pre>\tclass MyReceiver() extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging {\n  def onStart() {\n    new Thread(\"Splunk Receiver\") {\n       override def run() { receive() }\n    }.start()\n  }\n  def onStop() {\n  }\n  private def receive() {\n    try {\n      /*  My Code to run a process and get the stream */\n      val reader = new ResultsReader(job.getResults()); // ResultReader is reader for the appication\n      var event:String = reader.getNextLine;\n      while (!isStopped || event != null) {\n        store(event);\n        event = reader.getNextLine;\n      }\n      reader.close()\n    } catch {\n      case t: Throwable =&gt;\n        restart(\"Error receiving data\", t)\n    }\n  }\n} </pre><p>\n\tWhere did i go wrong.?\nProblems 1) The job and stream reading happening multiple times and same data is piling up. So, for 60 line of data, i am getting 1800 or greater some times, in total. </p><p>Streaming Code: </p><pre>val conf = new SparkConf\n    conf.setAppName(\"str1\");\n    conf.setMaster(\"local[2]\")\n    conf.set(\"spark.driver.allowMultipleContexts\", \"true\");\n    val ssc = new StreamingContext(conf, Minutes(2));\n    val customReceiverStream = ssc.receiverStream(new MyReceiver)\n    println(\" searching \");\n    //if(customReceiverStream.count() &gt; 0 ){\n     customReceiverStream.foreachRDD(x =&gt; {println(\"=====&gt;\"+ x.count());x.count()});\n    //}\n    ssc.start();\n    ssc.awaitTermination() \nNote: I am trying this in my local cluster, and with master as local[2].</pre>","tags":["stream-processing","streaming","spark-streaming","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-07 15:57:35.0","id":9122,"title":"Wget: unable to resolve host address in remote SSL sandbox login","body":"<p>I have downloaded HDP 2.3.2 Sandbox, and I am trying to run in on a Window 7 Professional machine using VirtualBox. The VM runs and I get to the 'main page' displaying the IP address to log on to when using SSH. </p><p>Now, I am able to log in on this IP address using ssh (on Cygwin), but then when I try to download anything I keep getting error messages. For instance:</p><pre>wget &lt;a href=\"http://www.google.com\"&gt;www.google.com&lt;/a&gt;</pre><p>results in:</p><pre>Resolving &lt;a href=\"http://www.google.com...\"&gt;www.google.com...&lt;/a&gt; failed: Temporary failure in name resolution. \nwget: unable to resolve host address \"www.google.com\"</pre><p>I tried to find any suggestions for this online but due to my lack of knowledge I find it hard to progress towards a solution. I also tried the other HDP Sandbox in the VMware player.</p><p>Update 12/1: I tried the sandbox at home and this all worked. My conclusion now is that it has something to do with my corporate network. I know also there are work-around methods, but I would very much like to tackle this problem at the core so I will not encounter it in the future. </p><p>Any suggestions would be very much appreciated! Also, let me know if you need more information of any sort.</p>","tags":["Sandbox","network","virtualbox","error"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-08 10:14:54.0","id":5407,"title":"Monitoring Streaming application Flume->Kafka->Storm->Hive Also some Oozie jobs","body":"<p>I have a question about monitoring an application that reads files from one datasource ( oozie ) and streaming logs through a streaming application Logs -&gt; Flume -&gt; Kafka -&gt; Storm -&gt; HDFS. </p><p>I would like to monitor this application and apart from Email actions in the Oozie workflow and Ambari monitoring errors in the setup I was wondering if anybody had done something like that before.</p><p>1) Storm Topology/Broker/Flume agent failures</p><p>Is there any way to add ambari alerts or an ambari view that shows this in one?</p><p>2) Data problems</p><p>For example if data stops flowing from the source</p><p>Some things I have tried:</p><p>Push number of received, inserted tuples from Storm into ambari metrics and show it on Ambari. Anybody did something like that? Are custom charts in Ambari supported now?</p><p>Any simpler solutions.</p>","tags":["ambari-metrics","Kafka","monitoring","Flume","Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-13 15:25:41.0","id":9729,"title":"Error: Attempted to add a rule for a principal with no realm: ambari-qa","body":"<p>\n\tI am trying to enable Kerberos on an HDP 2.3.2 Sandbox using FreeIPA on a standalone VM.  After stopping all of the HDP services the Kerberos wizard has a \"prepare\" task and that task is failing with this error message:</p>\n<pre>13 Jan 2016 04:56:25,610  WARN [Server Action Executor Worker 495] ServerActionExecutor:479 - Task #495 failed to complete execution due to thrown exception: java.lang.IllegalArgumentException:Attempted to add a rule for a principal with no realm: ambari-qa\njava.lang.IllegalArgumentException: Attempted to add a rule for a principal with no realm: ambari-qa\n        at org.apache.ambari.server.controller.AuthToLocalBuilder.addRule(AuthToLocalBuilder.java:147)\n        at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1671)\n        at org.apache.ambari.server.controller.KerberosHelperImpl.setAuthToLocalRules(KerberosHelperImpl.java:403)\n        at org.apache.ambari.server.serveraction.kerberos.PrepareKerberosIdentitiesServerAction.processAuthToLocalRules(PrepareKerberosIdentitiesServerAction.java:177)\n        at org.apache.ambari.server.serveraction.kerberos.PrepareEnableKerberosServerAction.execute(PrepareEnableKerberosServerAction.java:82)\n        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:537)\n        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:474)\n        at java.lang.Thread.run(Thread.java:745)\n</pre><p>\n\t<em>ambari-qa </em>was not mentioned in the CSV file that the wizard generated, but after seeing the above error I created a service for it, but I'm still getting the same error.  </p>\n<pre>Added service \"ambari-qa/sandbox.hortonworks.com@HORTONWORKS.COM\"\n-----------------------------------------------------------------\n  Principal: ambari-qa/sandbox.hortonworks.com@HORTONWORKS.COM\n  Managed by: sandbox.hortonworks.com\n</pre><p>What does the error message mean and what can I do to resolve it?</p>","tags":["Sandbox","kerberos","Ambari","freeipa"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-13 17:18:54.0","id":9766,"title":"How to ensure no cluster downtime?.","body":"<p>A general question for a heavily used prod cluster.</p><p>If we change a HDFS config property value in NN HA HDP 2.1 cluster how the restart operation will be executed by Ambari like sequentially NN1 and NN2 followed by DN’s or in parallel.</p><p>Q:whether it will impact current  running jobs ?.</p><p>Agenda is to make cluster available 24 hrs no downtime and shouldn’t affect current running jobs.</p><p>Ambari is 1.6 version with  HDP 2.1 stack</p>","tags":["Ambari","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-13 18:22:45.0","id":9779,"title":"track sqoop connection","body":"<p>i'm interested to know is there any tool available in market through which we can track what all the nodes sqoop job is hitting? both source and target..this will help especially when either source or target in firewall</p>","tags":["firewall","Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-13 17:50:19.0","id":9755,"title":"Where is  cache location for submitted applications in queues? How can we clear the cache for submitted applications?","body":"","tags":["help","jobs","queue","MapReduce"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-14 06:05:25.0","id":9855,"title":"Can Ambari server and agent run on same host ?","body":"<p>Hello,</p><p>I am a newbie to horton and ambari. I am trying to make a 2 node hadoop cluster with physical server. </p><p>I am thinking of using Ambari to do provisioning. However since i only have 2 nodes, can i use Ambari  server and ambari-agent on same node?</p><p>Right now if i do that, ambari-agent fails to startup and gives java unknown host exception . when i used FQDN</p><p>Thanks</p>","tags":["centos","ambari-2.0.1"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-14 14:53:20.0","id":9903,"title":"Ambari metrics display issue","body":"<p>I have a HDP 2.3 cluster with 9 nodes: 1 management node, 2 master nodes, and 6 slave nodes.</p><p>The Ambari dashboard shows the metrics (cpu usage, disk usage, etc.) from the management and master nodes. The metrics from the slave nodes are not displayed (metrics service may not be installed or inaccessible). I have checked and the metrics monitor service is installed, running, and sending messages to the collector. Ntp is also running.</p><p>So why am I not seeing those metrics?</p>","tags":["ambari-metrics","dashboard","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-14 14:54:30.0","id":9922,"title":"Can I create multiple Flume processes on same host?","body":"<p>Can I run different version of Flume with different classpath on same host? </p>","tags":["instance","process","Flume","Ambari"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-15 08:29:52.0","id":10023,"title":"Zeppelin notebook and user access","body":"<p>Where can I set access to a zeppelin notebook? So far all users can see all the notebooks.</p><p>Is there a way of setting a proxy user? \nExample:</p><p>I log in to zeppelin as user USER1, who has access to a folder in hdfs /user/user1\nI want to save my analysis results i made in zeppelin to that folder - /user/user1\nI get an error saying zeppelin user has no access to this folder.</p><p>How can I fix this? adding zeppelin user as proxy user in HDFS conf file did not do the trick.</p>","tags":["zeppelin-notebook","Spark","zeppelin","user-groups"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-13 11:35:05.0","id":44951,"title":"Find and fix Blocks with corrupt replicas","body":"<p>Hi all</p><p>There are many articles talk about corrupt blocks, and how to fix it, like this http://stackoverflow.com/questions/19205057/how-to-fix-corrupt-hadoop-hdfs</p><p>But I couldn't find any article which is talking about fixing corrupt replicas.</p><p>I got a few corrupt replicas, but don't know how to fix it.</p><p>I am not sure HDFS will fix it automatically or if I force it to reduce it's replication from 3 to 1 and increase it to 3 again may work.</p><pre>$ hdfs fsck /\nConnecting to namenode via http://xxx.com:50070/fsck?ugi=hdfs&path=%2F\nFSCK started by hdfs (auth:SIMPLE) from /xxx.xxx.xxx.xxx for path / at Wed Jul 13 20:32:51 JST 2016\n...............................................................Status: HEALTHY\n Total size:\t735023540 B\n Total dirs:\t100\n Total files:\t63\n Total symlinks:\t\t0 (Files currently being written: 2)\n Total blocks (validated):\t58 (avg. block size 12672819 B) (Total open file blocks (not validated): 2)\n Minimally replicated blocks:\t58 (100.0 %)\n Over-replicated blocks:\t0 (0.0 %)\n Under-replicated blocks:\t0 (0.0 %)\n Mis-replicated blocks:\t\t0 (0.0 %)\n Default replication factor:\t3\n Average block replication:\t1.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0 (0.0 %)\n Number of data-nodes:\t\t10\n Number of racks:\t\t1\nFSCK ended at Wed Jul 13 20:32:51 JST 2016 in 20 milliseconds\nThe filesystem under path '/' is HEALTHY\n\n$ hdfs dfsadmin -report | grep \"Blocks with corrupt replica\"\nBlocks with corrupt replicas: 3 </pre><p>Is there anyone who has good idea to fix this?</p>","tags":["corruption"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-15 03:59:53.0","id":45351,"title":"How to set the NameNode Heap memory in ambari?","body":"<p>how much amount to be set heap memory size? It means heap memory size is depend on which factor? </p>","tags":["namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-16 11:59:17.0","id":45510,"title":"Getting error Table not found 'tweets_simple'","body":"<p>Got below error while running the query as given in tutorial.\nquery:\nCREATE VIEW IF NOT EXISTS tweets_clean AS\nSELECT\n  t.tweet_id,\n  t.ts,\n  t.msg,\n  m.country\n FROM tweets_simple t LEFT OUTER JOIN time_zone_map m ON t.time_zone = m.time_zone;\nError:\nError while compiling statement: FAILED: SemanticException [Error 10001]: Line 7:6 Table not found 'tweets_simple' [ERROR_STATUS] </p>","tags":["hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-22 06:06:32.0","id":46617,"title":"nifi non secure cluster ui NiFi Web Server Returning Conflict response Error","body":"<p>Hi,</p><p>I ran into some problem with NiFi UI and hope someone here can help!</p><p>I tried to set up a NiFi cluster following the <a href=\"https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#clustering\">best practice</a>. To make things easier, I have one Master node and one Slave node. After I configured nifi.properties file on both servers respectively, I'm able to see both sides sending and receiving heartbeats with no trouble. However, when I tried to access the UI, http://master_external_ip:8090/nifi/ (I replaced 8080 with 8090), I got error like 'No nodes were able to process this request'. </p><p>The error in nifi.app.log is \"Connection Refused\" and \"o.a.n.c.m.e.NoConnectedNodesException org.apache.nifi.cluster.manager.exception.NoResponseFromNodesException: No nodes were able to process this request.. Returning Conflict response\". The error in nifi-user.log says \"o.a.n.w.a.c.IllegalStateExceptionMapper java.lang.IllegalStateException: Access tokens are only issued over HTTPS\". However, I configure my cluster with non secure (nifi.web.http.host and nifi.web.http.port) and never tried to set up HTTPS setting. </p><p>Please let me know if someone have any thought on this!</p><p>Thanks,</p><p>Stephanie </p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-22 10:42:59.0","id":46682,"title":"I have a two region server but both are not? I start the server using ambari UI but after few second again stop it?","body":"<table><tbody><tr><td><a href=\"http://192.166.4.27:8080/#\">RegionServers</a></td><td>0/2 \nRegionServers Live<a href=\"/storage/attachments/6001-hbase.jpg\">hbase.jpg</a></td></tr></tbody></table>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-03 15:06:23.0","id":2693,"title":"Old Nagios/Ganglia monitoring","body":"<p>A prospect with Ambari 1.7 / HDP 2.2 still planing upgrade to Ambari 2.1.2 needs to setup SNMP traps and provide rest metrics/service to his current monitoring tools.</p><p>Is it possible? Do we have any documentation for this using Nagios/Ganglia?</p>","tags":["Ambari","monitoring"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-04 13:09:38.0","id":2760,"title":"Is there a way to export the source code from a Zeppelin Notebook?","body":"<p>Is there a way to export the source code from a Zeppelin Notebook? </p>","tags":["zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-06 19:34:07.0","id":2975,"title":"storage requirement for fsimage and edit logs for typical customer implementation","body":"<p>A partner is working to offer Hadoop in Private cloud. Working with them on the sizing for their master nodes. A question came up is about the size of a fsimage and edit log files in a typical small/medium and large customer implementation of Hortonworks.</p><p>Based on this, partner can produre sufficient storage for master nodes</p><p>Any experiences to share about file sizes?</p>","tags":["HDFS","hdfs-policies"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-17 00:02:31.0","id":4098,"title":"Nifi sizing","body":"<p>What size Nifi system would we need to read 400 MB/s from a Kafka topic and store the output in HDFS ? The input is log lines, 100 B to 1KB in length each.</p>","tags":["benchmark","sizing","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-16 16:24:46.0","id":4024,"title":"How many files is too many on a modern HDP cluster?","body":"<p>Earlier Hadoop versions had problems with many small files because of the demands it placed on the NameNode. Modern machines and newer versions of NameNode seem to have mitigated this somewhat, but how much?  Is there a rule of thumb for what number of files is too many?  Small files also have proportionately more overhead per MB of data. Is there a rule of thumb for what is too small?</p>","tags":["data","HDFS","filesize"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-17 19:46:51.0","id":4148,"title":"Sqoop import from Sybase to Hive failing when the complete table name is given in the import statement","body":"<p>Trying to import table data from Sybase table to Hive using the below command:</p><pre> sqoop import --verbose  --driver com.sybase.jdbc4.jdbc.SybDriver --connect jdbc:sybase:Tds:dbgbl-tst:8032/DATABASE=trim_bw --username hrongali -P --table trim_bw..account --hive-database trim_bw --hive-table account --hive-import -m 1</pre><p>Sqoop is generating the below alias(<strong>AS trim_bw..account</strong>) which is failing to execute in Sybase and the below exception is thrown:</p><pre>2015-11-17 14:29:48,511 INFO [main] org.apache.sqoop.mapreduce.db.DBRecordReader: Executing query: SELECT col_1, col_2, col_3, col_4 FROM trim_bw..account AS trim_bw..account WHERE ( 1=1 ) AND ( 1=1 )\n2015-11-17 14:29:48,514 ERROR [main] org.apache.sqoop.mapreduce.db.DBRecordReader: Top level exception: \ncom.sybase.jdbc4.jdbc.SybSQLException: Incorrect syntax near '.'.\n\n\tat com.sybase.jdbc4.tds.Tds.processEed(Tds.java:4084)\n\tat com.sybase.jdbc4.tds.Tds.nextResult(Tds.java:3174)\n\tat com.sybase.jdbc4.tds.Tds.getResultSetResult(Tds.java:3940)\n\tat com.sybase.jdbc4.tds.TdsCursor.open(TdsCursor.java:328)\n\tat com.sybase.jdbc4.jdbc.SybStatement.executeQuery(SybStatement.java:2370)\n\tat com.sybase.jdbc4.jdbc.SybPreparedStatement.executeQuery(SybPreparedStatement.java:264)\n\tat org.apache.sqoop.mapreduce.db.DBRecordReader.executeQuery(DBRecordReader.java:111)\n\tat org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:235)\n</pre><p>Note: Tried with out giving the database name in the --table parameter, but the table object is not being recognized with that convention</p>","tags":["data-ingestion","Sqoop","sybase"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-20 18:20:11.0","id":4438,"title":"Hue does not show components and versions on Web UI after upgrade. Below is the snapshot.","body":"<p><img src=\"/storage/attachments/533-screen-shot-2015-11-20-at-121900-pm.png\"></p>","tags":["configuration","upgrade","operations","hue"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-25 05:57:08.0","id":4775,"title":"Has anyone done distcp between secured clusters but different REALM?","body":"<p>As long as reading http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Sys_Admin_Guides/content/ref-263ee41f-a0a9-4dea-ad4a-b3c257b8e188.1.html , looks like implying same realm.</p><p>Even Cloudera mentions Distinct Realms... </p><p>http://www.cloudera.com/content/www/en-us/documentation/enterprise/latest/topics/cdh_admin_distcp_data_cluster_migrate.html#concept_hcs_srr_sr_unique_1</p>","tags":["distcp","kerberos"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-30 12:48:38.0","id":4924,"title":"Falcon retention policy","body":"<p>Hello,</p><p>I have got one question regarding how Falcon implements retention policy for feed instances. I have observed that the retention policy action(i.e. DELETE in my case) is executed only within the dataset's validity interval. It means that several instances (how many it depends of the dataset's frequency) close to end of dataset's validity are kept forever even when some retention is defined.</p><p>Is it as expected or I am doing something wrong?</p><p>Thanks for any input,</p><p>Regards,</p><p>Pavel</p>","tags":["retention","Falcon"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-02 19:19:12.0","id":5081,"title":"When would HBaseMaster access HBase HDFS files with 'cmd=listStatus' ?","body":"<p>When I look at HDFS audit logs, I see hbase user from HBaseMaster node accessing hdfs files and the entry I see in audit log is with 'cmd=listStatus'. We regularly see about 3 million of them per hour and we have seen a hike of 6 million of them per hour which probably may have crashed NN. Any idea what HBaseMaster is doing here or if we can reduce any of this load on NN? </p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-02 12:01:32.0","id":5031,"title":"solr create_collection","body":"<p>Does anyone know if we have an option for solr create_collection command to specify the nodes for the index to be created on?</p>","tags":["SOLR","index","solrcloud"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-03 18:56:21.0","id":5122,"title":"What is the recommended value for dfs.datanode.handler.count in a large cluster?","body":"<p>Default for this is 10. I have seen it at 128 in a large (over 1000 nodes) cluster and I think this is causing load issues. What is the recommended value for this and when should this be increased from default 10. </p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-04 03:25:16.0","id":5182,"title":"How to run nnbench on HDP 2.3.2?","body":"<p>In HDP 2.3.0 this command used to be able to run nnbench:</p><pre>$ yarn jar /usr/hdp/2.3.0.0-2557/hadoop-hdfs/hadoop-hdfs-tests.jar nnbench –operation create_write\n</pre><p>In HDP 2.3.2 it seems nnbench is no longer part of hadoop-hdfs-tests.jar?</p><pre># yarn jar /usr/hdp/2.3.2.0-2950/hadoop-hdfs/hadoop-hdfs-tests.jar nnbench –operation create_write\nException in thread \"main\" java.lang.ClassNotFoundException: nnbench\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:278)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:214)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\n\n# jar -tvf /usr/hdp/2.3.2.0-2950/hadoop-hdfs/hadoop-hdfs-tests.jar | grep bench\n#</pre><p>Tried running a search in /usr for jars containing nnbench class but did not get any results</p><pre>find /usr -iname '*.jar' | xargs -i bash -c \"jar -tvf {} | tr / . | grep  nnbench && echo {}\"</pre><p>Has this class been renamed or depracated?</p>","tags":["nnbench","hdp-2.3.2","namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-04 15:32:57.0","id":5217,"title":"No output from Zeppelin on HDP 2.3 using Spark 1.4 with Zeppelin 0.6.","body":"<p>After following the Apache Zeppelin setup provided here - <a href=\"https://urldefense.proofpoint.com/v2/url?u=http-3A__hortonworks.com_hadoop-2Dtutorial_apache-2Dzeppelin_&d=CwMFAw&c=GC0NZZhaEw6GOQSjMHI2g15k_drElRoPmOYiK2k0eZ8&r=VzWNkTVsal7Ro4pQv62u5SIp14SQ2GVYp8Bn6GlOQHI&m=n_QkD2xpshcl6XJxMqIQxL_i7DImPXlnIqNUA-Ri-oQ&s=VTD4TqNr0-sveaMxWYFqK672nr73rnyc6CvYBhpx__E&e=.\">https://urldefense.proofpoint.com/v2/url?u=http-3A...</a> Zeppelin notebook does not show output after executing commands successfully.</p><p>Here's a subset of the errors seen in YARN logs:</p><pre>Stack trace: ExitCodeException exitCode=1: /grid/1/hadoop/yarn/local/usercache/root/appcache/application_1447968118518_0003/container_e03_1447968118518_0003_02_000004/launch_container.sh: line 23: :/usr/hdp/current/spark-client/bin/zeppelin-0.6.0-incubating-SNAPSHOT/interpreter/spark/dep/*:/usr/hdp/current/spark-client/bin/zeppelin-0.6.0-incubating-SNAPSHOT/interpreter/spark/*:/usr/hdp/current/spark-client/bin/zeppelin-0.6.0-incubating-SNAPSHOT/lib/*:/usr/hdp/current/spark-client/bin/zeppelin-0.6.0-incubating-SNAPSHOT/*::/usr/hdp/current/spark-client/bin/zeppelin-0.6.0-incubating-SNAPSHOT/conf:/usr/hdp/current/spark-client/bin/zeppelin-0.6.0-incubating-SNAPSHOT/conf:/usr/hdp/current/spark-client/bin/zeppelin-0.6.0-incubating-SNAPSHOT/conf:/etc/hadoop/conf:$PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure: bad substitution</pre><p>Noticed the mapred-site.xml had \"${hdp.version}\" variables that were not replaced. <strong>The workaround was replacing the variable with the actual hdp version in the mapred-site.xml then restarting</strong>. See the screenshot below:</p><p><img src=\"/storage/attachments/608-image002.png\"></p><p>This is posted as an FYI in case anyone else runs into a similar issue. I don't have a root cause for this behavior at this time.</p>","tags":["hdp-2.3.0","Spark","zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-08 11:49:24.0","id":5428,"title":"Using thin or thick client for Oracle","body":"<p>Hello,</p><p>We are mainly using thin option when connecting to Oracle via JDBC</p><p>Is there any issue running with thick option to get the benefit of Oracle HA ?</p><p>Thanks</p>","tags":["jdbc","namenode-ha","oracle"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-09 16:07:44.0","id":5582,"title":"EXPORT HIVE Database","body":"<p>I see the following option to export one table at a time but do we have a way to do this for all the tables on a particular hive db?</p><p><code>EXPORT TABLE tablename [PARTITION (part_column=</code><code>\"value\"</code><code>[, ...])]</code>\n<code>TO </code><code>'export_target_path'</code></p><p><code></code></p>","tags":["export","Hive","migration","distcp"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-09 09:19:12.0","id":5500,"title":"SAP DBTech JDBC: [403]: internal error: Error opening the cursor for the remote database [Hortonworks][HiveODBC] (35) Error from server: error code: '0' error message: 'ExecuteStatement finished with operation state: ERROR_STATE'.","body":"<p>Hello Experts,</p><p>we are trying to access a virtual table VT_ADDRESS_HIVE.</p><p>Trying to issue the following scripts:</p><p>-- 1\n<strong>select\n</strong>\"countrycode\",\n\"citycode\",\n<strong>count</strong>(*) \n<strong>from\n</strong>\"COM_DATA_HDPH01_ROC\".\"VT_ADDRESS_HIVE\"\n<strong>group</strong><strong>by\n</strong>\"countrycode\",\n\"citycode\";</p><p>-- 2 Schema w/o Double Quotes\n<strong>select\n</strong>\"countrycode\",\n\"citycode\",\n<strong>count</strong>(*) \n<strong>from\n</strong>COM_DATA_HDPH01_ROC.\"VT_ADDRESS_HIVE\"\n<strong>group</strong><strong>by\n</strong>\"countrycode\",\n\"citycode\";\t</p><p>-- 3 Schema, Table w/o Double Quotes</p><p><strong>select\n</strong>\"countrycode\",\n\"citycode\",\n<strong>count</strong>(*) \n<strong>from\n</strong>COM_DATA_HDPH01_ROC.VT_ADDRESS_HIVE\n<strong>group</strong><strong>by\n</strong>\"countrycode\",\n\"citycode\";\t</p><p>-- 4 W/o Count, w/o Double Quotes</p><p><strong>select\n</strong>\"countrycode\",\n\"citycode\"\n<strong>from\n</strong>COM_DATA_HDPH01_ROC.VT_ADDRESS_HIVE\n<strong>group</strong><strong>by\n</strong>\"countrycode\",\n\"citycode\";\t</p><p>lead to:</p><p>Could not execute 'select \"countrycode\", \"citycode\", count(*) from \"COM_DATA_HDPH01_ROC\".\"VT_ADDRESS_HIVE\" ...' in 3:40.971 minutes . </p><p>SAP DBTech JDBC: [403]: internal error: Error opening the cursor for the remote database [Hortonworks][HiveODBC] (35) Error from server: error code: '0' error message: 'ExecuteStatement finished with operation state: ERROR_STATE'. for query \"SELECT COUNT(*), VT_ADDRESS_HIVE.countrycode, VT_ADDRESS_HIVE.citycode FROM HIVE.roc.address VT_ADDRESS_HIVE GROUP BY VT_ADDRESS_HIVE.countrycode,VT_ADDRESS_HIVE.citycode \"</p><p>All statements are running 3 - 4 minutes and then ending up with the error.</p><p>Any advice or idea?</p><p>Thanks and best regards,\nMatthias Maier</p>","tags":["sap-hana","sap","Hive","odbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-09 08:31:13.0","id":5518,"title":"Can Impala be run on Hortonworks distribution?","body":"","tags":["impala"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-10 04:36:52.0","id":5757,"title":"Bug in Hive (HDP 2.1), Timestamp data overwritten during INSERT processing","body":"<p>\n\tWe have experienced an issue where (re)processing data in Hive overwrites timestamp data.\nThis occurs with HDP 2.1, but not 2.3.</p><p>\n\tWe are using Hive to run an ad hoc 'reorg' or 'reprocess' on existing Hive tables to reduce the number of files stored - improving query performance and reducing pressure on the cluster (found a nice explanation from @david.streever here \n\t<a href=\"https://community.hortonworks.com/questions/4024/how-many-files-is-too-many-on-a-modern-hdp-cluster.html\">https://community.hortonworks.com/questions/4024/how-many-files-is-too-many-on-a-modern-hdp-cluster.html</a>).</p><p>\n\tThe active Hive table is added to daily, creating at least one ORC file per day. The schema contains several timestamp columns (e.g. created_timestamp for when each record was origingally created on the source system).</p><p>\n\tWe then create a reorgTable with an identical schema to activeTable, copy the data from activeTable to the reorgTable which combines many of the smaller daily files reducing the overall number.</p><p>\n\tHowever, this process edits/overwrites timestamp data (and does not touch other columns):</p><p>\n\t1. Contents of activeTable</p><p>\n\tID    \n created_timestamp</p><p>\n\t\t01\n    2000-01-01 13:08:21.110</p><p>\n\t\t02\n    1970-01-01 01:02:03.450</p><p>\n\t\t03\n   \t\t1990-10-08 03:09:02.780</p><p>\n\t\t2. Copy data from activeTable to reorgTable</p>\n<pre>INSERT INTO TABLE reorgTable SELECT * FROM activeTable;\n</pre><p>\n\t\t3. Contents of the reorgTable</p><p>\n\t\tID\n\t\t    created_timestamp</p><p>\n\t\t01\n\t\t   1990-10-08 03:09:02.780</p><p>\n\t\t02\n\t\t   1990-10-08 03:09:02.780</p><p>\n\t\t03\n\t\t   1990-10-08 03:09:02.780</p><p>\nHas anyone else experienced this? Is there a solution other than upgrading?\nOr an alternative way to reprocess the data that might not have the same effect?\n\t</p><p>\n\tThank you!\n\t\t</p>","tags":["bug","Hive","timestamp"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-10 06:55:05.0","id":5762,"title":"After reboot, I start the ambri server using “ambari-server start” . Now the problem is that no service is running and having yellow icons.","body":"","tags":["ambari-server"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-09 09:38:24.0","id":5501,"title":"getting error while persisting spark output to hive","body":"<pre>scala&gt; df.select(\"name\",\"age\").write().format(\"com.databricks.spark.csv\").mode(SaveMode.Append).saveAsTable(\"PersonHiveTable\");\n&lt;console&gt;:39: error: org.apache.spark.sql.DataFrameWriter does not take parameters</pre>","tags":["hdp-2.3.0","spark-sql","Hive","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-10 18:55:18.0","id":5882,"title":"I have Hortonworks sandbox on azure . HBase crashes all the time and I have to restart the machine all the time is there a solution for this?","body":"<p></p><p>I have Hortonworks sandbox on azure . HBase crashes all the time and I have to restart the machine all the time is there a solution for this?</p>","tags":["Hbase"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-11 05:46:56.0","id":6023,"title":"org.apache.spark.SparkException: Task failed while writing rows.","body":"<p>\n\tHi,</p><p>\n\tI am using HDP2.3.2 with Spark 1.4.1 and trying to insert data in hive table using hive context.</p><p>Below is the sample code </p>\n<pre>spark-shell   --master yarn-client --driver-memory 512m --executor-memory 512m\n//Sample code \nimport org.apache.spark.sql.SQLContext\nimport sqlContext.implicits._\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\nval people = sc.textFile(\"/user/spark/people.txt\")\nval schemaString = \"name age\"\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.types.{StructType,StructField,StringType};\nval schema =\n  StructType(\n    schemaString.split(\" \").map(fieldName =&gt; StructField(fieldName, StringType, true)))\nval rowRDD = people.map(_.split(\",\")).map(p =&gt; Row(p(0), p(1).trim))\n//Create hive context \nval hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n//Apply the schema to the \nval df = hiveContext.createDataFrame(rowRDD, schema);\nval options = Map(\"path\" -&gt;  \"hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/personhivetable\")\ndf.write.format(\"org.apache.spark.sql.hive.orc.DefaultSource\").options(options).saveAsTable(\"personhivetable\")\n</pre><p>\n\tGetting below error :</p>\n<pre>org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$writeRows$1(commands.scala:191)\n\tat org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$anonfun$insert$1.apply(commands.scala:160)\n\tat org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$anonfun$insert$1.apply(commands.scala:160)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 1\n\tat $line30.$read$iwC$iwC$iwC$iwC$iwC$iwC$iwC$iwC$iwC$iwC$anonfun$2.apply(&lt;console&gt;:29)\n\tat $line30.$read$iwC$iwC$iwC$iwC$iwC$iwC$iwC$iwC$iwC$iwC$anonfun$2.apply(&lt;console&gt;:29)\n\tat scala.collection.Iterator$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$writeRows$1(commands.scala:182)\n\t... 8 more\n</pre><p>Is it configuration issue?</p><p>When I googled it I found out that Environment variable named HIVE_CONF_DIR should be there in spark-env.sh</p><p>Then I checked spark-env.sh in HDP2.3.2,I couldnt find the Environment variable named HIVE_CONF_DIR .</p><p>Do I need to add above mentioned variables to insert spark output data to hive tables.</p><p>Would really appreciate pointers. </p><p>\n\tThanks, </p><p>\n\tDivya</p>","tags":["hdp-2.3.2","orc","sparksql","Spark","Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-11 12:28:25.0","id":6035,"title":"Do people see benefit from SSDs and HDFS? Can we mix and match?","body":"<p>I've been looking around but am not sure what the answer to this is. </p><p>A client who is speccing up a hadoop cluster is keen to use SSDs. They have experience of using them for a variety of other applications like databases, OS, etc. I am happy for them to use SSDs for the OS, temp files, spill, log files, and so on, but I am still pushing them towards spindle disks for HDFS data as the best performance to cost ratio.</p><p>Is that everyone's opinion (\"spindle HDDs for storage\") or should I just let them spend the extra money on SSDs for storage. </p><p>PS I have seen the question </p><p><a href=\"https://community.hortonworks.com/questions/1405/can-you-please-advise-about-how-best-to-use-this-s.html?redirectedFrom=1711\">https://community.hortonworks.com/questions/1405/c...</a></p><p>Can you please advise about how best to use this SSD storage to boost performance in HDP on Azure?</p><p>The second part of this question is - if we use both SSD and HDD for storage is there a good way of mixing them in a cluster. I am thinking that *possibly* we might set up the SSD disks to be on data nodes which pretend that they are on a different rack to the HDD ones. That way they should always have at least one copy of each block. HOWEVER that wont work if I am trying to have HDDs on the same physical boxes. I can't have two different data nodes running on the same box with the same IP address. I would also need separate compute/YARN services on each box - one for each data node. </p><p>Thanks</p>","tags":["cluster","HDFS","ssd"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-12-12 02:12:57.0","id":6116,"title":"Is it possible to ingest data into kafka from REST API calls?","body":"<p>HDP 2.3.2 </p><p>We have data sitting on prem and we are planning to ingest data into Kafka using REST calls? Kafka will be running in the cloud. </p>","tags":["api","data-ingestion","Kafka"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 02:28:57.0","id":6273,"title":"HDP Sandbox Archive","body":"<p>Where can I find the HDP Sandbox archive for development purpose? Current the latest HDP 2.3.2 sandbox is available on hortonworks.com. There is also an archive link on the page, I cannot find the maintenance release version I need (HDP2.3.0.0-2557).</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-15 19:52:09.0","id":6584,"title":"Hbase Table dump to flat files","body":"<p>Hello,</p><p>I want to backup hbase tables to flat files but with the below command i can only backup single table. Is there a best way to backup all the tables with single command to flat files ?</p><p>hbase\norg.apache.hadoop.hbase.mapreduce.Export \"hbase:meta\"\n\"&lt;output_dir&gt;\"</p>","tags":["Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-16 04:57:38.0","id":6695,"title":"Ranger authorization for HDFS - Unable to change ownership of a directory in hdfs","body":"<p>I have configured once policy for hdfs via ranger. below are the details:</p><p>1. Policy configured for user admin</p><p>2. User admin can rwx into /user/oozie</p><p>3. Point number 2 tested successfully</p><p>4. When I went to change ownership of /user/oozie to admin by user admin then it fails with below error</p><pre>[admin@hdpambari ~]$ hdfs dfs -chown root /user/oozie/test1\nchown: changing ownership of '/user/oozie/test1': Non-super user cannot change owner</pre><p>I know that logically this is correct as user \"admin\" has rwx access to /user/oozie so no need to change the ownership. </p><p>Is my understanding correct ? is there any documentation that points to this ? </p>","tags":["ranger-admin","Ranger","hdfs-policies","user-groups"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-16 09:42:25.0","id":6705,"title":"ambari showing corrupt blocks but not fsck","body":"<p>Ambari UI shows corrupt blocks, fsck output shows not corrupt blocks and filesystem under / as healthy. Also when i run dfsadmin report it shows 'blocks with corrupt replicas ' same number as ambari showing</p>","tags":["HDFS","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-16 02:56:35.0","id":6674,"title":"Uninstall HDP 2.3.2 from SuSE","body":"<p>Found this link on how to unsinstall HDP. <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_installing_manually_book/content/ch_uninstalling_hdp_chapter.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-...</a></p><p>However for step 16 and 17 (uninstall hadoop and mysql), there are no alternatives for SuSE.</p><p>I tried like zypper remove hadoop\\* (just like others), but that didn't work.</p><p>zypper remove hadoop\\*</p><p>Loading repository data...</p><p>Reading installed packages...</p><p>No package matching 'hadoop*' are installed.</p><p>Can someone please suggest how to uninstall on SuSE.</p>","tags":["uninstall","suse"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-16 14:14:18.0","id":6775,"title":"I can't get establish a connection using Storm Syntax","body":"<p>Hi,</p><p>I have been following the available tutorials for sometime now and realized that there are quite a few examples which require the use of wget (stream/load a file via address).</p><p>The most recent tutorial example which resulted with this problem is from the \"Processing Streaming data in Hadoop with Apache Storm\" tutorial. </p><p>Additional Information: I have tried to establish a connection logged in as root, admin and hdfs, yet resulting in failure each time.</p><p>Please let me know if you require any other information and I look forward to getting this resolved. </p><p>Thank You!</p>","tags":["streaming","stream-processing","Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-17 08:51:26.0","id":7006,"title":"How to SSH into HortonWorks Sandbox using Putty?","body":"<p>Hi!</p><p>I have successfully downloaded and installed Hortonworks Sandbox. And i can bring it up in the Oracle Virtual Box. Now, i want to SSH into it. I am using Putty, i am n Windows 7. But every time i try to login as 'root' with password 'hadoop'; it says access denied. Could anyone please provide any pointers as to what could be going wrong? Screenshots below. </p><p>Thanks, in advance.</p><p><img src=\"/storage/attachments/904-sandbox.png\"></p><p><img src=\"/storage/attachments/905-putty1.png\"></p><p><img src=\"/storage/attachments/906-putty2.png\"></p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-21 07:46:28.0","id":7423,"title":"History Server Start failing on new cluster","body":"<p>starting History Server fails </p><pre>core/shell.py\", line 291, in _call\n    raise Fail(err_msg) </pre><p>resource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X GET '\"'\"'http://hadoop1.openwizards.com:50070/webhdfs/v1/app-logs?op=GETFILESTATUS&user.name=hdfs'\"'\"' 1&gt;/tmp/tmpIQifVJ 2&gt;/tmp/tmpE2a1W1'' returned 7.</p>","tags":["resource-manager","jhs","webhdfs","namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-23 21:30:32.0","id":7886,"title":"Getting \"500 status code received on POST method for API\" in ambari while trying to install Kerberos","body":"<ul>\n<li>OS: CentOS6.6 on all hosts</li><li>Ambari version 2.1 & HDP 2.3.0</li><li>Operation: Installing Kerberos With AD KDC</li><li>The error I get in Ambari, more precisely in step 3 and while performing Test Client:</li></ul><p>500 status codereceived on POST method for API: /api/v1/clusters/HdpCluster/requests</p><pre>Error message: Server Error</pre><ul>\n<li>Ambari-server log:</li></ul><p>23 Dec 2015 21:53:30,301 ERROR [pool-2-thread-1] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:\nLocal Exception Stack:</p><p>Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException\nInternal Exception: org.postgresql.util.PSQLException: </p><p><strong>ERROR: duplicate key value violates unique constraint \"uni_alert_group_name\"</strong></p><p>\nError Code: 0 </p><p>Call: INSERT INTO alert_group (group_id, cluster_id, group_name, is_default, service_name) VALUES (?, ?, ?, ?, ?)\n        bind =&gt; </p><p>[5 parameters bound]\n        at </p><p>org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)\n        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1611)</p><p>Any clue on how to solve this issue ?</p>","tags":["ambarialerts","Ambari","ambari-2.1.0","kerberos"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-30 00:26:09.0","id":8311,"title":"Ambari with mysql 5.6","body":"<p>I had a tough time with Ambari 1.7 pointing to Mysql 5.6 on remote host. AS is using all the available ports on the source to make a connection to remote host to port 3306. At one time i saw 28000 ports being used by ambari to connect. this is killing us because it even used 50070 at one time, and i had tough time to figure it out. Finally i stopped ambari-server and all is well. Did anyone face this issue? if so any idea on fix? apart from downgrading Mysql or upgrade ambari to 2.1.2?</p>","tags":["mysql","ambari-server","ambari-1.7"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-31 06:22:39.0","id":8458,"title":"Falcon tutorial - rawEmailIngestProcess shell-wf failing","body":"<p>Following the tutorial http://hortonworks.com/hadoop-tutorial/processing-data-pipeline-with-apache-falcon/ during rawEmailIngestProcess shell-wf the shellnode process fails.  shellnode is basically calling ingest.sh:</p><p>curl -sS <a href=\"http://bailando.sims.berkeley.edu/enron/enron_with_categories.tar.gz\">http://bailando.sims.berkeley.edu/enron/enron_wit...</a> | tar xz && hadoop fs -mkdir -p $1 && hadoop fs -put enron_with_categories/*/*.txt $1</p><p>I changed ingest.sh and perform a simple hadoop fs -ls and the shellnode process succeeds.  </p><p>Here is the oozie log:</p><p>2015-12-31 05:50:49,213  WARN ShellActionExecutor:523 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[shell-wf] JOB[0000420-151222214138596-oozie-oozi-W] ACTION[0000420-151222214138596-oozie-oozi-W@shell-node] Launcher ERROR, reason: Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]\n2015-12-31 05:50:49,239  INFO ActionEndXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[shell-wf] JOB[0000420-151222214138596-oozie-oozi-W] ACTION[0000420-151222214138596-oozie-oozi-W@shell-node] ERROR is considered as FAILED for SLA\n2015-12-31 05:50:49,261  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[shell-wf] JOB[0000420-151222214138596-oozie-oozi-W] ACTION[0000420-151222214138596-oozie-oozi-W@fail] Start action [0000420-151222214138596-oozie-oozi-W@fail] with user-retry state : userRetryCount [0], userRetryMax [0], userRetryInterval [10]</p><p>Any help would be appreciated.  <a rel=\"user\" href=\"/users/231/aahn.html\" nodeid=\"231\">@Anderw Ahn</a> </p>","tags":["how-to-tutorial","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-04 09:28:01.0","id":8593,"title":"is there any way to print or read PIG_HOME through terminal","body":"<p>I would like to print or read pig_home from terminal.Is there any way???</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-18 19:49:52.0","id":10423,"title":"Recommended user:group ownership for /apps/hive/warehouse","body":"<p>I'm working though this procedure to upgrade Ambari from 2.1.1 to 2.2.0 before I start an HDP upgrade from 2.3.0 to 2.3.4:</p><p><a href=\"https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_upgrading_Ambari/content/_preparing_to_upgrade_ambari_and_hdp.html\">https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_upgrading_Ambari/content/_preparing_to_upgrade_ambari_and_hdp.html</a></p><p>It says to run the service checks on installed components first. They all passed except the Hive check and I get this access error:</p><pre>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.security.AccessControlException: Permission denied: user=ambari-qa, access=WRITE, inode=\"/apps/hive/warehouse\":hive:hdfs:drwxr-xr-x\nat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\nat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)</pre><p>/apps/hive/warehouse user:group is to hive:hdfs. The ambari-qa user that is used when running the service checks on the node where Ambari is saying the checks are being run is ambari-qa:hadoop:</p><pre>[jupstats@vmwhadnfs01 hive]$ id ambari-qa\nuid=1001(ambari-qa) gid=501(hadoop) groups=501(hadoop),100(users)</pre><p>So, ambari-qa is a member of the hadoop group but that group has no write permission to hive managed tables which are owned by hive and allowed read access to only hdfs users. I'm not sure what Ambari service check is trying to do by it is clearly trying to write something in that managed table space. </p><p>As I understand it, the hadoop superuser is the user that starts the namenode, user \"hdfs\" in my case. So, my questions are:</p><p>1. Should \"hdfs\" really be the group for /apps/hive/warehouse or would it be better to have that be the \"hadoop\" group?</p><p>2. What are the best practice recommendations for the user:group permissions on /apps/hive/warehouse? For example, I have some Java and Python apps that run every 30 minutes to ingest data into hive management and external tables. Those processes run as a service user \"jupstats\" and group \"ingest\". My /apps/hive/warehouse/jupstats.db directory is where the managed tables lives and that directory is set to jupstats:ingest to restrict access appropriately. This seems right to me. Do you experts agree? Same for the directories where I also write some HDFS data that is accessed by external Hive tables. Those files are owned as jupstats:ingest.</p><p>3. I think I am generally lacking knowledge in how to best setup up access to various Hive tables that are eventually going to need to be accessed by various users. My thought was that all my jupstats.db tables, which are read only by group ingest, will be made made readable by these users by adding those users to the \"ingest\" group. Does that approach seem reasonable?</p><p>4. This still leaves me with the question of how to I setup Hive so that this Ambari service check can pass? Should I add ambari-qa to the \"hdfs\" group? That feels wrong and dangerous in that it is like adding ambari-qa to a root-like account since user \"hdfs\" is the hadoop superuser and can wack a lot of stuff.</p><p>Thanks for any help/tips on this...</p>","tags":["Hive","permissions","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-18 20:08:45.0","id":10440,"title":"How to connect to Hive through knox from Tableau.","body":"<li>While trying to connect facing the below exception</li><li>Unable to connect to the ODBC Data Source. Check that the necessary drivers are installed and that the connection properties are valid.</li><li>[Hortonworks][HiveODBC] (34) Error from server: SSL_connect: certificate verify failed.</li><li>Unable to connect to the server \"devehdp004.unix.gsm1900.org\". Check that the server is running and that you have access privileges to the requested database.</li>","tags":["knox-gateway","Knox","odbc","tableau","hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-19 08:05:24.0","id":10512,"title":"Enterprise Data architecture - HDFS","body":"<p>Hello</p><p>I am trying to understand how hadoop can fit in an enteprise data architecture. If we want incorporate hadoop into our enterprise data architecture, does this mean we have to copy all the data in our huge file system into HDFS if we want to have these files indexed? Wouldn't this lead to duplicating huge amounts of data (100s of TBs)?</p><p>Thanks</p>","tags":["data-model","data-architecture","HDFS","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-01 08:37:48.0","id":36929,"title":"Ranger KMS Kerberos Issue","body":"<p>Hi,</p><p>Getting below error while connecting to KMS repo:-</p><p>xa_portal.log:-</p><p>ERROR org.apache.ranger.biz.ServiceMgr (ServiceMgr.java:120) - ==&gt; ServiceMgr.validateConfig Error:java.util.concurrent.ExecutionException: org.apache.ranger.plugin.client.HadoopException: {\n  \"RemoteException\" : {\n    \"message\" : \"User:keyadmin not allowed to do 'GET_KEYS'\",\n    \"exception\" : \"AuthorizationException\",\n    \"javaClassName\" : \"org.apache.hadoop.security.authorize.AuthorizationException\"\n  }</p><p>2-3 days before it was working perfectly so I think I need to initialize(kinit) keytab and principal in every 24 hour but not sure which one. This is happening with me 4th time, previouly to resolve this, I just removed and reinstalled It worked but I need permanent resolution for this.</p><p>Please help me on this</p>","tags":["Ranger","ranger-0.5.0","kerberos","ranger-kms","ranger-admin"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-01 10:35:34.0","id":36963,"title":"how to Spark submit command with Scala","body":"<p>I an new bee in spark. I have create a Scala script.</p><p>I can run this script by \"spark-shell -p my_script.scala \" </p><p>how  to use spark-submit to use scala script ?</p>","tags":["spark-shell","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-01 05:56:53.0","id":36911,"title":"can we load data into hive orc table using NIFI?","body":"<p>i have excel data in hdfs and need to be store into hive ORC table using NIFI. please share the solution </p>","tags":["nifi-templates","nifi-streaming","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-01 05:40:07.0","id":36897,"title":"Best tools for file transfer and  ingest.","body":"<p>Currently reading different paper and article i'm wondering if their is a well known set of good tools/pattern to transfer,  process and land on hdfs large ingest files & logs. </p><p>I saw this <a target=\"_blank\" href=\"https://community.hortonworks.com/questions/23337/best-tools-to-ingest-data-to-hadoop.html\">article </a>, but apart from saying NIFI are their other solution ? </p><p>Currently we use SFTP, but this is not parallel FTP and may face performance issues based on size and latency. I had a look to flume but unfortunately it sounds a non production idea to use flume to tranfer gzipped files. You have to use a blob that load the all file in memory.</p><p>I'm a little surprised that nothing exist out of the box to chunk a file and send the data in parallel over several TCP connection. Likely the code exist for video transfer and i'm wondering if someone somewhere in apache have incoporate such code to transfer large log files and get them landed on hdfs.  </p><p>Any hints opinion welcome : </p><ul><li>Confirm that flume isn't appropriate or provide configuration for it ( many people asking but no firm config so far ) </li><li>Any other tools or pattern ? </li></ul>","tags":["linux","sftp","ingestion","Flume","files"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-01 16:06:49.0","id":37084,"title":"Wrong Zookeeper Client Information listed in Eclipse Java Program","body":"<p>The below environment variables are wrong and seem to be from my local machine. I would expect Zookeeper to list the variables from my Sandbox on VirtualBox</p><p>2016-05-31 16:04:05,796 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT </p><p>2016-05-31 16:04:05,796 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=daniel</p><p>2016-05-31 16:04:05,796 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_72</p><p>\n2016-05-31 16:04:05,796 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation </p><p>2016-05-31 16:04:05,796 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=C:\\Java\\jdk1.7.0_72\\jre</p>","tags":["eclipse","java","Hbase"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-02 09:43:06.0","id":37207,"title":"How to change keyadmin username on Ranger KMS UI?","body":"<p>Our Ranger Servers are integrated with Active Directory, and for a specific reason I'm not able to create a principal named 'keyadmin@REALM'. </p><p>I create a principal name 'keyadminhdp@REALM' and configured ti on Ambari to be used on KMS services. </p><p>On Ranger UI, I can access Service Manager created for KMS, and the 'Test Connection' check works perfectly.</p><p>But on Encryption Tab, when I select the service on the combobox I get an error on acces_log:</p><pre>X.X.X.X - - [01/Jun/2016:18:08:02 +0100] \"GET /kms/v1/keys/names?doAs=keyadmin HTTP/1.1\" 403 221</pre><p>and on kms-audit.log I have the following messages:</p><pre>2016-06-01 18:08:02,734 UNAUTHENTICATED RemoteHost:X.X.X.X Method:GET URL:http://HOSTNAME:9292/kms/v1/keys/names?doAs=keyadmin ErrorMsg:'Authentication required'\n2016-06-01 18:08:02,754 UNAUTHORIZED[op=GET_KEYS, user=keyadmin]\n</pre><p>As I understood this error happened because there is no principal create for user keyadmin (keyadmin@REALM) and so Ranger cannot allow access to the user.</p><p>I tried to find any place where I could change the username for keyadmin, but I was not able to find it (even on documentation).</p><p>Does anyone knows where can I change it?</p><p>Thanks</p><p>Leonardo</p>","tags":["Ranger","kms","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-01 12:22:26.0","id":37001,"title":"[FALCON] : force delete feed","body":"<p>Hi, </p><p>do you know how to force delete a feed for some reasons the directory of feed was deleted and now it unable to delete feed </p><pre>&lt;cluster xmlns='uri:falcon:cluster:0.1' name='next-rec-cluster' description='undefined' colo='nextRecColo'&gt;\n  &lt;interfaces&gt;\n    &lt;interface type='readonly' endpoint='hftp://clusterA002:50070' version='2.2.0'/&gt;\n    &lt;interface type='write' endpoint='hdfs://clusterA002:8020' version='2.2.0'/&gt;\n    &lt;interface type='execute' endpoint='clusterA002:8050' version='2.2.0'/&gt;\n    &lt;interface type='workflow' endpoint='http://clusterA003:11000/oozie/' version='4.0.0'/&gt;\n    &lt;interface type='messaging' endpoint='tcp://clusterA003:61616?daemon=true' version='5.1.6'/&gt;\n  &lt;/interfaces&gt;\n  &lt;locations&gt;\n    &lt;location name='staging' path='/apps/falcon/next-rec-cluster/staging'/&gt;\n    &lt;location name='temp' path='/apps/falcon/tmp'/&gt;\n    &lt;location name='working' path='/apps/falcon/next-rec-cluster/working'/&gt;\n  &lt;/locations&gt;\n  &lt;ACL owner='falcon' group='hadoop' permission='0755'/&gt;\n  &lt;properties&gt;\n    &lt;property name='dfs.namenode.kerberos.principal' value='nn/_HOST@FTI.NET'/&gt;\n    &lt;property name='hive.metastore.kerberos.principal' value='hive/_HOST@FTI.NET'/&gt;\n     &lt;property name='queueName' value='oozie-launcher'/&gt;\n    &lt;property name=\"hive.metastore.sasl.enabled\" value=\"true\"/&gt;\n  &lt;/properties&gt;\n&lt;/cluster&gt;\n\n</pre>\n<pre>$ falcon entity -delete -type feed -name next-vers-current\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.security.authentication.client.KerberosAuthenticator).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nERROR: Bad Request;default/org.apache.falcon.FalconWebException::org.apache.falcon.FalconException: Wrong FS: hdfs://clusterA001/apps/falcon/current-rec-cluster/staging/falcon/workflows/feed/next-vers-current/051001d36ac9092827ba3f011ccf1c35_1464596058207, expected hdfs://clusterA002\n\n</pre>\n\n<pre>java.lang.IllegalArgumentException: Wrong FS: hdfs://clusterA001/apps/falcon/current-rec-cluster/staging/falcon/workflows/feed/next-vers-current/051001d36ac9092827ba3f011ccf1c35_1464596058207, expected: hdfs://clusterA002:8020\n        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:646)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:194)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.j</pre>","tags":["Falcon","feed"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-01 15:46:02.0","id":37081,"title":"Spark Master UI","body":"<p>I have Sandbox 2.4 on a VirtualBox </p><p>when I try to access Spark Master UI by the browser from my windows machine I get access denied as you can see from the pic I attached </p><p>can you tell my what wrong with it.?<a href=\"/storage/attachments/4688-access-denide.png\">access-denide.png</a></p>","tags":["Sandbox","hortonwork","spark-ui","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-02 13:00:59.0","id":37272,"title":"Ranger is not creating service repositories","body":"<p>Hi,</p><p>Ranger is not creating repositories for services (Knox, Kafka). I am using HDP 2.3</p><p>I am using default settings. I only installed second Ranger Admin. What can be the issue? Nothing in logs</p><p>@EDIT</p><p>If I add them manually, it is not working. Not even logs in audit-&gt;plugins that I added new policy.</p>","tags":["ranger-admin","service","Ranger","repository","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-03 11:10:39.0","id":37515,"title":"Exporting / Migrating Apache Ranger Policies and Hadoop KMS keys to Ranger","body":"<p>Hi,</p><p>We presently have the situation where Apache Ranger has been deployed along with Hadoop KMS with HDP2.2. Plan is to upgrade ti HDP2.4 and move way from Apache Ranger and Hadoop KMS completely.</p><p>In order to do so, we need to export the data from the existing setup to HDP managed Ranger and Ranger KMS. </p><p>To make the things slightly complex, present setup of Ranger and KMS are shared by multiple clusters and hence migration mechanism need to ensure that only approariate policies and keys are moved to the specific cluster's Ranger and Ranger KMS.</p><p>Given the above,what is the best migration mechanism to get the data from the 2.2 cluster to 2.4 cluster?</p><p>Regards</p><p>Vijay</p>","tags":["upgrade","kms","security","Ranger","ranger-kms"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-03 08:35:35.0","id":37501,"title":"How to install Flume after HDP installation (windows) that it is included in HDP services?","body":"<p>I installed HDP 2.4.0 on windows (without flume) and installed a cluster with 8 nodes. That works perfect. But how can i now install flume on one of the nodes that flume is integrated in HDP and the flume agents start with other HDP services?</p>","tags":["installation","windows","hdp-2.4.0","flume-1.6","service"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-03 12:11:46.0","id":37542,"title":"Bind Ambari UI port 8080","body":"<p>Hi!</p><p>How do I bind ambari port 8080 to interfaces?</p><p>Thank yoU!</p>","tags":["help","Ambari","port"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-06-03 13:18:11.0","id":37557,"title":"Files created after running oozie shell action are owned by yarn user","body":"<p>Hi,</p><p>I am running simple shell action using HUE(logged in as hdfs user in hue) -</p><pre>$ cat test.sh\necho \"hello\" &gt; /tmp/test\n</pre><p>The workflow is getting executed successfully. When i check the files permission and ownership -</p><pre>$ ls -al /tmp/test\n-rw-r--r-- 1 yarn hadoop 6 2016-05-25 14:43 /tmp/test\n</pre><p>The above output shows the file created via shell action has ownership as yarn.</p><p>How can I make oozie shell action to get the ownership to be same as the user who is running the \"shell action/workflow\"(in this case \"hdfs\")</p><p>So i am expecting output as shown below -</p><pre>-rw-r--r-- 1 hdfs hadoop 6 2016-05-25 14:43 /tmp/test</pre>","tags":["hue","Oozie","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-03 14:49:13.0","id":37588,"title":"can't connect to the NiFi cluster, after starting Nifi","body":"<p>In  in nifi-app.log, file seeing this as only WARN level error:</p><p>WARN [main]\no.a.nifi.controller.StandardFlowService Failed to connect to cluster due to:\norg.apache.nifi.cluster.protocol.ProtocolException: Failed to create socket due\nto: java.net.ConnectException: Connection refused</p><p>org.apache.nifi.cluster.protocol.ProtocolException:\nFailed to create socket due to: java.net.ConnectException: Connection refused</p><p>  at\norg.apache.nifi.cluster.protocol.impl.NodeProtocolSenderImpl.createSocket(NodeProtocolSenderImpl.java:145)\n~[nifi-framework-cluster-protocol-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at\norg.apache.nifi.cluster.protocol.impl.NodeProtocolSenderImpl.requestConnection(NodeProtocolSenderImpl.java:68)\n~[nifi-framework-cluster-protocol-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at org.apache.nifi.cluster.protocol.impl.NodeProtocolSenderListener.requestConnection(NodeProtocolSenderListener.java:93)\n~[nifi-framework-cluster-protocol-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at\norg.apache.nifi.controller.StandardFlowService.connect(StandardFlowService.java:671)\n[nifi-framework-core-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at\norg.apache.nifi.controller.StandardFlowService.load(StandardFlowService.java:418)\n[nifi-framework-core-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at\norg.apache.nifi.web.server.JettyServer.start(JettyServer.java:774)\n[nifi-jetty-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at\norg.apache.nifi.NiFi.&lt;init&gt;(NiFi.java:137)\n[nifi-runtime-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at\norg.apache.nifi.NiFi.main(NiFi.java:227) [nifi-runtime-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>Caused by: java.net.ConnectException: Connection\nrefused</p><p>  at\njava.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_11]</p><p>  at\njava.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)\n~[na:1.8.0_11]</p><p>  at\njava.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n~[na:1.8.0_11]</p><p>  at\njava.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n~[na:1.8.0_11]</p><p>  at\njava.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_11]</p><p>  at\njava.net.Socket.connect(Socket.java:589) ~[na:1.8.0_11]</p><p>  at\njava.net.Socket.connect(Socket.java:538) ~[na:1.8.0_11]</p><p>  at\njava.net.Socket.&lt;init&gt;(Socket.java:434) ~[na:1.8.0_11]</p><p>  at\njava.net.Socket.&lt;init&gt;(Socket.java:211) ~[na:1.8.0_11]</p><p>  at\norg.apache.nifi.io.socket.SocketUtils.createSocket(SocketUtils.java:59)\n~[nifi-socket-utils-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>  at\norg.apache.nifi.cluster.protocol.impl.NodeProtocolSenderImpl.createSocket(NodeProtocolSenderImpl.java:143)\n~[nifi-framework-cluster-protocol-0.6.0.1.2.0.1-1.jar:0.6.0.1.2.0.1-1]</p><p>... 7 common frames omitted</p><p>Any idea or help would be greatly appreciated.</p>","tags":["cluster","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-03 20:29:19.0","id":37650,"title":"Ambari Hive View: How to make Hive queries run as current user?","body":"<p>We have a kerberized cluster and just finished configuring the Hive and HDFS views inside Ambari 2.1.2.1 by following the documentation here: http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.1/bk_ambari_views_guide/content/section_kerberos_setup_hive_view.html</p><p>I'm able to login to ambari with my username to use both the Hive and HDFS view OK. I'm testing these views with Ranger now. The problem is the Hive view seems to be running as the ambari-server user.</p><p><img src=\"/storage/attachments/4764-ranger-audit.png\"></p><p>How do make the hive view run queries as my username instead of ambari-server?</p><p>Note: The HDFS view seems to be working correctly because I can see the correct username in the audit logs.</p>","tags":["ambari-views","Hive","Ambari","Ranger"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-02 23:47:17.0","id":37431,"title":"Hive error while running query from Hive View","body":"<p>Hi, Does anyone have an idea about this error?</p><pre>02 Jun 2016 17:04:25,514 ERROR [qtp-ambari-client-6370] ATSRequestsDelegateImpl:125 - Error while reading from ATS\njava.net.ConnectException: Connection refused\n        at java.net.PlainSocketImpl.socketConnect(Native Method)\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n        at java.net.Socket.connect(Socket.java:579)\n        at sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\n        at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)\n        at sun.net.www.http.HttpClient.New(HttpClient.java:308)\n        at sun.net.www.http.HttpClient.New(HttpClient.java:326)\n        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)\n        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)\n        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)\n        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1300)\n        at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)\n        at org.apache.ambari.server.controller.internal.URLStreamProvider.processURL(URLStreamProvider.java:209)\n        at org.apache.ambari.server.view.ViewURLStreamProvider.getHttpURLConnection(ViewURLStreamProvider.java:239)\n        at org.apache.ambari.server.view.ViewURLStreamProvider.getInputStream(ViewURLStreamProvider.java:216)\n        at org.apache.ambari.server.view.ViewURLStreamProvider.readFrom(ViewURLStreamProvider.java:103)\n        at org.apache.ambari.server.view.ViewURLStreamProvider.readAs(ViewURLStreamProvider.java:117)\n        at org.apache.ambari.server.view.ViewURLStreamProvider.readAsCurrent(ViewURLStreamProvider.java:131)\n        at org.apache.ambari.view.hive.resources.jobs.atsJobs.ATSRequestsDelegateImpl.readFromWithDefault(ATSRequestsDelegateImpl.java:121)\n        at org.apache.ambari.view.hive.resources.jobs.atsJobs.ATSRequestsDelegateImpl.hiveQueryIdByOperationId(ATSRequestsDelegateImpl.java:88)\n        at org.apache.ambari.view.hive.resources.jobs.atsJobs.ATSParser.getHiveQueryIdByOperationId(ATSParser.java:83)\n        at org.apache.ambari.view.hive.resources.jobs.Aggregator.readATSJob(Aggregator.java:129)\n        at org.apache.ambari.view.hive.resources.jobs.JobService.jsonObjectFromJob(JobService.java:123)\n        at org.apache.ambari.view.hive.resources.jobs.JobService.getOne(JobService.java:106)\n        at sun.reflect.GeneratedMethodAccessor343.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:770)\n        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:196)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n        at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:216)\n        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:205)\n        at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        at org.eclipse.jetty.server.Server.handle(Server.java:370)\n        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n        at java.lang.Thread.run(Thread.java:745)\n02 Jun 2016 17:04:25,653  INFO [qtp-ambari-client-6370] JobService:276 - jobController.getStatus().status : Succeeded for job : 50\n</pre>","tags":["yarn-ats","hive-views","Hive","ambari-views"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-04 15:59:04.0","id":37705,"title":"Apache Airflow","body":"<p>Has anyone integrated Apache Airflow and HDP?</p><p>It looks interesting.</p>","tags":["airflow","hdp-2.4"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-06 02:39:11.0","id":37739,"title":"Unable to logon to Ambari web-client using default  credentials","body":"<p>I am running sandbox(2.4_1) on Oracle VM virtual box. </p><p>When I hit http://127.0.0.1:8080 , browser prompts me for the credentials. </p><p>I tried entering maria_dev/maria_dev but it doesnt work. Then I set the password for admin manually using shell but that doesnt work either.</p><p>It just keeps showing me the login popup again and again. No error.</p><p>Thanks,</p><p>Ankur</p>","tags":["Ambari","password","login"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-06 13:10:50.0","id":37844,"title":"[WEBHDFS] multi httpfs servers on the  HA cluster","body":"<p>Hi all,</p><p>It is possible to install multi httpfs servers on the same HA cluster, example (3 httpfs on one namenode, and 2 datanodes) ?</p><p>thanks</p>","tags":["httpfs","webhdfs"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-06-06 09:25:32.0","id":37827,"title":"Hi all, Ifollowed the tutorial to install jupyter on Hortonworks sandbox HDP 2.4. I have an issue when executing ./start_ipython_notebook.sh. I get an error that says \"NameError: name 'source' is not defined\". How can I fix it?","body":"<p>Below is the details of the message from the terminal:</p><p>[root@sandbox ~]# ./start_ipython_notebook.sh </p><p>[TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions.\n[TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future </p><p>[E 09:18:57.055 NotebookApp] Exception while loading config file /root/.jupyter/jupyter_notebook_config.py </p><p style=\"margin-left: 20px;\">    Traceback (most recent call last): </p><p style=\"margin-left: 40px;\">      File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/traitlets/config/application.py\", line 534, in _load_config_files</p><p style=\"margin-left: 60px;\"> \n        config = loader.load_config() </p><p style=\"margin-left: 40px;\">File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/traitlets/config/loader.py\", line 458, in load_config</p><p style=\"margin-left: 60px;\"> \n        self._read_file_as_dict() </p><p style=\"margin-left: 40px;\">      File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/traitlets/config/loader.py\", line 490, in _read_file_as_dict</p><p style=\"margin-left: 60px;\"> \n        py3compat.execfile(conf_filename, namespace) </p><p style=\"margin-left: 40px;\">      File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/ipython_genutils/py3compat.py\", line 288, in execfile</p><p style=\"margin-left: 60px;\"> \n        builtin_mod.execfile(filename, *where) </p><p style=\"margin-left: 40px;\">      File \"/root/.jupyter/jupyter_notebook_config.py\", line 2, in &lt;module&gt; </p><p style=\"margin-left: 60px;\">        source /opt/rh/python27/enable </p><p style=\"margin-left: 20px;\">    NameError: name 'source' is not defined </p><p>[I 09:18:57.081 NotebookApp] The port 8888 is already in use, trying another port. </p><p>[I 09:18:57.091 NotebookApp] Serving notebooks from local directory: /root </p><p>[I 09:18:57.091 NotebookApp] 0 active kernels </p><p>[I 09:18:57.091 NotebookApp] The Jupyter Notebook is running at: http://localhost:8889/ </p><p>[I 09:18:57.091 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). </p><p>[W 09:18:57.092 NotebookApp] No web browser found: could not locate runnable browser.</p><p style=\"margin-left: 40px;\"></p>","tags":["Spark","hdp-2.4.0","tutorial-380","jupyter"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-06 13:58:04.0","id":37812,"title":"Solr-HBase-Lily","body":"<p>Hi ,</p><p>I'm trying to integrate lily-solr-hbase .</p><p>I'm using Hortonworks distribution (2.4 version) Solr (5.2.1 version) Lily (2.0 version)</p><p>I'm following the lily documentation from official link : <a href=\"http://docs.ngdata.com/lily-docs-2_0/414-lily/432-lily.html\">http://docs.ngdata.com/lily-docs-2_0/414-lily/432-lily.html</a></p><p>But while running the steps as mentioned to start the lily server .. I'm getting the below exception .</p><pre>[root@BAN-SDU-OVS2-VM7 lily-2.0]# bin/lily-server [INFO ][11:32:41,606][main ] org.kauriproject.runtime.info - Starting the Kauri Runtime. [INFO ][11:32:42,163][main ] org.kauriproject.runtime.info - Reading module configurations of 12 modules. [INFO ][11:32:43,152][main ] org.kauriproject.runtime.info - Starting the modules. [INFO ][11:32:43,162][main ] org.kauriproject.runtime.info - Starting module pluginregistry - /root/lily-2.0/lily-2.0/lily-2.0/lib/org/lilyproject/lily-pluginregistry-impl/2.0/lily-pluginregistry-impl-2.0.jar [INFO ][11:32:44,203][main ] org.kauriproject.runtime.info - Starting module general - /root/lily-2.0/lily-2.0/lily-2.0/lib/org/lilyproject/lily-general-module/2.0/lily-general-module-2.0.jar org.kauriproject.runtime.KauriRTException: Error constructing module defined at /root/lily-2.0/lily-2.0/lily-2.0/lib/org/lilyproject/lily-general-module/2.0/lily-general-module-2.0.jar at org.kauriproject.runtime.module.build.ModuleBuilder.buildInt(ModuleBuilder.java:152) at org.kauriproject.runtime.module.build.ModuleBuilder.build(ModuleBuilder.java:55) at org.kauriproject.runtime.KauriRuntime.start(KauriRuntime.java:240) at org.kauriproject.runtime.cli.KauriRuntimeCli.run(KauriRuntimeCli.java:292) at org.kauriproject.runtime.cli.KauriRuntimeCli.main(KauriRuntimeCli.java:63) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.kauriproject.launcher.RuntimeCliLauncher.run(RuntimeCliLauncher.java:79) at org.kauriproject.launcher.RuntimeCliLauncher.launch(RuntimeCliLauncher.java:58) at org.kauriproject.launcher.RuntimeCliLauncher.main(RuntimeCliLauncher.java:54) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'hbaseConfiguration' defined in KAURI-INF/spring/services.xml in /root/lily-2.0/lily-2.0/lily-2.0/lib/org/lilyproject/lily-general-module/2.0/lily-general-module-2.0.jar: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [org.lilyproject.server.modules.general.HadoopConfigurationFactoryImpl]: Constructor threw exception; nested exception is org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for after 1 tries. at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:254) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:925) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:835) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:440) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:409) at java.security.AccessController.doPrivileged(Native Method) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:380) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:264) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:261) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:185) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164) at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:429) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:728) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:380) at org.kauriproject.runtime.module.build.ModuleBuilder.buildInt(ModuleBuilder.java:89) ... 11 more Caused by: org.springframework.beans.BeanInstantiationException: Could not instantiate bean class [org.lilyproject.server.modules.general.HadoopConfigurationFactoryImpl]: Constructor threw exception; nested exception is org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for after 1 tries. at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:115) at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:87) at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:248) ... 26 more Caused by: org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for after 1 tries. at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:914) at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:820) at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:788) at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:249) at org.apache.hadoop.hbase.client.HTable.(HTable.java:213) at org.lilyproject.server.modules.general.HadoopConfigurationFactoryImpl.waitOnHBase(HadoopConfigurationFactoryImpl.java:67) at org.lilyproject.server.modules.general.HadoopConfigurationFactoryImpl.(HadoopConfigurationFactoryImpl.java:53) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:100) ... 28 more Startup failed. Will try to shutdown and exit. [INFO ][11:33:45,632][main ] org.kauriproject.runtime.info - Shutting down the modules. [INFO ][11:33:45,633][main ] org.kauriproject.runtime.info - Stopping the restservice manager.</pre><p>Can anyone help so as how to fix this and proceed ?</p><p>Thanks in advance .</p>","tags":["hortonwork","Hbase","SOLR","lily"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-07 07:05:28.0","id":38091,"title":"hive context save file","body":"<p>Hello</p><p>I work with Hive- Context to load and manipulate data in my orc format. I\nwould now please know how to save in the hdfs file the results of a sql queries ?</p><p>Help me please?</p><p>Here is my Hive-Context code, I would like to save the contents of hive_context in a file on my hdfs :</p><p>Thanks you in advance</p><pre>from pyspark.sql import HiveContext from pyspark import SparkContext\nsc =SparkContext()\nhive_context = HiveContext(sc) qvol = hive_context.table(\"&lt;bdd_name&gt;.&lt;table_name&gt;\") qvol.registerTempTable(\"qvol_temp\") hive_context.sql(\"select * from qvol_temp limit 10\").show()\n</pre>","tags":["Hive","python"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-07 11:50:43.0","id":38167,"title":"How does Ranger HDFS plugin avoid reading tampered local policy cache file?","body":"<p>I modified(tampered) a local policy file written by Ranger HDFS Plugin to test against illegal or malicious operation, but authorization rules are not changed. For example:</p><p>1. user \"ohide\" cannot read /user/ohide</p><p>2. admit user \"ohide\" to read /user/ohide by Ranger</p><p>3. confirm user \"ohide\" can read /user/ohide</p><p>4. delete an entry added by step 2 from a local policy cache file in NameNode host (where Ranger HDFS Plugin running)</p><p>5. try to read /user/ohide by user \"ohide\" and succeeded.</p><p>This behavior is appropriate I think, but I do not know and want to know how to avoid not to read tampered policy cache file. Does anyone know the answer of my question?</p>","tags":["HDFS","Ranger","ranger-0.5.0"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-07 14:55:32.0","id":38213,"title":"Apache sentry in Ambari Cluster","body":"<p>I want to do POC on Apache Sentry in our Hadoop cluster (which was set up through Ambari)</p><p>Now Apache Sentry is Cloudera project. And I am not able to find any relevant documents for Sentry configuration in hadoop/ Ambari clusters. </p><p>Please help me by explaining pre-requisites and steps to configure the same.</p>","tags":["HDFS","security","Ambari","Falcon","hadoop"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-07 15:06:35.0","id":38205,"title":"Is there a collaborative document for service \"Custom commands\"?","body":"<p>Some of the custom commands I know are :</p><p>DECOMMISSION/RECOMMISSION and </p><p>CLEAN for hive.</p><p>Is there an API to get all the custom commands? Or a doc with details of custom commands that can be run for services?</p>","tags":["hdp-2.4.0","Ambari","notify-docs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-07 16:08:54.0","id":38231,"title":"Number of Region Servers, Phoenix Server on HBase?","body":"<p>Hi,</p><p>I have a couple of questions in installing HBase, I want to install HBase in the existing cluster only.</p><p>1. Do I need to install Region Server on all Datanodes? </p><p>2. If I don't install Region Servers on all the datanodes, what will be the impact?</p><p>3. Do I need to install Phoenix Query Server on all the Region Server nodes?</p><p>4. If I install only 3 Phoenix Query Server on top of any 3 Region servers node out of 20, what will be the impact?</p><p>5. If I install only 3 Phoenix Query Server on the separate node where I don't have Region Server, what will be the impact?</p><p>Any needful help is highly appreciated and thanks in advance.</p>","tags":["regionserver","Phoenix","installation","Hbase","datanode"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-08 01:43:53.0","id":38308,"title":"How to extract Ambari metrics using Hortonworks DataFlow ?","body":"<p>Hello,</p><p>In order to build a centralized monitoring platform using the ELK stack I'm trying to to get the metrics from Ambari using HDF.</p><p>For instance, using HDF (Nifi) and the getHTTP processor, I would like to get the \"cluster disk\" metrics, csv formatted as when you export them manually from Ambari dashboard  <a href=\"https://community.hortonworks.com/storage/attachments/2349-2200-1.png\">as seen here</a></p><p>- Has anyone already did that ?</p><p>- what is the REST-API call ?</p><p>- How do I have to configure my processor if the ambari server is configured to use HTTPS ?</p><p>Thxx!!</p>","tags":["Nifi","ambari-metrics","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-07 18:01:18.0","id":38279,"title":"question on executing HDFS command on Kerberized cluster through oozie Shell action","body":"<p>Hi,</p><p>I am trying to read a file in HDFS using hadoop fs -cat command in oozie Shell action. Mine is a kerberized cluster.</p><p>oozie workflow is submitted using my ID, A. The file can be only read using ID B. I am doing kinit -kt using B's keytab.</p><p>Inside the shell script, I did kinit -kt and i also did klist. Klist displayed B as default principal and it showed a valid ticket.</p><p>But even though the klist shows B's valid ticket, hadoop fs -cat is executed using my ID (A) and not B. This results in insufficient privilege issue.  Why is the hadoop fs -cat command not using B's ticket and using my ID ?</p><p>The same thing works when I run from linux as individual commands instead of oozie shell action. I login to linux using my ID. klist just shows my principal. I do a kinit for B and now Klist shows B's ticket. and in the same shell (not from oozie, from linux command line) when I issue hadoop fs -cat filename, it displays the content of the file. </p><p>Why is this working from linux directly but not working when executed from oozie shell action?</p><p>After doing a kinit on a different user, all the hadoop commands seem to be executing using the second user in linux CLI, I thought this is how the oozie shell action would work too. Please help me understand this.</p><p>Note: When I login using my ID and do kinit as second user before even submitting the oozie workflow, and submit the oozie workflow after kinit to second user, this seems to work and all actions inside shell action of workflow now seems to exxecuted by second user rather than my ID. and this way it does not create any issues. Please help me understand this as well.</p>","tags":["Oozie","kerberos","permission-denied","kinit","hdfs-permissions"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-07 20:45:51.0","id":38319,"title":"Problems importing tables from SAP that have / characters in the table name","body":"<p>Is there a way to make sqoop accept / characters in table and field names?</p><p>I have tried the following:</p><p>leaving the name as-is:  --table /BI0/TCUSTOMER</p><p>wrapping in quotes:  --table \"/BI0/TCUSTOMER\"</p><p>escaping each /:  --table \"\\/BI0\\/TCUSTOMER\"</p><p>It produces this error each time for all three options above:</p><p>com.sap.db.jdbc.exceptions.JDBCDriverException: SAP DBTech JDBC: [257]: sql syntax error: incorrect syntax near \"\\\": line 1 col 17 (at pos 17)</p>","tags":["import","sap-hana","sap","Sqoop","sql"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-07 18:05:12.0","id":38264,"title":"OpenStack hardware architecture for running HDP","body":"<p>Hello all,</p><p>I'm planning to have my own private cloud using OpenStack. I've already \nseen some sample architectures to do so, like the following one: \nhttp://docs.openstack.org/openstack-ops/content/example_architecture.html</p><p>My\n ideas is to have a regular private cloud but also capable to run HDP efficiently.</p><p>My question then is the next one: Which architecture should I use in \norder to run a HDP in my OpenStack cloud? There is any architecture \nsample out there to achieve that?</p>My guess is about having 4 compute nodes (with enough RAM, CPU and local storage, Cinder, to run many VMs\n as Hadoop\n nodes), 1 controller node (API services, high availability services, Neutron) and 1 Swift node (for all storage but Hadoop\n nodes storage).<p>What do you think about this possible architecture? Does it make sense?</p><p>Thanks!</p>","tags":["hardware","architecture","Cloudbreak","hdp-2.4","openstack"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-08 11:17:17.0","id":38442,"title":"Logstash infrastructure to monitor my Hadoop/Hue and Hive .","body":"<p>I have already a server configured with logstash and also Kibana is configured and showing the logs currently on the server.\nNow in another server where Hadoop is running I want also to capture the logs generated by a certain component (e.g HDFS) and send the logs to the central server where logstash is running.</p><p>\nMy questions are:</p><p>\n - What do I need to install on the Hadoop server elasticsearch or some .jar files? And in what exactly will consists the configuration so that some logs (e.g in the directory \\vat\\log\\XX </p><p> - where to specify the agent configuration (remote logstash serve/output)?  </p>","tags":["hadoop","logstash","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-08 13:11:45.0","id":38479,"title":"Issue when using parquet org.kitesdk.data.DatasetNotFoundException: Descriptor location does not exist","body":"<p>I am getting this issue when using sqoop with parquet</p>","tags":["hadoop","hadoop-ecosystem","Sqoop","parquet"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-09 13:27:46.0","id":38776,"title":"Wy data is not being profiled using waterline?","body":"<p>Hi,</p><p>I have download Sandbox machine for waterlinedata and I am getting waterline UI with per-loaded/per-profiled data everything is working fine(default).</p><p>Now I want to profile some files which are present under <strong>/user/waterlinedata/newStaggingData directory.</strong>After copying from local to HDFS I am running command<strong> ./waterline profileOnly </strong><strong>/user/waterlinedata/newStaggingData and </strong>now accoding to my knowledge profiling is nothing but identify file format,calculate data quality matrics and store all details in inventory etc. but am not able to see such details in front of my files within waterline UI.</p><p>Please attached images.<a href=\"/storage/attachments/4889-capture1.png\">capture1.png</a></p><p>I know that after executing above command waterline runs map-reduce job and I am sure that it's running perfectly but still not getting any fileformat/data quality metrics in UI.</p><p>otherwise send me the steps to for how to profile data which are present inside of  particular directory.</p><p>Thanks in advance.</p>","tags":["profiling","waterline"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-10 10:18:19.0","id":39017,"title":"can someone point me to a good tutorial on spark streaming to use with kafka","body":"<p>I am trying to fetch json format data from kafka through spark streaming and want to create a temp table in spark to query json data like normal table.</p><p>i tried several tutorials available on internet but did'nt get success. I am able to read a text file from hdfs and process it through spark, but stuck using json data from kafka.</p><p>can somebody guide me on this.</p>","tags":["spark-sql","spark-streaming","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-09 22:18:37.0","id":38929,"title":"While using Ranger authorization (through policies) for say, a Hive command, is the command's user's group determined through Ranger DB's x_group_users table or through Hadoop? What is the process of determining a user's group?","body":"<a rel=\"user\" href=\"/users/218/rmani.html\" nodeid=\"218\">@Ramesh Mani</a>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-10 15:39:06.0","id":39090,"title":"Does Microsoft SQL Server 2016 Polybase Support HDP 2.4","body":"<p>https://msdn.microsoft.com/en-us/library/mt163689.aspx</p><p>They mention HDP 2.3, I am wondering if 2.4 is supported and will work.</p><p>I don't see why it wouldn't.</p>","tags":["sql-server","polybase","hdp-2.4"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-10 12:24:51.0","id":39045,"title":"We have HDP 2.1.5","body":"<p>We have HDP 2.1.5 and I have confusion that i just want to install spldf pkg in R, it is asking depende of tseries and tseries depn of quadprog and while i installed quadprog ERROR display that make: gfortran command not found..is it gfortran and gcc-fortran are same // i have downlaod gcc-fortran rpm can i installed it..?  suggest me</p>","tags":["help"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-06-09 18:07:41.0","id":38871,"title":"Hadoop Job Status from YARN after restart","body":"<p>Hello,</p><p>I want to get the list of all (successful, failed, or killed) the previous jobs launched with YARN and using the YARN REST API to collect this information. But, in case of YARN restart on I can see only successful jobs from past 7 days. </p><p>Any pointers on how to get this going?</p><p>I see that following param can be set to longer than 7 days to get that but still would not get me all the job.</p><pre>mapreduce.jobhistory.max-age-ms</pre><p>Thanks in advance!</p>","tags":["YARN","yarntimeline"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-10 08:55:01.0","id":38997,"title":"Can I use the PutCassandraQL NiFi processor to insert data into Cassandra 3.5 or higher?","body":"<p>I have successfully used the PutCassandraQL NiFi processor to insert data into Cassandra 2.2. It works perfectly.</p><p>However I'm not able to use the PutCassandraQL NiFi processor to insert data into Cassandra 3.5.</p><p>Is there a solution  for using NiFi 1.6.1 or HDF to insert data into Cassandra 3.5, or a workaround?</p><p>Are there any plans for upgrading the NiFi code to support Cassandra 3.5.</p>","tags":["nifi-processor","cassandra"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-10 12:43:08.0","id":39060,"title":"Sqoop to import data to Hive through oozie shell action fails","body":"<p>Hi,</p><p>I am running a oozie shell action to run a sqoop command to import data to Hive. When I run the sqoop command directly, it works fine, but when I run it through oozie shell action, it aborts with </p><pre>Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]</pre><p>Based on this link, </p><p><a href=\"https://community.hortonworks.com/questions/25121/oozie-execute-sqoop-falls.html#answer-25290\">https://community.hortonworks.com/questions/25121/oozie-execute-sqoop-falls.html#answer-25290</a></p><p>I have added hive-site.xml also using &lt;file&gt; tag in oozie shell action and also based on other link I have added </p><pre>export HIVE_CONF_DIR=`pwd` </pre><p>before running the sqoop command. But nothing worked. When I add full hive-site.xml it resulted in the same error above, when I added just the important properties mentioned in this link <a href=\"http://ingest.tips/2014/11/27/how-to-oozie-sqoop-hive/\">http://ingest.tips/2014/11/27/how-to-oozie-sqoop-hive/</a>, I get this error</p><pre>FAILED: IllegalStateException Unxpected Exception thrown: Unable to fetch table XYZ. java.net.SocketException: Connection resetFailing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]</pre><p>Both the times, the sqoop command successfully creates the file in target-directory but fails while loading this data to hive. </p><p>Hadoop cluster is kerberos enabled. I have a kinit done before submitting the workflow and also kinit is done again inside the oozie shell action.</p><p>Can someone please throw some light on how to fix this one? below is the sqoop command used.  </p><pre>Sqoop command:sqoop import \\\n--connect \"jdbc:teradata://${server}/database=${db},logmech=ldap\" \\\n--driver \"com.teradata.jdbc.TeraDriver\" \\\n--table \"XYZ\" \\\n--split-by \"col1\" \\\n--hive-import \\\n--delete-target-dir \\\n--target-dir \"/user/test/\" \\\n--hive-table \"default.XYZ\" \\\n--username \"terauser\" \\\n--password tdpwd \\\n--where \"${CONDITION}\" \\\n--m 2 \\\n--fetch-size 1000 \\\n--hive-drop-import-delims \\\n--fields-terminated-by '\\001' \\\n--lines-terminated-by '\\n' \\\n--null-string '\\\\N' \\\n--null-non-string '\\\\N'</pre>","tags":["oozie-shell-action","Hive","Sqoop","kerberos","import"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-10 22:52:54.0","id":39141,"title":"Sqoop Changing Datatype from Datetime to Varchar","body":"<p>I'm new to Hadoop and trying to move many tables from an existing sql database to a Hadoop cluster (and into hive). I'm using sqoop because it should automatically set up all the tables for me, so I don't have to know about the hundreds of columns involved and all their details. For whatever reason, sqoop is automatically changing the datetime columns into varchar columns.</p><p>Searching online, it looks like people have encountered this kind of error before, at least with oracle databases, but the only fixes I've found seem specific to problems with the oracle jdbc, or rely on listing out every relevant column's details, which defeats my purpose for using sqoop- and it seems like this is something I should be able to fix in the initial sqoop import in some kind of setting.</p><p>Thanks in advance for any help!</p>","tags":["date","sql","Sqoop","sql-server","import"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-11 00:19:34.0","id":39160,"title":"Combined Application Logs","body":"<p>yarn logs -applicationId &lt;&gt; only seems to be giving logs from \nthe Mapper, but if I wanted my logs \nfrom the Driver as well.  Is there a way to get that?</p><p>My driver \nclass is logging the arguments.  When the mappers are done with writing data into HFiles, the driver invokes LoadIncrementalHFiles\n to bulk load data into HBase.  I would like to get a log file with all my Application's logs - Driver + Mappers.  At the moment, I haven't found the logs from the Driver that I see on the console.</p><p>Also, how can I set an Appender only for my application logs?  I have seen properties for setting log levels, but I'd like to set a log configuration file when the map reduce job runs.</p>","tags":["MapReduce","logs","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-11 13:34:09.0","id":39178,"title":"Sqoop Import Error from Teradata","body":"<p>Hi ,</p><p>I am doing sqoop import from Teradata to HDFS in hortonworks 2.2 using below command .</p><p>Below settings have been done</p><p>Placed below the jar files @ /usr/hdp/2.2.0.0-2041/sqoop/lib</p><p>1.teradata-connector-1.3.4-hdp2.2.noarch.rpm</p><p>2.terajdbc4.jar</p><p>3.tdgssconfig.jar</p><p>4.hdp-connector-for-teradata-1.4.1.2.3.2.0-2950-distro.tar</p><p>Run the below command from /usr/hdp/current/sqoop-client/bin</p><p>sqoop import  --connection-manager org.apache.sqoop.teradata.TeradataConnManager \\                                                                --connect jdbc:teradata://192.168.146.130/Database=retail \\                                                                                                               --connection-manager org.apache.sqoop.teradata.TeradataConnManager \\ --table client --username dbc --password dbc \\           --target-dir /usr/hue/Abhishek</p><p>Getting below error</p><p>Warning: /usr/hdp/2.2.0.0-2041/accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. 16/06/11 13:28:04 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5.2.2.0.0-2041 16/06/11 13:28:04 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 16/06/11 13:28:04 ERROR sqoop.ConnFactory: Sqoop could not found specified connection manager class org.apache.sqoop.teradata.TeradataConnManager. Please check that you've specified the class correctly. 16/06/11 13:28:04 ERROR tool.BaseSqoopTool: Got error creating database manager: java.io.IOException: java.lang.ClassNotFoundException: org.apache.sqoop.teradata.TeradataConnManager at org.apache.sqoop.ConnFactory.getManager(ConnFactory.java:166) at org.apache.sqoop.tool.BaseSqoopTool.init(BaseSqoopTool.java:249) at org.apache.sqoop.tool.ImportTool.init(ImportTool.java:89)       at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:589) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) Caused by: java.lang.ClassNotFoundException: org.apache.sqoop.teradata.TeradataConnManager at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:191) at org.apache.sqoop.ConnFactory.getManager(ConnFactory.java:146) ... 9 more</p><p>------------------------</p><p>Please help me on this and let me know the solution.</p><p>Regards</p><p>Abhishek</p>","tags":["teradata","import","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-11 14:43:51.0","id":39192,"title":"Where to store a really wide table?","body":"<p>If you have 600+ columns and you need to access 20-30 columns at a time.   What is the optimal type of storage:</p><ul><li>Hive Table Stored as ORC with compression, vectorization and optimization; access with Tez and properly partition and bucket</li><li>HBase</li><li>HBase in Phoenix Table</li><li>Parquet File</li><li>AVRO File</li><li>Accumulo</li></ul>","tags":["Phoenix","Accumulo","Hive","Hbase","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-13 17:51:58.0","id":39431,"title":"Mount question","body":"<p>Hi,</p><p>Since the largest folder in HDP is under /usr/hdp, we are planning to mount it in advance as 50G prior to running the installation. Will it delete and recreate the folder  while running the installation? This was the recommendation from unix team. Please let me know</p><p>Thanks</p>","tags":["mount","hdp-2.3.4","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-13 15:37:22.0","id":39418,"title":"How can I use Spark to empty/delete data from an S3 bucket?","body":"<p>My ingest pipeline writes small files to S3 frequently. I have a periodic job that aggregates these into bigger files. Is there a way to use Spark to empty an S3 path? Something like \"insert overwrite s3://bucket/my_folder\" with an empty DataFrame?</p>","tags":["s3","Spark","aws"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-13 21:22:25.0","id":39447,"title":"Unable to delete files using Ambari HDFS View: UnrecognizedPropertyException: Unrecognized field \"path\"","body":"<p>I uploaded a file into my HDFS user directory using the Ambari HDFS view and moved it into my temp folder.</p><p>I tried to delete the file and get a 500 Server Error:</p><p><img src=\"/storage/attachments/4978-ambari-hdfs-view.png\"></p><p>I looked in the Ambari Server logs and see this error:</p><pre>javax.servlet.ServletException: org.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field \"path\" (Class org.apache.ambari.view.filebrowser.FileOperationService$MultiRemoveRequest), not marked as ignorable\n at [Source: org.eclipse.jetty.server.HttpInput@28b380db; line: 1, column: 10] (through reference chain: org.apache.ambari.view.filebrowser.MultiRemoveRequest[\"path\"])\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:420)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:196)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n        at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:216)\n        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:205)\n        at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:152)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        at org.eclipse.jetty.server.Server.handle(Server.java:370)\n        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n        at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)\n        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)\n        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)\n        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)\n        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field \"path\" (Class org.apache.ambari.view.filebrowser.FileOperationService$MultiRemoveRequest), not marked as ignorable\n at [Source: org.eclipse.jetty.server.HttpInput@28b380db; line: 1, column: 10] (through reference chain: org.apache.ambari.view.filebrowser.MultiRemoveRequest[\"path\"])\n        at org.codehaus.jackson.map.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:53)\n        at org.codehaus.jackson.map.deser.StdDeserializationContext.unknownFieldException(StdDeserializationContext.java:267)\n        at org.codehaus.jackson.map.deser.std.StdDeserializer.reportUnknownProperty(StdDeserializer.java:649)\n        at org.codehaus.jackson.map.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:635)\n        at org.codehaus.jackson.map.deser.BeanDeserializer.handleUnknownProperty(BeanDeserializer.java:1355)\n        at org.codehaus.jackson.map.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:717)\n        at org.codehaus.jackson.map.deser.BeanDeserializer.deserialize(BeanDeserializer.java:580)\n        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:2695)\n        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1308)\n        at org.codehaus.jackson.jaxrs.JacksonJsonProvider.readFrom(JacksonJsonProvider.java:419)\n        at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:490)\n        at com.sun.jersey.server.impl.model.method.dispatch.EntityParamDispatchProvider$EntityInjectable.getValue(EntityParamDispatchProvider.java:123)\n        at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:86)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:203)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n        ... 66 more\n</pre><p>Any idea what this is saying? We recently upgraded Ambari to 2.2.2.0 and I'm pretty sure this was working in Ambari 2.1.2.1.</p>","tags":["ambari-views","Ambari","webhdfs"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-14 04:28:03.0","id":39533,"title":"Fill 'Null' With Previous Row Values in Hive","body":"<p>Hi,</p><p>I just wanted to know is there a way to fill up <strong>null </strong>values with previous value/record in Hive?</p><p><strong>Example:</strong></p><p>date                employee          salary</p><p>5/16/16           Dave                 25,000</p><p>5/17/16           Richard             10,000</p><p>5/18/16           <strong>NULL                 NULL</strong></p><p>5/17/16 Howard 50,000</p><p>5/18/16 <strong>NULL NULL</strong></p><p><strong>INTO:</strong></p><p>date                employee          salary</p><p>5/16/16           Dave                 25,000</p><p>5/17/16           Richard             10,000</p><p>5/18/16 <strong>Richard 10,000</strong></p><p>5/17/16 Howard 50,000</p><p>5/18/16 <strong>Howard 50,000</strong></p><p>Assuming there's thousand records with several <strong>null values. </strong>Appreciate your help on this.</p><p>To understand further, attached is my primary objective.</p><p><img src=\"/storage/attachments/4991-1.png\"></p><p>Regards,</p><p>Bruce</p>","tags":["query","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-14 11:05:44.0","id":39628,"title":"Hive in-built functions","body":"<p>We are planning to migrate from HAWQ to Hive. HAWQ has 2000+ in-built functions and in existing code  around 400+ in-built functions are used.\nWe have a task to make all 400+ available in HIVE and noticed only 150+ are available in HIVE sanbox environment we are using.\nWe noticed below URL has few, which we  deployed in Hive and are working as expected.</p><p>https://github.com/brndnmtthws/facebook-hive-udfs/tree/master/src/main/java/com/facebook/hive/udf </p><p>Please share any opensource repositories like above URL where we can find code of UDFs and use them directly.</p>","tags":["hive-odbc","hive-udf","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-06 07:45:25.0","id":55001,"title":"error: java.lang.IllegalArgumentException: Field \"label_idx\" does not exist","body":"<p>Hello,</p><p>I have the following error: java.lang.IllegalArgumentException: Field \"label_idx\" does not exist.</p><p>After executing this code:</p><p>import org.apache.spark.mllib.tree.RandomForest</p><p>import org.apache.spark.mllib.tree.model.RandomForestModel</p><p>import org.apache.spark.mllib.util.MLUtils</p><p>import org.apache.spark.mllib.linalg.Vectors</p><p>import org.apache.spark.mllib.regression.LabeledPoint</p><p>import org.apache.spark.mllib.evaluation.MulticlassMetrics</p><p>import org.apache.spark.ml.Pipeline import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}</p><p>import org.apache.spark.ml.classification.RandomForestClassifier</p><p>import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator</p><p>import org.apache.spark.sql.types._ import sqlContext.implicits._</p><p>import org.apache.spark.ml.attribute.NominalAttribute</p><p>import org.apache.spark.ml.feature.StringIndexer</p><p>val unparseddata = sc.textFile(\"hdfs:///tmp/epidemiological16.csv\")</p><p>val data = unparseddata.map { line =&gt; val parts = line.split(',').map(_.toDouble) LabeledPoint(parts.last%2, Vectors.dense(parts.slice(0, parts.length - 1))) }</p><p>val splits = data.randomSplit(Array(0.7, 0.3))</p><p>val (trainingData2, testData2) = (splits(0), splits(1))</p><p>val trainingData = trainingData2.toDF</p><p>val nFolds: Int = 10</p><p>val NumTrees: Int = 3</p><p>val rf = new RandomForestClassifier() .setNumTrees(NumTrees) .setFeaturesCol(\"features\")</p><p>val indexer = new StringIndexer() .setInputCol(\"label\") .setOutputCol(\"label_idx\") .fit(trainingData)</p><p>rf.setLabelCol(\"label_idx\").fit(indexer.transform(trainingData))</p><p>val pipeline = new Pipeline().setStages(Array(rf))</p><p>val paramGrid = new ParamGridBuilder().build()</p><p>val evaluator = new MulticlassClassificationEvaluator() .setLabelCol(\"label\") .setPredictionCol(\"prediction\")</p><p>val cv = new CrossValidator() .setEstimator(pipeline) .setEvaluator(evaluator) .setEstimatorParamMaps(paramGrid) .setNumFolds(nFolds)</p><p>val model = cv.fit(trainingData)</p><p>Do you know where can be the problem?</p><p>Thanks,</p><p>Laia</p>","tags":["spark-mllib","code","scala"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-09-06 16:52:02.0","id":55132,"title":"How we can distcp between secured and unsecured cluster?","body":"","tags":["security","distcp"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-09-06 13:38:53.0","id":55077,"title":"Hi - I am getting the following error Resolving docs.google.com... failed: Temporary failure in name resolution. wget: unable to resolve host address “docs.google.com” rm: `/tmp/littlelog.csv': No such file or directory","body":"","tags":["hdp-2.4.0","tutorial-370"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-08 16:19:16.0","id":55545,"title":"flume-ng throwing exception","body":"<p></p><p>the following command throws exception , i have installed latest flume (1.6.0  ),  I have also tried installing the  \nflume-sources-1.0-SNAPSHOT.jar  file by doing mvn install in  cdh-twitter-example-master  folder and then copying it \nto $FLUME_HOME/lib/  with no luck. </p><p>flume-ng agent --conf-file twitter-to-hdfs.properties --name agent1 --conf $FLUME_HOME/conf/twitter.conf -Dflume.root.logger=WARN,console -Dtwitter4j.http.proxyHost=dotatofwproxy.tolls.dot.state.fl.us -Dtwitter4j.http.proxyPort=8080</p><p>16/09/08 11:46:58 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: sink1 started\n16/09/08 11:46:58 INFO twitter4j.TwitterStreamImpl: Connection established.\n16/09/08 11:46:58 INFO twitter4j.TwitterStreamImpl: Receiving status stream.\n<strong>Exception in thread \"Twitter4J Async Dispatcher[0]\" java.lang.NoSuchMethodError: twitter4j</strong>.Status.getRetweetCount()J\n  at org.apache.flume.source.twitter.TwitterSource.extractRecord(TwitterSource.java:248)\n  at org.apache.flume.source.twitter.TwitterSource.onStatus(TwitterSource.java:157)\n  at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)</p>","tags":["Flume"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-08 18:58:25.0","id":55580,"title":"Ambari Hive view, Files view, Tez view does not work. throws null pointer exception","body":"<p>I cannot get into ambari hive view, I get the following errors. I have restarted HS2 and hive metastore, webHcat server couple of times. It does not fix the problem. Webhcat failed to start after the install (not sure if webhcat server has to do something with the hive view, files view and Tez view). I can run a job from hive shell successfully.   \n</p><p>E090 NullPointerException</p><p></p><pre>java.lang.NullPointerException\n\njava.lang.NullPointerException\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:168)\n\tat org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)\n\tat org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\n\tat org.apache.ambari.view.hive.client.Connection.openConnection(Connection.java:107)\n\tat org.apache.ambari.view.hive.client.Connection.(Connection.java:96)\n\tat org.apache.ambari.view.hive.client.ConnectionFactory.create(ConnectionFactory.java:68)\n\tat org.apache.ambari.view.hive.client.UserLocalConnection.initialValue(UserLocalConnection.java:42)\n\tat org.apache.ambari.view.hive.client.UserLocalConnection.initialValue(UserLocalConnection.java:26)\n\tat org.apache.ambari.view.utils.UserLocal.get(UserLocal.java:66)\n\tat org.apache.ambari.view.hive.resources.browser.HiveBrowserService.databases(HiveBrowserService.java:87)\n\tat sun.reflect.GeneratedMethodAccessor384.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:770)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:196)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:216)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:205)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:152)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\n  </pre><li><p></p><p>F060 Couldn't open connection to HDFS</p><pre>org.apache.ambari.view.utils.hdfs.HdfsApiException: HDFS070 fs.defaultFS is not configured\n\norg.apache.ambari.view.utils.hdfs.HdfsApiException: HDFS070 fs.defaultFS is not configured\n\tat org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.getDefaultFS(ConfigurationBuilder.java:115)\n\tat org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.parseProperties(ConfigurationBuilder.java:86)\n\tat org.apache.ambari.view.utils.hdfs.ConfigurationBuilder.buildConfig(ConfigurationBuilder.java:227)\n\tat org.apache.ambari.view.utils.hdfs.HdfsApi.(HdfsApi.java:65)\n\tat org.apache.ambari.view.utils.hdfs.HdfsUtil.connectToHDFSApi(HdfsUtil.java:126)\n\tat org.apache.ambari.view.hive.utils.SharedObjectsFactory.getHdfsApi(SharedObjectsFactory.java:128)\n\tat org.apache.ambari.view.hive.resources.savedQueries.SavedQueryService.getDefaultSettings(SavedQueryService.java:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:770)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:196)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:216)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:205)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)</pre></li>","tags":["Hive","hadoop","metastore","HDFS","hiveserver2","webhcat"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-12 18:38:23.0","id":56087,"title":"Is there a easy way to test spark applications within Eclipse instead of running the jars from the terminal?","body":"<p>I am using Eclipse to build spark applications and every time I need to export the jar and run it from the shell to test the application. I am using a VM running CDH5.5.2 quick start vm in it. I have my eclipse installed in my windows (Host) and I create spark applications which is then exported as Jar file from Eclipse and copied over to Linux(Guest) and then, I run the spark application using spark-submit. This is very annoying sometimes because if you miss something in your program and the build was successful, the application will fail to execute and I need to fix the code and again export the Jar to run and so on. I am wondering if there is a much simpler way to run the job right from eclipse(Please note that I don't want to run spark in local mode) where the input file will be in HDFS? Is this a better way of doing? What are the Industry standards that are followed to develop. test and deploying spark applications in Production?</p>","tags":["Spark"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-13 05:35:43.0","id":56155,"title":"Error in metron dashboard deployment","body":"<p>Hi. I am doing full-dev-platfrom of Metron and have reached the stage where kibana dashboard is getting deployed but am getting the following error:</p><p>: [node1]: FAILED! =&gt; {\"changed\": true, \"cmd\": \"elasticdump --output=http://node1:9200/.kibana --input=/tmp/kibana-index.json\", \"delta\": \"0:00:00.494524\", \"end\": \"2016-09-13 05:13:18.150559\", \"failed\": true, \"rc\": 1, \"start\": \"2016-09-13 05:13:17.656035\", \"stderr\": \"Tue, 13 Sep 2016 05:13:18 GMT | Error Emitted =&gt; Cannot read property 'body' of undefined\", \"stdout\": \"Tue, 13 Sep 2016 05:13:18 GMT | starting dump\\nTue, 13 Sep 2016 05:13:18 GMT | got 34 objects from source file (offset: 0)\\nTue, 13 Sep 2016 05:13:18 GMT | Total Writes: 0\\nTue, 13 Sep 2016 05:13:18 GMT | dump ended with error (set phase)  =&gt; TypeError: Cannot read property 'body' of undefined\", \"stdout_lines\": [\"Tue, 13 Sep 2016 05:13:18 GMT | starting dump\", \"Tue, 13 Sep 2016 05:13:18 GMT | got 34 objects from source file (offset: 0)\", \"Tue, 13 Sep 2016 05:13:18 GMT | Total Writes: 0\", \"Tue, 13 Sep 2016 05:13:18 GMT | dump ended with error (set phase)  =&gt; TypeError: Cannot read property 'body' of undefined\"], \"warnings\": []}</p><p>Please inform how to fix this.</p>","tags":["Metron","dashboard","installation"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-09-13 11:18:34.0","id":56206,"title":"Unable to import data to Hbase using nifi through puthbaseshell.","body":"<p>Unable to import data to Hbase using nifi through puthbaseshell.. geting the following issue </p><p>\"Habse due to org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action\norg.apache.hadoop.hbase.security.<strong>AccessDeniedException</strong>\n org.apache.hadoop.hbase.security.access.AccessController.prePut(AccessController java:1642)\" </p><p>we already checked in hbase shell that particular user(nifi/HDF@XXX.COM) have complete authority </p><pre>hbase(main):009:0&gt; user_permission  'employee_1'\nUser                  Namespace,Table,Family,Qualifier:Permission\nnifi/HDF@XXX.COM     default,employee_1,personalInfo,: [Permission: actions=READ,WRITE,CREATE,EXEC,ADMIN]</pre><p>Please find the attached Ni-fi_issue screen shot for reference. Please suggest,</p>","tags":["nifi-streaming","nifi-processor","nifi-templates","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-14 02:59:32.0","id":56366,"title":"Time synchronization issue among nodes that cause HBase failure","body":"<p>I have installed HDP 2.5 on 3 nodes in Ubuntu 14, HBase Master is on Node #1 and HBase client was installed on #2 and #3, but I found during automated installation the Ntp server was installed on all of the nodes. This may cause the time not to be synched on each node since they are not getting nptdate from the same place - each node has its own nptserver. Is this true? </p><p>When I run ntpdate it will prompt to me that the NTP is in use. </p><p>Can you please explain why need the ntpserver on the node where I installed HBase client? Any workaround for the time synchnization?</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-14 07:11:37.0","id":56401,"title":"Hue install on HDP 2.4","body":"<p>I have installed Hue on HDP2.4 using yum and below is the status it show:-</p><p>[root@sandbox Mukesh]# /etc/init.d/hue status\nsupervisor (pid  253395) is running...</p><p>But when try to invoke UI using URL \"<a href=\"http://127.0.0.1:8000/\">http://127.0.0.1:8000/</a>\" i amd getting below error:-</p><p>Error screen shot with the attachment. Is this expected or i am missing valid port number for Hue. Please Help!!</p><pre>NotImplementedError at /accounts/login/\nRequest Method:GETRequest URL:http://10.51.0.24:8000/accounts/login/?next=/about/Django Version:1.2.3Exception Type:NotImplementedErrorException Value:Exception Location:/usr/lib/hue/build/env/lib/python2.6/site-packages/Django-1.2.3-py2.6.egg/django/contrib/auth/models.py in save, line 427Python Executable:/usr/bin/python2.6Python Version:2.6.6</pre><p><img src=\"/storage/attachments/7622-hue-error.png\"></p><p>.</p>","tags":["hue"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-14 13:39:03.0","id":56487,"title":"Tableau and Spark Thrift Server Integration","body":"<p>I have been successful in integrating Tableau with Spark Thrift server using Samba ODBC. I have tried using the cache table during the initial SQL and the performance has been great till now. I am now looking for a way to cache and un cache few of the frequently used tables when they are updated using through our data pipelines.</p><p>The challlenge that I am facing is that the cache table done via Tableau will remain in cache through the lifetime of the thrift server but when I write my data pipleline process and submit spark jobs it will use a different spark context.</p><p>Can anyone please suggest how can I connect to the thrift server context through the backend process.</p><p>1. Is there a way to re-use the thrift services from spark-submit or spark shell?</p><p>2. At the end of my data pipeline will it be a good idea to invoke a simple shell script that will connect to the thrift service and refresh the cache?</p><p>Note both my backend and the BI tool are using the same cluster as I have used the same yarn cluster while starting the thrift service as well as submitting the backend jobs.</p><p>Thanks,</p><p>Jayadeep</p>","tags":["thrift","tableau","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-09-15 08:28:22.0","id":56683,"title":"Sqoop : Compression not working for teradata manager","body":"<p>I am trying to import data from teradata to HDFS using both teradata manager and jdbc driver . Using jdbc driver it is working fine but for teradata manager it is not working as expected. I am not getting any error. Below is the sqoop commands.</p><p><strong>Using JDBC Driver:</strong></p><pre>sqoop import --driver com.teradata.jdbc.TeraDriver --connect jdbc:teradata://**.***.***.***/DATABASE=****** --username ****** --password **** --table mytable --target-dir /user/aps/test87 --compress -m 1</pre><p>Output:</p><p>-rw-r--r--   3 ***** hdfs          0 2016-09-15 13:45 /user/aps/test87/_SUCCESS </p><p>-rw-r--r--   3 ***** hdfs         38 2016-09-15 13:45 /user/aps/test87/<strong>part-m-00000.gz</strong></p><p><strong>Using Teradata Manager :</strong></p><pre>sqoop import --connection-manager org.apache.sqoop.teradata.TeradataConnManager --connect jdbc:teradata://**.***.***.***/DATABASE=****** --username ****** --password **** --table mytable --target-dir /user/aps/test88 --compress -m 1</pre><p>Output:</p><p>-rw-r--r--   3 ****** hdfs          0 2016-09-15 13:46 /user/aps/test88/_SUCCESS </p><p>-rw-r--r--   3 ****** hdfs         18 2016-09-15 13:46 /user/aps/test88/<strong>part-m-00000</strong></p><p><strong>\n</strong></p><p>For Teradata Manager output should be .gz file. Am I doing something wrong. Please help.</p><p>I am facing same issue for snappy, parquet, BZip2, avro . Please help asap.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-16 10:11:32.0","id":56890,"title":"How to load data from HDFS to teradata using java api","body":"","tags":["teradata","hadoop","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-17 17:46:53.0","id":57070,"title":"While running talend job it is requesting for [ERROR]: org.apache.hadoop.util.Shell - Failed to locate the winutils binary in the hadoop binary path and also asking for jar file (hadoop-conf-hortonworks.jar ) . Please help on this.","body":"","tags":["HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-17 09:24:54.0","id":57049,"title":"hbase table copy from one cluster to other","body":"<p>Is there a way to copy hbase( phoenix) tables from one cluster to the other. If  so can anyone tell what is the best option?</p>","tags":["Phoenix","Hbase","administration"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-19 14:38:02.0","id":57234,"title":"clients in alert state on master node","body":"<p>Hi all,</p><p>I have one master node with all the main components installed. Also all the client libraries are installed there but in failed state as you can see on the picture:</p><p><img src=\"/storage/attachments/7759-master-clients.png\"></p><p>I am not using this master node as a client or \"edge node\" but I would like to know why are in this state.</p><p>Thanks and regards,</p>","tags":["client-tools","client"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-09-20 10:30:38.0","id":57439,"title":"Password Encryption in Sqoop Job not working","body":"<p>I am using password encryption method in Sqoop job for data ingestion into Hadoop. Used Dhadoop.security.credential.provider.path to encrypt the password.</p><p>But when I try to create the Sqoop job in CLI, it is unable to parse the arguments. Below is the code I used and error I got also mentioned below.</p><p></p><p><strong>CODE</strong></p><p>sqoop job --create password-test --meta-connect jdbc:hsqldb:hsql://&lt;hostname&gt;:&lt;port&gt;/sqoop -- import \\\n-Dhadoop.security.credential.provider.path=jceks://hdfs/user/&lt;username&gt;/&lt;username&gt;.password.jceks \\\n--connect \"jdbc:oracle:thin:&lt;hostname&gt;:&lt;Port&gt;:&lt;sid&gt;\" \\\n--username &lt;username&gt; \\\n--table &lt;tablename&gt; \\\n--password-alias &lt;password-alias-name&gt; \\\n--fields-terminated-by '\\001' \\\n--null-string '\\\\N' \\\n--null-non-string '\\\\N'  \\\n--lines-terminated-by '\\n' \\\n--target-dir '/user/&lt;username&gt;/&lt;staging loc&gt;' \\\n--incremental append \\\n--check-column &lt;colname&gt; \\\n--last-value &lt;value&gt; \\\n--num-mappers 8</p><p>ERROR</p><p></p><p><img src=\"/storage/attachments/7819-119ti.png\"></p>","tags":["encryption","password","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-21 19:05:43.0","id":57761,"title":"Nifi UI configured to be accessed over  HTTPS not displayed in Internet Explorer","body":"<p>I installed a Nifi 1.0.0 instance and configured the User Interface to be accessed over HTTPS. </p><p>I installed the client certificate in IE and Firefox. </p><p>Everything works fine with Firefox when connecting to https://host1:9443/nifi/.</p><p>When I try to connect using IE, the tab is correctly called \"Nifi\" but the Canvas is not displayed. Instead, a simple blue screen is displayed. </p><p>Have someone else had the same issue ? </p><p>Thanks and regards</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-22 02:13:43.0","id":57805,"title":"how to compile sqoop source?","body":"<p>https://github.com/apache/sqoop</p><p>I got sqoop source from github.According to the compile.txt in the source file,I use ant to compile it.I didn't change anything of the source.When I used it to finish hiveimport from mysql,there is nothing in hive.</p><p>Here is a screenshot when the sqoop it ran.</p><p><img src=\"/storage/attachments/7890-sqoop147-wuqi.png\"></p><p>We can find it didn't tell us the hiveimport complete.</p>","tags":["Sqoop","help"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-24 08:49:11.0","id":58209,"title":"USYD毕业证[澳洲真实毕业证Q微信956290760]办理澳洲悉尼大学USYD毕业证成绩单学历认证文凭The University of Sydney","body":"<p>QQ/微信956290760办理澳洲文凭|澳洲毕业证|澳洲学历认证,澳洲成绩单 澳洲offer,教育部学历认证及使馆认证永久可查 |国外毕业证|国外学历认证|国外学历文凭证书\nCQU毕业证，UOW毕业证，MQ毕业证，UTS毕业证，US毕业证，UNSW毕业证，ADELAIDE毕业证，RMIT毕业证，USQ毕业证\n专业为留学生办理毕业证、成绩单、使馆留学回国人员证明、教育部学历学位认证、录取通知书、Offer、在读证明、雅思托福成绩单、网上存档永久可查！\nQ956290760微信：q956290760办理澳洲毕业证、成绩单、使馆认证、真实教育部认证.永久可查\nQQ:956290760               微信：q956290760\n这些单位是不查询毕业证真伪的，而且国内没有渠道去查询国外文凭的真假，也不需要提供真实教育部认证。其他私营、外企企业，无需提供！办理教育部认证所需资料众多且烦琐，所有材料您都必须提供原件，我们凭借丰富的经验，帮您快速整合材料，让您少走弯路。事业性用人单位如银行，国企，公务员，在您应聘时都会需要您提供这个认证。鉴于此，办理一份毕业证成绩单即可\n★毕业证、成绩单等全套材料，从防伪到印刷，从水印到钢印烫金，与学校100%相同\n面向澳洲留学生提供以下服务：\n二、回国进私企、外企、自己做生意的情况\n★真实教育部认证，教育部存档，教育部留服网站永久可查\n二：教育部认证的用途：\n诚招代理：本公司诚聘当地合作代理人员，如果你有业余时间，有兴趣就请联系我们。专业面向“英国、加拿大、意大利，澳洲、新西兰、美国 ”等国的学历学位真实教育部认证、使馆认证。\n办理一份毕业证成绩单，递交材料到教育部，办理真实教育部认证\n★毕业证、成绩单等全套材料，从防伪到印刷，从水印到钢印烫金，与学校100%相同\n一、工作未确定，回国需先给父母、亲戚朋友看下文凭的情况\n一：回国证明的用途：\n★真实使馆认证,留学人员回国证明,使馆存档可通过大使馆查询确认.\n面向澳洲留学生提供以下服务：\n特别关注：【业务选择办理准则】\n联系人gavin  QQ: 956290760   微信：q956290760\n★真实教育部认证，教育部存档，教育部留服网站永久可查】\n ★真实留信认证，留信网入库存档，永久可查】\n如果您计划在国内发展，那么办理国内教育部认证是必不可少的</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-26 13:21:21.0","id":58404,"title":"How to debug MapReduce job ?","body":"<p>I am trying execute one MapReduce job in java but it is getting stuck at the middle and finally it was timed out. Below is the log</p><pre>16/09/26 18:46:42 INFO mapreduce.Job: Running job: job_1474692614849_0070\n16/09/26 18:46:50 INFO mapreduce.Job: Job job_1474692614849_0070 running in uber mode : false\n16/09/26 18:46:50 INFO mapreduce.Job:  map 0% reduce 0%\n16/09/26 18:47:01 INFO mapreduce.Job:  map 33% reduce 0%\n16/09/26 18:52:19 INFO mapreduce.Job: Task Id : attempt_1474692614849_0070_m_000000_0, Status : FAILED\nAttemptID:attempt_1474692614849_0070_m_000000_0 Timed out after 300 secs\n16/09/26 18:52:20 INFO mapreduce.Job:  map 0% reduce 0%\n16/09/26 18:52:30 INFO mapreduce.Job:  map 33% reduce 0%\n16/09/26 18:57:49 INFO mapreduce.Job: Task Id : attempt_1474692614849_0070_m_000000_1, Status : FAILED\nAttemptID:attempt_1474692614849_0070_m_000000_1 Timed out after 300 secs\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n\n16/09/26 18:57:50 INFO mapreduce.Job:  map 0% reduce 0%\n16/09/26 18:57:59 INFO mapreduce.Job:  map 33% reduce 0%\n16/09/26 19:03:19 INFO mapreduce.Job: Task Id : attempt_1474692614849_0070_m_000000_2, Status : FAILED\nAttemptID:attempt_1474692614849_0070_m_000000_2 Timed out after 300 secs\n16/09/26 19:03:20 INFO mapreduce.Job:  map 0% reduce 0%\n16/09/26 19:03:31 INFO mapreduce.Job:  map 33% reduce 0%\n16/09/26 19:08:50 INFO mapreduce.Job:  map 100% reduce 100%\n16/09/26 19:08:50 INFO mapreduce.Job: Job job_1474692614849_0070 failed with state FAILED due to: Task failed task_1474692614849_0070_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n\n16/09/26 19:08:50 INFO mapreduce.Job: Counters: 13\n        Job Counters\n                Failed map tasks=4\n                Killed reduce tasks=1\n                Launched map tasks=4\n                Other local map tasks=3\n                Rack-local map tasks=1\n                Total time spent by all maps in occupied slots (ms)=1311303\n                Total time spent by all reduces in occupied slots (ms)=0\n                Total time spent by all map tasks (ms)=1311303\n                Total time spent by all reduce tasks (ms)=0\n                Total vcore-seconds taken by all map tasks=1311303\n                Total vcore-seconds taken by all reduce tasks=0\n                Total megabyte-seconds taken by all map tasks=2685548544\n                Total megabyte-seconds taken by all reduce tasks=0\n\n\n</pre><p>Is there any way to debug this mapreduce job ? Please help.</p>","tags":["java","MapReduce","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-27 04:58:29.0","id":58531,"title":"Hive ACID performance testing?","body":"<p>Does TPC-DS test hive acid performance?  if not what tool do you suggest I use to benchmark my cluster for hive updates.</p>","tags":["performance","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-26 14:31:48.0","id":58421,"title":"Hive UDF issue","body":"<p>Hi, \n\nI've created a Hive UDF out of a number of jar files using HiveServer2. One of these jar files (on which I made the UDF) leverages a shared object (.so file) which is copied across all Hadoop nodes in the same path. Let's name it \"library.so\". \nThe Hive UDF works fine at the beginning, however it stops working after a few calls with this error message at bottom of the stack the admin gets: \n\n\"Caused\nby: java.lang.UnsatisfiedLinkError: Native Library /opt/library.so\nalready loaded in another classloader\" \n\nTo make the UDF work agian, I restart HiveServer2, this would temporarily fix the issue, however, I get the same exception again after a few times I call the UDF. </p><p>\nP.S The jar files on which I make the Hive UDF already exist at hive/lib directory of all nodes running HiveServer2.</p>","tags":["Hive","hiveserver2","hive-udf"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-27 20:28:08.0","id":58719,"title":"How to create S3 triggers in Apache Nifi","body":"<p>Hi all,</p><p>I have a scenario where I want to trigger a signal for a flow to start processing whenever there is some data available on S3 to process. In such a scenario, all processors will be EventDriven (except for the trigger), and only run if there is any data to process (or somehow we trigger them to start processing).</p><p>Scenario:</p><p>- Whenever a file(or files) lands on S3, launch SQL queries (create table, copy data etc)</p><p>So, what would be a good way for defining such a trigger?   </p><p>Thanks</p>","tags":["Nifi","s3"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-28 10:55:29.0","id":58831,"title":"hadoop.tmp.dir issue.","body":"<p>HI I am using HDP 2.1 and Ambari 1.7.</p><p>I have added the property to core-site.xml. But property is not working properly.</p><p>Nedded urgent help</p><p>Thanks in advance.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-29 06:34:17.0","id":58972,"title":"FireEye topology not parsing","body":"<p>I am trying to parse the following FireEye log-</p><p>CEF:0|FireEye|MPS|6.1.0.69991|MC|malware-callback|9|src=195.2.252.157 spt=80 smac=00:0d:66:4d:fc:00 rt=May 08 2016 14:24:45 dst=128.12.95.64 dpt=0 dmac=00:18:74:1c:a1:80 cn1Label=vlan cn1=0 cn2Label=sid cn2=33331600 cs1Label=sname cs1=Trojan.Piptea.2 msg=https://mil.fireeye.com/edp.php?sname\\=Trojan.Piptea.2 cs4Label=link cs4=https://172.16.127.7/event_stream/events?event_id\\=111 cs5Label=ccName cs5=195.2.252.157 cn3Label=ccPort cn3=80 proto=tcp shost=rescomp-09-149735.Stanford.EDU dvcHost=mslms dvc=172.16.127.7 externalId=111 \nCEF:0|FireEye|MPS|5.1.0.55701|WI|web-infection|9|src=3.0.0.0 spt=0 smac=00:00:00:00:00:00 dproc=InternetExplorer 6.0 rt=May 05 2016 12:36:22 dst=64.22.138.10 dpt=555 dmac=92:73:75:00:00:35 cs2Label=anomaly cs2=anomaly-tag misc-anomaly cn2Label=sid cn2=0 msg=https://mil.fireeye.com/edp.php?sname\\=Exploit.Browser cs4Label=link cs4=https://172.16.127.7/event_stream/events?event_id\\=15 fileType=text/html request=vip2.51.la/go.asp?we\\=a-free-service-forwebmasters& svid\\=22&id\\=1153797&tpages\\=1&ttimes\\=1&tzone\\=- 8&tcolor\\=24&ssize\\=800,600&referrer\\=http%3a//88.88 cs1Label=sname cs1=Exploit.Browser shost=web155.discountasp.net dvcHost=mslms dvc=172.16.127.7 externalId=3 \nCEF:0|FireEye|MPS|6.1.0.69991|MO|malware-object|9|src=195.2.252.153 spt=880 smac=00:0d:66:4d:fc:00 rt=May 10 2016 11:09:31 dst=128.12.95.64 dpt=0 dmac=00:18:74:1c:a1:80 cs2Label=anomaly cs2=anomaly-tag misc-anomaly cn1Label=vlan cn1=0 cn2Label=sid cn2=33331724 cs1Label=sname cs1=Trojan.Piptea.2 msg=https://mil.fireeye.com/edp.php?sname\\=Trojan.Piptea.2 cs4Label=link cs4=https://172.16.127.7/event_stream/events?event_id\\=254 cs5Label=ccName cs5=ahohonline.com cn3Label=ccPort cn3=80 proto=tcp cs6Label=ccChannel cs6=GET /ufwnltbz/evmhfzlfe.php?id\\=1812198572&p\\=1 HTTP/1.1::~~User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; InfoPath.1)ver52::~~Host: ahohonline.com::~~::~~ shost=rescomp-09-149735.Stanford.EDU dvcHost=mslms dvc=172.16.127.7 externalId=224  \nCEF:0|FireEye|CMS|7.6.0.334042|WI|web-infection|4|rt=May 25 2016 22:07:50 UTC src=192.168.1.1 dproc=InternetExplorer 7.0 cs3Label=osinfo cs3=Microsoft WindowsXP 32-bit 5.1 sp3 15.0210 filePath=xxx.xx.x.xx:xxxx/metasploit dvchost=axhwmps dvc=192.168.5.6 smac=00:0c:29:d9:2e:e1 cn1Label=vlan cn1=0 externalId=11646 cs4Label=link cs4=https://www.fireeye.com/event_stream/events_for_bot?inc_id\\=11646 act=notified cs2Label=anomaly cs2=misc-anomaly cs1Label=sname cs1=Malware.Binary.url </p><p>The Exception log from Storm UI is as below -</p><pre>2016-09-29 06:13:25.115 STDIO [INFO] {\"original_string\":\"CEF:0|FireEye|MPS|6.1.0.69991|MC|malware-callback|9|src=195.2.252.157 spt=80 smac=00:0d:66:4d:fc:00 rt=May 08 2016 14:24:45 dst=128.12.95.64 dpt=0 dmac=00:18:74:1c:a1:80 cn1Label=vlan cn1=0 cn2Label=sid cn2=33331600 cs1Label=sname cs1=Trojan.Piptea.2 msg=https:\\/\\/mil.fireeye.com\\/edp.php?sname\\\\=Trojan.Piptea.2 cs4Label=link cs4=https:\\/\\/172.16.127.7\\/event_stream\\/events?event_id\\\\=111 cs5Label=ccName cs5=195.2.252.157 cn3Label=ccPort cn3=80 proto=tcp shost=rescomp-09-149735.Stanford.EDU dvcHost=mslms dvc=172.16.127.7 externalId=111 \\n \\nCEF:0|FireEye|MPS|5.1.0.55701|WI|web-infection|9|src=3.0.0.0 spt=0 smac=00:00:00:00:00:00 dproc=InternetExplorer 6.0 rt=May 05 2016 12:36:22 dst=64.22.138.10 dpt=555 dmac=92:73:75:00:00:35 cs2Label=anomaly cs2=anomaly-tag misc-anomaly cn2Label=sid cn2=0 msg=https:\\/\\/mil.fireeye.com\\/edp.php?sname\\\\=Exploit.Browser cs4Label=link cs4=https:\\/\\/172.16.127.7\\/event_stream\\/events?event_id\\\\=15 fileType=text\\/html request=vip2.51.la\\/go.asp?we\\\\=a-free-service-forwebmasters& svid\\\\=22&id\\\\=1153797&tpages\\\\=1&ttimes\\\\=1&tzone\\\\=- 8&tcolor\\\\=24&ssize\\\\=800,600&referrer\\\\=http%3a\\/\\/88.88 cs1Label=sname cs1=Exploit.Browser shost=web155.discountasp.net dvcHost=mslms dvc=172.16.127.7 externalId=3 \\n\\nCEF:0|FireEye|MPS|6.1.0.69991|MO|malware-object|9|src=195.2.252.153 spt=880 smac=00:0d:66:4d:fc:00 rt=May 10 2016 11:09:31 dst=128.12.95.64 dpt=0 dmac=00:18:74:1c:a1:80 cs2Label=anomaly cs2=anomaly-tag misc-anomaly cn1Label=vlan cn1=0 cn2Label=sid cn2=33331724 cs1Label=sname cs1=Trojan.Piptea.2 msg=https:\\/\\/mil.fireeye.com\\/edp.php?sname\\\\=Trojan.Piptea.2 cs4Label=link cs4=https:\\/\\/172.16.127.7\\/event_stream\\/events?event_id\\\\=254 cs5Label=ccName cs5=ahohonline.com cn3Label=ccPort cn3=80 proto=tcp cs6Label=ccChannel cs6=GET \\/ufwnltbz\\/evmhfzlfe.php?id\\\\=1812198572&p\\\\=1 HTTP\\/1.1::~~User-Agent: Mozilla\\/4.0 (compatible; MSIE 7.0; Windows NT 5.1; InfoPath.1)ver52::~~Host: ahohonline.com::~~::~~ shost=rescomp-09-149735.Stanford.EDU dvcHost=mslms dvc=172.16.127.7 externalId=224  \\n\\nCEF:0|FireEye|CMS|7.6.0.334042|WI|web-infection|4|rt=May 25 2016 22:07:50 UTC src=192.168.1.1 dproc=InternetExplorer 7.0 cs3Label=osinfo cs3=Microsoft WindowsXP 32-bit 5.1 sp3 15.0210 filePath=xxx.xx.x.xx:xxxx\\/metasploit dvchost=axhwmps dvc=192.168.5.6 smac=00:0c:29:d9:2e:e1 cn1Label=vlan cn1=0 externalId=11646 cs4Label=link cs4=https:\\/\\/www.fireeye.com\\/event_stream\\/events_for_bot?inc_id\\\\=11646 act=notified cs2Label=anomaly cs2=misc-anomaly cs1Label=sname cs1=Malware.Binary.url \\n\"}\n2016-09-29 06:13:25.125 o.a.m.p.f.BasicFireEyeParser [WARN] Unable to find timestamp in message: CEF:0|FireEye|MPS|6.1.0.69991|MC|malware-callback|9|src=195.2.252.157 spt=80 smac=00:0d:66:4d:fc:00 rt=May 08 2016 14:24:45 dst=128.12.95.64 dpt=0 dmac=00:18:74:1c:a1:80 cn1Label=vlan cn1=0 cn2Label=sid cn2=33331600 cs1Label=sname cs1=Trojan.Piptea.2 msg=https://mil.fireeye.com/edp.php?sname\\=Trojan.Piptea.2 cs4Label=link cs4=https://172.16.127.7/event_stream/events?event_id\\=111 cs5Label=ccName cs5=195.2.252.157 cn3Label=ccPort cn3=80 proto=tcp shost=rescomp-09-149735.Stanford.EDU dvcHost=mslms dvc=172.16.127.7 externalId=111 \n \nCEF:0|FireEye|MPS|5.1.0.55701|WI|web-infection|9|src=3.0.0.0 spt=0 smac=00:00:00:00:00:00 dproc=InternetExplorer 6.0 rt=May 05 2016 12:36:22 dst=64.22.138.10 dpt=555 dmac=92:73:75:00:00:35 cs2Label=anomaly cs2=anomaly-tag misc-anomaly cn2Label=sid cn2=0 msg=https://mil.fireeye.com/edp.php?sname\\=Exploit.Browser cs4Label=link cs4=https://172.16.127.7/event_stream/events?event_id\\=15 fileType=text/html request=vip2.51.la/go.asp?we\\=a-free-service-forwebmasters& svid\\=22&id\\=1153797&tpages\\=1&ttimes\\=1&tzone\\=- 8&tcolor\\=24&ssize\\=800,600&referrer\\=http%3a//88.88 cs1Label=sname cs1=Exploit.Browser shost=web155.discountasp.net dvcHost=mslms dvc=172.16.127.7 externalId=3 \n\nCEF:0|FireEye|MPS|6.1.0.69991|MO|malware-object|9|src=195.2.252.153 spt=880 smac=00:0d:66:4d:fc:00 rt=May 10 2016 11:09:31 dst=128.12.95.64 dpt=0 dmac=00:18:74:1c:a1:80 cs2Label=anomaly cs2=anomaly-tag misc-anomaly cn1Label=vlan cn1=0 cn2Label=sid cn2=33331724 cs1Label=sname cs1=Trojan.Piptea.2 msg=https://mil.fireeye.com/edp.php?sname\\=Trojan.Piptea.2 cs4Label=link cs4=https://172.16.127.7/event_stream/events?event_id\\=254 cs5Label=ccName cs5=ahohonline.com cn3Label=ccPort cn3=80 proto=tcp cs6Label=ccChannel cs6=GET /ufwnltbz/evmhfzlfe.php?id\\=1812198572&p\\=1 HTTP/1.1::~~User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; InfoPath.1)ver52::~~Host: ahohonline.com::~~::~~ shost=rescomp-09-149735.Stanford.EDU dvcHost=mslms dvc=172.16.127.7 externalId=224  \n\nCEF:0|FireEye|CMS|7.6.0.334042|WI|web-infection|4|rt=May 25 2016 22:07:50 UTC src=192.168.1.1 dproc=InternetExplorer 7.0 cs3Label=osinfo cs3=Microsoft WindowsXP 32-bit 5.1 sp3 15.0210 filePath=xxx.xx.x.xx:xxxx/metasploit dvchost=axhwmps dvc=192.168.5.6 smac=00:0c:29:d9:2e:e1 cn1Label=vlan cn1=0 externalId=11646 cs4Label=link cs4=https://www.fireeye.com/event_stream/events_for_bot?inc_id\\=11646 act=notified cs2Label=anomaly cs2=misc-anomaly cs1Label=sname cs1=Malware.Binary.url \n\n2016-09-29 06:13:25.126 STDIO [ERROR] java.lang.NullPointerException\n2016-09-29 06:13:25.126 STDIO [ERROR] at org.apache.metron.parsers.utils.ParserUtils.convertToEpoch(ParserUtils.java:51)\n2016-09-29 06:13:25.126 STDIO [ERROR] at org.apache.metron.parsers.fireeye.BasicFireEyeParser.getTimeStamp(BasicFireEyeParser.java:122)\n2016-09-29 06:13:25.126 STDIO [ERROR] at org.apache.metron.parsers.fireeye.BasicFireEyeParser.parse(BasicFireEyeParser.java:97)\n2016-09-29 06:13:25.126 STDIO [ERROR] at org.apache.metron.parsers.bolt.ParserBolt.execute(ParserBolt.java:70)\n2016-09-29 06:13:25.126 STDIO [ERROR] at backtype.storm.daemon.executor$fn__5495$tuple_action_fn__5497.invoke(executor.clj:670)\n2016-09-29 06:13:25.126 STDIO [ERROR] at backtype.storm.daemon.executor$mk_task_receiver$fn__5418.invoke(executor.clj:426) </pre><p>The BasicFireEyeParser is not able to find out the Timestamp properly and hence failing to process.</p><p>What is the remedy for this problem?</p>","tags":["Storm","parsers","logs"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-30 05:45:01.0","id":59212,"title":"Does HDP2.5 sandbox work on vmware player on windows?","body":"<p>Hi,</p><p>has this support changed with HDP2.5? Or can wait for some more time?</p><p>Thanks,</p><p>Avijeet</p>","tags":["vmware","hdp2.5"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-10-02 15:06:14.0","id":59409,"title":"Keep displaying stack when starting docker image of HDP-2.5","body":"<p>Hello all,</p><p>I am using HDP-2.5 on Sandbox starting it with docker.</p><p>I keep receiving strange stack when starting the application. I am unable to know if it is important for the moment...</p><pre>sam@sam-dell:~$ sudo ./dev/sandbox-docker/sandbox-start.sh \n[sudo] password for sam: \nWaiting for docker daemon to start up:\na903e2dc7993        sandbox             \"/usr/sbin/sshd -D\"   8 days ago          Exited (1) 44 hours ago   0.0.0.0:1000-&gt;1000/tcp, 0.0.0.0:1100-&gt;1100/tcp, 0.0.0.0:1220-&gt;1220/tcp, 0.0.0.0:1988-&gt;1988/tcp, 0.0.0.0:2100-&gt;2100/tcp, 0.0.0.0:4040-&gt;4040/tcp, 0.0.0.0:4200-&gt;4200/tcp, 0.0.0.0:5007-&gt;5007/tcp, 0.0.0.0:5011-&gt;5011/tcp, 0.0.0.0:6001-&gt;6001/tcp, 0.0.0.0:6003-&gt;6003/tcp, 0.0.0.0:6008-&gt;6008/tcp, 0.0.0.0:6080-&gt;6080/tcp, 0.0.0.0:6188-&gt;6188/tcp, 0.0.0.0:8000-&gt;8000/tcp, 0.0.0.0:8005-&gt;8005/tcp, 0.0.0.0:8020-&gt;8020/tcp, 0.0.0.0:8040-&gt;8040/tcp, 0.0.0.0:8042-&gt;8042/tcp, 0.0.0.0:8050-&gt;8050/tcp, 0.0.0.0:8080-&gt;8080/tcp, 0.0.0.0:8082-&gt;8082/tcp, 0.0.0.0:8086-&gt;8086/tcp, 0.0.0.0:8088-&gt;8088/tcp, 0.0.0.0:8090-8091-&gt;8090-8091/tcp, 0.0.0.0:8188-&gt;8188/tcp, 0.0.0.0:8443-&gt;8443/tcp, 0.0.0.0:8744-&gt;8744/tcp, 0.0.0.0:8765-&gt;8765/tcp, 0.0.0.0:8886-&gt;8886/tcp, 0.0.0.0:8888-8889-&gt;8888-8889/tcp, 0.0.0.0:8983-&gt;8983/tcp, 0.0.0.0:8993-&gt;8993/tcp, 0.0.0.0:9000-&gt;9000/tcp, 0.0.0.0:9090-&gt;9090/tcp, 0.0.0.0:9995-9996-&gt;9995-9996/tcp, 0.0.0.0:10000-10001-&gt;10000-10001/tcp, 0.0.0.0:10500-&gt;10500/tcp, 0.0.0.0:11000-&gt;11000/tcp, 0.0.0.0:15000-&gt;15000/tcp, 0.0.0.0:16010-&gt;16010/tcp, 0.0.0.0:16030-&gt;16030/tcp, 0.0.0.0:18080-&gt;18080/tcp, 0.0.0.0:19888-&gt;19888/tcp, 0.0.0.0:21000-&gt;21000/tcp, 0.0.0.0:42111-&gt;42111/tcp, 0.0.0.0:50070-&gt;50070/tcp, 0.0.0.0:50075-&gt;50075/tcp, 0.0.0.0:50095-&gt;50095/tcp, 0.0.0.0:50111-&gt;50111/tcp, 0.0.0.0:60000-&gt;60000/tcp, 0.0.0.0:60080-&gt;60080/tcp, 0.0.0.0:2222-&gt;22/tcp   sandbox\nsandbox\nStarting Flume                                            [  OK  ]\nStarting Postgre SQL                                      [  OK  ]\nStarting mysql                                            [  OK  ]\nStarting Ranger-admin                                     [  OK  ]\nStarting data node                                        [  OK  ]\nStarting name node                                        [  OK  ]\nStarting Ranger-usersync                                  [  OK  ]\nStarting Zookeeper nodes                                  [  OK  ]\n16/10/02 06:20:05 WARN ipc.Client: Failed to connect to server: sandbox.hortonworks.com/172.17.0.2:8020: try once and fail.\njava.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n    at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:650)\n    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:745)\n    at org.apache.hadoop.ipc.Client$Connection.access$3200(Client.java:397)\n    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1618)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1449)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1396)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n    at com.sun.proxy.$Proxy10.setSafeMode(Unknown Source)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setSafeMode(ClientNamenodeProtocolTranslatorPB.java:711)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)\n    at com.sun.proxy.$Proxy11.setSafeMode(Unknown Source)\n    at org.apache.hadoop.hdfs.DFSClient.setSafeMode(DFSClient.java:2657)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1340)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1324)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.setSafeMode(DFSAdmin.java:611)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:1916)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2107)\n16/10/02 06:20:05 WARN retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.setSafeMode over null. Not retrying because try once and fail.\njava.net.ConnectException: Call From sandbox.hortonworks.com/172.17.0.2 to sandbox.hortonworks.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\n    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\n    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1556)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1496)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1396)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n    at com.sun.proxy.$Proxy10.setSafeMode(Unknown Source)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setSafeMode(ClientNamenodeProtocolTranslatorPB.java:711)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)\n    at com.sun.proxy.$Proxy11.setSafeMode(Unknown Source)\n    at org.apache.hadoop.hdfs.DFSClient.setSafeMode(DFSClient.java:2657)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1340)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1324)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.setSafeMode(DFSAdmin.java:611)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:1916)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2107)\nCaused by: java.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n    at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:650)\n    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:745)\n    at org.apache.hadoop.ipc.Client$Connection.access$3200(Client.java:397)\n    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1618)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1449)\n    ... 20 more\nsafemode: Call From sandbox.hortonworks.com/172.17.0.2 to sandbox.hortonworks.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\nmake: [datanode] Error 255 (ignored)\nStarting NFS portmap                                      [  OK  ]\nStarting Hdfs nfs                                         [  OK  ]\nStarting Hive server                                      [  OK  ]\nStarting Hiveserver2                                      [  OK  ]\nStarting Oozie                                            [  OK  ]\nStarting Yarn history server                              [  OK  ]\nStarting Node manager                                     [  OK  ]\nStarting Webhcat server                                   [  OK  ]\nStarting Spark                                            [  OK  ]\nStarting Resource manager                                 [  OK  ]\n16/10/02 06:20:44 WARN retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.setSafeMode over null. Not retrying because try once and fail.\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): NameNode still not started\n    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkNNStartup(NameNodeRpcServer.java:2057)\n    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setSafeMode(NameNodeRpcServer.java:1172)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setSafeMode(ClientNamenodeProtocolServerSideTranslatorPB.java:747)\n    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)\n\n    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1496)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1396)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n    at com.sun.proxy.$Proxy10.setSafeMode(Unknown Source)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setSafeMode(ClientNamenodeProtocolTranslatorPB.java:711)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)\n    at com.sun.proxy.$Proxy11.setSafeMode(Unknown Source)\n    at org.apache.hadoop.hdfs.DFSClient.setSafeMode(DFSClient.java:2657)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1340)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.setSafeMode(DistributedFileSystem.java:1324)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.setSafeMode(DFSAdmin.java:611)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:1916)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n    at org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2107)\nsafemode: NameNode still not started\nStarting Zeppelin                                         [  OK  ]\nTStarting Ambari server                                    [  OK  ]\nStarting Ambari agent                                     [WARNINGS]\ntput: No value for $TERM and no -T specified\ntput: No value for $TERM and no -T specified\nStarting Mapred history server                            [  OK  ]</pre><p>It seems related to a non started Namenode but at the beginning of the service starting, the name node is reported as started ok. I really don't understand this.</p><p>Thanks for giving me any help</p>","tags":["Sandbox","hdp-2.5.0","docker"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-10-01 19:20:56.0","id":59388,"title":"How to copy the executed comments through PIG  into dataset ?","body":"<p>Hi Dear Sir, \nCould you please let me know how to store executed comments through pig in some data set . \nExample : history -n \nWill give all executed comments. Suppose i want to copy this executed comments into another dataset without copy and paste it manually .</p>","tags":["Pig","executeprocess"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-03 06:09:13.0","id":59447,"title":"Kafka produce count and kafka consumer count is not equal..?","body":"<p>Hi All,</p><p>I am using java producer to produce messages and Scala Consumer to consume messages from producer, with this i am able to produce correct no.of messages from hdfs and unable to consume all the messages , Here is my java producer and consumer code please find attached.<a href=\"/storage/attachments/8189-scalaconsumer.txt\">scalaconsumer.txt</a></p><p>please let me know what is the reason.</p><p>Thanks in advance.</p>","tags":["spark-streaming","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-04 05:54:55.0","id":59673,"title":"When hortonworks release hbase-spark library for hbase? ,When hortonworks release hbase-spark stable version for spark?","body":"<p>When horton works release hbase-spark library for hbase, please reply here.</p>,<p>When hortonworks release stable version of hbase-spark library in hbase, please reply on this thread.</p>","tags":["2.5","2.4"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-05 20:46:46.0","id":60097,"title":"Spark launching in local mode but not on yarn clusrer","body":"<p>Hi,</p><p>I have an issue on a Yarn cluster. I am able to run my application in local mode on the entry/main node in the cluster but when I am launching it on cluster (using client or cluster mode) it just does not start on the cluster. The error is simply that yarn container is not launching with an exit code of -1. What could be the issue? </p><p>Tried many things and surprisingly the configuration on this cluster is same as another independent cluster where the Spark application is running in cluster mode.</p>","tags":["Spark","YARN"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-10-05 19:22:52.0","id":60075,"title":"Is the \"STREAMTABLE\" hint still needed in Hive 0.14?","body":"<p>Does the \"STREAMTABLE\" hint still\nneeded in Hive 0.14 or is it being handled internally as part of hive join\noptimization? Thanks. </p>","tags":["Hive","join"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-06 08:53:36.0","id":60189,"title":"How do I make a configuration parameter not final?","body":"<p>Hi.</p><p>I mistakenly set a parameter to final (please see screenshot below). Is there any way I can override this? TIA!!</p><p><img src=\"/storage/attachments/8303-screenshot-from-2016-10-06-094642.png\"></p>","tags":["configuration","YARN","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-06 08:15:00.0","id":60167,"title":"NIFI : Cluster with Multiple Instance","body":"<p>Hello all,</p><p>I've trying configure two instances NIFI in the same cluster (3 nodes)</p><p>My configuration :</p><p>Instance 1 :</p><pre>-bash-4.1# grep port conf/nifi.properties\nnifi.remote.input.socket.port=10443\nnifi.web.http.port=\nnifi.web.https.port=9443\nnifi.cluster.node.protocol.port=11443\n\n-bash-4.1# more conf/zookeeper.properties\n\nclientPort=2181\n\nserver.1=nifi001:2888:3888\nserver.2=nifi002:2888:3888\nserver.3=nifi003:2888:3888\n\n</pre><p>Instance 2 :</p><pre>-bash-4.1# grep port conf/nifi.properties\nnifi.remote.input.socket.port=10444\nnifi.web.http.port=\nnifi.web.https.port=9444\nnifi.cluster.node.protocol.port=11444\n\n-bash-4.1# more conf/zookeeper.properties\n\nclientPort=2182\n\nserver.1=nifi001:2889:3889\nserver.2=nifi002:2889:3889\nserver.3=nifi003:2889:3889</pre><p>It's missing some things, because my nifi002 and nifi003 can joint the cluster.</p>","tags":["clustering","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-06 08:15:12.0","id":60173,"title":"Hortonworks installation","body":"<p>Hi i am  trying to install hortonworks , but i couldn't been able to do it .And i got a message teeling me to chechthe log file which you may find attached  .I'll be greatfull for an assistance .Thabk youu very much </p>","tags":["windows","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-06 10:57:56.0","id":60216,"title":"Error while installing Hortonworks~Ambari","body":"<p>I am at the following step:\n<a href=\"http://localhost:8080/#/installer/step2\">http://localhost:8080/#/installer/step2</a></p><div>When I enter the host(deepak-HP or localhost) and the SSH private key, it gives me the following error:\n\n****************************************************************************\n****************************************************************************\n****************************************************************************\n\n<em>==========================\nCreating target directory...\n==========================\n\nCommand start time 2016-10-06 14:51:24\n\nPermission denied (publickey,password).\nSSH command execution finished\nhost=localhost, exitcode=255\nCommand end time 2016-10-06 14:51:24\n\nERROR: Bootstrap of host localhost fails because previous action finished with non-zero exit code (255)\nERROR MESSAGE: Permission denied (publickey,password).\n\nSTDOUT: \nPermission denied (publickey,password).</em><em></em><em><div>\n****************************************************************************\n****************************************************************************\n****************************************************************************I tried resolving it by setting up a passwordless SSH, however the error persist. How do I resolve this?</div></em><p></p></div>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-05 21:14:41.0","id":60105,"title":"Using the Ambari API to enable Demo LDAP in Knox","body":"<p>In the Ambari GUI, I can enable and disable the Demo LDAP under service actions on the Knox page.</p><p><img src=\"/storage/attachments/8300-screen-shot-2016-10-05-at-41238-pm.png\"></p><p>I am wonder if it is enable to enable / disable the Demo LDAP using the Ambari API &lt;ambri-ip&gt;:8080/api/v1/clusters/:clustername/</p><p>If it is, what is the endpoint?</p><p>Changing category as this is more of an Ambari related question</p><p>Thanks,</p><p>Eric</p>","tags":["ambari-server","Ambari","Knox","demo","ldap","ambari-api","api"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-06 20:25:19.0","id":60329,"title":"Alerts in Spark Sreaming","body":"<p>I was just wondering if spark streaming can send back enriched events to Kafka or can it send jms alerts directly....I may have a scenario of the users getting alert for any specific anamoly...</p>","tags":["spark-streaming","Spark","jms"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-07 14:49:58.0","id":60456,"title":"getkafka don't working in nifi","body":"<p>When we trying to use getkafka we see following error:</p><p>2016-10-07 17:37:39,469 INFO [pool-24-thread-1-EventThread] org.I0Itec.zkclient.ZkClient zookeeper state changed (Expired)</p><p>2016-10-07 17:37:39,470 INFO [ZkClient-EventThread-465-hdp-name1.lab.croc.ru:2181] k.consumer.ZookeeperConsumerConnector [95446e62-0157-1000-7951-fd4244e9aec2_###############-1475841346967-f0d261ce], exception during rebalance</p><p>kafka.common.KafkaException: Failed to parse the broker info from zookeeper: {\"jmx_port\":-1,\"timestamp\":\"1475501559373\",\"endpoints\":[\"PLAINTEXT://############:6667\"],\"host\":\"#############\",\"version\":3,\"port\":6667}</p><p>next we see:</p><p>Caused by: kafka.common.KafkaException: Unknown version of broker registration. Only versions 1 and 2 are supported.{\"jmx_port\":-1,\"timestamp\":\"1475501559373\",\"endpoints\":[\"PLAINTEXT://#########:6667\"],\"host\":\"##########\",\"version\":3,\"port\":6667}</p><p>Our hdp version is 2.5 and hdf version is 2.0.</p>","tags":["zookeeper","Nifi","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-10 00:22:18.0","id":60694,"title":"Is the DistCp NameNode path the active HDFS NameNode or the active ResourceManager?","body":"<p>When specifying fully-qualified paths to copy data between two HA clusters with DistCp, e.g:</p><pre>hdfs://nn1:8020/foo/bar</pre><p>Is the address of nn1 really referring to the where the active HDFS NameNode is, or is it looking for the active ResourceManager?</p><p>Thanks!</p>","tags":["high-availability","distcp","namenode","resource-manager"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-10-10 11:39:15.0","id":60732,"title":"phoenix python csv utility","body":"<p>i need to install phoenix on machine where hadoop and hbase is not present. want to make use of the phoenix csv bulk loader (python utility). do i need to install anything else apart from python on this machine. and that too any specfic version of python.</p>","tags":["csv","Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-09 15:06:28.0","id":60668,"title":"Empty lines in JSON files causing duplicate records. ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'","body":"<p>Im trying to create a exteral hive database with JsonSerde as follows :</p><p>CREATE EXTERNAL TABLE test1\n(\nuser string\n)\nROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' \nLOCATION '/user/ec2-user/test_data/';</p><p>Problem is I have a input file with empty blank lines between data.</p><p>{\"user\":\"chill1\"}</p><p>\n{\"user\":\"chill2\"}</p><p>\n{\"user\":\"chill3\"}</p><p>\n{\"user\":\"chill4\"}</p><p>\n{\"user\":\"chill5\"}</p><p>If I do count on hive table, I get 9 lines. If I remove lines from source text file then I get the total count as 5. Anybody can help on this one please.</p>","tags":["hive-serde"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-10 15:11:10.0","id":60770,"title":"Kafka:- No partition metadata for topic due to kafka.common.TopicAuthorizationException for topic","body":"<p>Hi all,</p><p>Trying to produce messages but getting the below errors.</p><p>[2016-10-10 20:22:10,947] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: test11 (kafka.producer.async.DefaultEventHandler)\n[2016-10-10 20:22:11,049] WARN Error while fetching metadata [{TopicMetadata for topic test11 -&gt; \nNo partition metadata for topic test11 due to kafka.common.TopicAuthorizationException}] for topic [test11]: class kafka.common.TopicAuthorizationException  (kafka.producer.BrokerPartitionInfo)\n[2016-10-10 20:22:11,051] WARN Error while fetching metadata [{TopicMetadata for topic test11 -&gt; \nNo partition metadata for topic test11 due to kafka.common.TopicAuthorizationException}] for topic [test11]: class kafka.common.TopicAuthorizationException  (kafka.producer.BrokerPartitionInfo)\n[2016-10-10 20:22:11,051] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: test11 (kafka.producer.async.DefaultEventHandler)\n[2016-10-10 20:22:11,153] WARN Error while fetching metadata [{TopicMetadata for topic test11 -&gt; </p><pre>No partition metadata for topic test11 due to kafka.common.TopicAuthorizationException}] for topic [test11]: class kafka.common.TopicAuthorizationException  (kafka.producer.BrokerPartitionInfo)</pre><p>[2016-10-10 20:22:11,154] ERROR Failed to send requests for topics test11 with correlation ids in [0,8] (kafka.producer.async.DefaultEventHandler)\n[2016-10-10 20:22:11,155] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread)\nkafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.\n   at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:91)\n   at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105)\n   at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:88)\n   at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:68)\n   at scala.collection.immutable.Stream.foreach(Stream.scala:547)\n   at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67)\n   at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45)</p><p>I have disabled the kerberos.</p><p>But I guess Kafka was still trying to get authorise from Kerbors.</p><p>Please suggest me.</p><p>Thank you.</p><p>Mohan.V</p>","tags":["topic","producer","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-11 08:18:20.0","id":60872,"title":"docker for windows 10 port forwarding?","body":"<p>\n\tso I downloaded docker for windows, hortonwork sandbox docker version, followed the instruction and installed the sandbox.</p><p>\n\tthe code I used to install sandbox is as following</p><p>docker run -v hadoop:/hadoop --name sandbox --hostname \"sandbox.hortonworks.com\"--privileged -p -d 6080:6080-p 9090:9090-p 9000:9000-p 8000:8000-p 8020:8020-p 42111:42111-p 10500:10500-p 16030:16030-p 8042:8042-p 8040:8040-p 2100:2100-p 4200:4200-p 4040:4040-p 8050:8050-p 9996:9996-p 9995:9995-p 8080:8080-p 8088:8088-p 8886:8886-p 8889:8889-p 8443:8443-p 8744:8744-p 8888:8888-p 8188:8188-p 8983:8983-p 1000:1000-p 1100:1100-p 11000:11000-p 10001:10001-p 15000:15000-p 10000:10000-p 8993:8993-p 1988:1988-p 5007:5007-p 50070:50070-p 19888:19888-p 16010:16010-p 50111:50111-p 50075:50075-p 50095:50095-p 18080:18080-p 60000:60000-p 8090:8090-p 8091:8091-p 8005:8005-p 8086:8086-p 8082:8082-p 60080:60080-p 8765:8765-p 5011:5011-p 6001:6001-p 6003:6003-p 6008:6008-p 1220:1220-p 21000:21000-p 6188:6188-p 61888:61888-p 2222:22 sandbox /usr/sbin/sshd -D</p><p>however, I can only access those ports, for example 8888 and 2222 from local machine. </p><p>I can access 192.168.2.105(the IP of that windows 10 machine):2222 with putty no problem. </p><p>but when I use another computer under the same LAN and try to access 192.168.2.105:2222, it just won't connect..</p><p>I think it has something to do with the following in the instruction \"You then need to spawn a container with the –privileged flag from the image making sure to forward the following ports:....\"</p><p>I'm so sorry I'm a completely newbie to all of this, does anyone know what should I do??</p><p>thanks a lot!</p>","tags":["port","ports","docker","windows"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-10-11 12:54:09.0","id":60910,"title":"Install a HDP version - using Ambari - specified up to the HDP build number","body":"<p>Can Ambari install a specific HDP version, with a complete HDP version number like for instance 2.4.1.1-3?  In other words a version number consisting out of the HDP Major Version, HDP Minor Version, HDP Maintenance Release, HDP Patch Level, HDP Build Number.  </p><p>Or does Ambari only support specifying up to the HDP Minor Version number?</p>","tags":["hdp-2.4.0","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-11 09:36:10.0","id":60890,"title":"Sqoop import to avro failing - which jars to be used ?","body":"<p>HDP-2.5.0.0 using Ambari  2.4.0.1</p><p>A Sqoop import to avro fails with the following error :</p><pre>16/10/11 08:26:32 INFO mapreduce.Job: Job job_1476162030393_0002 running in uber mode : false\n16/10/11 08:26:32 INFO mapreduce.Job:  map 0% reduce 0%\n16/10/11 08:26:40 INFO mapreduce.Job:  map 25% reduce 0%\n16/10/11 08:26:40 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000001_0, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:40 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000000_0, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:40 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000003_0, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\n16/10/11 08:26:41 INFO mapreduce.Job:  map 0% reduce 0%\n16/10/11 08:26:42 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000002_0, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:46 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000001_1, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:47 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000000_1, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:47 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000003_1, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:48 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000002_1, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:51 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000001_2, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n16/10/11 08:26:51 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000002_2, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\n16/10/11 08:26:51 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000003_2, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\n16/10/11 08:26:52 INFO mapreduce.Job: Task Id : attempt_1476162030393_0002_m_000000_2, Status : FAILED\nError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\n16/10/11 08:26:57 INFO mapreduce.Job:  map 100% reduce 0%\n16/10/11 08:26:57 INFO mapreduce.Job: Job job_1476162030393_0002 failed with state FAILED due to: Task failed task_1476162030393_0002_m_000002\nJob failed as tasks failed. failedMaps:1 failedReduces:0</pre><p>The YARN application log ends with :</p><pre>FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.NoSuchMethodError: org.apache.avro.reflect.ReflectData.addLogicalTypeConversion(Lorg/apache/avro/Conversion;)V\n        at org.apache.sqoop.mapreduce.AvroOutputFormat.getRecordWriter(AvroOutputFormat.java:97)\n        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.&lt;init&gt;(MapTask.java:647)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:767)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)</pre><p>The original installation had the following libraries under /usr/hdp/2.5.0.0-1245/sqoop/lib:</p><p>avro-mapred-1.8.0-hadoop2.jar, parquet-avro-1.4.1.jar, avro-1.8.0.jar</p><p>I tried first replacing(<strong>ONLY one jar at a time under the lib</strong>) <strong>avro-mapred-1.8.0-hadoop2.jar</strong> <strong>with </strong><strong>avro-mapred-1.8.1-hadoop2.jar</strong> <strong>and </strong><strong>avro-mapred-1.7.7-hadoop2.jar</strong>. When that didn't help, I tried using the jars from the <strong>HDP 2.4</strong> distribution viz. <strong>avro-1.7.5.jar</strong> and <strong>avro-mapred-1.7.5-hadoop2.jar</strong>, yet the error persisted.</p><p>How shall I fix the error ? </p>","tags":["Sqoop","avro"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-13 09:10:16.0","id":61344,"title":"Add an Host to ambari from another datacenter (and behind a firewall)","body":"<p>Hi,</p><p>I have a running HDP 2.5 cluster in AWS, and want to add a few hosts to it from our in-house DC for the sole purpose of having the relevant configuration files to connect to different Hadoop components. No other services will be installed on those hosts. </p><p>The hosts have ambari-agent running, I can see that they are known from ambari by looking at /var/log/ambari-server/ambari-server.log</p><p>Trying to add them from ambari fails, with the 'helpful' error message:</p><pre>Registering with the server...\nRegistration with the server failed.</pre><p>I cannot find more useful info in /var/log.</p><p>Those hosts are thus in another DC, and cannot be accessible directly from outside (they only have private IP addresses, but can connect to outside, including ambari). Is it possible to still add them? If not, is there any workaround to achieve what I am trying to do?</p>","tags":["cloud","Ambari","ambari-agent","aws"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-13 04:20:29.0","id":61306,"title":"Nifi - putsql for phoenix upsert very slow - improve performace ??","body":"<p>Nifi - putsql for phoenix upsert very slow , getting records 1000/sec , phoenix putsql is unabel toinsert it seems . </p><p>Please let us know how to improve phoenix insertion fast . </p>","tags":["Nifi","Phoenix"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-14 08:43:12.0","id":61583,"title":"How does Atlas implement Hive Hook and monitor the hive command line?","body":"<p>I am trying to write a Hbase hook and HBase Bridge, yet when I am reading the HiveHook.java sourcecode, I did not quite get how the hive hook capture the updates or any new hive operations on hive via the hive command line? </p><p>Does anyone have any idea of that? </p>","tags":["atlas-api","Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-10-14 10:03:13.0","id":61594,"title":"Atlas UI: http://localhost:21000 is not accessible,","body":"<p>I am using HDP-2.5.0.0, added Atlas using add services.</p><p>but not be able to access atlas webUI also there is an alert as below</p><p>Metadata Server Web UI - \n  Connection failed to http://localhost:21000/api/atlas/admin/status (&lt;urlopen error timed out&gt;)\n  </p>","tags":["Atlas","atlas-api"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-10-14 04:03:46.0","id":61561,"title":"smartsense hdfs dashboard connection issue","body":"<p>On HDP 2.5 smartsense 1.3 zeppelin instance for hdfs dashboards I am unable to see any reports.  I see errors for each report:</p><pre>java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)</pre><p> any insights?</p>","tags":["smartsense"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-16 19:11:19.0","id":51881,"title":"Oozie workflow Hive2 action completes successful with no result","body":"<p>I ran a Oozie workflow with hive2 action on Sandbox 2.3. The workflow status shows completed successfully but no result. However, I could execute the same query on \"beeline\" CLI and it worked as expected.</p><p>workflow.xml</p><pre>&lt;workflow-app xmlns=\"uri:oozie:workflow:0.5\" name=\"hive2-wf\"&gt;\n    &lt;start to=\"hive2-node\"/&gt;\n    &lt;action name=\"hive2-node\"&gt;\n        &lt;hive2 xmlns=\"uri:oozie:hive2-action:0.1\"&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n            &lt;jdbc-url&gt;${jdbcURL}&lt;/jdbc-url&gt;\n            &lt;script&gt;/user/hue/A_OozieWF/hivescript.hql&lt;/script&gt;\n        &lt;/hive2&gt;\n        &lt;ok to=\"end\"/&gt;\n        &lt;error to=\"fail\"/&gt;\n    &lt;/action&gt;\n    &lt;kill name=\"fail\"&gt;\n        &lt;message&gt;Hive2 (Beeline) action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n    &lt;/kill&gt;\n    &lt;end name=\"end\"/&gt;\n&lt;/workflow-app&gt;\n</pre><p>hivescript.hql</p><pre>use test_db;\ninsert into temp_test(col_a) values(1);\n</pre><p>If I run the query from beeline CLI, I can see multiple entries in resource manager UI e.g.: hive#, tez# for the request. However, there is only one entry if I run the same query through workflow, an entry for Tez is missing.</p><p>I am facing the same issue in my production cluster as well.</p><p>Am I missing any configuration?</p>","tags":["Hive","hiveserver2","oozie-hive","hive-jdbc","beeline"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-17 01:02:39.0","id":51943,"title":"Partital deletion of Spark 1.6 (under HDP 2.4.2.0-258), leads to inability to completely uninstall","body":"<p>In an attempt to deploy Spark 2.0 (manually), a partial removal of the previous version was removed, but for some reason not all of it.  When I attempt again to remove it I get the following error message:</p><p><em>HTTP/1.1 500 Internal Server Error\nX-Frame-Options: DENY\nX-XSS-Protection: 1; mode=block\nUser: admin\nSet-Cookie: AMBARISESSIONID=1ff3e14nyrju26q9y7izm5hp6;Path=/;HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nContent-Type: text/plain\nContent-Length: 212\nServer: Jetty(8.1.17.v20150415)\n{\n  \"status\" : 500,\n  \"message\" : \"org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Cannot remove DELTA/SPARK. SPARK_JOBHISTORYSERVER is in a non-removable state.\"</em></p><p>How can complete the deletion of this package, so I can install the newer one? </p>","tags":["Spark","delete","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-16 22:14:53.0","id":51900,"title":"Several components in the HDP gray out. how can i make it green again","body":"<p>Hi </p><p>I'm new to hadoop and also new to the sandbox of Hortonworks sandbox. I installed the sandbox and after a while of practicing, some components in the dashboard grayed out (as highlighted in the following pic) and are not able to execute anything on those grayed-out components. How can I bring those components back green status?</p><p><img src=\"/storage/attachments/6680-capture.png\"></p>","tags":["Sandbox","hdp-2.4.0","tutorial-160"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-08-16 22:41:28.0","id":51913,"title":"After addig new nodes into the cluster, copying from local to HDFS is taking more time. What may be the reason?","body":"","tags":["performance","HDFS","nodes","reason","add"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-17 08:54:41.0","id":51984,"title":"HortonWorks License Vs HortonWorks Free Comparison","body":"<p>Dear </p><p>One of our client want Hortonworks License edition they dont want to use opensource. Plus comparison of free vs license hortonworks is required, also detail about its license cost and annual support</p><p>Regards</p><p>Rehan</p>","tags":["licensing"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-08-17 19:07:10.0","id":52115,"title":"Pig Script Call locally","body":"<p>Hi All,</p><p>  I have a pig script named sample.pig  that has following statements:</p><p>  REGISTER /usr/local/hadoop/hive/hcatalog/share/hcatalog/hive-hcatalog-core-1.2.1.jar</p><p>   test = LOAD 'userdb.dataset' using org.apache.hive.hcatalog.pig.HCatLoader;</p><p>  I'm using this script because using the test relation load manually gives me error ERROR 1070: Could not resolve      org.apache.hive.hcatalog.pig.HCatLoader.</p><p>  Now i have the script sample.pig stored on my desktop so i'm using ./pig -useHCatalog -f sample.pig while starting pig which   is   not   working.Can   anyone suggest how to make it work??</p>","tags":["Hive","hadoop","Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-18 10:53:27.0","id":52326,"title":"I have on cluster setup in hortonworks having ambari 2.1 and HDP 2.3. I have to create an automated Script for creating HDFS directories and Quota Setup. Anyone is having any kind of script related to this or any kind of suggestions, kindly post here.","body":"","tags":["hadoop","setup","script","hortonwork","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-19 01:05:26.0","id":52459,"title":"HBaseSink HDP 2.3.2 HBase client version","body":"<p>We recently upgraded from HDP 2.1.4 to HDP 2.3.2 and I have some Flume agents that use HBaseSink to publish logs to HBase.  After the upgrade I've been receiving the error found below.  I compiled my customer event serializer for my events against HBase version 1.1.2.2.3.2.0-2950 and Hadoop version 2.7.1.2.3.2.0-2950 but something still isn't right. Is there  something special I need to do to get this working?\n\nHDP 2.3.2 contains HBase 1.1.2 + Hadoop 2.7.1 + Flume 1.5.2</p><pre>18 Aug 2016 19:02:24,873 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hbase.HBaseSink.process:356)  - Failed to commit transaction.Transaction rolled back.\njava.lang.NoSuchMethodError: org.apache.hadoop.hbase.client.Put.setWriteToWAL(Z)V\n\tat org.apache.flume.sink.hbase.HBaseSink$3.run(HBaseSink.java:379)\n\tat org.apache.flume.sink.hbase.HBaseSink$3.run(HBaseSink.java:374)\n\tat org.apache.flume.sink.hbase.HBaseSink.runPrivileged(HBaseSink.java:427)\n\tat org.apache.flume.sink.hbase.HBaseSink.putEventsAndCommit(HBaseSink.java:374)\n\tat org.apache.flume.sink.hbase.HBaseSink.process(HBaseSink.java:344)\n\tat org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)\n\tat org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)\n\tat java.lang.Thread.run(Thread.java:745)\n</pre>","tags":["Hbase","Flume"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-19 06:38:12.0","id":52490,"title":"I want to create a tool which can convert Teradata queries to Hive. Can somebody suggest how I can achieve this?","body":"","tags":["hadoop-ecosystem"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-22 05:44:33.0","id":52693,"title":"Containers Still Running Even After Job Is Finished","body":"<p>Hello,</p><p>I am launching 7 sqoop jobs in parallel which launches 14 containers. Even after the jobs are Finished and Final Status is Succeeded in RM, I see that there are still 7 more containers running holding up the resources. They stop only after 10 minutes. Is there a configuration which is affecting this? I see the part_success files getting generated before itself. I am not able to figure out what extra work it is doing in these 10 minutes.</p>","tags":["memory","MapReduce","YARN","resource-manager"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-21 14:00:10.0","id":52672,"title":"How to retrieve the guid with an Atlas search","body":"<p>I can select name and other attributes in a DSL (e.g. <code>Sometype select name, date</code>), but can't find any way to retrieve the guids of the results. How should I do this?</p>","tags":["Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-08-22 10:03:31.0","id":52726,"title":"Link Analysis using Spark Python","body":"<p>Hi,\n\nI need to create some graphs using PySpark to elaborate some link analysis research. I already see this link:\n\nhttp://kukuruku.co/hub/algorithms/social-network-analysis-spark-graphx\n\nBut this algorithm is implemented in Scala which is very more complex to understand. \n\nAnyone have an idea on a white paper or some tutorial that do some link analysis research using PySpark?\n\nThanks!</p>","tags":["pyspark","analysis"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-08-22 11:36:51.0","id":52741,"title":"Ranger policy configuration","body":"<p>Hi I have started to learn ranger. I have installed the ranger on hdp now can any one plz share docs to configure ranger and ranger policy. I use only ranger not kerbores not know and not ldap. Please suggest the docs from where i can deploy the policy.</p><p>Thank you in advance</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-08-22 19:43:19.0","id":52861,"title":"PIG relation gets dropped","body":"<p>HI All,</p><p>Whenever i use pig as a local or mapreduce fresh evryday i see the relation which i created yesterday is not there .So i need to load again and then store again.Why i happens like this ??Is there any way to make a relation permanent or in other words can we just re-use a  relation  again which we made some other day ??</p>","tags":["Pig","Hive","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-22 14:00:39.0","id":52776,"title":"When doing MR2 Service Check getting below error","body":"<pre>Container killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n.Failing this attempt.. Failing the application.\n16/08/22 19:11:38 INFO mapreduce.Job: Counters: 0</pre>","tags":["MapReduce","mr2","hadoop-core"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-23 05:26:38.0","id":52892,"title":"EC2 AMI for slave nodes","body":"<p>I am planning to use EC2 for setting up a cluster and for elasticity purposes, we plan to have a variable number of data nodes. IS it possible to install the slave components (datanodes, node manager) on a machine and take an AMI (amazon machine image) of that machine. And in the future if i want to bring a slave node, will it be possible to make use of the AMI.</p>","tags":["hdp-2.4","ec2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-23 11:04:41.0","id":52934,"title":"Nifi 0.7 putsplunk processor to send log files to splunk for nifi alerts?","body":"<p>Iam new to Alerting & Monitoring. If we want to setup alerts for nifi using splunk can we use putsplunk nifi processor or send log files directly to splunk?</p><p>Currently we are having applications use splunk where they send the log files directly to splunk for alerting. Which is the effective way to acheive monitoring and alerting for nifi using splunk? Thank you</p>","tags":["monitoring","putsplunk","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-23 11:16:08.0","id":52939,"title":"Create a Falcon job to backup files greater than 6months old from one server to another","body":"<p>Create a new process which just backs up all the files/folders which are greater than 6months old to another server</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-08-23 09:14:52.0","id":52928,"title":"How to change log location for solr cloud","body":"<p>Team,</p><p>Once we installed solr then by default log location is /opt/lucidworks-hdpsearch/solr/server/logs but I want to change it to /var/log/solr, so can someone please help me to change it. </p><p>Also note that I have tried ln -s /opt/lucidworks-hdpsearch/solr/server/logs /var/log/solr but it is symlink and still store log data in /opt location. So is there any property which I can change it to defined location ?</p>","tags":["logs","SOLR"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-24 03:28:34.0","id":53067,"title":"How to configure hiveserver2 high availability on Tableau? Does anyone worked on it?","body":"","tags":["hadoop","hiveserver2","configuration","ha","tableau"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-08-24 20:45:39.0","id":53259,"title":"compilation of hadoop word count example","body":"<p>I am following apache instructions at </p><p><a href=\"https://hadoop.apache.org/docs/r2.5.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\">https://hadoop.apache.org/docs/r2.5.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</a></p><p>when I try to compile the java code I get the error :</p><pre>[root@hadoop1 sami]# hadoop  WordCount.java\nError: Could not find or load main class WordCount.java\n[root@hadoop1 sami]# hadoop com.sun.tools.javac.Main WordCount.java\nError: Could not find or load main class com.sun.tools.javac.Main\n[root@hadoop1 sami]#\n</pre>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-07 05:29:23.0","id":55236,"title":"Module Failure Error in Ambari","body":"<p>Hi,</p><p>I am getting this error while i am trying to install the latest apache metron using quick-dev-platform</p><pre>TASK [ambari_config : Start the ambari cluster - no wait] **********************\nchanged: [node1]\nTASK [ambari_config : Start the ambari cluster - wait] *************************\nfatal: [node1]: FAILED! =&gt; {\"changed\": false, \"failed\": true, \"module_stderr\": \"\", \"module_stdout\": \"\", \"msg\": \"MODULE FAILURE\", \"parsed\": false}\nPLAY RECAP *********************************************************************\nnode1                      : ok=15   changed=1    unreachable=0    failed=1  \nAnsible failed to complete successfully. Any error output should be\nvisible above. Please fix these errors and try again.</pre><p>Following is the output of my platform</p><pre> ./metron-deployment/scripts/platform-info.shMetron 0.2.0BETA\nfatal: Not a git repository (or any of the parent directories): .git\n--\nansible 2.0.0.2\n  config file = \n  configured module search path = Default w/o overrides\n--\nVagrant 1.8.1\n--\nPython 2.7.11\n--\nApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T21:41:47+05:00)\nMaven home: /usr/share/apache-maven\nJava version: 1.8.0_45, vendor: Oracle Corporation\nJava home: /usr/java/jdk1.8.0_45/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"linux\", version: \"2.6.32-642.el6.x86_64\", arch: \"amd64\", family: \"unix\"\n--\nLinux CentOS17 2.6.32-642.el6.x86_64 #1 SMP Tue May 10 17:27:01 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</pre><p>Any help will be appreciated.</p>","tags":["vagrant","Metron","Ambari"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-09-07 18:34:37.0","id":55386,"title":"intergrate Livy with OOzie","body":"<p>Is it possible to integrate Livy with OOzie?</p><p>I would like to use the Livy commands to submit the job and also schedule the jobs in OOzie.  Is it possible?</p><p>What is the difference between Livy and OOzie?</p><p>Please let me know,</p><p>Thanks,</p><p>Narender.</p>","tags":["livy","Oozie"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-08 05:28:25.0","id":55461,"title":"How to check whether NIFI has completed the job remotely?","body":"<p>I have a requirement to read huge CSV file from Kafka topic to Cassandra. I configured<a href=\"https://cwiki.apache.org/confluence/display/NIFI/Example+Dataflow+Templates\">Apache Nifi</a>to achieve the same.</p><p><strong>Flow:</strong></p><p>User does not have a control on Nifi setup. He only specifies the URL where the CSV is located. The web application writes the URL into kafka topic. Nifi fetches the file and inserts into Cassandra.</p><p>How will I know that Nifi has inserted all the rows from the CSV file into Cassandra? I need to let the user know that inserting is done and display a page where he can see the unique values from the CSV.</p><p>Any help would be appreciated.</p>","tags":["nifi-streaming","Kafka","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-07 15:49:21.0","id":55365,"title":"Hadoop enviroment performance root cause??","body":"<p>We are working on some proof of concepts on the Hadoop dev\nenvironments and are running into perceived performance and memory issues using\nHive and SQL. Is there a way we can run something like \"explains\" on\nthe SQL  or assess the environment. Need to determine where the bottlenecks\nmight be.   It takes about about 20 minutes to do an average calculation\nin SQL for about 26 million rows, when we increase that volume we run of of\nmemory. We need to take a look at what the issue might be at root cause.</p>","tags":["hadoop-core","hadoop-ecosystem","hadoop","hadoop-maintenance"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-08 11:13:36.0","id":55492,"title":"what are the topics for HDPCD Associate level exam (like hive,pig etc .,) ????","body":"","tags":["hdpcd","Pig","Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-08 18:08:55.0","id":55571,"title":"Ambari Https (Broken HTTPS)","body":"<p>While securing Ambari Sever for Https, we can successfully login to https and default port 8443, however the https  is stoked out and says <strong>This page is insecure (broken HTTPS).</strong></p><p>We are using wildcard certs initially in .cer format however have to convert it to .pem format using openssl.</p><p>What is the preferred format and encryption for the Certs.</p><p>The current error says</p><p>1) SHA-1 Certificate\nThe certificate for this site expires in 2017 or later, and the certificate chain contains a certificate signed using SHA-1.</p><p>2) Certificate Error\nThere are issues with the site's certificate chain (net::ERR_CERT_COMMON_NAME_INVALID).</p><p>Thanks</p><p>Mayank</p>","tags":["Ambari","ssl"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-09-09 10:12:21.0","id":55706,"title":"Prod. HDP sandbox/clones for organizational users","body":"<p>A prod. cluster is already in place - HDP-2.4.2.0-258 installed using Ambari 2.2.2.0.</p><p>Following are the existing and upcoming scenarios :</p><ol><li>There are various 'actors' - hadoop developers and admin., data scientists, enthusiasts etc. who currently download and use the HDP sandbox on their local machines</li><li>The prod. cluster has lot of data and it is NOT advisable to have a large no. of users right away</li><li>The idea is to have a central system using which a large no. of users can 'spawn'/download & install their own sandboxes which are a tiny image of the prod. cluster in terms of the data and the services</li><li>It's indispensable for this system to allow the users to decide what subset of data they want to include in their sandbox</li></ol><p>I have a few thoughts :</p><ol><li>Maybe, it's sensible to provide a centralized download of the latest HDP sandbox, however, this may be version and otherwise different(maybe, far ahead !) from the prod. cluster</li><li>While the users would be willing to execute the queries/drag-drop tables, files etc. to select the data they want, almost none would be prepared to load this data manually from the production to their own sandboxes</li><li>Maybe, there are some existing tools that can be used to do this</li></ol><p>Can the community help me to assess the viability of this requirement/suggest alternatives ?</p>","tags":["Sandbox"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-09 18:24:37.0","id":55816,"title":"Unable to select 'Files' option in Ambari","body":"<p>And get the following error:</p><p><strong>500</strong>HdfsApi connection failed. Check \"webhdfs.url\" property</p><p>And \"java.lang.NullPointerException\"</p><p>It is interesting to note that yesterday i was able to select. But today after changing password to 'admin' and restarting, i am unable to select this option. </p>","tags":["tutorial-160","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-10 13:44:20.0","id":55886,"title":"Nifi/DataFlow example that loops through a list?","body":"<p>I'm a total dataflow/nifi rookie.</p><p>I'm trying to accomplish something like the following:</p><p>Given a database table like this</p><p>Customer_ID (varchar), DoA (boolean), DoB (boolean), DoC (boolean)</p><p>I want to:</p><p>1) query the table (select *)</p><p>2) for each customer:</p><p>3a) if DoA, execute some steps (move some files around, etc)</p><p>3b) if DoB, execute some steps</p><p>3c) if DoC, execute some steps</p><p>4) Update some logs files, etc.</p><p>I've been playing with some of the example templates here: https://cwiki.apache.org/confluence/display/NIFI/Example+Dataflow+Templates</p><p>But I haven't found anything to show me how to accomplish step 2 above. </p><p>Is it possible to work through a loop like this? </p><p>In the nifi training class, the instructor said that this is a common use case, but I can't seem to find a template that looks like this.</p><p>Can someone point me at an example to get me going?</p>","tags":["dataflow","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-12 03:39:47.0","id":55953,"title":"How to temporarily bypass Metron enrichments","body":"<p>I am running an 8 node physical Metron cluster (2 search ES nodes, and a 6 node HDP 2.4 cluster that also runs the additional Metron services).  </p><p>I started pushing bro logs onto the bro topic and have noticed it taking extremely long times to process (roughly 50 minutes from send to hitting the enrichment bolts).  I'd like to speed this up and, at least for the short term, could live with bypassing the enrichment topic.  Is there a simple way to do this?  </p>","tags":["Metron"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-09-11 08:37:32.0","id":55916,"title":"what is the usecase of Strict mode in Dynamic partitioning in Hive?? When would some one need that mode to do partitioning??","body":"","tags":["Hive","partitioning","dynamic"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-12 10:14:05.0","id":55999,"title":"Best Practices for Setting up Nifi in different modes","body":"<p>Hello everyone,</p><p>Please share the best practices for setting up Nifi in Cluster mode and Standalone mode. </p><p>Best Regards</p>","tags":["integration","Nifi","best-practices"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-12 15:16:44.0","id":56052,"title":"we are getting below error/exception while executing sqoop action via oozie ?","body":"<p><a href=\"/storage/attachments/7537-img.png\">img.png</a>we are getting below error/exception while executing sqoop\naction via oozie .</p><p><strong>Error</strong> - Failing Oozie Launcher, Main class\n[org.apache.oozie.action.hadoop.SqoopMain], exception invoking main(),\njava.lang.ClassNotFoundException: Class\norg.apache.oozie.action.hadoop.SqoopMain not found</p><p>It seems some jars related to sqoop action not available on\noozie share lib path i.e. “/user/oozie/share/lib/lib_20160807001458/sqoop”.\n We are getting access issues while trying to update the oozie share lib\npath with required jars.  </p><p><img style=\"height: 0.343in; width: 11.312in;\"></p>","tags":["oozie-shell-action","hadoop-ecosystem","Oozie","oozie-shell"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-09-12 20:06:47.0","id":56104,"title":"Apache nifi additional data info","body":"<p>Dear collegues,</p><p>I am trying to make apache nifi control data flow. However had not found any possiblity to make NiFi provide the data volume information (number of lines or entities in a file, number of imported data points from stream). Had anyone found such?</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-12 22:01:58.0","id":56113,"title":"Trying to create phoenix interpreter using %jdbc in Zeppelin using 2.5","body":"<p>I am trying to create phoenix interpreter using %jdbc in Zeppelin using 2.5 and am not succeeding.</p><p>Steps are:</p><ol><li>Log into Zeppelin (sandbox 2.5)</li><li>Create new interpreter as follows\n<p><img src=\"/storage/attachments/7540-screen-shot-2016-09-12-at-52103-pm.png\"></p></li><li>restart (just to be paranoid)</li><li>go to my notebook and bind interpreter\n<p><img src=\"/storage/attachments/7541-screen-shot-2016-09-12-at-54305-pm.png\"></p></li><li>when I run with <strong>%jdbc(phoenix) </strong>I get <strong>Prefix not found.</strong></li><li>when I run it with <strong>%jdbc.phoenix</strong> I get <strong>jdbc.phoenix interpreter not found</strong></li></ol><p>What am I missing?</p>","tags":["zeppelin-notebook","interpreter","jdbc","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-13 08:06:53.0","id":56183,"title":"Ambari upgrade .","body":"<p>HI,</p><p>I was trying to upgrade the ambari 1.7 from 2.4 and getting error. So I want to know is  possible to upgrade the ambari 1.7 to latest version of ambari 2.4.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-13 05:55:37.0","id":56156,"title":"fffffffffff","body":"<p>fffffffffffff</p>","tags":["datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-13 02:27:23.0","id":56140,"title":"How to connect/integrate HDP Hive and MicroStrategy Analytics Desktop?","body":"","tags":["hiveserver2","Hive","hive-jdbc"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-13 17:45:33.0","id":56278,"title":"how can i do these two steps in Hortonworks on centos ?","body":"<ol>\n<li>Kill your hiveserver2 & metastore & Restart them again. </li><li>check hiveserver2 & metastore are running using 'jps' command.</li></ol>","tags":["hadoop-maintenance"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-13 13:39:01.0","id":56215,"title":"HiveServer2 Thrift Thread Pool","body":"<p>We have Hive 1.2.1.2.3 Thrift service installed with Atlas Plugin, Ranger Plugin. After some days, we exhaust the running threads, receive an error such as below in the logs and the service stops responding, requiring a restart.</p><p>org.apache.hive.service.cli.HiveSQLException: Error while processing \nstatement: FAILED: Hive Internal Error: \njava.util.concurrent.RejectedExecutionException(Task \njava.util.concurrent.FutureTask@1c9f4873 rejected from \njava.util.concurrent.ThreadPoolExecutor@1bacbbcc[Running, pool size = 1,\n active threads = 1, queued tasks = 10000, completed tasks = 345]) </p><p>Caused by: java.util.concurrent.RejectedExecutionException: Task \njava.util.concurrent.FutureTask@1c9f4873 rejected from \njava.util.concurrent.ThreadPoolExecutor@1bacbbcc[Running, pool size = 1,\n active threads = 1, queued tasks = 10000, completed tasks = 345]</p><p>Does anyone have further information on troubleshooting this issue or means to determine what is exhausting the thread pool?</p><p>thanks.</p>","tags":["hiveserver2","thrift"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-13 14:20:12.0","id":56225,"title":"Nifi InvokeHTTP handshake alert unrecognized_name","body":"<p>Good after-noon,</p><p>The context of the question - to monitor the url      https://research.cs.wisc.edu/dbworld/browse.html</p><p>It requires HTTPS, so from browser I downloaded the certificate and saved it as:</p><p>/home/user/dbworld_certif.pem</p><p>Next, I imported this one in the truststore (where I had previously added other certificates as indicated by https://community.hortonworks.com/questions/9509/connecting-to-datasift-https-api-using-nifi.html):</p><p>keytool -noprompt -importcert -keystore /home/user/nifi_certs/truststore.jks -storepass changeit -file /home/user/dbworld_certif.pem -alias /home/user/dbworld_certif.pem</p><p>And finally I configured the StandardSSLContextService :</p><p>Keystore Filename: /home/user/nifi_certs/truststore.jks</p><p>Keystore Password: changeit</p><p>Keystore Type: JKS</p><p>Truststore Filename: /home/user/nifi_certs/truststore.jks</p><p>Truststore Password: changeit</p><p>Truststore Type: JKS</p><p>SSL Protocol: TLS</p><p>I enabled this StandardSSLContextService and used it in the InvokeHTTP processor. So, the InvokeHTTP processor has as configuration:</p><p>It is connected to a PutFile processor.</p><p>HTTP Method: GET</p><p>Remote URL: https://research.cs.wisc.edu/dbworld/browse.html</p><p>SSL Context Service: StandardSSLContextService</p><p>...</p><p>Basic Authentication Username: NO VALUE SET</p><p>Basic Authentication Password: NO VALUE SET</p><p>...</p><p>Use Digest Authentication: false</p><p>Trusted Hostname: https://research.cs.wisc.edu/dbworld/browse.html (with or without this value set is the same error)</p><p>Content-Type: ${mime.type}</p><p>When I execute the flow, I get after a couple of seconds the following error:</p><p>Yielding processor due to exception encountered as a source processor:</p><p>javax.net.ssl.SSLProtocolException: handshake alert : unrecognized_name</p><p>Thank you in advance for suggestions on how to solve this.</p><p>Best regards,</p><p>Camelia</p>","tags":["invokehttp"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-25 15:29:55.0","id":63380,"title":"NiFi queues too large?","body":"<p>I pulled in a large number of files into nifi using the <strong>getFile </strong>processor... I am looking to store these files in HDFS using <strong>putHDFS</strong>. The process ran overnight and didn't have any back pressure set so the queue got backed up. It seems that this is causing <strong>threads to hang</strong> when trying to start <em><strong>any</strong></em> processor. </p><p>I am also unable to empty the queue... the process just hangs indefinitely at 0%. </p><p>Some things I have tried so far:</p><ol><li>Restarting Server (removes hanged threads, but threads hang whenever processors are started again)</li><li>Increasing Java heap size to 16g</li><li>Using G1GC garbage collector</li><li>Increasing swap size of queue to 40,000 since both of the backed up queues have 29,000 (8GB) - 25,000 (800MB compressed) flow files in them respectively.</li></ol><p>Does anyone have any suggestions for what I should do next? Has anyone run into this issue before? I am on nifi version 0.6.1. Thanks for you help! :)</p>","tags":["queues","Nifi","memory"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-26 06:20:34.0","id":63498,"title":"Unable to download tutorial data file","body":"<p>http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/#what-is-the-sandbox</p><p style=\"margin-left: 20px;\">The link is part of tutorial, in 1.5 SEND DATA BETWEEN SANDBOX & LOCAL MACHINE make typing this command:</p><pre>&lt;code&gt;scp -P 2222~/Downloads/HDF-1.2.0.1-1.tar.gz root@localhost:/root</pre><p style=\"margin-left: 20px;\">but it need password and there are no password in tutorial...</p><p style=\"margin-left: 20px;\">Anybody, could you please help me?</p>","tags":["Sandbox","linux"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-10-26 09:42:34.0","id":63517,"title":"Hive Ranger policy is not applied","body":"<p>Hi,</p><p>I am trying to apply Ranger policies for Hive. I have created a policy but it seems that the policy is not applied. The audit logs that are shown in Ranger-&gt; Audit are also confusing. I am trying to execute queries from Hive CLI. </p><p>I have a database called 'employee'. I have created a table empdetails having columns empno, empname and salary.</p><p>When I query 'select empno from empdetails' , it still shows me all the records as the policy states only 'empname' must be accessible by user 'mohang'.</p><p>It would be helpful if some one can provide some solution and suggestions. Attached are the screenshots.</p><p>Thanks.</p>","tags":["Hive","Ranger"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-10-25 22:05:16.0","id":63454,"title":"Implementation of cross data center replication with SolrCloud 5.5 and HDP 2.5","body":"<p>How can DR be implemented with active-passive or active-active in solrcloud 5.5 on HDP 2.5. I am using HDFS as storage for index. </p>","tags":["solrcloud","restore","backup","hdp-2.5.0"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-10-26 09:17:17.0","id":63513,"title":"Helping setting up cron-based nifi processor","body":"<p>I'm trying to setup a nifi processor to run once daily, using the 'cron' option under scheduling.</p><p>Under \"run schedule\", I put \"01 18 * * * ?\" , which should be 6:01pm (I couldn't get nifi to accept it without the question mark).</p><p>But this causes the scheduler to run on the hour, every hour.</p><p>Can someone please help me with my syntax here?</p>","tags":["Nifi","dataflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-26 15:13:18.0","id":63577,"title":"Namenode not starting after kerberizing (“Cannot find key of appropriate type to decrypt AP REP - RC4 with HMAC” )","body":"<p>I have a cluster with HA set up for the namenodes and after a manual kerberization process using an existing AD. I am unable to start the namenodes. Other services that do not rely on a namenode start fine, and the journal nodes and datanodes start fine as well. In the error log I see “Cannot find key of appropriate type to decrypt AP REP - RC4 with HMAC”. Any ideas?</p><p>I've done a klist and confirmed that Arcfour with Hmac/md5 is one of the included keys in the keytab, what else could be the issue here?</p><p>Also I am missing the kerberos option on the right hand side of the ambari UI, is this normal for a manual setup? I'm used to it being there. </p>","tags":["namenode","namenode-ha","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-10-27 17:28:19.0","id":63826,"title":"Hi,  Is there any connector for teradata to spark.We have scenarios to get the data from teradata by using SparkSQl. I am using spark 1.6.0.Please let me know if anyone tired connecting teradata $ spark.Thanks!","body":"","tags":["spark-sql","sparksql","teradata","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-10-28 05:21:23.0","id":63936,"title":"About hbase-1.1.2.2.3.4.0-3485 has too many close_wait on regionserver  bug","body":"<p>I used hbase-1.1.2.2.3.4.0-3485 version .</p><p>Now I find my regionserver has too many close_wait, restart doesn’t slove it</p><p>[root@hdfs009 ~]# lsof |wc -l </p><p>35908</p><p>\n[root@hdfs009 ~]# lsof |grep hbase |wc -l</p><p>\n30690</p><p>when it grows up to 65535,regionserver will shutdown with too many open files in regionserver log.</p><p>I try to patch it with this article:</p><p> <strong>https://issues.apache.org/jira/browse/HBASE-9393</strong></p><p>but hbase-1.1.2.2.3.4.0-3485version is too older,only newly hbase-branch in github can be patched.</p><p>[root@hdfs002 hbase-release-HDP-2.3.4.0-tag]# git apply ../HBASE-9393.v15.patch </p><p>error: patch failed: hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java:74 </p><p>error: hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java: patch does not apply</p><p>\nerror: patch failed: hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java:477</p><p>\nerror: hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java: patch does not apply </p><p>error: patch failed: hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java:1351</p><p>\nerror: hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java: patch does not apply </p><p>error: hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java: No such file or directory</p><p>I really don't want to upgrade my hdp2.3 ,please give me some help .</p><p>thanks so much.</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-28 14:40:26.0","id":63991,"title":"HIve location","body":"<p>Location clause is Mandatory in create table statement in External tables and not required with  External  partiotioned table.what about managed table and Managed  partiotioned tables.Is it mandatory or optional?</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-28 17:01:38.0","id":64005,"title":"Phoenix security and initial system table creation","body":"<p>There are numerous references on how SYSTEM tables are created the first time that a user logs in to Phoenix. Thus, they would require Create and Write permissions in the HBase default namespace.</p><p>1. Does this happen for each user?</p><p>2. Does this happen for each time they login?</p><p>I ask this because we have users that were encountering \"Insufficient permissions\" errors. Then granted them 'RWXCA' permissions in HBase. Then everything worked well. After the first login we tried removing permissions (trying to create a read-only user). However, when we removed the 'CW' permissions, they could no longer login and starting getting Insufficient Permissions error.</p><pre>Error: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions (user=svc.xyx@FOO.BAR, scope=SYSTEM.CATALOG, family=, action=CREATE)\n\ngrant 'xyz', 'RWXCA', '@default' \n-- All good!\n\ngrant 'xyz', 'RX', '@default' \n-- No good! Even after first time\n</pre><p>3. DO THE USERS ALWAYS HAVE TO HAVE 'CW' access to Hbase default namespace? And if so, what is the best way to control table-level security in Phoenix?</p>","tags":["security","Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-28 16:08:14.0","id":64001,"title":"Adding users to demo LDAP in Hortonworks sandbox?","body":"<p>I am not so familiar with LDAP servers. How can I add more users and groups to the Demo LDAP server?</p>","tags":["Knox","ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-10-28 20:19:14.0","id":64068,"title":"hive upgrade script 2..0.0 to 2.1.0.mysql error duplicate key","body":"<p>Express upgrade to hdp 2.5.0.0-1245 from 2.4 fails when upgrading hive schema</p><p>Starting upgrade metastore schema from version 1.2.1000 to 2.1.0\nUpgrade script upgrade-1.2.1000-to-2.0.0.mysql.sql\nCompleted upgrade-1.2.1000-to-2.0.0.mysql.sql\nUpgrade script upgrade-2.0.0-to-2.1.0.mysql.sql\nError: Duplicate key name 'CONSTRAINTS_PARENT_TABLE_ID_INDEX' (state=42000,code=1061)\norg.apache.hadoop.hive.metastore.HiveMetaException: Upgrade FAILED! Metastore state would be inconsistent !!\nUnderlying cause: java.io.IOException : Schema script failed, errorcode 2\nUse --verbose for detailed stacktrace.\n*** schemaTool failed ***</p>","tags":["upgrade","Hive","hdp-2.5.0"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-28 21:01:27.0","id":64066,"title":"Unable to use lzo codec","body":"<p>I'm trying to get LZO compression to work on our HDP 2.3.2 cluster and getting nowhere.  Here's what I've done:</p><p>  - Installed the hadooplzo and hadoop-lzo-native RPMs</p><p>  - Made the documented changes to add the codec and the lzo class spec to core-site.xml</p><p>When I try to run a job thusly:</p><pre>yarn jar /usr/hdp/2.3.2.0-2950/hadoop/lib/hadoop-lzo-0.6.0.2.3.2.0-2950.jar com.hadoop.compression.lzo.LzoIndexer /path/to/lzofiles</pre><p>It tells me:</p><pre>[hirschs@sees24-lin ~]$ yarn jar /usr/hdp/2.3.2.0-2950/hadoop/lib/hadoop-lzo-0.6.0.2.3.2.0-2950.jar com.hadoop.compression.lzo.LzoIndexer /xxxx/yyy\n16/10/28 16:44:56 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library\njava.lang.UnsatisfiedLinkError: no gplcompression in java.library.path\n        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1886)\n        at java.lang.Runtime.loadLibrary0(Runtime.java:849)\n        at java.lang.System.loadLibrary(System.java:1088)\n        at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:32)\n        at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:71)\n        at com.hadoop.compression.lzo.LzoIndexer.&lt;init&gt;(LzoIndexer.java:36)\n        at com.hadoop.compression.lzo.LzoIndexer.main(LzoIndexer.java:134)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n16/10/28 16:44:56 ERROR lzo.LzoCodec: Cannot load native-lzo without native-hadoop\n16/10/28 16:44:57 INFO lzo.LzoIndexer: LZO Indexing directory /xxxxx/yyyyy...\n16/10/28 16:44:57 INFO lzo.LzoIndexer:   [INDEX] LZO Indexing file hdfs://correct_path_to_file, size 1.08 GB...\n16/10/28 16:44:57 INFO compress.LzoCodec: Bridging org.apache.hadoop.io.compress.LzoCodec to com.hadoop.compression.lzo.LzoCodec.\n16/10/28 16:44:57 ERROR lzo.LzoIndexer: Error indexing hdfs://correct_path_to_file\njava.io.IOException: Could not find codec for file hdfs://correct_path_to_file - you may need to add the LZO codec to your io.compression.codecs configuration in core-site.xml\n        at com.hadoop.compression.lzo.LzoIndex.createIndex(LzoIndex.java:212)\n        at com.hadoop.compression.lzo.LzoIndexer.indexSingleFile(LzoIndexer.java:117)\n        at com.hadoop.compression.lzo.LzoIndexer.indexInternal(LzoIndexer.java:98)\n        at com.hadoop.compression.lzo.LzoIndexer.indexInternal(LzoIndexer.java:86)\n        at com.hadoop.compression.lzo.LzoIndexer.index(LzoIndexer.java:52)\n        at com.hadoop.compression.lzo.LzoIndexer.main(LzoIndexer.java:137)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\n</pre><p>I get the feeling I'm missing a step somewhere.  The shared libraries appear to be in place:</p><pre>[hirschs@sees24-lin native]$ rpm -ql hadoop-lzo-native\n/usr/hdp/current/share/lzo/0.6.0/lib/native\n/usr/hdp/current/share/lzo/0.6.0/lib/native/Linux-amd64-64\n/usr/hdp/current/share/lzo/0.6.0/lib/native/Linux-amd64-64/libgplcompression.a\n/usr/hdp/current/share/lzo/0.6.0/lib/native/Linux-amd64-64/libgplcompression.la\n/usr/hdp/current/share/lzo/0.6.0/lib/native/Linux-amd64-64/libgplcompression.so\n/usr/hdp/current/share/lzo/0.6.0/lib/native/Linux-amd64-64/libgplcompression.so.0\n/usr/hdp/current/share/lzo/0.6.0/lib/native/Linux-amd64-64/libgplcompression.so.0.0.0\n/usr/hdp/current/share/lzo/0.6.0/lib/native/docs\n</pre><p>In core-site.xml:</p><pre>    &lt;property&gt;\n      &lt;name&gt;io.compression.codecs&lt;/name&gt;\n      &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;\n    &lt;/property&gt;\n</pre><p>In hdfs-site.xml:</p><pre>   &lt;property&gt;\n      &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;\n      &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;\n    &lt;/property&gt;\n</pre><p>What more do I need to do in order for this to run?</p><p>Even a guess would be helpful at this point.  </p>","tags":["compression","configuration"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-29 13:47:22.0","id":64110,"title":"Impala -Pig Files - Parquet file?","body":"<p>Hi experts,</p><p>I've created a Script using Apache PIG to do some jobs on my data (that are from a text file). After my script I'm getting a big list of files (\"part-m-001\",\"part-m-002\",...). What I'm asking is:\n\nUsing Impala is possible to concatenate all the data into one table? The data follows a structured schema so using Parquet Files is a good option?\n\nThanks!</p>","tags":["apache-impala","files","parquet","Pig","impala"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-30 04:41:21.0","id":64119,"title":"issue with Cloudbreak automatically creating SPOT instances on AWS instead of regular on-demand instances","body":"<p>I use the network option \"Use an existing subnet in an existing VPC\" to launch my cluster in AWS. Cloudbreak automatically creates spot instances instead of the regular instances. I have not specified anywhere about spot instances. I checked my instance types created through \"Manage Template\" also. There is no reference to spot instances anywhere. Does anybody have any idea as to where this setting exists? I use my cloudbreak deployment with version: 1.6</p>","tags":["Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-30 17:27:27.0","id":64124,"title":"How to run spark action through oozie..?","body":"<p>Hi All,</p><p>I am new to oozie trying to run my simple word count spark job through oozie ,i am successfully submitting the job, but i am getting this exception <em><strong>Launcher exception: java.lang.ClassNotFoundException: Class org.apache.oozie.action.hadoop.SparkMain not found\njava.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.oozie.action.hadoop.SparkMain not found, </strong></em></p><p><em><strong>Here is my workflow.xml and job.properties i followed these link to run the spark job</strong></em></p><p><em><strong>https://community.hortonworks.com/articles/48920/how-to-run-spark-action-in-oozie-of-hdp-230.html</strong></em></p><p><em>w</em>e tried a sample hive job through oozie we successfully executed that, for this i am getting this error.please let me know the root cause of this exception, for this i have tried different test cases,we copied spark-assembly-jar to usr/oozie/share/lib although i am getting this exception,<a href=\"/storage/attachments/9007-error.txt\">error.txt</a> <strong><em>please find attached error log aso</em></strong></p><pre>Workflow.xml\n&lt;workflow-app xmlns='uri:oozie:workflow:0.5' name='Sparkjob'&gt;\n    &lt;start to='spark-node' /&gt;\n    &lt;action name='spark-node'&gt;\n        &lt;spark xmlns=\"uri:oozie:spark-action:0.1\"&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n            &lt;master&gt;local[*]&lt;/master&gt;\n&lt;mode&gt;client&lt;/mode&gt;\n            &lt;name&gt;Spark-FileCopy&lt;/name&gt;\n            &lt;class&gt;org.examples.WordCounte&lt;/class&gt;\n            &lt;jar&gt;${nameNode}/anji/oozie/lib/Spark_test.jar&lt;/jar&gt;\n            &lt;arg&gt;${input}&lt;/arg&gt;\n            &lt;arg&gt;${output}&lt;/arg&gt;\n        &lt;/spark&gt;\n        &lt;ok to=\"end\" /&gt;\n        &lt;error to=\"fail\" /&gt;\n    &lt;/action&gt;\n    &lt;kill name=\"fail\"&gt;\n        &lt;message&gt;Workflow failed, error\n            message[${wf:errorMessage(wf:lastErrorNode())}]\n        &lt;/message&gt;\n    &lt;/kill&gt;\n    &lt;end name='end' /&gt;\n&lt;/workflow-app&gt;\n\njob.properties:nameNode=hdfs://nn:8020\njobTracker=jT:8032\nqueueName=default\ninput =${nameNode}/sample2.txt\noutput=${nameNode}/output3\noozie.system.lib.path = true\noozie.libpath=${nameNode}/user/oozie/share/lib\noozie.action.sharelib.for.spark=${namenode}/user/oozie/share/lib/spark/\ndryrun=False\noozie.wf.application.path=hdfs://quickstart.cloudera:8020/anji/oozie/\n\n\n\n\n\n\n\n\n</pre>","tags":["Oozie","oozie-sharelib","hadoop","oozie-shell-hue","oozie-coordinator","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-10-30 13:37:21.0","id":64131,"title":"sqoop import data to hive throw ERROR org.apache.sqoop.hive.HiveConfig?","body":"<p>hi,</p><p>1. I have installed HUE 3.10 on ambari HDP 2.5.0 </p><p>2. config the hue.ini fully </p><p>My problem is var sqoop sync data from mysql to hive, it throw an exception:</p><pre>[main] ERROR org.apache.sqoop.hive.HiveConfig – Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly. \n\n[main] ERROR org.apache.sqoop.hive.HiveConfig – Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly. \n\n[main] ERROR org.apache.sqoop.tool.ImportTool – Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf\n\nat org.apache.sqoop.hive.HiveConfig.getHiveConf(HiveConfig.java:50)\n\tat org.apache.sqoop.hive.HiveImport.getHiveArgs(HiveImport.java:397)\n\tat org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:384)\n\tat org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:342)\n\tat org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:246)\n\tat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:524)\n\tat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)\n\tat org.apache.sqoop.Sqoop.run(Sqoop.java:147)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)\n\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:225)\n\tat org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)\n\tat org.apache.sqoop.Sqoop.main(Sqoop.java:243)\n\tat org.apache.oozie.action.hadoop.SqoopMain.runSqoopJob(SqoopMain.java:202)\n\tat org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:182)\n\tat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:51)\n\tat org.apache.oozie.action.hadoop.SqoopMain.main(SqoopMain.java:48)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:242)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)</pre><p>But, if execute the same sqoop script in command line, it works!</p><p>Added environment variable HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/hdp/current/hive-client/lib to /etc/profile. It still not work. I tried some times to slove this issue by myself, but faild.</p><p>The script is /usr/hdp/2.5.0.0-1245/hive/bin/hive. It seem like ${HADOOP_CLASSPATH} point to /usr/hdp/2.5.0.0-1245/atlas/hook/hive/* ?</p><pre>#!/bin/bash\n\n\nif [ -d \"/usr/hdp/2.5.0.0-1245/atlas/hook/hive\" ]; then\n  if [ -z \"${HADOOP_CLASSPATH}\" ]; then\n    export HADOOP_CLASSPATH=/usr/hdp/2.5.0.0-1245/atlas/hook/hive/*\n  else\n    export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:/usr/hdp/2.5.0.0-1245/atlas/hook/hive/*\n  fi\nfi\n\n\nBIGTOP_DEFAULTS_DIR=${BIGTOP_DEFAULTS_DIR-/etc/default}\n[ -n \"${BIGTOP_DEFAULTS_DIR}\" -a -r ${BIGTOP_DEFAULTS_DIR}/hbase ] && . ${BIGTOP_DEFAULTS_DIR}/hbase\n\n\n\n\nexport HIVE_HOME=${HIVE_HOME:-/usr/hdp/2.5.0.0-1245/hive}\nexport HADOOP_HOME=${HADOOP_HOME:-/usr/hdp/2.5.0.0-1245/hadoop}\nexport ATLAS_HOME=${ATLAS_HOME:-/usr/hdp/2.5.0.0-1245/atlas}\n\n\nHCATALOG_JAR_PATH=/usr/hdp/2.5.0.0-1245/hive-hcatalog/share/hcatalog/hive-hcatalog-core-1.2.1000.2.5.0.0-1245.jar:/usr/hdp/2.5.0.0-1245/hive-hcatalog/share/hcatalog/hive-hcatalog-server-extensions-1.2.1000.2.5.0.0-1245.jar:/usr/hdp/2.5.0.0-1245/hive-hcatalog/share/webhcat/java-client/hive-webhcat-java-client-1.2.1000.2.5.0.0-1245.jar\n\n\nif [ -z \"${HADOOP_CLASSPATH}\" ]; then\n  export HADOOP_CLASSPATH=${HCATALOG_JAR_PATH}\nelse\n  export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${HCATALOG_JAR_PATH}\nfi\n\n\nexec \"${HIVE_HOME}/bin/hive.distro\" \"$@\"\n\n\n</pre><p>how to slove this issue? urgent!!!</p><p>many thanks!</p>","tags":["Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-07 07:34:35.0","id":32050,"title":"Pig Error while running script and loading data into a already created table in hive","body":"<pre>The script is emp.pig -useHCatalog\n\nA = LOAD '/user/maria_dev/empdata' using PigStorage(',') AS (ename:chararray, esal:float, eid:float);\nB = FILTER A BY ($1 matches 'N/A') and ($2 matches 'Null');\nSTORE B INTO 'emp' USING org.apache.hive.hcatalog.pig.HCatStorer();\n</pre>","tags":["Pig","Falcon"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-09 00:02:28.0","id":32179,"title":"spark sql interaction with hive doubts","body":"<p>Hi, Im studing the interaction of spark with hive, to execute queries over hive tables with spark sql using hiveContex. But, Im having some doubts to understanding the logic.</p><p> From the spark documentation, the basic code for this is this: </p><pre>// sc is an existing SparkContext. \nvar hiveContext = new org.apache.spark.sql.hive.HiveContext(sc) \nvar query = hiveContext.sql(\"select * from customers\"); \nquery.collect() </pre><p>I have three main doubts. I read that spark works with rdds, and then spark can apply actions or transformations in that rdds. </p><p><strong>1) </strong>It seems that we can create a rdd by loading an external dataset, so in this above code where the RDD is created? Is here \"query = hiveContext.sql(\"select * from customers\");\" ? var query is the RDD? </p><p><strong>2)</strong> And then after the RDD is created we can do transformations and actions, but in this case of execute queries over hive tables we just do actions right? There is no need for transformations right? And the action here is collect() right? </p><p><strong>3)</strong> And third, I also read that spark computes rdds in a lazy way to save storage space. In this use case of execute queries over hive tables with above code, where or how this lazy evaluation mechanism happens, so the spark can save storage space?</p><p>Can you give some help to understand this better?</p>","tags":["Hive","spark-sql","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-07 17:04:50.0","id":32071,"title":"tez.queue.name vs hive.server2.tez.default.queues in HiveServer configuration","body":"<p>In documentation page for \"<a href=\"https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_installing_manually_book/content/ref-d677ca50-0a14-4d9e-9882-b764e689f6db.1.html\">Configure Hive and HiveServer2 for Tez</a>\" there are two properties that looks similar to me: </p><ul>\n<li>tez.queue.name: property to specify which queue will be used for Hive-on-Tez jobs.</li><li>hive.server2.tez.default.queues: A list of comma separated values corresponding to YARN queues of the same name. When HiveServer2 is launched in Tez mode, this configuration needs to be set for multiple Tez sessions to run in parallel on the cluster.</li></ul><p>The only difference that I see is that when using \"hive.server2.tez.default.queues\" we can specify several queues so I guess jobs will be distributed over these queues. Hence, if we need all Hive jobs running in one queue we should use \"tez.queue.name\".</p><p>Am I missing something here ? </p>","tags":["queue","Hive","hiveserver2","capacity-scheduler","YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-08 13:45:14.0","id":32109,"title":"Hive on Tez Progress of TaskAttempt is always 1.0 and it will never change to another state","body":"<p>hi guys,</p><p style=\"margin-left: 20px;\">My cluster is using hdp-2.3.2.0, anaylse data with hive on tez.</p><p style=\"margin-left: 20px;\">Jobs always go well, but sometimes job can't not be finished, the log of the taskattempt is always 1.0, such like this \"org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1462372008131_4318_m_000000_0 is : 1.0\", and it never change to another state if i do nothing.</p><p style=\"margin-left: 20px;\">And job progress on \"All applications\" ui shows 95%.</p><p style=\"margin-left: 20px;\">\n</p><p><img src=\"/storage/attachments/4101-image.png\"></p><p style=\"margin-left: 20px;\">So I have to kill the job never finished by tez to avoid holding up next jobs, how can i do fix it , i have no idea with this problem</p><p style=\"margin-left: 20px;\">My sense is hive on tez job executed by oozie coordinators, 8 workflow action run concurrently at one time.</p><p style=\"margin-left: 20px;\">Please help . Thank you very much</p><p style=\"margin-left: 20px;\">tips about our cluster: node manager, datanode, regionserver, recource manager started on the same node</p><p style=\"margin-left: 20px;\">and attachement is more detail info about my issue and the container log, Thanks again</p><p><img src=\"/storage/attachments/4100-rm-memory.png\"></p><p><a href=\"/storage/attachments/4102-container.txt\">container.txt</a></p>","tags":["jobs","Hive","oozie-hive","Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-08 16:28:31.0","id":32137,"title":"sqoop job for incremental import execution from oozie","body":"<p>I hear that metastore in sqoop can take care of the incremental imports and that way I do not need to keep track of the last updated id/datetime myself. </p><p>I am trying to execute this from an oozie WF but my question is </p><p>1,what goes into last-value parameter in sqoop command in that case(when I have a sqoop job and metastore configured)?(Do I need to even pass the parameter )?</p><p>2. Also, can I give multiple import statements in single sqoop job?</p><p>3. If yes, How?</p><p>4. Is it a good idea to execute multiple table imports in parallel? (I really would like to know the pros and cons attached to it). </p><p>5. If I plan to have table imports in parallel, do I just fork and execute jobs in oozie?</p>","tags":["Sqoop","import"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-09 03:00:04.0","id":32165,"title":"Improve performance in loading Hbase table using Pig","body":"<p>I am trying to load a HBase table using Pig from HDFS file. The file is just 3 GB with 30350496 records.</p><p>It takes a long time to load the table. Pig is running in tez. Can you please suggest me any ways to improve the performance?</p><p>How to identify where the performance bottle neck is? I am not able to get much from the pig explain. </p><p>Any ways to identify if single Hbase region server is overloaded or if it is getting distributed properly.</p><p>How to identify Hbase regionserver splits ?</p>","tags":["Pig","Hbase","Tez","performance","loader"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-09 08:37:49.0","id":32225,"title":"The 'out-of-box' way to read Avro files present on HDFS","body":"<p>Stack : Installed HDP-2.3.2.0-2950 using Ambari 2.1</p><p>The background can be found <a href=\"https://community.hortonworks.com/questions/31885/basics-unclear-about-external-table-and-hdfs-file.html\">here</a>, I have also read the <a href=\"https://community.hortonworks.com/questions/14661/read-a-avro-file-stored-in-hdfs.html\">existing community thread</a>.</p><p>I did a 'find' on the cluster machine, which revealed several relevant(!) libraries :</p><p>/usr/hdp/2.3.2.0-2950/sqoop/lib/avro-1.7.5.jar</p><p>/usr/hdp/2.3.2.0-2950/sqoop/lib/avro-mapred-1.7.5-hadoop2.jar</p><p>/usr/hdp/2.3.2.0-2950/pig/lib/avro-tools-1.7.5-nodeps.jar</p><p>My objective is straightforward, I have a set of avro files created after a Sqoop import from SQL Server :</p><p><img src=\"/storage/attachments/4089-sqoop-table-import-from-sql-server-to-hdfs.png\"></p><p>I am sure the existing libraries can be used to simply read the avro files to confirm they are formed correctly(I am having a doubt about the import itself !) - what is the quickest, out-of-the-box way ?</p>","tags":["avro","Sqoop","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-09 11:05:17.0","id":32259,"title":"Installing sqoop server","body":"<p>I am using HDP 2.4 and installed sqoop using ambari. But i could just see the sqoop client installed. How to install the sqoopserver/sqoopserver2 using ambari?. can that be done or it should be done manually?</p>","tags":["Sqoop","Ambari"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-11 10:20:03.0","id":32657,"title":"Can we use ORC fileformat in Impala?","body":"<p>Also can we access ORC table stored in Hive metastore in Impala.</p>","tags":["hiveserver2","Hive","orc","impala"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-11 11:05:30.0","id":32690,"title":"Hive compactions on External table","body":"<p>\tHi, </p><p>\tI am currently using Spark streaming to write to an external hive table every 30 mins.</p>\n<pre>rdd.toDF().write.partitionBy(\"dt\").options(options).format(\"orc\").mode(SaveMode.Append).saveAsTable(\"table_name\")\n</pre><p>\tThe issue with this is it creates lots of small files in HDFS, like so</p><pre>\tpart-00000\n\tpart-00000_copy_1</pre><p>My table was created with transactions enabled, and I have enabled ACID transactions on the Hive instance however, I can't see any compactions running nor do any get created when I force compaction with ALTER TABLE command. I would expect compaction to run and merge these files as they are very small 200 KB's in size. </p><p>Any idea's or help greatly appreciated</p>","tags":["Hive","spark-streaming","HDFS"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-10 22:07:00.0","id":32597,"title":"ranger  ldap configuration after enable ranger ssl","body":"<p>Hi,</p><p>I have one question regarding to ranger ldap configuration.</p><p>Is there any value needs to be changed in ranger config if enable ranger ssl? Like must enable ldap ssl or something? Thanks.</p>","tags":["ranger-ldap","ssl"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-11 12:25:27.0","id":32686,"title":"SAS -Connecting to Hive 1.2.1 from SAS 9.3 using odbc driver","body":"<p>Can we connect to hive 1.2.1 from SAS 9.3 using odbc driver?If yes what are steps?</p><p>Thanks!</p>","tags":["hive-odbc","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-11 17:37:30.0","id":32771,"title":"PIG UDF ERROR","body":"<p>I managed to create a UDF, successfully compiled it and packaged it in a customudf.jar file that is located here on a HDP 2.4 sandbox: /user/maria_dev/customudf.jar. I start my PIG script with </p><p>REGISTER customudf.jar </p><p>...and I created a entry the web interface for UDF as follows: </p><div>\n  <div>\n    <h3>UDFs</h3>\n    \n</div></div><p>  \n    \n \n      \n\n        \nName:  customudf</p><p> \n        \nPath:  /user/maria_dev/customudf.jar</p><p> \n        \nOwner : maria_dev</p><p>Yet, when I run my PIG script the line customudf.xformation(f1) generates a non descript error:</p><pre>ERROR org.apache.pig.tools.grunt.Grunt - ERROR 101: file 'customudf.jar' does not exist.</pre><p>What am I missing?</p>","tags":["udf","webhcat","error","Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-11 19:03:49.0","id":32794,"title":"HDP Cluster Health Report","body":"<p>We would like to generate Cluster Health Report. What is the best way to do this?</p><p>This is what I see in the Ambari Documentation</p><p>https://cwiki.apache.org/confluence/display/AMBARI/Ambari+Metrics+API+specification</p><p>Is there any better way to get Health Report from about the Cluster and the Cluster Layout?</p>","tags":["monitoring","Ambari","metrics-collector"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-12 04:40:10.0","id":32871,"title":"phoenix logs","body":"<p>I am using phoenix to create a table in hbase and insert some rows. If the insert is successful, i dont see any message on the console. But if i do see some error, i do see the error in the console. I just want to know where should i look for the logs so that i can check whether any operation through phoenix is failure or success. Is there any place where i could look for phoenix logs. </p><p>I am usign HDP 2.4</p>","tags":["Hbase","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-11 19:21:37.0","id":32808,"title":"Should I be able to view Metron on code-lab platform?","body":"<p>I've been getting all sorts of interesting error messages when I try to view Metron (http://node1:8080) after I installed my code-lab platform. Everything from \"<strong>Upgrade Required </strong>Your version of Elasticsearch is too old. Kibana requires Elasticsearch 0.90.9 or above.\" to  \"Node1 not responding\" even though I can view Ambari and Storm UIs just fine. I've been able to view the Metron UI when I installed apache-metron-0.1BETA-RC7-incubating.tar.gz. Should I be able to view Metron in the code-lap platform? If yes, any thoughts on what I need to do differently. Thanks for your help.</p>","tags":["Metron","ui","vagrant","elasticsearch","cybersecurity"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-05-13 13:50:33.0","id":33193,"title":"How to do logging in Spark Applications without using actions in logger statements?","body":"<p>I am trying to capture the logs for my application before and after the Spark Transformation statement. Being Lazy in evaluation the logs get printed before a transformation is actually evaluated. Is there a way to capture logs without calling any Spark action in log statements, avoiding unnecessary CPU consumption?</p>","tags":["Spark","spark-history-server","logs"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-13 14:09:00.0","id":33218,"title":"Display lineage and table schema after working with tables on Hive","body":"<p>I try to use Altlas for data governance in Hadoop. </p><p>My senario: Load data from MySQL to HDFS using Sqoop and Work with data in HDFS with Hive by creating some view or supplement tables (select or aggregation)</p><p>I want, I've done and I couldn't: </p><p>- I use the Sandbox Hortonworks (http://hortonworks.com/hadoop-tutorial/cross-component-lineage-apache-atlas/) </p><p>- Load data from MySQL to HDFS using Sqoop ==&gt; OK</p><p>==&gt; I see the tables loaded by Sqoop in Atlas UI with their lineage but not their Schema </p><p>- by using import-hive.sh in /bin, atlas create another table in Atlas UI with its schema but not lineage ==&gt; Why? </p><p>How can I build the lineage schema for this table and Why does Atlas UI create another table? </p><p>For example: Using sqoop to load data from MySQL, in Atlas webUI, I have default.test@erietp, after import-hive.sh, I see also default.test@primary? </p><p>- In Hive, I want to create another table (or view) based on the existing table (aggregation, sum, ...), these tables are not showed on Atlas WebUI and I have to rerun import-hive.sh, but it doesn't show the lineage. </p><p>Thank for any suggestion</p>","tags":["ldap","Hive","Atlas","Sqoop"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-13 22:12:27.0","id":33312,"title":"What is currently supported in terms of masking specific fields in HDP ? Is there any way to reverse the masking to original?","body":"","tags":["HDFS"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-13 12:14:01.0","id":33176,"title":"Upgrading from Pivotal HD to Pivotal/Hortonworks HDP","body":"<p>I'm unable to find any documentation on upgrading from Pivotal HD to Pivotal/Hortonworks HDP. Can I follow the usual upgrade process using Ambari?</p>","tags":["upgrade","Ambari","hdp-2.3.0","pivotal"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-13 20:00:27.0","id":33266,"title":"Vagrant password","body":"<p>I am following these instructions http://hortonworks.com/blog/building-hadoop-vm-quickly-ambari-vagrant/ for setting up a cluster using vagrant. When I try to SSH to the VM, I get prompted for password. The default vagrant password \n\"vagrant\" doesn't work. What can I do to bypass the password requirement?</p>","tags":["vagrant"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-14 00:28:50.0","id":33315,"title":"SERVICE_CHECK MAPREDUCE2 - failed","body":"<p>I got this error while upgrading to one of our environment.   I didn't get this error in another environment.</p><p>2016-05-14 00:24:55,618 - Execute['hadoop --config /usr/hdp/2.4.0.0-169/hadoop/conf jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples-2.*.jar wordcount /user/ambari-qa/mapredsmokeinput /user/ambari-qa/mapredsmokeoutput'] {'logoutput': True, 'try_sleep': 5, 'environment': {}, 'tries': 1, 'user': 'ambari-qa', 'path': ['/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/var/lib/ambari-agent:/usr/hdp/2.4.0.0-169/hadoop/bin:/usr/hdp/current/hadoop-yarn-client/bin']}</p><pre>WARNING: Use \"yarn jar\" to launch YARN applications.\n16/05/14 00:24:58 INFO impl.TimelineClientImpl: Timeline service address: http://usw2dzdpma03:8188/ws/v1/timeline/\n16/05/14 00:24:58 INFO client.RMProxy: Connecting to ResourceManager at usw2dzdpma03.glassdoor.local/172.17.213.152:8050\njava.io.FileNotFoundException: File does not exist: hdfs://dfs-nameservices/hdp/apps/2.4.0.0-169/mapreduce/mapreduce.tar.gz\n\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:134)\n\tat org.apache.hadoop.fs.AbstractFileSystem.resolvePath(AbstractFileSystem.java:467)\n\tat org.apache.hadoop.fs.FileContext$25.next(FileContext.java:2193)\n\tat org.apache.hadoop.fs.FileContext$25.next(FileContext.java:2189)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.resolve(FileContext.java:2189)\n\tat org.apache.hadoop.fs.FileContext.resolvePath(FileContext.java:601)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.addMRFrameworkToDistributedCache(JobSubmitter.java:457)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:142)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)\n\tat org.apache.hadoop.examples.WordCount.main(WordCount.java:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)</pre>","tags":["upgrade","hdp-2.4.0","hdp-2.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-14 10:32:34.0","id":33325,"title":"Structured Unstructured Data for Pig and Hive","body":"<p>Hi, </p><p>Can anyone elaborate on why pig and hive are better suited for unstructured and structured respectively?</p><p>My understanding of structured data is data that follows a particular schema and after that I've very little knowledge. </p><p>Is there a limitation with CSV files and variable length fields that Pig can handle easily?   </p>","tags":["Pig","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-15 16:44:12.0","id":33366,"title":"Zeppelin - Spark Filter Function Crashes Zeppelin/Spark","body":"<p>Trying out Zeppelin on HDP 2.4.0 Sandbox and having issues with filter functions when trying with a data frame which seems to crash Spark and/or Zeppelin. I’ve listed below what I'm trying to do and exceptions as well. Is this a known issue? Would love to get a patch if it's available. If we don’t have it, what workaround can we do?</p><p>case class CallColumnMap(LongName: String, ShortName: String, Parent: String)</p><p>val callcolmapDF = sc.textFile(\"hdfs://sandbox.hortonworks.com:8020/tmp/maria_dev/data/CallColumnMap.csv\").map(_.split(\",\")).map(p =&gt; CallColumnMap(p(0), p(1), p(2))).toDF()</p><p>//lookup short name</p><p>val ln = \"time\"</p><p>val prnt = \"detail\"</p><p>callcolmapDF.filter($\"LongName\" === ln && $\"Parent\" === prnt).select(\"ShortName\").head().getAs[String](0)</p><p>org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, <a href=\"http://sandbox.hortonworks.com/\">sandbox.hortonworks.com</a>): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e22_1462973801804_0003_01_000003 on host: <a href=\"http://sandbox.hortonworks.com/\">sandbox.hortonworks.com</a>. Exit status: 50. Diagnostics: Exception from container-launch.</p><p>Container id: container_e22_1462973801804_0003_01_000003</p><p>Exit code: 50</p><p>Stack trace: ExitCodeException exitCode=50:</p><p>  at org.apache.hadoop.util.Shell.runCommand(Shell.java:576)</p><p>  at org.apache.hadoop.util.Shell.run(Shell.java:487)</p><p>  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753)</p><p>  at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)</p><p>  at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303)</p><p>  at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)</p><p>  at java.util.concurrent.FutureTask.run(FutureTask.java:262)</p><p>  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</p><p>  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</p><p>  at java.lang.Thread.run(Thread.java:745)</p><p>Container exited with a non-zero exit code 50</p><p>Driver stacktrace:</p><p>  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)</p><p>  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)</p><p>  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)</p><p>  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</p><p>  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)</p><p>  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)</p><p>  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)</p><p>  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)</p><p>  at scala.Option.foreach(Option.scala:236)</p><p>  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)</p><p>  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)</p><p>  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)</p><p>  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)</p><p>  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</p><p>  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)</p><p>  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)</p><p>  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)</p><p>  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)</p><p>  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:212)</p><p>  at org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)</p><p>  at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)</p><p>  at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)</p><p>  at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)</p><p>  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)</p><p>  at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)</p><p>  at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1537)</p><p>  at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1544)</p><p>  at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1414)</p><p>  at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1413)</p><p>  at org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2138)</p><p>  at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1413)</p><p>  at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1422)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.lookupShortName(&lt;console&gt;:46)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(&lt;console&gt;:93)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(&lt;console&gt;:91)</p><p>  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)</p><p>  at scala.collection.immutable.Map$Map3.foreach(Map.scala:154)</p><p>  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:91)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:101)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:103)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:105)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:107)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:109)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:111)</p><p>  at $iwC$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:113)</p><p>  at $iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:115)</p><p>  at $iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:117)</p><p>  at $iwC$$iwC.&lt;init&gt;(&lt;console&gt;:119)</p><p>  at $iwC.&lt;init&gt;(&lt;console&gt;:121)</p><p>  at &lt;init&gt;(&lt;console&gt;:123)</p><p>  at .&lt;init&gt;(&lt;console&gt;:127)</p><p>  at .&lt;clinit&gt;(&lt;console&gt;)</p><p>  at .&lt;init&gt;(&lt;console&gt;:7)</p><p>  at .&lt;clinit&gt;(&lt;console&gt;)</p><p>  at $print(&lt;console&gt;)</p><p>  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</p><p>  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>  at java.lang.reflect.Method.invoke(Method.java:606)</p><p>  at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)</p><p>  at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)</p><p>  at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)</p><p>  at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)</p><p>  at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)</p><p>  at org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)</p><p>  at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:673)</p><p>  at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:666)</p><p>  at org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)</p><p>  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)</p><p>  at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:295)</p><p>  at org.apache.zeppelin.scheduler.Job.run(Job.java:171)</p><p>  at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)</p><p>  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)</p><p>  at java.util.concurrent.FutureTask.run(FutureTask.java:262)</p><p>  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)</p><p>  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)</p><p>  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</p><p>  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</p><p>  at java.lang.Thread.run(Thread.java:745)</p><p>For some reason my spark-defaults.conf file is getting overwritten and losing the spark.jars entry.</p><p>My Zeppelin notebooks are now unusable, getting lots of odd errors when trying to run anything (no spark context, or timeouts).  Exiting the virtual machine and restarting does not help.</p>","tags":["Sandbox","zeppelin-notebook","Spark","zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-16 10:17:23.0","id":33456,"title":"how to install hadoop on ambari-server node & Can iInstall Ambari-agent on the same node and proceed for further","body":"<p>I am unable to install hadoop on which ambari is installed coz i dont have ssh paswordless maually installed on ambari agent on second node.</p><p>I hve 2 nodes where node 1 consists of only ambari-server and second one is ambari-agent all nn,dn,yarn everything installed on node 2 .</p><p>My question is i am unable to install hadoop services on node 1 coan somebody guide me to to so</p>","tags":["Ambari","migration","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-16 16:27:10.0","id":33541,"title":"Oozie capture error message in Shell action","body":"<p>I am using Shell action to call Spark code. I have used Shell action as I was getting kerberos error in the Spark action. I am unable to capture the actual error message in oozie when the code fails. Instead, I am getting generic error message as below. </p><p><strong>Oozie EL Function:</strong> ${wf:errorMessage(wf:lastErrorNode())}\n<strong></strong></p><p><strong>Error Message:</strong> <em>Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]</em></p><p>Is there any way to capture the underlying error message while using Shell action?</p>","tags":["oozie-shell-action","Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-16 11:08:32.0","id":33470,"title":"Ambari dashboard : HDFS and YARN alerts","body":"<p>Stack : Installed HDP-2.3.2.0-2950 using Ambari 2.1</p><p>There are several alerts seen on the Ambari dashboard(Currently, I am ignoring Accumulo alerts) :</p><p><img src=\"/storage/attachments/4234-dashboard-alerts.png\"></p><p>The HDFS alerts are :</p><p><img src=\"/storage/attachments/4235-alert-nn-checkpoint.png\" style=\"background-color: initial;\"></p><p><img src=\"/storage/attachments/4237-alert-failed-dir-count.png\"></p><p>When I checked the YARN alerts(some NodeManager unreachable and all), I checked one of the hosts and found the NodeManager stopped :</p><p><img src=\"/storage/attachments/4238-nm-stopped.png\"></p><p>When I attempted to start it, I got the following error but couldn't figure out the root cause :</p><pre>stderr:   /var/lib/ambari-agent/data/errors-1005.txt\nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py\", line 35, in &lt;module&gt;\n    BeforeAnyHook().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py\", line 29, in hook\n    setup_users()\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py\", line 41, in setup_users\n    groups = params.user_to_groups_dict[user],\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 152, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 118, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/accounts.py\", line 51, in action_create\n    if getattr(self.resource, option_name) != None and getattr(self.resource, option_name) != attributes[0](self):\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/accounts.py\", line 35, in &lt;lambda&gt;\n    gid=(lambda self: grp.getgrgid(self.user.pw_gid).gr_name, \"-g\"),\nKeyError: 'getgrgid(): gid not found: 7165'\nError: Error: Unable to run the custom hook script ['/usr/bin/python2.6', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py', 'ANY', '/var/lib/ambari-agent/data/command-1005.json', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY', '/var/lib/ambari-agent/data/structured-out-1005.json', 'INFO', '/var/lib/ambari-agent/tmp']\nstdout:   /var/lib/ambari-agent/data/output-1005.txt\n2016-05-16 12:54:38,599 - Group['hadoop'] {}\n2016-05-16 12:54:38,600 - Group['users'] {}\n2016-05-16 12:54:38,600 - Group['knox'] {}\n2016-05-16 12:54:38,600 - Group['spark'] {}\n2016-05-16 12:54:38,601 - User['oozie'] {'gid': 'hadoop', 'groups': ['users']}\nError: Error: Unable to run the custom hook script ['/usr/bin/python2.6', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py', 'ANY', '/var/lib/ambari-agent/data/command-1005.json', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY', '/var/lib/ambari-agent/data/structured-out-1005.json', 'INFO', '/var/lib/ambari-agent/tmp']</pre><p>The (abridged due to size limit) nodemanager log is attached as follows :</p><p><a href=\"/storage/attachments/4248-yarn-yarn-nodemanager-l1034labssssecom-part-1log.txt\">yarn-yarn-nodemanager-l1034labssssecom-part-1log.txt</a></p><p>Another part of the log is attached in the comments below(due to the size and no. of attachments limit)</p><p>How shall I proceed ?</p>","tags":["HDFS","Ambari","nodemanager","ambari-alerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-17 00:54:18.0","id":33621,"title":"input path on sandbox for loading data into spark shell","body":"<h1></h1><p>hi - i am trying to load my json file using spark and\n cannot seem to do it correctly. the path at the end of this bit of \nscala. the file is located on my sandbox in the tmp folder. i've tried:</p><p>val df2 = sqlContext.read.format(\"json\").option(\"samplingRatio\", \"1.0\").load(\"/tmp/rawpanda.json\")</p><p>any help would be great thanks.</p><p>mark</p>","tags":["json","spark-shell"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-09 09:48:23.0","id":32233,"title":"when i import table from sqlserver to hive iam getting this error message while executing query in  FAILED: Hive Internal Error: com.sun.jersey.api.client.ClientHandlerException(java.net.SocketTimeoutException: Read timed out)","body":"<p><img src=\"/storage/attachments/4272-4208-re1.png\"></p><p><img src=\"/storage/attachments/4273-4209-re2.png\"></p><p><img src=\"/storage/attachments/4274-4210-re3.png\"></p>","tags":["hiveserver2","Ambari","Sqoop","HDFS","Hive"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-17 07:41:45.0","id":33679,"title":"Error Config inconsistency exists: unknown configType after Ambari 2.2.2.0 upgrade","body":"<p>I have upgraded Ambari to 2.2.2.0 as described here:\nhttps://docs.hortonworks.com/HDPDocuments/Ambari-2.2.2.0/bk_upgrading_Ambari/bk_upgrading_Ambari-20160509.pdf</p><p>Ambari works and all, the only problem that I have is in the ambari-server.log. I am getting these ERROR messages constantly:</p><p><img src=\"/storage/attachments/4258-config-inconsistency-exists.jpg\"></p><p>What is interesting here is that in this cluster, I dont have hive, pig, ranger and tez installed.\nAny ideas how to fix this?</p><p>EDIT:\nI see now that I am getting metrics correctly from the datanode, while the name node is giving me No Data Available.</p>","tags":["upgrade","ambari-2.2.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-17 05:49:02.0","id":33665,"title":"upgrading hive metastore from 0.12 to 0.13 - No class found  org.apache.hive.beeline.HiveSchemaTool","body":"<p>while starting hive metastore after upgrading getting thhe bellow error </p><p>Exception in thread \"main\" java.lang.ClassNotFoundException: org.apache.hive.beeline.HiveSchemaTool\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n  at java.lang.Class.forName0(Native Method)\n  at java.lang.Class.forName(Class.java:270)\n  at org.apache.hadoop.util.RunJar.run(RunJar.java:214)\n  at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p>","tags":["upgrade","stack-upgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-25 05:09:11.0","id":29298,"title":"I want to import certain tables from multiple SQL sever databases using sqoop to HDFS. Can someone guide me how to do it? an automated script would do well.","body":"","tags":["Sqoop","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-25 02:10:53.0","id":29290,"title":"Ambari alerts getting generated even if the Maintenance mode for the service is ON","body":"<p>Hi All,</p><p>Need your valuable inputs/suggestions on below issues.</p><p>1. Ambari service alerts are generating even if the maintenance mode is turned ON for the respective services. </p><p>2. Also if we turn ON maintenance mode for all Hosts, not able stop the Services using Ambari.</p><p>What could be the reason for these issues?</p><p>Appreciate your help on this.</p>","tags":["Ambari","ambari-alerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-25 19:55:33.0","id":29478,"title":"Hitting GSSException: Defective token detected (Mechanism level: GSSHeader did not find the right tag)","body":"<p>HDP 2.3.4, Ambari 2.1.2, Keberos in place</p><p>Centrify integration is in place.</p><p> I have seen this https://community.hortonworks.com/questions/2580/accessing-hdp-web-ui-from-windows-pc-causes-gsshea.html</p><p>We are using Open JDK Java 1.8.0_65. </p><p>Exact error:</p><p>Error while accessing webhcat</p><p><a href=\"http://10.216.24.11:50111/templeton/v1/status\"><strong>http://xxxxx:50111/templeton/v1/status</strong></a></p><p><strong>HTTP ERROR: 403</strong></p><p>Problem accessing /templeton/v1/status. Reason:</p><p>  GSSException: Defective token detected (Mechanism level: GSSHeader did not find the right tag)</p><p><em>Powered by Jetty://</em></p>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-26 06:41:44.0","id":29547,"title":"Ranger not found in Ambari 2.2.1.1 and HDP 2.4.0","body":"<p>Made a fresh install of HDP-2.4.0 with Ambari-2.2.1.1. We can't seem to find Ranger when choosing Services in the Ambari Registration Wizard. Any clues?</p><p>It's an ubuntu14 system. These are the tarballs we downloaded:</p><p>http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.4.0.0/HDP-2.4.0.0-ubuntu14-deb.tar.gz</p><p>http://public-repo-1.hortonworks.com/ambari/ubuntu14/2.x/updates/2.2.1.1/ambari-2.2.1.1-ubuntu14.tar.gz</p>","tags":["installation","Ranger","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-26 02:55:55.0","id":29534,"title":"Sqooping data with avro file format in hive","body":"<p>I am using Impala , so sqooping as a AvroTable, The scrip is as follows </p><p>hadoop fs -rm -r /user/mcodev/* hdfsdir= \n/hivestage/mco/dev/data/VISHAL/avro/data_provider \npositiondate=\"31-DEC-2014\" hadoop fs -test -d $hdfsdir if [ $? == 0 ]; \nthen echo \"Cleaning up $hdfsdir\" hadoop fs -rm -r $hdfsdir fi export \nSQOOP_HOME=/opt/cloudera/parcels/CDH/lib/sqoop export \nPATH=$PATH:$SQOOP_HOME/bin export \nHIVE_HOME=/opt/cloudera/parcels/CDH/lib/hive export \nHCAT_HOME=/opt/cloudera/parcels/CDH/lib/hive-hcatalog sqoop import \n--verbose --connect jdbc:oracle:thin:@10.91.35.23:1521/MCOUAT \\ --query \n\"SELECT TO_CHAR(TIMEKEY, 'DD-MON-YYYY') AS TIMEKEY, AGGREGATION_KEY, \nCONTRACT_REFERENCE, BALANCE, CPTY_CODE, SYS_PARTITION_KEY, CNTRY_CDE, \nCPTY_BASEL_ENTITY_CLASS, TO_CHAR(MATURITY_DATE, 'DD-MON-YYYY \nHH24:MI:SS') AS MATURITY_DATE, ACCRUED_INTEREST, \nTO_CHAR(NEXT_INTEREST_DATE, 'DD-MON-YYYY HH24:MI:SS') AS \nNEXT_INTEREST_DATE, N_COUNTER FROM MV_MCO_ACCOUNT_DEALS WHERE TIMEKEY = \n'$positiondate' AND \\$CONDITIONS\" \\ --username mcodbowner --password \nMcouatDbOwner_2015 \\ --split-by AGGREGATION_KEY \\ --target-dir \n'$hdfsdir' \\ --as-avrodatafile \\ --compression-codec snappy</p><p> The problem here is I cant find any output in the target directory but can find many files genrated in /user/$UserName/ </p><p>directory. </p><p>How can I import the sqoop data into my avro table.</p><p>I have followed steps as from the website </p><p><a href=\"http://dev.gbif.org/wiki/display/DEV/Sqoop+to+Hive+using+Avro\">http://dev.gbif.org/wiki/display/DEV/Sqoop+to+Hive+using+Avro</a></p><p>But no success, It will be really helpfull If you can guide me in sqooping oracle tables into avro table in HDFS</p>","tags":["avro","Sqoop","hiveserver2"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-27 07:09:24.0","id":29815,"title":"when i am downloading jdbc driver by curl -L 'http://download.microsoft.com/download/0/2/A/02AAE597-3865-456C-AE7F-613F99F850A8/sqljdbc_4.0.2206.100_enu.tar.gz' i am getting this error  curl: (6) Couldn't resolve host 'download.microsoft.com'","body":"","tags":["Sqoop","centos","jdbc"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-27 12:19:24.0","id":29857,"title":"Unable to connect from squirrel to phoenix","body":"<p>Hi all,</p><p>While connecting to phoenix using squirrel it is taking my local system userid instead of taking the cluster credentials.</p><p>See the stack trace below -</p><p>java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.phoenix.exception.PhoenixIOException: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user 'DRawat' (action=create)\nat org.apache.ranger.authorization.hbase.AuthorizationSession.publishResults(AuthorizationSession.java:254)\nat org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.authorizeAccess(RangerAuthorizationCoprocessor.java:595)\nat org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor.requirePermission(RangerAuthorizationCoprocessor.java:664)</p><p>Thanks in advance.</p>","tags":["squirrell","Phoenix"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-30 20:56:53.0","id":30611,"title":"Node labels examples","body":"<p>Hello,</p><p>I configured a yarn cluster to be used with node labels. Could you please provide a simple example by using the \"pi\" or wordcount  applications with node labels.   I tried to use  \"hadoop jar /HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapred*examples.jar  pi  10 10  -node_labels_expression node1\" but I got an error because of the command is not well formed . Without -node_labels_expression the execution is hanged without errors. </p><p>Thanks</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-01 17:30:03.0","id":30676,"title":"Error when running python ez_setup.py","body":"<p>See below. Thank you in advance for resolution.</p><p>[root@sandbox ~]# python ez_setup.py </p><p>Downloading https://pypi.python.org/packages/source/s/setuptools/setuptools-20.10.1.zip\nExtracting in /tmp/tmpT5KgYk\nTraceback (most recent call last):\n  File \"ez_setup.py\", line 415, in &lt;module&gt;\n    sys.exit(main())\n  File \"ez_setup.py\", line 412, in main\n    return _install(archive, _build_install_args(options))\n  File \"ez_setup.py\", line 52, in _install\n    with archive_context(archive_filename):\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"ez_setup.py\", line 103, in archive_context\n    with ContextualZipFile(filename) as archive:\n  File \"ez_setup.py\", line 87, in __new__\n    return zipfile.ZipFile(*args, **kwargs)\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/zipfile.py\", line 766, in __init__\n    self._RealGetContents()\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/zipfile.py\", line 807, in _RealGetContents\n    raise BadZipfile, \"File is not a zip file\"\nzipfile.BadZipfile: File is not a zip file</p>","tags":["tutorial-380","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-04 18:24:59.0","id":31532,"title":"Ambari install without derby","body":"<p>What is the recommended way of installing HDP 2.3 with Ambari 2.2.1 and not use Derby?</p>","tags":["hdp-2.3.0","derby","ambari-2.2.1"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-06 06:26:54.0","id":31844,"title":"Connecting to phoenix using jdbc driver is taking lot of time","body":"<p>Hi,</p><p>I am using the phoenixclient.jar and hbaseclient.jar to connect to phoenix and i have written a very simple jave program</p><p>It is taking lot of time to connect. Is it the behaviour witnessed by anyone else. Any help would be great.</p><p>I even tried to run from the same machien where zookeeper is installed too.</p>","tags":["Phoenix","jdbc","Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-06 21:29:19.0","id":32016,"title":"HBASE Check failed during Ambari install of HDP 2.3.4.7","body":"<p>I have uploaded a file that is a copy of the Ambari information about the failure of the HBASE Check event during an Ambari 2.2.1 install of HDP 2.3.4.7.  This happens during the initial install of a cluster on Openstack instances.  What is wrong?  What do I need to do to correct it on the next install?\n<a href=\"/storage/attachments/4083-check-hbase-fails-in-hdp-2347.txt\"></a></p><p><a href=\"/storage/attachments/4083-check-hbase-fails-in-hdp-2347.txt\">check-hbase-fails-in-hdp-2347.txt</a></p><p>Also is there a way to rerun the HBASE Check on an existing cluster to see if the problem is corrected?</p>","tags":["ambari-2.2.1","hdp-2.3.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-06 16:21:08.0","id":31970,"title":"HDP upgrade fails at pre check with erify Ambari and Ranger Password Synchronization error","body":"<p>Hi,</p><p>I am trying to do express upgrade to upgrade HDp-2.3 to HDP-2.4 via Ambari-2.2.1.1.</p><p>I am getting below error during the pre check</p><p>Verify Ambari and Ranger Password Synchronization\n        </p><pre>Reason: Unexpected server error happened\nFailed on: </pre><p>In ambari-server.logs I founf below error messages</p><p>06 May 2016 10:51:41,576 ERROR [qtp-ambari-client-295] CheckHelper:72 - Check SERVICES_RANGER_PASSWORD_VERIFY failed\njava.lang.IllegalStateException: Can't get secure connection to https://server1:6182/service/public/api/repository/count.  Truststore path or password is not set.</p><p>I see that there is a bug which is fixed in Ambari-2.2.2 which is not releases by Hortonworks</p><p>BUG: https://issues.apache.org/jira/browse/AMBARI-15980</p><p>Please let me know the workaround to get over this precheck fail.</p><p>Thanks,</p><p>Venkat</p>","tags":["upgrade","hdp-2.4.0","ambari-2.2.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-06 11:26:40.0","id":31908,"title":"Why only two nodes are running Map tasks?","body":"<p>I have a 4  node cluster. I am running a MapReduce job on this cluster.  The input file is a JSON file  of the size 1.53 GB. The Mapper task is reading a JSON record and manipulating the text. I observed the following, after I executed the Job.</p><p>1) There are 15 Mapper tasks, which is correct. (no issues here)</p><p>2) Only 1% of the job is processed in 50 minutes, which is very slow.</p><p>3) Only 4 mapper task is shown running. </p><p>4) Two mappers are running on Machine1 and other two mapers are running on Machine2.</p><p>5) Mapper task 1 in Machine 1 is showing total 21627027 as read and keeps increasing after a few seconds.  </p><p>Following is what I need to understand:</p><p>1) Why only two Nodes have all the Mapper tasks running. Why are the other nodes not running any mapper?</p><p>2) If one mapper is per 128 MB file block, why the mapper task on machine 1 is showing 21627027 byes (21 MB) of data ? (Edited: I had mentioned  21120 MB, which was a calculation mistake. The correct figure is 21 MB.)</p>","tags":["hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-07 18:44:07.0","id":32085,"title":"optimized oozie workflow to import multiple tables","body":"<p>what would be the most optimal way to import tables in oozie (let's say 8-10 tables ) from a database that has 100's of them. Now, I can not use exclude option with import all and specify 90 tables which I need to exclude.</p><p>Also, I believe that the table imports should be parallel jobs in oozie? Since these are independent of each other, why not? In that case, </p><p>1. how exactly do I specify tables imports in parallel? (from same and different databases).</p><p>2. How do I have incremental imports every day i.e I import only the newly added data everyday at a particular time?(I have a co-ordinator.xml but not sure about passing the date / recordnum. for incremental imports).</p><p>3. How do I specify different frequencies ? i.e some of the imports happen daily and rest may be once in 3 days?</p><p>4. The APPEND option can limit data movement only to new or updated records. How exactly? how is it different from incremental imports? Which one is performance-wise better and faster)?</p><p>Also, Thank you so much for maintaining this wonderfully informed community :).</p>","tags":["import","optimization","Oozie","workflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-08 16:28:16.0","id":32136,"title":"YARN ACLS creation from Ambari and Command line","body":"<p>How to create YARN acls from Ambari and Command line?</p>","tags":["yarn-scheduler"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-09 13:46:53.0","id":32300,"title":"HDFS encryption and 3rd party KMS","body":"<p>Is it possible to integrate HDFS encryption with 3rd party KMS solutions? for example: Microsoft KMS</p><p>I do see that there is Ranger KMS https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4/bk_Ranger_KMS_Admin_Guide/content/ch_ranger_kms_overview.html but we are looking into the option to integrate HDFS encryption with the existing KMS.</p>","tags":["kms","ranger-kms"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-09 17:04:00.0","id":32327,"title":"Using hash functions in hive to figure out which records changed/added to a large table","body":"<p>I have a hive table to which new partitions get added (say daily). \nAnd I want to write a daily hive query that tells me which records changed or were added that day.</p><p>A unique record is a combination of multiple columns.</p><p>Would using hive's hash or sha (with 256) udf be the best and most performant route to writing such a query? And will using a 256 hash be good enough to prevent collisions?</p>","tags":["Hive","hive-udf"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-09 19:40:23.0","id":32360,"title":"Number of Zookepers on small cluster","body":"<p>Given the best practice of separating master and slave node configuration, for the sake of argument, if you have a 2/3 master/slave node configuration, is it recommended to have 3 zookeeper masters and have the other zookeeper installed on a slave node or simply install one zookeeper one of the master nodes? Appreciate the input. </p>","tags":["cluster","zookeeper"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-05-10 03:39:10.0","id":32390,"title":"JobHistory UI is not showing the running jobs.","body":"<p>I have a job running in the cluster, but I am unable to see that job through the JobHistory UI. I can only see the job if I execute the command \"hadoop job -list\" in the linux command prompt. I have observed that if I go to \"ResourceManager UI\" I see a running application, but I do not see any jobs of that running application through \"JobHistory UI\".  In the ResourceManager UI, I have also observed that the latest application that I executed is associated with \"ApplicationMaster' under \"Tracking UI\" field. Rest of the other Applications are associated with \"History\" under \"Tracking UI\" field. Is this the reason why I cannot see all the jobs under JobHistory UI for this application because it is associated with ApplicationMaster?.  </p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-04 16:29:06.0","id":31499,"title":"How to load and store nvarchar","body":"<p>Stack : Installed HDP-2.3.2.0-2950 using Ambari 2.1</p><p>The steps that I am following :</p><ol>\n<li>Load SQL server tables onto HDFS using Sqoop</li><li>Create EXTERNAL tables in Hive</li></ol><p>I didn't use anything pertaining to charset/unicode/utf-8 while executing the sqoop import commands.</p><p>While creating the Hive external table, I was wondering what data type shall I select for the nvarchar columns in the original sql server table, now I am worried that even in Sqoop while importing that needs to be addressed.</p><ol>\n<li>Couldn't find relevant charset/nvarchar etc. options in Sqoop import</li><li>In Hive, can varchar/string blindly be used in place of nvarchar</li></ol><p>****************************edit : Added further code</p><p>I now have a problem, Sqoop is auto. converting nvarchar to String</p><ul>\n<li>When I mention --as-avrodatafile, .avro files and a schema file is created which is as follows :</li></ul><pre>[sqoop@l1038lab root]$ hadoop fs -cat /dataload/tohdfs/reio/odpdw/may2016/DimSampleDesc/DimSampleDesc.avsc\n{\n  \"type\" : \"record\",\n  \"name\" : \"DimSampleDesc\",\n  \"doc\" : \"Sqoop import of DimSampleDesc\",\n  \"fields\" : [ {\n    \"name\" : \"SmapiName_ver\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"SmapiName_ver\",\n    \"sqlType\" : \"12\"\n  }, {\n    \"name\" : \"SmapiColName\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"SmapiColName\",\n    \"sqlType\" : \"12\"\n  }, {\n    \"name\" : \"ChartType\",\n    \"type\" : [ \"null\", \"int\" ],\n    \"default\" : null,\n    \"columnName\" : \"ChartType\",\n    \"sqlType\" : \"4\"\n  }, {\n    \"name\" : \"X_Indexet\",\n    \"type\" : [ \"null\", \"int\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_Indexet\",\n    \"sqlType\" : \"4\"\n  }, {\n    \"name\" : \"Y_Indexet\",\n    \"type\" : [ \"null\", \"int\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_Indexet\",\n    \"sqlType\" : \"4\"\n  }, {\n    \"name\" : \"X_Tick\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_Tick\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"Y_Tick\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_Tick\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"X_TickRange\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_TickRange\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"X_TickRangeFrom\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_TickRangeFrom\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"X_TickRangeTom\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_TickRangeTom\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"Y_TickRange\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_TickRange\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"Y_TickRangeFrom\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_TickRangeFrom\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"Y_TickRangeTom\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_TickRangeTom\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"IndexCount\",\n    \"type\" : [ \"null\", \"int\" ],\n    \"default\" : null,\n    \"columnName\" : \"IndexCount\",\n    \"sqlType\" : \"4\"\n  }, {\n    \"name\" : \"X_IndexCount\",\n    \"type\" : [ \"null\", \"int\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_IndexCount\",\n    \"sqlType\" : \"4\"\n  }, {\n    \"name\" : \"Y_IndexCount\",\n    \"type\" : [ \"null\", \"int\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_IndexCount\",\n    \"sqlType\" : \"4\"\n  }, {\n    \"name\" : \"X_Symbol\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_Symbol\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"X_SymbolName\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_SymbolName\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"X_SymbolDescr\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"X_SymbolDescr\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"Y_Symbol\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_Symbol\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"Y_SymbolName\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_SymbolName\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"Y_SymbolDescr\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"Y_SymbolDescr\",\n    \"sqlType\" : \"-9\"\n  }, {\n    \"name\" : \"SmapiName\",\n    \"type\" : [ \"null\", \"string\" ],\n    \"default\" : null,\n    \"columnName\" : \"SmapiName\",\n    \"sqlType\" : \"12\"\n  }, {\n    \"name\" : \"Incorrect_Ver_FL\",\n    \"type\" : [ \"null\", \"boolean\" ],\n    \"default\" : null,\n    \"columnName\" : \"Incorrect_Ver_FL\",\n    \"sqlType\" : \"-7\"\n  } ],\n  \"tableName\" : \"DimSampleDesc\"\n}[sqoop@l1038lab root]$</pre><ul>\n<li>I then simply create a hive external table</li></ul><pre>hive&gt;\n    &gt; CREATE EXTERNAL TABLE DimSampleDesc  ROW FORMAT SERDE  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'  STORED as INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'  OUTPUTFORMAT  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'  TBLPROPERTIES (    'avro.schema.url'='hdfs://l1031lab.sss.se.com:8020/dataload/tohdfs/reio/odpdw/may2016/DimSampleDesc/DimSampleDesc.avsc');\nOK\nTime taken: 0.37 seconds\nhive&gt;</pre><ul>\n<li>The table described as follows :</li></ul><pre>hive (odp_dw_may2016)&gt;\n                     &gt;\n                     &gt; describe formatted dimsampledesc;\nOK\ncol_name        data_type       comment\n# col_name              data_type               comment\nsmapiname_ver           string\nsmapicolname            string\ncharttype               int\nx_indexet               int\ny_indexet               int\nx_tick                  string\ny_tick                  string\nx_tickrange             string\nx_tickrangefrom         string\nx_tickrangetom          string\ny_tickrange             string\ny_tickrangefrom         string\ny_tickrangetom          string\nindexcount              int\nx_indexcount            int\ny_indexcount            int\nx_symbol                string\nx_symbolname            string\nx_symboldescr           string\ny_symbol                string\ny_symbolname            string\ny_symboldescr           string\nsmapiname               string\nincorrect_ver_fl        boolean\n# Detailed Table Information\nDatabase:               odp_dw_may2016\nOwner:                  hive\nCreateTime:             Mon May 09 16:37:08 CEST 2016\nLastAccessTime:         UNKNOWN\nProtect Mode:           None\nRetention:              0\nLocation:               hdfs://l1031lab.sss.se.com:8020/apps/hive/warehouse/odp_dw_may2016.db/dimsampledesc\nTable Type:             EXTERNAL_TABLE\nTable Parameters:\n        COLUMN_STATS_ACCURATE   false\n        EXTERNAL                TRUE\n        avro.schema.url         hdfs://l1031lab.sss.se.com:8020/dataload/tohdfs/reio/odpdw/may2016/DimSampleDesc/DimSampleDesc.avsc\n        numFiles                6\n        numRows                 -1\n        rawDataSize             -1\n        totalSize               8566342\n        transient_lastDdlTime   1462804628\n# Storage Information\nSerDe Library:          org.apache.hadoop.hive.serde2.avro.AvroSerDe\nInputFormat:            org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\nOutputFormat:           org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat\nCompressed:             No\nNum Buckets:            -1\nBucket Columns:         []\nSort Columns:           []\nStorage Desc Params:\n        serialization.format    1\nTime taken: 0.441 seconds, Fetched: 56 row(s)\nhive (odp_dw_may2016)&gt;</pre><p>What's the risk here and how do I proceed ?</p>","tags":["hive-serde","Hive","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-11 02:57:33.0","id":32614,"title":"HDPCD Exam Code \"hdp-cp-24\"","body":"<p>While searching for the exam on \"<a href=\"http://www.examslocal.com/\">www.examslocal.com</a>\" I am not getting \"Hortonworks : HDP Certified Developer (HDPCD) – English\". I am getting this one : \"Hortonworks. : HDP Certified Developer (hdp-cp-24) - English\".</p><p>Is \"hdp-cp-24\" the right one to take up for HDP Certified Developer Certification (Non-Java) ??</p>","tags":["hdpcd"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-12 12:13:17.0","id":32912,"title":"Hive external table : java.lang.RuntimeException","body":"<pre>Stack : Installed HDP-2.3.2.0-2950 using Ambari 2.1</pre><p>I imported a table from SQL Server :</p><pre>sqoop import --connect 'jdbc:sqlserver://dbserver;database=dbname' --username username --password somepassword --as-textfile --fields-terminated-by '|&|' --num-mappers 8 --table DimECUDTCCode  --warehouse-dir /dataload/tohdfs/reio/odpdw/may2016 --verbose</pre><p>The contents on hdfs :</p><p><img src=\"/storage/attachments/4190-sqoop-text-files.png\"></p><p>I created external table :</p><pre>CREATE EXTERNAL TABLE IF NOT EXISTS DimECUDTCCode (`ECU_DTC_ID` int,`DTC_CDE` char(20),`ECU_NAME` varchar(15),`ECU_FAMILY_NAME` varchar(15),`DTC_DESC` varchar(65355),`INSERTED_BY`varchar(64),`INSERTION_DATE` timestamp,`DTC_CDE_DECIMAL` int) ROW FORMAT DELIMITED\n   FIELDS TERMINATED BY '|' STORED AS ORC LOCATION '/dataload/tohdfs/reio/odpdw/may2016/DimECUDTCCode';</pre><p>I get the following exception :</p><pre>hive (odp_dw_may2016_orc)&gt;\n                         &gt;\n                         &gt;\n                         &gt; select * from DimECUDTCCode limit 10;\nOK\ndimecudtccode.ecu_dtc_id        dimecudtccode.dtc_cde   dimecudtccode.ecu_name  dimecudtccode.ecu_family_name   dimecudtccode.dtc_desc  dimecudtccode.inserted_by       dimecudtccode.insertion_date dimecudtccode.dtc_cde_decimal\nFailed with exception java.io.IOException:java.lang.RuntimeException: serious problem\nTime taken: 0.075 seconds\nhive (odp_dw_may2016_orc)&gt;</pre><p>In /tmp/hive/hive.log, I get the following exception(NullPointer is the root cause but I don't understand 'no space left' , which device ?) :</p><pre>2016-05-12 13:01:25,605 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - &lt;PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl&gt;\n2016-05-12 13:01:25,620 ERROR [main]: CliDriver (SessionState.java:printError(960)) - Failed with exception java.io.IOException:java.lang.RuntimeException: serious problem\njava.io.IOException: java.lang.RuntimeException: serious problem\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)\nat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)\nat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.lang.RuntimeException: serious problem\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1025)\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1052)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:363)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:295)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:446)\n... 15 more\nCaused by: java.lang.NullPointerException\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$BISplitStrategy.getSplits(OrcInputFormat.java:564)\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1014)\n... 19 more\n2016-05-12 13:01:25,620 INFO  [main]: exec.TableScanOperator (Operator.java:close(613)) - 0 finished. closing... \n2016-05-12 13:01:25,620 INFO  [main]: exec.SelectOperator (Operator.java:close(613)) - 1 finished. closing... \n2016-05-12 13:01:25,620 INFO  [main]: exec.LimitOperator (Operator.java:close(613)) - 2 finished. closing... \n2016-05-12 13:01:25,620 INFO  [main]: exec.ListSinkOperator (Operator.java:close(613)) - 4 finished. closing... \n2016-05-12 13:01:25,620 INFO  [main]: exec.ListSinkOperator (Operator.java:close(635)) - 4 Close done\n2016-05-12 13:01:25,620 INFO  [main]: exec.LimitOperator (Operator.java:close(635)) - 2 Close done\n2016-05-12 13:01:25,621 INFO  [main]: exec.SelectOperator (Operator.java:close(635)) - 1 Close done\n2016-05-12 13:01:25,621 INFO  [main]: exec.TableScanOperator (Operator.java:close(635)) - 0 Close done\n2016-05-12 13:01:25,625 INFO  [main]: CliDriver (SessionState.java:printInfo(951)) - Time taken: 0.091 seconds\n2016-05-12 13:01:25,626 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - &lt;PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver&gt;\n2016-05-12 13:01:25,626 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - &lt;/PERFLOG method=releaseLocks start=1463050885626 end=1463050885626 duration=0 from=org.apache.hadoop.hive.ql.Driver&gt;\n2016-05-12 13:01:25,657 INFO  [Atlas Logger 3]: security.SecureClientUtils (SecureClientUtils.java:getClientConnectionHandler(91)) - Real User: hive (auth:SIMPLE), is from ticket cache? false\n2016-05-12 13:01:25,657 INFO  [Atlas Logger 3]: security.SecureClientUtils (SecureClientUtils.java:getClientConnectionHandler(94)) - doAsUser: hive\n2016-05-12 13:01:26,302 INFO  [Atlas Logger 3]: hook.HiveHook (HiveHook.java:run(168)) - Atlas hook failed\norg.apache.atlas.AtlasServiceException: Metadata service API SEARCH_GREMLIN failed with status 400(Bad Request) Response Body ({\"error\":\"javax.script.ScriptException: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\",\"stackTrace\":\"org.apache.atlas.discovery.DiscoveryException: javax.script.ScriptException: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat org.apache.atlas.discovery.graph.GraphBackedDiscoveryService.searchByGremlin(GraphBackedDiscoveryService.java:175)\\n\\tat org.apache.atlas.GraphTransactionInterceptor.invoke(GraphTransactionInterceptor.java:41)\\n\\tat org.apache.atlas.web.resources.MetadataDiscoveryResource.searchUsingGremlinQuery(MetadataDiscoveryResource.java:155)\\n\\tat sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:606)\\n\\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\\n\\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\\n\\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\\n\\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\\n\\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\\n\\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\\n\\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\\n\\tat org.apache.atlas.web.filters.AuditFilter.doFilter(AuditFilter.java:67)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\\n\\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\\n\\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\\n\\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\\n\\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\\n\\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\\n\\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\\n\\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\\n\\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\\n\\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\\n\\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\\n\\tat org.mortbay.jetty.Server.handle(Server.java:326)\\n\\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\\n\\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\\n\\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\\n\\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\\n\\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\\n\\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\\n\\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\\nCaused by: javax.script.ScriptException: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:94)\\n\\tat javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:233)\\n\\tat org.apache.atlas.discovery.graph.GraphBackedDiscoveryService.searchByGremlin(GraphBackedDiscoveryService.java:172)\\n\\t... 48 more\\nCaused by: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:221)\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:90)\\n\\t... 50 more\\nCaused by: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.newTransaction(StandardTitanGraph.java:276)\\n\\tat com.thinkaurelius.titan.graphdb.transaction.StandardTransactionBuilder.start(StandardTransactionBuilder.java:220)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.newThreadBoundTransaction(StandardTitanGraph.java:265)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getAutoStartTx(TitanBlueprintsGraph.java:104)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.query(TitanBlueprintsGraph.java:225)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.query(TitanBlueprintsGraph.java:27)\\n\\tat com.tinkerpop.pipes.transform.GraphQueryPipe.processNextStart(GraphQueryPipe.java:34)\\n\\tat com.tinkerpop.pipes.transform.GraphQueryPipe.processNextStart(GraphQueryPipe.java:17)\\n\\tat com.tinkerpop.pipes.AbstractPipe.next(AbstractPipe.java:89)\\n\\tat com.tinkerpop.pipes.IdentityPipe.processNextStart(IdentityPipe.java:19)\\n\\tat com.tinkerpop.pipes.AbstractPipe.next(AbstractPipe.java:89)\\n\\tat com.tinkerpop.pipes.IdentityPipe.processNextStart(IdentityPipe.java:19)\\n\\tat com.tinkerpop.pipes.AbstractPipe.hasNext(AbstractPipe.java:98)\\n\\tat com.tinkerpop.pipes.util.Pipeline.hasNext(Pipeline.java:105)\\n\\tat org.codehaus.groovy.runtime.DefaultGroovyMethods.toList(DefaultGroovyMethods.java:1946)\\n\\tat org.codehaus.groovy.runtime.dgm$836.invoke(Unknown Source)\\n\\tat org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:271)\\n\\tat org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:53)\\n\\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)\\n\\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)\\n\\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)\\n\\tat Script176.run(Script176.groovy:1)\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:219)\\n\\t... 51 more\\nCaused by: com.thinkaurelius.titan.diskstorage.PermanentBackendException: Could not start BerkeleyJE transaction\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJEStoreManager.beginTransaction(BerkeleyJEStoreManager.java:144)\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJEStoreManager.beginTransaction(BerkeleyJEStoreManager.java:34)\\n\\tat com.thinkaurelius.titan.diskstorage.keycolumnvalue.keyvalue.OrderedKeyValueStoreManagerAdapter.beginTransaction(OrderedKeyValueStoreManagerAdapter.java:52)\\n\\tat com.thinkaurelius.titan.diskstorage.Backend.beginTransaction(Backend.java:465)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.openBackendTransaction(StandardTitanGraph.java:282)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.newTransaction(StandardTitanGraph.java:272)\\n\\t... 73 more\\nCaused by: com.sleepycat.je.LogWriteException: (JE 5.0.73) Environment must be closed, caused by: com.sleepycat.je.LogWriteException: Environment invalid because of previous exception: (JE 5.0.73) \\/var\\/lib\\/atlas\\/data\\/berkeley java.io.IOException: No space left on device LOG_WRITE: IOException on write, log is likely incomplete. Environment is invalid and must be closed.\\n\\tat com.sleepycat.je.LogWriteException.wrapSelf(LogWriteException.java:72)\\n\\tat com.sleepycat.je.dbi.EnvironmentImpl.checkIfInvalid(EnvironmentImpl.java:1512)\\n\\tat com.sleepycat.je.Environment.checkEnv(Environment.java:2185)\\n\\tat com.sleepycat.je.Environment.beginTransactionInternal(Environment.java:1313)\\n\\tat com.sleepycat.je.Environment.beginTransaction(Environment.java:1284)\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJEStoreManager.beginTransaction(BerkeleyJEStoreManager.java:134)\\n\\t... 78 more\\nCaused by: com.sleepycat.je.LogWriteException: Environment invalid because of previous exception: (JE 5.0.73) \\/var\\/lib\\/atlas\\/data\\/berkeley java.io.IOException: No space left on device LOG_WRITE: IOException on write, log is likely incomplete. Environment is invalid and must be closed.\\n\\tat com.sleepycat.je.log.FileManager.writeLogBuffer(FileManager.java:1652)\\n\\tat com.sleepycat.je.log.LogBufferPool.writeBufferToFile(LogBufferPool.java:260)\\n\\tat com.sleepycat.je.log.LogBufferPool.writeCompleted(LogBufferPool.java:345)\\n\\tat com.sleepycat.je.log.LogManager.serialLogWork(LogManager.java:716)\\n\\tat com.sleepycat.je.log.LogManager.serialLogInternal(LogManager.java:493)\\n\\tat com.sleepycat.je.log.SyncedLogManager.serialLog(SyncedLogManager.java:42)\\n\\tat com.sleepycat.je.log.LogManager.multiLog(LogManager.java:395)\\n\\tat com.sleepycat.je.log.LogManager.log(LogManager.java:335)\\n\\tat com.sleepycat.je.txn.Txn.logCommitEntry(Txn.java:957)\\n\\tat com.sleepycat.je.txn.Txn.commit(Txn.java:719)\\n\\tat com.sleepycat.je.txn.Txn.commit(Txn.java:584)\\n\\tat com.sleepycat.je.Transaction.commit(Transaction.java:317)\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJETx.commit(BerkeleyJETx.java:81)\\n\\tat com.thinkaurelius.titan.diskstorage.keycolumnvalue.cache.CacheTransaction.commit(CacheTransaction.java:198)\\n\\tat com.thinkaurelius.titan.diskstorage.BackendTransaction.commitStorage(BackendTransaction.java:117)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.commit(StandardTitanGraph.java:670)\\n\\tat com.thinkaurelius.titan.graphdb.transaction.StandardTitanTx.commit(StandardTitanTx.java:1337)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.commit(TitanBlueprintsGraph.java:60)\\n\\tat org.apache.atlas.GraphTransactionInterceptor.invoke(GraphTransactionInterceptor.java:42)\\n\\tat org.apache.atlas.services.DefaultMetadataService.createEntity(DefaultMetadataService.java:231)\\n\\tat org.apache.atlas.web.resources.EntityResource.submit(EntityResource.java:96)\\n\\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:606)\\n\\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\\n\\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\\n\\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\\n\\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\\n\\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\\n\\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\\n\\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\\n\\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\\n\\tat org.apache.atlas.web.filters.AuditFilter.doFilter(AuditFilter.java:67)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\\n\\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\\n\\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\\n\\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\\n\\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\\n\\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\\n\\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\\n\\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\\n\\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\\n\\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\\n\\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\\n\\tat org.mortbay.jetty.Server.handle(Server.java:326)\\n\\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\\n\\tat org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\\n\\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\\n\\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\\n\\t... 3 more\\nCaused by: java.io.IOException: No space left on device\\n\\tat java.io.RandomAccessFile.writeBytes0(Native Method)\\n\\tat java.io.RandomAccessFile.writeBytes(RandomAccessFile.java:520)\\n\\tat java.io.RandomAccessFile.write(RandomAccessFile.java:550)\\n\\tat com.sleepycat.je.log.FileManager.writeToFile(FileManager.java:1757)\\n\\tat com.sleepycat.je.log.FileManager.writeLogBuffer(FileManager.java:1637)\\n\\t... 65 more\\n\"})\nat org.apache.atlas.AtlasClient.callAPIWithResource(AtlasClient.java:365)\nat org.apache.atlas.AtlasClient.callAPIWithResource(AtlasClient.java:346)\nat org.apache.atlas.AtlasClient.searchByGremlin(AtlasClient.java:294)\nat org.apache.atlas.hive.bridge.HiveMetaStoreBridge.getEntityReferenceFromGremlin(HiveMetaStoreBridge.java:227)\nat org.apache.atlas.hive.bridge.HiveMetaStoreBridge.getProcessReference(HiveMetaStoreBridge.java:183)\nat org.apache.atlas.hive.hook.HiveHook.registerProcess(HiveHook.java:297)\nat org.apache.atlas.hive.hook.HiveHook.fireAndForget(HiveHook.java:202)\nat org.apache.atlas.hive.hook.HiveHook.access$200(HiveHook.java:54)\nat org.apache.atlas.hive.hook.HiveHook$2.run(HiveHook.java:166)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)</pre><p>How shall I proceed ?</p>","tags":["Sqoop","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-12 12:13:24.0","id":32943,"title":"HDPCD(non java) exam not available to register in examslocal website.​Is HDPCD exam retired?","body":"<p>I am not able to find HDPCD exam in the examslocal website.Please help.</p>","tags":["hdpcd"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-13 06:57:38.0","id":33131,"title":"record password for sqoop job in sqoop-site.xml","body":"<p>I have created a sqoop job:</p><p>sqoop job --create test_products -- import --driver com.mysql.jdbc.Driver --connect jdbc:mysql://IP/DB?zeroDateTimeBehavior=convertToNull --username root --password PASSWORD --table products --m 1 --warehouse-dir test/</p><p>Now, I have set record.password to true in sqoop-site.xml.</p><p>As long as I was adding the passwords manually though the command line, it works.But when I record the passwords,it throws:</p><pre>ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: Access denied for user 'root'@'10.10.10.9' (using password: YES)\njava.sql.SQLException: Access denied for user 'root'@'10.10.10.9' (using password: YES)\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:935)\nat com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:4101)\nat com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1300)\nat com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2337)\nat com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2370)\nat com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2154)\nat com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:792)\nat com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:49)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\nat com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:381)\nat com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:305)\nat java.sql.DriverManager.getConnection(DriverManager.java:664)\nat java.sql.DriverManager.getConnection(DriverManager.java:247)\nat org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:885)\nat org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:52)\nat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:744)\nat org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:767)\nat org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:270)\nat org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:241)\nat org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:227)\nat org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:295)\nat org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1845)\nat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1645)\nat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)\nat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)\nat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)\nat org.apache.sqoop.tool.JobTool.execJob(JobTool.java:228)\nat org.apache.sqoop.tool.JobTool.run(JobTool.java:283)\nat org.apache.sqoop.Sqoop.run(Sqoop.java:148)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\nat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:184)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:226)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:235)\nat org.apache.sqoop.Sqoop.main(Sqoop.java:244)\n16/05/13 12:20:49 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter\nat org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1651)\nat org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)\nat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)\nat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)\nat org.apache.sqoop.tool.JobTool.execJob(JobTool.java:228)\nat org.apache.sqoop.tool.JobTool.run(JobTool.java:283)\nat org.apache.sqoop.Sqoop.run(Sqoop.java:148)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\nat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:184)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:226)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:235)\nat org.apache.sqoop.Sqoop.main(Sqoop.java:244)</pre><p>I made sure that the password is correcy. I made sure that root user can access the server with given password but somehow the recorded passwords throw this exception. How do I debug this or may be fix this? What could possibly be the problem here?</p><p>Note:** I am using MySQL for sqoop metastore. Though some of the sources say none other than HSQLDB is supported,some others say it does. </p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-22 08:30:49.0","id":34740,"title":"HDPCD exam not launched.","body":"<p><a rel=\"user\" href=\"/users/164/rich.html\" nodeid=\"164\">@Rich Raposa</a> </p><p>Hi,The above mentioned exam which was scheduled at 11 AM on 20th MAY 2016 was not launched giving following status message-\nLaunch Status: Delivery partner doesn't have the exam ready,</p><p>I had a chat conversation with your Mr.Mauro. He had suggested to wait for 30 minutes. But after 30 minutes also the exam is not launched. \nFollowing are the details of the exam-\n</p><div><div><table><tbody><tr><td><div><strong>Hortonworks</strong>\nHDP Certified Developer\nHDPCD - English\nLaunch Status: Delivery partner doesn't have the exam ready\n</div></td></tr><tr>\n<td><img src=\"https://ci5.googleusercontent.com/proxy/n45NZF5YQNRyxqMv9YR0FPlX4eGeFACKWu6iBjWemtruWgfedUfRK8aEmd8llD5vF67hjOVHiR7Lbelp4wfaNBxCGO79XC3ctB8mNQc=s0-d-e1-ft#https://www.examslocal.com/Content/Images/icon_date.png\" alt=\"date\" style=\"margin: 0px; padding: 0px; outline: 0px; vertical-align: baseline; background: transparent;\"></td></tr><tr>\n<td><img src=\"https://ci5.googleusercontent.com/proxy/H4_ay7ydinoCrs5qwX4nhO3vl7S2Q86jg0w5isPWkmu2ONdaRc_WtLZRLUhiVJGbNDykpwHPvOsv2A1ibhgG9pzHhQli-aAfSWytuKk=s0-d-e1-ft#https://www.examslocal.com/Content/Images/icon_time.png\" alt=\"time\" style=\"margin: 0px; padding: 0px; outline: 0px; vertical-align: baseline; background: transparent;\"></td><td>11:00 AM - 1:00 PM - (UTC+05:30) Chennai, Kolkata, Mumbai, New Delhi</td></tr></tbody></table></div><p><strong>Confirmation Code</strong>\n007-A10</p><p><strong>Transaction Date</strong>\n5/18/2016</p>\n<p><strong>Transaction Details</strong></p><table><tbody><tr><td>Exam Fee</td><td>250 USD</td></tr><tr><td>Total Amount</td><td>250 USD</td></tr></tbody></table>I had logged in again at 12.30 to checked rescheduling status.\nThere i could see launch exam option . But nobody responding on the live chat.\nGetting the following message -\nPlease wait while your exam monitor rejoins the session, reviews your information and releases your exam.</div><div>I have taken screen shots of the errors also sent emails to your support teams time to time.</div><p>Please suggest the further course of action.\n</p>","tags":["hdpcd","error"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-23 08:10:40.0","id":34846,"title":"passing multiple parameters through connection string","body":"<p>I am trying to create a sqoop job that imports data from MySQL. </p><pre>sqoop job --meta-connect jdbc:hsqldb:hsql://FQDN:16000/sqoop --create JOB_NAME -- import --connect jdbc:mysql://IP/DB?zeroDateTimeBehavior=convertToNull&dontTrackOpenResources=true&defaultFetchSize=1000&useCursorFetch=true --username USERNAME --password 'PASSWORD' --driver com.mysql.jdbc.Driver --table sales_flat_order_item --incremental lastmodified --check-column updated_at --last-value 0 --merge-key field_name --split-by field_name --hive-import --hive-overwrite --hive-database dbName</pre><p>However, it works fine as long as I pass only one parameter to the connection string like:</p><pre>jdbc:mysql://IP/DB?zeroDateTimeBehavior=convertToNull</pre><p>but as soon as I pass multiple key-values in connection string separated by &(which I believe is the right way to do it?)</p><pre>jdbc:mysql://IP/DB?zeroDateTimeBehavior=convertToNull&dontTrackOpenResources=true&defaultFetchSize=1000&useCursorFetch=true</pre><p>It throws, </p><pre>username :command not found.</pre><p>I have tried replacing & with &&, comma(,), semi-colon(;)  and (&&) but none of these worked. </p><p>How do I pass multiple parameters with values in JDBC connection string ?</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-22 22:46:10.0","id":34806,"title":"Hive vs Beeline for join queries","body":"<p>Hi guys, Need help..currently I am using HDP2.2 when I am running join queries in Hive shell with auto convert mapjoin true  configuration where I am able to get the results...but the same query when I am running in beeline it is failing ..when I turn off mapjoin it is working fine..anyone know what could be the reason..</p>","tags":["beeline"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-05-23 13:23:13.0","id":34934,"title":"​Please how do i do a hot disk swap on a node without restarting the HDP 2.4 cluster?","body":"","tags":["hdp2.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-25 15:33:47.0","id":35559,"title":"Concerns with installing Python 3.X on Hadoop Cluster","body":"<p>Hi All,</p><p>I am aware that HDP and Ambari need Python 2.X to run it's services. However are there any concerns with installing Python 3.5.1 to be used for data processing?</p><p>Thanks,</p><p>Andrew</p>","tags":["installation","Ambari","python"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-13 01:50:21.0","id":27251,"title":"NameNode and other all services or not running excepting zookeeper, giving connection refused error 111","body":"<p>Traceback (most recent call last):</p><pre>  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py\", line 401, in &lt;module&gt;\n    NameNode().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py\", line 102, in start\n    namenode(action=\"start\", hdfs_binary=hdfs_binary, upgrade_type=upgrade_type, env=env)\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py\", line 89, in thunk\n    return fn(*args, **kwargs)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py\", line 146, in namenode\n    create_log_dir=True\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/utils.py\", line 267, in service\n    Execute(daemon_cmd, not_if=process_id_exists_command, environment=hadoop_env_exports)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 238, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh --config /usr/hdp/current/hadoop-client/conf start namenode'' returned 1. starting namenode, logging to /var/log/hadoop/hdfs/hadoop-hdfs-namenode-qubida-hadoop-1.out</pre>","tags":["YARN","namenode","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-13 08:52:04.0","id":27303,"title":"Striping compactions in Hbase","body":"<p>Hbase has an option called striping compactions. From the looks of it, it\nappears to be quite promising. Are end users using it and is it reliable/ stable? </p><p>Just want to be sure it is production hardened and its field acceptance.</p>","tags":["compaction","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-13 13:42:57.0","id":27360,"title":"Directory/File visablity issue in Sandbox Azure VM","body":"<p><a rel=\"user\" href=\"/users/527/rmolina.html\" nodeid=\"527\">@rmolina - </a>I am running the Sandbox 2.4 VM in Azure and I am experiencing what may be a bug. </p><p>In <a href=\"http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#section_3\">Lab 1: Loading Sensor Data into HDFS</a> Step 1.2 Load the sensor data into HDFS, whenever I create the folders as instructed and then load the data, if I browse up one level the folder is no longer visible. If I create the folder again, I can, but if I upload the file again, it fails with the following error \"<strong>500 </strong>/tmp/maria_dev/data/geolocation.csv for client 10.1.0.5 already exists\". If you want the stack trace, I have it.</p>","tags":["hdp-2.4.0","tutorial-100"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-13 16:31:47.0","id":27395,"title":"Hadoop security - Encryption at rest and motion","body":"<p><strong>Encrypting data in motion</strong></p><p>What is the difference between wire encryption mentioned <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_Security_Guide/content/ch_hdp-security-guide-wire-encryption.html \">here </a>and the commercial tools like voltage, data guise etc? When to use what?</p><p><strong>Encryption on data at rest</strong></p><p>Is TDE the only solution for data at rest?</p><p>Would the above mentioned commercial tools also help here?</p><p>Is it possible to encrypt only specific columns in both data at rest and motion? If so, please share the details.</p><p><b>Data masking</b></p><p>What is the recommendation for data masking?</p><p><strong>\n</strong></p><p><strong>\n</strong></p>","tags":["hadoop","encryption","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-14 22:48:09.0","id":27661,"title":"Can I use NiFi to launch Spark (or other YARN) jobs?","body":"<p>I'm aware of ExecuteProcess, which could invoke spark-submit, but I'm not running NiFi on an HDP node.</p><p>I receive lots of arbitrary CSV and JSON files that I don't have pre-existing tables for. Instead of trying to script DDL creation inside NiFi, it would be nice to invoke a Spark job that infers schema and creates tables from data already loaded to HDFS.</p>","tags":["Nifi","Hive","Spark"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-16 14:20:46.0","id":27882,"title":"I could not change the tez container size on Azure","body":"<p>Login in Azure as maria_dev, running tutorial and encountered the outofmemory issue, try to increase the tez containner size, but under that account it does not allow me to modify the configuration.</p><p>What should I do?</p>","tags":["Tez"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-16 22:06:52.0","id":27899,"title":"The Product.tsv could not be load correctly using the the script in the tutorial","body":"<p>Try it out yourself, the name.tsv is not even products.tsv, it is urlmap.tsv instead, even if I proceed to the end of this tut, it display wrong results.</p><p>http://hortonworks.com/hadoop-tutorial/loading-data-into-the-hortonworks-sandbox/</p>","tags":["hdp-2.4.0","tutorial-230"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-18 15:10:09.0","id":28062,"title":"Is there documentation on how to configure secure WAL replication and secure CopyTable for cluster replication with clusters configured with kerberos and Hbase?","body":"","tags":["kerberos","wal","Hbase","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-18 16:04:29.0","id":28073,"title":"How do you force the number of reducers in a map reduce job to be higher?","body":"<p>I'm attempting to copy 30 billion rows from one hive table into another hive table.   The tables are both created the same and are partitioned on date (DT).   Currently there are 1173 partitions.   I'm using the following query:    insert into accesslog_new PARTITION (DT) select * from accesslog;</p><p>This query has been running for almost 3 days straight on a cluster with 18 data nodes.    </p><p>My issue is that the Map-Reduce job only creates one reducer step.   Btw, we are using MR2.   I'm guessing this is drastically slowing things down.  Is there a way to force the number of reducers to be much larger?   How do you also figure out what an appropriate number of reducers would be for that volume of data? </p>","tags":["Hive","MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-19 08:14:09.0","id":28239,"title":"how to add third party jars for a mapreduce program","body":"<p>Hi All, </p><p>Am getting this error </p><pre>cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.vertica.hadoop.VerticaOutputFormat not found\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.vertica.hadoop.VerticaOutputFormat not found\n\n</pre><p>However , I have added the third party jar into the /usr/hdp/2.3.4.0-***/hadoop/lib </p><p>and have also used -libjar option while running the command .</p><p>can anyone let me know where to set the hadoop classpath correctly  , and if we are copying the third party jars into the hadoop source folder  there are many folder within like hadoop-hdfs,hadoop-mapreduce,hadoop-httpf,hadoop-yarn do we copy the third party jars here too ????</p>","tags":["MapReduce","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-18 19:45:09.0","id":28140,"title":"Clients issue in Ambari","body":"<p>All the services are up and running, but all these clients are down. How to bring these up manually?</p><p>HBase Client,HCat Client,HDFS Client,Hive Client,MapReduce2 Client,Pig,Spark Client,Sqoop,Tez Client,YARN Client,ZooKeeper Client</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-19 12:14:18.0","id":28266,"title":"Storm kafkaspout at SimpleConsumer kerberos error","body":"<p>In log We see that is correct info for broker:</p><p>7435 [Thread-13-$mastercoord-bg0] INFO  s.k.DynamicBrokersReader - Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=nabajk11-sc.corpnet.pl:6667}}\n7435 [Thread-17-spout0] INFO  s.k.DynamicBrokersReader - Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=nabajk11-sc.corpnet.pl:6667}}\n7435 [Thread-9-$spoutcoord-spout0] INFO  s.k.DynamicBrokersReader - Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=nabajk11-sc.corpnet.pl:6667}}</p><p>But is error with connection to broker:</p><p>7465 [Thread-17-spout0-EventThread] INFO  o.a.s.c.f.s.ConnectionStateManager - State change: CONNECTED\n7525 [Thread-9-$spoutcoord-spout0] INFO  b.s.d.executor - Prepared bolt $spoutcoord-spout0:(2)\n7811 [Thread-17-spout0] INFO  k.c.SimpleConsumer - Reconnect due to socket error: java.io.EOFException\n7817 [Thread-17-spout0] ERROR b.s.util - Async loop died!\njava.lang.RuntimeException: java.io.EOFException\n  at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]\n  at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]\n  at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]\n  at backtype.storm.daemon.executor$fn__7245$fn__7258$fn__7309.invoke(executor.clj:808) ~[storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]\n  at backtype.storm.util$async_loop$fn__544.invoke(util.clj:475) [storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]\n  at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]\n  at java.lang.Thread.run(Thread.java:745) [?:1.7.0_79]\nCaused by: java.io.EOFException\n  at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:83) ~[storm-kafka-example-0.10.0.2.4.0.0-169-jar-with-dependencies.jar:?]</p><p>config for broker:</p><p>cat\n/etc/kafka/conf/server.properties</p><p># Generated by Apache Ambari. Wed Apr  6 12:18:42 2016</p><p>authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</p><p>auto.create.topics.enable=true</p><p>auto.leader.rebalance.enable=true</p><p>compression.type=producer</p><p>controlled.shutdown.enable=true</p><p>controlled.shutdown.max.retries=3</p><p>controlled.shutdown.retry.backoff.ms=5000</p><p>controller.message.queue.size=10</p><p>controller.socket.timeout.ms=30000</p><p>default.replication.factor=1</p><p>delete.topic.enable=false</p><p>external.kafka.metrics.exclude.prefix=kafka.network.RequestMetrics,kafka.server.DelayedOperationPurgatory,kafka.server.BrokerTopicMetrics.BytesRejectedPerSec</p><p>external.kafka.metrics.include.prefix=kafka.network.RequestMetrics.ResponseQueueTimeMs.request.OffsetCommit.98percentile,kafka.network.RequestMetrics.ResponseQueueTimeMs.request.Offsets.95percentile,kafka.network.RequestMetrics.ResponseSendTimeMs.request.Fetch.95percentile,kafka.network.RequestMetrics.RequestsPerSec.request</p><p>fetch.purgatory.purge.interval.requests=10000</p><p>kafka.ganglia.metrics.group=kafka</p><p>kafka.ganglia.metrics.host=localhost</p><p>kafka.ganglia.metrics.port=8671</p><p>kafka.ganglia.metrics.reporter.enabled=true</p><p>kafka.metrics.reporters=org.apache.hadoop.metrics2.sink.kafka.KafkaTimelineMetricsReporter</p><p>kafka.timeline.metrics.host=nabajk10-sc.corpnet.pl</p><p>kafka.timeline.metrics.maxRowCacheSize=10000</p><p>kafka.timeline.metrics.port=6188</p><p>kafka.timeline.metrics.reporter.enabled=true</p><p>kafka.timeline.metrics.reporter.sendInterval=5900</p><p>leader.imbalance.check.interval.seconds=300</p><p>leader.imbalance.per.broker.percentage=10</p><p>listeners=SASL_PLAINTEXT://:6667</p><p>log.cleanup.interval.mins=10</p><p>log.dirs=/kafka/kafka-logs</p><p>log.index.interval.bytes=4096</p><p>log.index.size.max.bytes=10485760</p><p>log.retention.bytes=1807745024</p><p>log.retention.hours=1</p><p>log.roll.hours=1</p><p>log.segment.bytes=1073741824</p><p>message.max.bytes=1000000</p><p>min.insync.replicas=1</p><p>num.io.threads=8</p><p>num.network.threads=3</p><p>num.partitions=1</p><p>num.recovery.threads.per.data.dir=1</p><p>num.replica.fetchers=1</p><p>offset.metadata.max.bytes=4096</p><p>offsets.commit.required.acks=-1</p><p>offsets.commit.timeout.ms=5000</p><p>offsets.load.buffer.size=5242880</p><p>offsets.retention.check.interval.ms=600000</p><p>offsets.retention.minutes=86400000</p><p>offsets.topic.compression.codec=0</p><p>offsets.topic.num.partitions=50</p><p>offsets.topic.replication.factor=3</p><p>offsets.topic.segment.bytes=104857600</p><p>principal.to.local.class=kafka.security.auth.KerberosPrincipalToLocal</p><p>producer.purgatory.purge.interval.requests=10000</p><p>queued.max.requests=500</p><p>replica.fetch.max.bytes=1048576</p><p>replica.fetch.min.bytes=1</p><p>replica.fetch.wait.max.ms=500</p><p>replica.high.watermark.checkpoint.interval.ms=5000</p><p>replica.lag.max.messages=4000</p><p>replica.lag.time.max.ms=10000</p><p>replica.socket.receive.buffer.bytes=65536</p><p>replica.socket.timeout.ms=30000</p><p>security.inter.broker.protocol=SASL_PLAINTEXT</p><p>socket.receive.buffer.bytes=102400</p><p>socket.request.max.bytes=104857600</p><p>socket.send.buffer.bytes=102400</p><p>super.users=user:kafka</p><p>zookeeper.connect=nabajk10-sc.corpnet.pl:2181,nabajk11-sc.corpnet.pl:2181,nabajk200-sc.corpnet.pl:2181</p><p>zookeeper.connection.timeout.ms=25000</p><p>zookeeper.session.timeout.ms=30000</p><p>zookeeper.sync.time.ms=2000</p><p>This mean that We should not use buildin KafkaSpout from storm\nlibraries? If not, how to setup parameters for KafkaSpout? In addition our kafka broker in cluster works on default port 6667 with\nKerberos authentication. </p>","tags":["Storm","kerberos"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-18 22:49:48.0","id":28197,"title":"Where can I get start up scripts for nodemanager, hivemeta, hbase master, regions, history server etc? HDP installed version is 2.3.0","body":"<p>Where can I get start up scripts for nodemanager, hivemeta, hbase master, regions, history server etc? HDP installed version is 2.3.0.</p><p>I got few start up scripts from the link below:</p><p>https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_HDP_Reference_Guide/content/starting_hdp_services.html</p>","tags":["script"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-20 07:17:59.0","id":28419,"title":"How to compare two hive tables that are in different clusters?","body":"<p>I have similar hive tables in two clusters, say my Staging and Prod. I would like to compare the tables in Staging and Prod. Is there a straight way to do it using Hive or Beeline?</p>","tags":["Hive","beeline"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-20 15:47:48.0","id":28529,"title":"Sqoop is still not recognizing the command","body":"<p>HDP with windows environment and trying to import oracle database using sqoop. </p><p>Command is :</p><p>sqoop import --connect  jdbc:oracle:thin:hud_reader/\nhud_reader_n1le@huiprd_nile:1527--username\n&lt;hud_reader&gt;--password&lt;\nhud_reader_n1le&gt;\n--table&lt;DATAAGGRUN&gt;\n--C:\\hadoop\\hdp\\hadoop-2.7.1.2.4.0.0-169\\sqoop_out\n -m 1</p><p>error/message file not able to find.</p><p>Please help if you have any ideas to resolve this issue. </p><p>thanks,</p>","tags":["Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-20 21:32:39.0","id":28589,"title":"Hive Metastore won't start after enabling Kerberos","body":"<p>I'm setting up a test cluster to match a client site and so it's HDP 2.2.4 with Ambari 2.1.0. 3 masters and 5 slaves, Centos 6, primarily HDFS, Yarn, Hive, HBase, Oozie and ZK. HDFS is in HA. Cluster was working fine pre-kerberos.</p><p>After enabling kerberos (fresh MIT KDC) everything seems to be happy except Hive - the metastore won't come up. The hivemetastore.log says:</p><p>2016-04-20 17:10:10,976 ERROR [main]: metastore.HiveMetaStore (HiveMetaStore.java:main(5654)) - Metastore Thrift Server threw an exception...</p><p>org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Error creating path /hive/cluster/delegationMETASTORE/keys</p><p>        at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ensurePath(ZooKeeperTokenStore.java:165)</p><p>        at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.initClientAndPaths(ZooKeeperTokenStore.java:235)</p><p>        at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.init(ZooKeeperTokenStore.java:468)</p><p>        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server.startDelegationTokenSecretManager(HadoopThriftAuthBridge20S.java:438)</p><p>        at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:5724)</p><p>        at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:5650)</p><p>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</p><p>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>        at java.lang.reflect.Method.invoke(Method.java:497)</p><p>        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</p><p>        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p><p>Caused by: org.apache.zookeeper.KeeperException$InvalidACLException: KeeperErrorCode = InvalidACL for /hive/cluster/delegationMETASTORE/keys</p><p>        at org.apache.zookeeper.KeeperException.create(KeeperException.java:121)</p><p>        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)</p><p>        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)</p><p>        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:423)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:257)</p><p>        at org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:205)</p><p>        at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ensurePath(ZooKeeperTokenStore.java:159)</p><p>        ... 11 more</p><p>I've checked zookeeper and the only node under hive is /hive/cluster/delegationHIVESERVER2</p><p>Any advice on where to start? </p>","tags":["Hive","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-20 11:36:00.0","id":28476,"title":"Spark Submit Multiple Jobs in Cluster Environment","body":"<p>Is it possible to submit multiple jobs in Spark-Submit ?</p><p>I have created multiple jobs to be submitted in the spark-submit</p><p>I have to submit these created jobs in a cluster deploy mode</p><p>Is it possible to submit the jobs to desired cores in the cluster deploy mode?</p><p>like job 1 - core 1</p><p>job 2 - cores 2 and 3 </p><p>similarly....</p><p>Is it possible to schedule the jobs to the desired cores?</p><p>Note: Cores should be assigned manually by the user</p>","tags":["jobs","Spark","spark-1.6.1"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-21 11:14:17.0","id":28683,"title":"Overwriting a column in HIVE","body":"<p>Hello there,</p><p>I need to overwrite a column in a HIVE table with data from another HIVE table.</p><p>In my exemple I have a a table employees and a table salary. I need to overwrite just ONE column of the employees table....let's a say a from start_date with values from called salary_date in the salary table. Both salary and employees table have a a common key which is the coluns employee_number. </p><p>Here is what I have been trying to do without success. </p><p>     INSERT OVERWRITE TABLE employees SELECT employees.start_date salary.salary_date FROM salary employees WHERE salary.employee_number =  employee.employee_number</p><p>Any insights on where I might be making a mistake here would help. I see some exemples suggesting to use partitions but I am not sure how to use this in this case, as I  am considering to overwrite the whole start_date column and not just some values. </p><p>Thanks!</p><p>Wellington</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-21 12:18:03.0","id":28673,"title":"Tag based policies with Apache Ranger and Apache Atlas","body":"<p>In the above <a href=\"http://de.hortonworks.com/hadoop-tutorial/tag-based-policies-atlas-ranger/\">Tutorial</a> the virtual machine <a href=\"https://s3.amazonaws.com/demo-drops.hortonworks.com/HDP-Atlas-Ranger-TP.ova\">HDP-Atlas-Ranger-TP</a> Ambari UI should be available at <a href=\"http://127.0.0.1:8080/\">http://127.0.0.1:8080/</a>. This is a problem because the VM has no GUI preinstalled or not stated in the tutorial.  </p>","tags":["hdp-2.4.0","tutorial-600"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-21 20:49:29.0","id":28796,"title":"NiFi GetTwitter","body":"<p>I have a simple question, as I am new to NiFi.</p><p>I have a GetTwitter processor set up and configured (assuming \ncorrectly). I have the Twitter Endpoint set to Sample Endpoint. I run \nthe processor and it runs, but nothing happens. I get no input/output</p><p>How do I troubleshoot what it is doing (or in this case not doing)?</p>","tags":["Nifi","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-22 00:06:38.0","id":28829,"title":"Did anyone try installing ambari-server with custom name i.e. ambariProd vs. default ambari on Ubuntu 14.04?","body":"<p>I can't get past the first few options and it keeps failing saying database ambariProd already exists.</p>","tags":["ambari-2.2.1","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-13 09:57:37.0","id":27309,"title":"Cloud Break and Azure Pack","body":"<p>Hi,</p><p>Is there a SPI to run Cloudbreak with an <strong>azure pack</strong> cloud platform(private cloud) ?</p><p>I saw that Cloudbreak support <strong>microsoft azure </strong>but does it also support <strong>azure pack</strong> ?</p>","tags":["Cloudbreak","azure"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-14 07:02:09.0","id":27524,"title":"Getting error \"Exception in doCheckpoint java.io.IOException: Unable to download to any storage directory\"","body":"<p>Hello All,</p><p>Below are the errors seen in secondary namenode, there are about 6000 under replicated blocks too, not sure if its related to this issue only. DN health is fine. Appreciate any pointers.</p><p>==</p><pre>2016-04-12 15:45:03,660 INFO  namenode.SecondaryNameNode (SecondaryNameNode.java:run(453)) - Image has not changed. Will not download image.\n2016-04-12 15:45:03,661 INFO  namenode.TransferFsImage (TransferFsImage.java:getFileClient(394)) - Opening connection to http://ey9omprna005.vzbi.com:50070/imagetransfer?getedit=1&startTxId=236059442&endTxId=23608...\n2016-04-12 15:45:03,665 ERROR namenode.SecondaryNameNode (SecondaryNameNode.java:doWork(399)) - Exception in doCheckpoint\njava.io.IOException: Unable to download to any storage directory\n</pre>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-14 16:07:04.0","id":27621,"title":"sqoop import multiple table","body":"<p>Hi,</p><p>I know the command to import a data table with Sqoop is :</p><p>  sqoop import \\\n  --connect \"jdbc:jtds:sqlserver://xxxx:xxxx;databaseName=xxxx;user=xxxx;password=xxxx;instance=xxxx\" \\\n  --driver net.sourceforge.jtds.jdbc.Driver \\\n  --username xxxx \\\n  --table table_name \\\n  --target-dir /test \\\n  --as-textfile \\\n  --fields-terminated-by '\\t' \\\n  --verbose</p><div>But I would like to learn how to change this command to import the table several of my choice ?</div><div> thank you\n</div>","tags":["Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-15 10:53:38.0","id":27742,"title":"Hive connection to MongoDB with userid/pw","body":"<p>Hi,</p><p>I am trying to connect to mongodb and  read the collection from the db.  It is working ok with non secured db when no need of authorization.  With the secured access</p><p>We have provided the username and password to the connector (eg. mongo.input.uri = \"mongodb://myuser:mypassword@host:27017/mydb.mycollection\"), for an Hadoop Job that pulls data from the database. I am geeting the  message similar to the following:\njava.io.IOException: java.io.IOException: java.lang.RuntimeException: Unable to calculate input splits from collection stats: not authorized on mydb  to execute command { collstats: \"mycollectiont\" }</p><p>Here is the hive query :</p><p>CREATE EXTERNAL TABLE uploadresult\n(\nid                  STRING  ,                                                  \nconfirmationCode    STRING  ,                                                        \ndateCreated         STRING  ,                                                        \nfailedMessages      STRING  ,                                                        \nfileName            STRING  ,                                                        \nreportingPeriod     STRING  ,                                                        \nstatus              STRING  ,                                                        \nuser_id             STRING  ,\ncontract_id         STRING\n)\nSTORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'\nWITH SERDEPROPERTIES('mongo.columns.mapping'='{\"id\":\"_id\",\"confirmationCode\":\"confirmationCode\",\n \"dateCreated\":\"dateCreated\", \"failedMessages\":\"failedMessages\", \"fileName\":\"fileName\",  \n  \"reportingPeriod\":\"reportingPeriod\", \"status\":\"status\",\"user_id\":\"user_id\",\"contract_id\":\"contract_id\"}')\nTBLPROPERTIES(\n  'mongo.uri' = 'mongodb://&lt;host&gt;:27017/mydb.uploadresult',\n  'mongo.input.uri' = 'mongodb://user:pw@&lt;host&gt;:27017/mydb.uploadresult'                \n     );</p>","tags":["mongodb"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-15 13:09:41.0","id":27769,"title":"Assume you reboot a datanode in a running cluster Is there a way to auto restart all the Hadoop processes? Is there a way to do so in Ambari? Or is a script required here?","body":"","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-15 14:12:33.0","id":27781,"title":"how can we select  jars(for hbase,hive) that are present in the hortonworks sandbox(generally in a folder)","body":"","tags":["Sandbox","Hbase"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-15 16:16:19.0","id":27802,"title":"How to update Nifi on my HDP sandbox 2.4 and ambari to pick the latest version","body":"","tags":["Nifi","Sandbox"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-15 02:04:26.0","id":27675,"title":"Oozie Service (HDP 2.4.0) Failed To Start on Windows Server 2012","body":"<p>Hi,</p><p>I set up a HDP cluster on VMs with OS Windows Server 2012. For Oozie set up, I am using default derby server. </p><p>However, I am not able to start the Oozieservices on Oozie host.</p><p>The service keeps restarting. And the error in oozie-error.log is attached below.</p><p>Thanks!</p><p>Jinling</p><pre>2016-04-14 17:37:29,445 FATAL Services:514 - SERVER[HADOOPHDP] Runtime Exception during Services Load. Check your list of 'oozie.services' or 'oozie.services.ext'\n2016-04-14 17:37:29,452 FATAL Services:514 - SERVER[HADOOPHDP] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Schema 'SA' does not exist)\norg.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Schema 'SA' does not exist)\n\tat org.apache.oozie.service.Services.loadServices(Services.java:309)\n\tat org.apache.oozie.service.Services.init(Services.java:213)\n\tat org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)\n\tat org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4210)\n\tat org.apache.catalina.core.StandardContext.start(StandardContext.java:4709)\n\tat org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:802)\n\tat org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:779)\n\tat org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)\n\tat org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:676)\n\tat org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:602)\n\tat org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:503)\n\tat org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322)\n\tat org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325)\n\tat org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1068)\n\tat org.apache.catalina.core.StandardHost.start(StandardHost.java:822)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1060)\n\tat org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)\n\tat org.apache.catalina.core.StandardService.start(StandardService.java:525)\n\tat org.apache.catalina.core.StandardServer.start(StandardServer.java:759)\n\tat org.apache.catalina.startup.Catalina.start(Catalina.java:595)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)\n\tat org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)\nCaused by: &lt;openjpa-2.2.2-r422266:1468616 fatal general error&gt; org.apache.openjpa.persistence.PersistenceException: Cannot create PoolableConnectionFactory (Schema 'SA' does not exist)\n\tat org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:102)\n\tat org.apache.openjpa.jdbc.conf.JDBCConfigurationImpl.getDBDictionaryInstance(JDBCConfigurationImpl.java:603)\n\tat org.apache.openjpa.jdbc.meta.MappingRepository.endConfiguration(MappingRepository.java:1518)\n\tat org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:531)\n\tat org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:456)\n\tat org.apache.openjpa.lib.conf.PluginValue.instantiate(PluginValue.java:120)\n\tat org.apache.openjpa.conf.MetaDataRepositoryValue.instantiate(MetaDataRepositoryValue.java:68)\n\tat org.apache.openjpa.lib.conf.ObjectValue.instantiate(ObjectValue.java:83)\n\tat org.apache.openjpa.conf.OpenJPAConfigurationImpl.newMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:967)\n\tat org.apache.openjpa.conf.OpenJPAConfigurationImpl.getMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:958)\n\tat org.apache.openjpa.kernel.AbstractBrokerFactory.makeReadOnly(AbstractBrokerFactory.java:644)\n\tat org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:203)\n\tat org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:156)\n\tat org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:227)\n\tat org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:154)\n\tat org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:60)\n\tat org.apache.oozie.service.JPAService.getEntityManager(JPAService.java:500)\n\tat org.apache.oozie.service.JPAService.init(JPAService.java:201)\n\tat org.apache.oozie.service.Services.setServiceInternal(Services.java:386)\n\tat org.apache.oozie.service.Services.setService(Services.java:372)\n\tat org.apache.oozie.service.Services.loadServices(Services.java:305)\n\t... 26 more\nCaused by: org.apache.commons.dbcp.SQLNestedException: Cannot create PoolableConnectionFactory (Schema 'SA' does not exist)\n\tat org.apache.commons.dbcp.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:1549)\n\tat org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1388)\n\tat org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044)\n\tat org.apache.openjpa.lib.jdbc.DelegatingDataSource.getConnection(DelegatingDataSource.java:110)\n\tat org.apache.openjpa.lib.jdbc.DecoratingDataSource.getConnection(DecoratingDataSource.java:87)\n\tat org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:91)\n\t... 46 more\nCaused by: java.sql.SQLSyntaxErrorException: Schema 'SA' does not exist\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(Unknown Source)\n\tat org.apache.commons.dbcp.DelegatingStatement.executeQuery(DelegatingStatement.java:208)\n\tat org.apache.commons.dbcp.PoolableConnectionFactory.validateConnection(PoolableConnectionFactory.java:658)\n\tat org.apache.commons.dbcp.BasicDataSource.validateConnectionFactory(BasicDataSource.java:1558)\n\tat org.apache.commons.dbcp.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:1545)\n\t... 51 more\nCaused by: java.sql.SQLException: Schema 'SA' does not exist\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 63 more\nCaused by: ERROR 42Y07: Schema 'SA' does not exist\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.sql.catalog.DataDictionaryImpl.getSchemaDescriptor(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.QueryTreeNode.getSchemaDescriptor(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.QueryTreeNode.getSchemaDescriptor(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.FromBaseTable.bindTableDescriptor(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.FromBaseTable.bindNonVTITables(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.FromList.bindTables(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.SelectNode.bindNonVTITables(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.DMLStatementNode.bindTables(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.DMLStatementNode.bind(Unknown Source)\n\tat org.apache.derby.impl.sql.compile.CursorNode.bindStatement(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericStatement.prepMinion(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericStatement.prepare(Unknown Source)\n\tat org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(Unknown Source)\n\t... 57 more\n\n\n</pre>","tags":["hdp-2.4.0","Oozie","derby"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-15 03:47:52.0","id":27687,"title":"HDF and Windows Azure.","body":"<p>I would like to understand whether HDF (Hortonworks Data Flow) is supported by Windows Azure. I was interested in sentiment analysis approach mentioned in the link http://hortonworks.com/hadoop-tutorial/how-to-refine-and-visualize-sentiment-data/. I was wondering if Azure is ready for deploying the mentioned sentiment analysis sample.</p>","tags":["hdp-2.3.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-22 14:41:57.0","id":28976,"title":"Job history server ui not getting connected","body":"<p>Dear Hortonworks,\nAm using hdp2.4 sandbox, once my job finished (eg. sqoop import job) not able to view the history logs through resource manager UI. If i click history am getting error as below. All the quick links in MR2 getting the same even if history server is started and running fine.</p><p><a href=\"http://sandbox.hortonworks.com:19888/\">http://sandbox.hortonworks.com:19888/</a> –&gt; Job history server UI</p><p>This site can’t be reached</p><p>sandbox.hortonworks.com’s server DNS address could not be found.</p><p>Looking yours support!</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-26 09:21:11.0","id":58346,"title":"max size for oozie workflow definition exceeded","body":"<p>I am facing below exception on submitted co-ordinator job.</p><pre>Caused by: org.apache.oozie.workflow.WorkflowException: E0736: Workflow definition length [637,397] exceeded maximum allowed length [100,000]\n\tat org.apache.oozie.service.WorkflowAppService.readDefinition(WorkflowAppService.java:130)\n\tat org.apache.oozie.service.LiteWorkflowAppService.parseDef(LiteWorkflowAppService.java:45)\n\tat org.apache.oozie.command.wf.SubmitXCommand.execute(SubmitXCommand.java:179)\n</pre><p>is there any way to bypass this ?</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-10-01 06:32:49.0","id":59364,"title":"Few Questions on Hadoop 2.x architecture","body":"<p>I was going through the 2.x architecture, I got few question about the name node and resource manager</p><p> To resolve Single point of failure of Namenode in 1.x arch,In Hadoop 2.x have standby namenode.</p><p>  to reduce the load of the Job Tracker , In 2.x  we have Resource Manager.</p><p> Wanted to know, </p><p>  1. What is the role of Namenode  and Resource Manager ?</p><p>   2. As only one resource manager available/cluster , then It could be a Single Point of failure</p><p>   3. If NameNode storing meta data info about the blocks (as Hadoop 1.x ) , Then which service is responsible for getting data block information after submitting the job. </p><p>      see the image :  http://hortonworks.com/wp-content/uploads/2014/04/YARN_distributed_arch.png  </p><p>   Resource manager directly interacting with Nodes </p><p>4. can anyone tell me, how the flow goes for below commands</p><p>       $ hadoop fs -put  &lt;source&gt; &lt;dest&gt;</p><p>       $ hadoop jar app.jar  &lt;app&gt;  &lt;inputfilepath&gt; &lt;outputpath&gt;</p>","tags":["hadoop-core"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-10-01 23:19:08.0","id":59395,"title":"LLAP Application","body":"<p>Hi,</p><p>I've been trying to set up a small cluster using HDP 2.5 on OL 7.2 with the purpose of running an academic project in Hive and after some days cleaning errors and alerts I still have one that I can't get rid of.</p><p>HiveServer2 Interactive reports the following alert: </p><p style=\"margin-left: 20px;\">Service - Hive</p><p style=\"margin-left: 20px;\">Alert Definition Name: LLAP Application</p><p style=\"margin-left: 20px;\">Status: Crit</p><p style=\"margin-left: 20px;\">Response: The application reported a 'NOT RUNNING' state.</p><p style=\"margin-left: 20px;\"></p><p>Can please someone point me into the direction I should be looking?</p><p>Thank you very much!</p><p>,</p>","tags":["llap","hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-30 14:51:53.0","id":64133,"title":"How to set owner of Storm topology as the user who submits the topology.","body":"<p>narasimha8177@gmail.com</p>","tags":["Storm"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-06-16 16:40:17.0","id":40233,"title":"Deleting dead/decommisioned datanodes","body":"<p>I completed the decommissioning process for two datanodes, stopped all services on them when the state changed to \"<strong>decommissioned</strong>\", and then using Ambari, chose to Delete the two nodes. They are gone from the Ambari Host screen but we still have alerts  \"DataNode Health: [Live=8, Stale=0, Dead=2]\". The \"Overview\" page (namenode port 50070) reports the nodes \"Dead, decommissioned\".</p><p>So how/when does one delete dead/decommissioned nodes? </p><p>I've tried adding them to the conf/dfs.excludes file and ran <em>hdfs dfsadmin -refreshNodes. </em>No luck. <em>hdfs dfsadmin -report</em> still list the two nodes as \"Decommission Status : Decommissioned\".</p>","tags":["delete","decommission"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-16 19:31:46.0","id":40266,"title":"Hive indexes without improvement of performance","body":"<p>Hello! </p><p> I use Hive 1.1.0-cdh5.5.0 and try to use indexes support. </p><p>My index creation: </p><pre>CREATE INDEX doc_id_idx on TABLE my_schema_name.doc_t (id) AS 'COMPACT' WITH DEFERRED REBUILD; \nALTER INDEX doc_id_idx ON my_schema_name.doc_t REBUILD; \n</pre><p>Then I set configs: </p><pre>set hive.optimize.autoindex=true;\nset hive.optimize.index.filter=true; \nset hive.optimize.index.filter.compact.minsize=0; \nset hive.index.compact.query.max.size=-1; \nset hive.index.compact.query.max.entries=-1; \n</pre><p>\nAnd my query is: </p><pre>select count(*) from my_schema_name.doc_t WHERE id = '3723445235879'; </pre><p>Sometimes I have improvement of performance, but most of cases - not. </p><p>In cases when I have improvement: </p><p>1. my query is </p><pre>select count(*) from my_schema_name.doc_t WHERE id = '3723445235879'; </pre><p>give me NullPointerException (in logs I see that Hive doesn't find my index table) </p><p>2. then I write: </p><pre>USE my_schema_name; \nselect count(*) from doc_t WHERE id = '3723445235879'; \n</pre><p>and have result with improvement\n(172 sec) </p><p>In case when I don't have improvement, I can use either </p><pre>select count(*) from my_schema_name.doc_t WHERE id = '3723445235879'; </pre><p>without exception, either </p><pre>USE my_schema_name; \nselect count(*) from doc_t WHERE id = '3723445235879'; \n</pre><p>and have result\n(1153 sec) </p><p>My table is about 6 billion rows. </p><p>I tried various combinations on index configs, including only these two: </p><pre>set hive.optimize.index.filter=true; \nset hive.optimize.index.filter.compact.minsize=0; </pre><p>My hadoop version is 2.6.0-cdh5.5.0 </p><p>What I do wrong? </p><p>Thank you.</p>","tags":["Hive","indexing","index"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-17 11:07:04.0","id":40396,"title":"want to check usecache content after submission of user job but its got cleared quickly. is there a way to hold this for a moment so that I can check what certificate I am submitting with my job.","body":"<p></p><p>want to check usecache content after submission of user job but its got cleared quickly. is there a way to hold this for a moment so that I can check what certificate I am submitting with my job.</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-23 22:14:46.0","id":41537,"title":"Adding libraries to Zeppelin","body":"<p>I want to add a library and use it in Zeppelin (ex. Spark-csv). I succeeded in adding it to Spark and using it by putting my Jar in all nodes and adding spark.jars='path-to-jar' in conf/spark-defaults.conf.</p><p>However when I call the library from Zeppelin it doesn't work (class not found). From my understanding Zeppelin do a Spark-submit so if the package is already added in Spark it should work. Also, I tried adding using export\nSPARK_SUBMIT_OPTIONS=”--jars\n/path/mylib1.jar,/path/mylib2.jar\" to zeppelin-env.sh but same problem.</p><p>Has anyone suceeded in adding libraries to Zeppelin ? have you seen this problem ?</p>","tags":["zeppelin","zeppelin-notebook","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-24 20:55:19.0","id":41724,"title":"nifi - puthdfs and/or gethdfs with 'knox authentication'","body":"<p>Hi, </p><p>I have a small HDF/Nifi node outside of my hdp hadoop cluster.</p><p>I want to use puthdfs to move some data in; I can see properties for \"kerberos principal\" and \"kerberos keytab\".  </p><p>Is there anyway i can use knox authentication to puthdfs (and gethdfs) and mask the kerberos complexities?</p><p>If not, is this a feature that is in the works?  thanks, -Joe</p>","tags":["Knox","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-27 09:25:55.0","id":41868,"title":"Oozie purge script","body":"<p>Is it ok to run purge scripts on WF_JOBS & COORD_JOBS tables that are in oozie Database configured in MySQL? </p><p>Will the purge scripts remove the running workflows and coordinators?</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-27 14:24:42.0","id":41905,"title":"Ranger Hive policy for schema only","body":"<p>Is there a Hive policy that we can enable a user to read schema only without accessing the data? Since Ranger plugin is in HS2 only, could Hive metastore be a choice? </p>","tags":["Hive","Ranger"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-27 09:04:11.0","id":41855,"title":"[FALCON] : Feed failed wrong HDFS uri","body":"<p>Hi all, </p><p>what's wrong in this feed and cluster entities ?</p><p>FALCON server is run on clusterA003.</p><p>The feed run a replication data from clusterB001 (source) to clusterA001 (target)</p><pre>&lt;cluster xmlns='uri:falcon:cluster:0.1' name='nextCur001' description='cluster Next sur Master001' colo='nextCur'&gt;\n  &lt;interfaces&gt;\n    &lt;interface type='readonly' endpoint='hftp://clusterA001:50070' version='2.2.0'/&gt;\n    &lt;interface type='write' endpoint='hdfs://clusterA001:8020' version='2.2.0'/&gt;\n    &lt;interface type='execute' endpoint='clusterA001:8050' version='2.2.0'/&gt;\n    &lt;interface type='workflow' endpoint='http://clusterB003:11000/oozie/' version='4.0.0'/&gt;\n    &lt;interface type='messaging' endpoint='tcp://clusterA003:61616?daemon=true' version='5.1.6'/&gt;\n  &lt;/interfaces&gt;\n  &lt;locations&gt;\n    &lt;location name='staging' path='/apps/falcon/bigdata-next-cluster/staging'/&gt;\n    &lt;location name='temp' path='/apps/falcon/tmp'/&gt;\n    &lt;location name='working' path='/apps/falcon/bigdata-next-cluster/working'/&gt;\n  &lt;/locations&gt;\n  &lt;ACL owner='falcon' group='hadoop' permission='0755'/&gt;\n  &lt;properties&gt;\n    &lt;property name='dfs.namenode.kerberos.principal' value='nn/_HOST@FTI.NET'/&gt;\n    &lt;property name='hive.metastore.kerberos.principal' value='hive/_HOST@FTI.NET'/&gt;\n    &lt;property name=\"hive.metastore.uris\" value=\"thrift://clusterA003:9083\"/&gt;\n    &lt;property name='queueName' value='oozie-launcher'/&gt;\n    &lt;property name=\"hive.metastore.sasl.enabled\" value=\"true\"/&gt;\n  &lt;/properties&gt;\n&lt;/cluster&gt;</pre><pre>&lt;cluster xmlns='uri:falcon:cluster:0.1' name='curNext001' description='undefined' colo='nextCur'&gt;\n  &lt;interfaces&gt;\n    &lt;interface type='readonly' endpoint='hftp://clusterB001:50070' version='2.2.0'/&gt;\n    &lt;interface type='write' endpoint='hdfs://clusterB001:8020' version='2.2.0'/&gt;\n    &lt;interface type='execute' endpoint='clusterB001:8050' version='2.2.0'/&gt;\n    &lt;interface type='workflow' endpoint='http://clusterB003:11000/oozie/' version='4.0.0'/&gt;\n    &lt;interface type='messaging' endpoint='tcp://clusterA003:61616?daemon=true' version='5.1.6'/&gt;\n\n  &lt;/interfaces&gt;\n  &lt;locations&gt;\n    &lt;location name='staging' path='/apps/falcon/bigdata-current-cluster/staging'/&gt;\n    &lt;location name='temp' path='/apps/falcon/tmp'/&gt;\n    &lt;location name='working' path='/apps/falcon/bigdata-current-cluster/working'/&gt;\n  &lt;/locations&gt;\n  &lt;ACL owner='falcon' group='hadoop' permission='0755'/&gt;\n  &lt;properties&gt;\n    &lt;property name='dfs.namenode.kerberos.principal' value='nn/_HOST@FTI.NET'/&gt;\n    &lt;property name='hive.metastore.kerberos.principal' value='hive/_HOST@FTI.NET'/&gt;\n    &lt;property name=\"hive.metastore.uris\" value=\"thrift://clusterB003:9083\"/&gt;\n    &lt;property name='queueName' value='oozie-launcher'/&gt;\n    &lt;property name=\"hive.metastore.sasl.enabled\" value=\"true\"/&gt;\n\n  &lt;/properties&gt;\n&lt;/cluster&gt;\n</pre><pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;feed name=\"curVersNext001\" description=\"next-vers-current master001\" xmlns=\"uri:falcon:feed:0.1\"&gt;\n &lt;frequency&gt;hours(2)&lt;/frequency&gt;\n &lt;timezone&gt;UTC&lt;/timezone&gt;\n &lt;late-arrival cut-off=\"hours(1)\"/&gt;\n\n &lt;clusters&gt;\n  &lt;cluster name=\"curNext001\" type=\"source\"&gt;\n &lt;validity start=\"2016-06-01T12:00Z\" end='2016-06-30T11:00Z'/&gt;\n &lt;retention limit=\"days(6)\" action=\"delete\"/&gt;\n &lt;locations&gt;\n &lt;location type=\"data\" path=\"/tmp/falcon/next-vers-current/${YEAR}-${MONTH}-${DAY}-${HOUR}\"/&gt;\n &lt;/locations&gt;\n &lt;/cluster&gt;\n\n &lt;cluster name=\"nextCur001\" type=\"target\"&gt;\n &lt;validity start=\"2016-06-01T13:00Z\" end=\"2016-06-30T11:00Z\"/&gt;\n &lt;retention limit=\"days(6)\" action=\"delete\"/&gt;\n &lt;locations&gt;\n &lt;location type=\"data\" path=\"/tmp/falcon/next-vers-current/${YEAR}-${MONTH}-${DAY}-${HOUR}\"/&gt;\n &lt;/locations&gt;\n &lt;/cluster&gt;\n &lt;/clusters&gt;\n\n &lt;locations&gt;\n &lt;location type=\"data\" path=\"/tmp/falcon/next-vers-current/${YEAR}-${MONTH}-${DAY}-${HOUR}\"/&gt;\n &lt;location type=\"stats\" path=\"/none\"/&gt;\n &lt;location type=\"meta\" path=\"/none\"/&gt;\n &lt;/locations&gt;\n &lt;ACL owner=\"falcon\" group=\"hadoop\" permission=\"0755\"/&gt;\n &lt;schema location=\"/none\" provider=\"none\"/&gt;\n &lt;properties&gt;&lt;property name=\"queueName\" value=\"oozie-launcher\"/&gt;&lt;/properties&gt;\n&lt;/feed&gt;\n</pre><pre>[falcon@clusterA003 CURNEXT]$ falcon entity  -type feed -submit -file next-master001-vers-current-master001.xml\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.security.authentication.client.KerberosAuthenticator).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nfalcon/default/Submit successful (feed) curVersNext001\n\n\n[falcon@clusterA003 CURNEXT]$ falcon entity  -type feed -schedule -name curVersNext001\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.security.authentication.client.KerberosAuthenticator).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\ndefault/curVersNext001(feed) scheduled successfully\n\n[falcon@clusterA003 CURNEXT]$\n[falcon@clusterA003 CURNEXT]$\n[falcon@clusterA003 CURNEXT]$ falcon entity -type feed -name curVersNext001 -status\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.security.authentication.client.KerberosAuthenticator).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nERROR: Bad Request;default/org.apache.falcon.FalconWebException::org.apache.falcon.FalconException: Wrong FS: hdfs://clusterB001:8020/apps/falcon/bigdata-current-cluster/staging/falcon/workflows/feed/curVersNext001/7e307c2292e9b897d6a51f68ed17ac51_1467016396742, expected: hdfs://clusterA001:8020\n</pre><p>Falcon error : </p><pre>\n2016-06-27 10:55:48,797 INFO  - [452378800@qtp-794075965-11 - 1e55add6-9a8f-42ca-9ad0-3885f55a5fe0:] ~ HttpServletRequest RemoteUser is null (Servlets:47)\n2016-06-27 10:55:48,798 INFO  - [452378800@qtp-794075965-11 - 1e55add6-9a8f-42ca-9ad0-3885f55a5fe0:] ~ HttpServletRequest user.name param value is falcon (Servlets:53)\n2016-06-27 10:55:48,798 DEBUG - [452378800@qtp-794075965-11 - 1e55add6-9a8f-42ca-9ad0-3885f55a5fe0:] ~ Audit: falcon/10.98.138.87 performed request http://clusterA003:15000/api/options?user.name=falcon (10.98.138.87) at time 2016-06-27T08:55Z (FalconAuditFilter:86)\n2016-06-27 10:55:49,045 INFO  - [452378800@qtp-794075965-11 - c89efc19-496b-4281-a9d3-8f2632913218:] ~ HttpServletRequest RemoteUser is null (Servlets:47)\n2016-06-27 10:55:49,045 INFO  - [452378800@qtp-794075965-11 - c89efc19-496b-4281-a9d3-8f2632913218:] ~ HttpServletRequest user.name param value is falcon (Servlets:53)\n2016-06-27 10:55:49,045 DEBUG - [452378800@qtp-794075965-11 - c89efc19-496b-4281-a9d3-8f2632913218:] ~ Audit: falcon/10.98.138.87 performed request http://clusterA003:15000/api/options?user.name=falcon (10.98.138.87) at time 2016-06-27T08:55Z (FalconAuditFilter:86)\n2016-06-27 10:55:49,818 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:] ~ HttpServletRequest RemoteUser is falcon (Servlets:47)\n2016-06-27 10:55:49,818 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Logging in falcon (CurrentUser:65)\n2016-06-27 10:55:49,818 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Request from authenticated user: falcon, URL=/api/entities/delete/feed/curVersNext001, doAs user: null (FalconAuthenticationFilter:185)\n2016-06-27 10:55:49,818 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Authorizing user=falcon against request=RequestParts{resource='entities', action='delete', entityName='curVersNext001', entityType='feed'} (FalconAuthorizationFilter:78)\n2016-06-27 10:55:49,819 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Authenticated user falcon is proxying entity owner falcon/hadoop (CurrentUser:118)\n2016-06-27 10:55:49,819 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Authenticated user falcon is proxying entity owner falcon/hadoop (AUDIT:120)\n2016-06-27 10:55:49,819 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Authenticated user falcon is proxying entity owner falcon/hadoop (CurrentUser:118)\n2016-06-27 10:55:49,819 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Authenticated user falcon is proxying entity owner falcon/hadoop (AUDIT:120)\n2016-06-27 10:55:49,819 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Authorization succeeded for user=falcon, proxy=falcon (FalconAuthorizationFilter:88)\n2016-06-27 10:55:49,820 DEBUG - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Invoking method delete on service org.apache.falcon.resource.ConfigSyncService (IPCChannel:45)\n2016-06-27 10:55:49,820 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Creating Oozie client object for http://master003.current.rec.mapreduce.m1.p.fti.net:11000/oozie/ (OozieClientFactory:50)\n2016-06-27 10:55:49,919 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Creating FS for the login user falcon, impersonation not required (HadoopClientFactory:191)\n2016-06-27 10:55:49,919 ERROR - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Unable to reach workflow engine for deletion or deletion failed (AbstractEntityManager:266)\njava.lang.IllegalArgumentException: Wrong FS: hdfs://clusterB001:8020/apps/falcon/bigdata-current-cluster/staging/falcon/workflows/feed/curVersNext001/7e307c2292e9b897d6a51f68ed17ac51_1467016396742, expected: hdfs://clusterA001:8020\n        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:646)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:194)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)\n        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1424)\n        at org.apache.falcon.entity.EntityUtil.isStagingPath(EntityUtil.java:639)\n        at org.apache.falcon.workflow.engine.OozieWorkflowEngine.findBundles(OozieWorkflowEngine.java:294)\n        at org.apache.falcon.workflow.engine.OozieWorkflowEngine.doBundleAction(OozieWorkflowEngine.java:377)\n        at org.apache.falcon.workflow.engine.OozieWorkflowEngine.doBundleAction(OozieWorkflowEngine.java:371)\n        at org.apache.falcon.workflow.engine.OozieWorkflowEngine.delete(OozieWorkflowEngine.java:355)\n        at org.apache.falcon.resource.AbstractEntityManager.delete(AbstractEntityManager.java:253)\n        at org.apache.falcon.resource.ConfigSyncService.delete(ConfigSyncService.java:62)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.falcon.resource.channel.IPCChannel.invoke(IPCChannel.java:49)\n        at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$3.doExecute(SchedulableEntityManagerProxy.java:230)\n        at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$EntityProxy.execute(SchedulableEntityManagerProxy.java:577)\n        at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$3.execute(SchedulableEntityManagerProxy.java:219)\n        at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.delete_aroundBody2(SchedulableEntityManagerProxy.java:232)\n        at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure3.run(SchedulableEntityManagerProxy.java:1)\n        at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n        at org.apache.falcon.aspect.AbstractFalconAspect.logAroundMonitored(AbstractFalconAspect.java:51)\n        at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.delete(SchedulableEntityManagerProxy.java:206)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.falcon.security.FalconAuthorizationFilter.doFilter(FalconAuthorizationFilter.java:108)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.falcon.security.FalconAuthenticationFilter$2.doFilter(FalconAuthenticationFilter.java:188)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:615)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:574)\n        at org.apache.falcon.security.FalconAuthenticationFilter.doFilter(FalconAuthenticationFilter.java:197)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.falcon.security.FalconAuditFilter.doFilter(FalconAuditFilter.java:64)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.falcon.security.HostnameFilter.doFilter(HostnameFilter.java:82)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n2016-06-27 10:55:49,921 ERROR - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Action failed: Bad Request\nError: Wrong FS: hdfs://clusterB001:8020/apps/falcon/bigdata-current-cluster/staging/falcon/workflows/feed/curVersNext001/7e307c2292e9b897d6a51f68ed17ac51_1467016396742, expected: hdfs://clusterA001:8020 (FalconWebException:83)\n2016-06-27 10:55:49,922 ERROR - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ Action failed: Bad Request\nError: default/org.apache.falcon.FalconWebException::org.apache.falcon.FalconException: Wrong FS: hdfs://clusterB001:8020/apps/falcon/bigdata-current-cluster/staging/falcon/workflows/feed/curVersNext001/7e307c2292e9b897d6a51f68ed17ac51_1467016396742, expected: hdfs://clusterA001:8020\n (FalconWebException:83)\n2016-06-27 10:55:49,922 INFO  - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:falcon:DELETE//entities/delete/feed/curVersNext001] ~ {Action:delete, Dimensions:{colo=NULL, entityType=feed, entityName=curVersNext001}, Status: FAILED, Time-taken:102470925 ns} (METRIC:38)\n2016-06-27 10:55:49,923 DEBUG - [452378800@qtp-794075965-11 - ea0ed7de-a518-4b54-9db2-51e33fde0176:] ~ Audit: falcon/10.98.138.87 performed request http://clusterA003:15000/api/entities/delete/feed/curVersNext001 (10.98.138.87) at time 2016-06-27T08:55Z (FalconAuditFilter:86)\n\n</pre>","tags":["feed","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-09-30 06:25:12.0","id":59214,"title":"Oozie shell action gets stuck in running state","body":"<p>I am stuck with error like job gets stuck in running state it might be due to scheduler i guess. Which scheduler is better to use in a 3 node  hadoop 2.x cluster? Fair or capacity? can any one help me out with configuration of these 2 and how it works? How queues are being used? Any help would be appreciated... Thanks</p>","tags":["Oozie","scheduler"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-09-30 08:58:53.0","id":59234,"title":"Hadoop Cluster Sizing for QJ & NN","body":"<p> What is adequate size for the quorum journal and for the Name Node</p><p>Thank you for your responses</p>","tags":["architecture"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-09-29 19:46:37.0","id":59162,"title":"telnet connection refused","body":"<p>I have installed flume and flume agent. When i try to connect to the localhost using telnet i m getting the connection refused.Please help me with how to resolve this error in hortonworks sandbox.</p>","tags":["connection-refused"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-10-02 09:48:10.0","id":59420,"title":"HDFS-Sharepoint","body":"<p>Hello</p><p>I want to ingest data from a sharepoint repository, containing documents, into HDFS. What is the best way to do this? Nifi doesn't have a connector to a sharepoint repository. Has anyone faced a case like this?</p><p>Thanks</p>","tags":["hadoop","sharepoint"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-10-05 10:58:49.0","id":59941,"title":"How can we find the updated records in Hadoop when they are imported from tera data?","body":"<p>SCD TYPE 2 process </p>","tags":["Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-10-14 16:30:23.0","id":61675,"title":"Adding jars to spark lib","body":"<p>I have a few external jars such as elasticsearch-spark_2.10-2.4.0.jar. Currently I use --jars option to load it for spark-shell. Is there a way to get this or other jars to load with spark for my cluster? I see through Ambari there is the spark-defaults but was wondering if I could just copy X.jar to /usr/hdp&lt;Ver&gt;/spark/lib and it would get picked up.</p><p>A side question somewhat related, its in the same command line, is I use the following: \"--packages com.databricks:spark-avro_2.10:2.0.1\". I notice the first time this is done, spark goes out and grabs the jars like maven would. But I could not find these and wonder if I need this argument as well or can I get databrick libs installed permanently as with elasticsearch?</p><p>Thanks, Mike</p>","tags":["spark-shell","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-10-17 21:10:54.0","id":62002,"title":"KiteSDK HDP 2.5","body":"<p>I recently swapped sandboxes from HDP 2.4 to HDP 2.5 and I'm running into all sorts of issues with the KiteSDK. I created the directory /hdp/apps/2.5.0.0-1245/mapreduce/ and copied in mapreduce.tar.gz which got me a little further, but now I'm running into a \"org.kitesdk.tools.CopyTask: Kite(dataset:file:/tmp/413a41a2-8813-4056-9433-3c5e073d80... ID=1 (1/1)(1): java.io.FileNotFoundException: File does not exist: hdfs://sandbox.hortonworks.com:8020/tmp/crunch-283520469/p1/REDUCE\" that I can't seem to overcome. \nHas anyone successfully gotten KiteAPI to work on  HDP 2.5? I can't figure out what I'm  doing wrong here.\n\nI'd be happy to go back to 2.4 but I can't seem to find a download for it.</p>","tags":["kitesdk","hdp-2.5.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-10-21 02:44:02.0","id":62690,"title":"PublishJMS as plaintext/xml instead of binary","body":"<p>hi, im currently using the PublishJMS processor to send messages to my messaging layer (couldnt use PutJMS as its specific to ActiveMQ only). The contents are in binary format, although my flow file is actually a xml. How can i define the message type in PublishJMS to ensure its delivered as a xml?</p><p>Thanks</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-20 21:19:04.0","id":62670,"title":"Is Ambari Infra open source?","body":"<p>If yes, from where can i find the code?</p>","tags":["ambari-infra"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-10-26 06:14:31.0","id":63487,"title":"Ambari agent fail to start with errors after upgrade from Ambari 2.2.1.1 to 2.4.0.1-1","body":"<p>We have upgraded the Ambari Server and agent from 2.2.1.1 to 2.4.0.1-1</p><p>Ambari Server and Agent on the same server is running fine. We have additional 2 nodes and we are getting following error on both the nodes while starting Ambari agent. </p><p>- we have updated ambari-agent.ini to point to Ambari Server</p><p>- We do not see /var/log/ambari-agent/ambari-agent.log file created.</p><p>- We have tried uninstall and re-install of the ambari-agent but still getting same following error.</p><p>% ambari-agent restart --verbose </p><pre>Restarting ambari-agent\nVerifying Python version compatibility...\nUsing python  /usr/bin/python\nambari-agent is not running. No PID found at /var/run/ambari-agent/ambari-agent.pid\nVerifying Python version compatibility...\nUsing python  /usr/bin/python\nChecking for previously running Ambari Agent...\nStarting ambari-agent\nVerifying ambari-agent process status...\nERROR: ambari-agent start failed. For more details, see /var/log/ambari-agent/ambari-agent.out:\n====================\n    from Controller import AGENT_AUTO_RESTART_EXIT_CODE\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/Controller.py\", line 37, in &lt;module&gt;\n    import AmbariConfig\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/AmbariConfig.py\", line 26, in &lt;module&gt;\n    from NetUtil import NetUtil\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/NetUtil.py\", line 22, in &lt;module&gt;\n    from HeartbeatHandlers import HeartbeatStopHandlers\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/HeartbeatHandlers.py\", line 21, in &lt;module&gt;\n    from ambari_commons.exceptions import FatalException\nImportError: No module named ambari_commons.exceptions\n====================\nAgent out at: /var/log/ambari-agent/ambari-agent.out\nAgent log at: /var/log/ambari-agent/ambari-agent.log</pre><pre>% yum info ambari-agent \nLoaded plugins: security\nInstalled Packages\nName        : ambari-agent\nArch        : x86_64\nVersion     : 2.4.0.1\nRelease     : 1\nSize        : 38 M\nRepo        : installed\nFrom repo   : Updates-ambari-2.4.0.1\nSummary     : Ambari Agent\nURL         : http://www.apache.org\nLicense     : (c) Apache Software Foundation\nDescription : Maven Recipe: RPM Package.</pre>","tags":["ambari-agent"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-10-26 16:30:47.0","id":63614,"title":"Issue in starting the HDFS in sandbox","body":"<p>Was successfully able to run over NiFi tutorials but now I am unable to proceed with Spark with HDFS not starting. Takes longer time to hours when clicked on restart all affected and end result with error as connection error. Please advice</p>","tags":["Sandbox"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-10-27 22:24:16.0","id":63918,"title":"Need help using append replacement strategy in ReplaceText Processor on NiFi","body":"<p>I am trying to take in an attribute, example \"ID\" attribute, from an incoming file and append it into the content of the file. I know that I can use  ${allAttributes(\"ID\")} as the replacement value in the ReplaceText processor and then change the replacement strategy to append and it works but I want to leave a \" | \" between the attribute \"ID\" and the rest of the message. Something like \" ID | rest of message\". Does someone have an idea of how this could be done? </p>","tags":["nifi-processor","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-20 07:50:39.0","id":40679,"title":"Kafka 0.9 new-producer on kerberized HDP 2.4","body":"<p>I am trying to run kafka-console-producer.sh script on a kerberized HDP 2.4 sandbox with Kafka 0.9. I have created a topic and provided user permissions to the topic.</p><p>Below is the command I am using to start the producer:</p><pre>./kafka-console-producer.sh --topic test-topic --broker-list sandbox.hortonworks.com:6667 --security-protocol PLAINTEXTSASL --new-producer</pre><p>Below is the error I get when I try to produce something:</p><pre>ERROR Error when sending message to topic test-topic with key: null, value: 8 bytes with error: Failed to update metadata after 60000 ms. (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)\n</pre><p>It works fine when I don't use the new-producer, but I need to use the new producer API.</p>","tags":["Kafka","hdp-2.4"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-22 04:22:14.0","id":41080,"title":"sqoop incremental load failed with timestamp","body":"<p>Hi,</p><p>I'm trying to run sqoop incremental imports from Teradata to HDFS but running into issues with time stamp.</p><p>my sqoop command:</p><p>sqoop import \\ </p><p>--connect jdbc:teradata://PH/DATABASE=hadoop,CHARSET=UTF8 \\ </p><p>--driver \"com.teradata.jdbc.TeraDriver\" \\ </p><p>--username hadoop \\ </p><p>--password Hadoop \\ </p><p>--table item \\ </p><p>--target-dir /diva/item_text \\ </p><p>--m 1 \\ </p><p>--fields-terminated-by '|' \\ </p><p>--incremental append -check-column dw_modify_ts --last-value '2013-01-01 02:00:00'</p><p>Error Message:</p><p>Error: java.io.IOException: SQLException in nextKeyValue</p><p>Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 15.00.00.20] [Error 6760] [SQLState HY000] Invalid timestamp.</p><p>Log Message:</p><p>-----------------------------------</p><p>16/06/22 04:13:43 INFO tool.ImportTool: Incremental import based on column dw_modify_ts </p><p>16/06/22 04:13:43 INFO tool.ImportTool: Lower bound value: '2013-01-01 02:00:00' </p><p>16/06/22 04:13:43 INFO tool.ImportTool: Upper bound value: '2013-07-10 22:56:01.0' </p><p>16/06/22 04:13:43 INFO mapreduce.ImportJobBase: Beginning import of e2_item_category_text </p><p>Here as per above log sqoop giving milliseconds as well in Upper bound Value \"'2013-07-10 22:56:01.0'?</p><p>I'm suspecting that this milliseconds format having an issue while importing data from Teradata.</p><p>Can you suggest me if you have any idea?</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-22 23:38:47.0","id":41252,"title":"What's purpose of shell(/bin/true) in HDFS HA fencer?","body":"<p>In the process of exploring HDFS HA with ZKFC, I noticed 'dfs.ha.fencing.methods' is configured as 'shell(/bin/true)'. Would anyone explain what's the purpose of this conf? As a bonus, it's better to highlight high level failover flow within which how this conf is applied? Thanks.</p>","tags":["hdfs-ha","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-23 02:50:15.0","id":41268,"title":"My sqoop list-databases works but sqoop import is failing with communication link failure","body":"<p>I'm using the following to create my cluster:</p><p>https://cwiki.apache.org/confluence/display/AMBARI/Quick+Start+Guide</p><p>When I tried to issue the command on my that has both mySql and sqoop, sqoop list database works fine. But when I try to use sqoop import i get the following error:</p><pre>2016-06-23 02:44:00,844 FATAL [IPC Server handler 0 on 45899] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1466646282939_0010_m_000002_0 - exited : java.lang.RuntimeException: java.lang.RuntimeException: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure\n\nThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.\n\tat org.apache.sqoop.mapreduce.db.DBInputFormat.setConf(DBInputFormat.java:167)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:749)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162) </pre><p>Caused by: java.lang.RuntimeException: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-22 17:43:16.0","id":41214,"title":"Hi guys, I would like to know if it is possible to use Metron without installing Ansible (so much expensive now, isnt'it?). Thank you in advance.","body":"","tags":["ansible"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-06-23 11:28:35.0","id":41335,"title":"Can not install hadoop single node cluster with ambari","body":"<p>Hi,</p><p>I am a hadoop-newbie and I have a hard time installing a hadoop single node cluster using ambari 2.2.2.</p><p>I am trying to install on a dual core machine with 2 GB of RAM running CentOS 6.7 (a real machine - no VM).</p><p>I installed by following each single step of this howto:</p><p>http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.1.1/bk_Installing_HDP_AMB/content/ch_Getting_Ready.html</p><p>I installed the following services (all on the same machine as I do not have other machines available):</p><p>HDFS, \nMaReduce2\nYarn\nTez\nHive\nHBase\nPig\nZookeeper\nAmbari-Metrics\nSmartSense</p><p>After the installation has completed I get several warnings that several services could not be started.</p><p>The machine seems to run on heavy load and there are huge lags after any action which makes troubleshooting extremely hard.</p><p>The Alert page looks as follows:</p><p><img src=\"/storage/attachments/5189-alerts.jpg\"></p><p>The Dashboard:</p><p><img src=\"/storage/attachments/5190-dashboard.jpg\"></p><p>Can anyone provide some hints on how to resolve this?</p><p>Thank you in advance,</p><p>chris</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-29 14:36:11.0","id":42384,"title":"Falcon atlas hook - No information known about type","body":"<p>Hi</p><p>After submitting a process I'll get following error message in log - No information known about type</p><p>Where do I have search a reason?</p><pre>2016-06-29 17:21:27,411 INFO  - [1402323266@qtp-443721024-6 - d125c836-a3f4-4dd3-82c9-5faa53631f11:falcon:POST//entities/submit/process] ~ PROCESS/demo-process16 is published into config store (AUDIT:264)\n2016-06-29 17:21:27,411 INFO  - [Atlas Logger 0:] ~ Entered Atlas hook for Falcon hook operation ADD_PROCESS (FalconHook:148)\n2016-06-29 17:21:27,411 INFO  - [1402323266@qtp-443721024-6 - d125c836-a3f4-4dd3-82c9-5faa53631f11:falcon:POST//entities/submit/process] ~ Submit successful: (PROCESS): demo-process16 (AbstractEntityManager:711)\n2016-06-29 17:21:27,412 INFO  - [Atlas Logger 0:] ~ Creating process Instance : demo-process16 (FalconHook:179)\n2016-06-29 17:21:27,412 INFO  - [1402323266@qtp-443721024-6 - d125c836-a3f4-4dd3-82c9-5faa53631f11:falcon:POST//entities/submit/process] ~ {Action:submit, Dimensions:{colo=NULL, entityType=process}, Status: SUCCEEDED, Time-taken:87014165 ns} (METRIC:38)\n2016-06-29 17:21:27,412 DEBUG - [1402323266@qtp-443721024-6 - d125c836-a3f4-4dd3-82c9-5faa53631f11:] ~ Audit: falcon/127.0.0.1 performed request http://localhost:15000/api/entities/submit/process (127.0.0.1) at time 2016-06-29T14:21Z (FalconAuditFilter:86)\n2016-06-29 17:21:27,412 INFO  - [1402323266@qtp-443721024-6 - d125c836-a3f4-4dd3-82c9-5faa53631f11:] ~ Audit: falcon@127.0.0.1 performed http://localhost:15000/api/entities/submit/process (127.0.0.1) at 2016-06-29T14:21Z (AUDIT:48)\n2016-06-29 17:21:27,412 INFO  - [1402323266@qtp-443721024-6 - d125c836-a3f4-4dd3-82c9-5faa53631f11:] ~ Audit: falcon@127.0.0.1 performed http://localhost:15000/api/entities/submit/process (127.0.0.1) at 2016-06-29T14:21Z (AUDIT:48)\n2016-06-29 17:21:27,418 INFO  - [Atlas Logger 0:] ~ Adding entity for type: hive_db (AtlasHook:72)\n2016-06-29 17:21:27,794 INFO  - [Atlas Logger 0:] ~ Adding entity for type: hive_table (AtlasHook:72)\n2016-06-29 17:21:27,795 INFO  - [Atlas Logger 0:] ~ Adding entity for type: falcon_process (AtlasHook:72)\n2016-06-29 17:21:27,821 INFO  - [Atlas Logger 0:] ~ Atlas hook failed (FalconHook:140)\norg.json4s.package$MappingException: No usable value for values\nNo information known about type\n        at org.json4s.reflect.package$.fail(package.scala:96)\n        at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:443)\n        at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:463)\n        at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:463)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n        at scala.collection.AbstractTraversable.map(Traversable.scala:105)\n        at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$instantiate(Extraction.scala:451)\n        at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:491)\n        at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:488)\n        at org.json4s.Extraction$.org$json4s$Extraction$$customOrElse(Extraction.scala:500)\n        at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:488)\n        at org.json4s.Extraction$.extract(Extraction.scala:332)\n        at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$mkWithTypeHint(Extraction.scala:483)\n        at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:490)\n        at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$result$6.apply(Extraction.scala:488)\n        at org.json4s.Extraction$.org$json4s$Extraction$$customOrElse(Extraction.scala:500)\n        at org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:488)\n        at org.json4s.Extraction$.extract(Extraction.scala:332)\n        at org.json4s.Extraction$.extract(Extraction.scala:42)\n        at org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:21)\n        at org.json4s.native.Serialization$.read(Serialization.scala:71)\n        at org.apache.atlas.typesystem.json.InstanceSerialization$.fromJsonReferenceable(InstanceSerialization.scala:335)\n        at org.apache.atlas.typesystem.json.InstanceSerialization.fromJsonReferenceable(InstanceSerialization.scala)\n        at org.apache.atlas.notification.hook.HookNotification$EntityCreateRequest.&lt;init&gt;(HookNotification.java:152)\n        at org.apache.atlas.hook.AtlasHook.notifyEntities(AtlasHook.java:78)\n        at org.apache.atlas.falcon.hook.FalconHook.fireAndForget(FalconHook.java:150)\n        at org.apache.atlas.falcon.hook.FalconHook.access$200(FalconHook.java:63)\n        at org.apache.atlas.falcon.hook.FalconHook$2.run(FalconHook.java:138)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.json4s.package$MappingException: No information known about type\n\n</pre>","tags":["Falcon","Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-07-06 15:36:25.0","id":43721,"title":"Could not find port with name 'portName' for remote NiFi instance","body":"<p>Hello, I cannot connect my NifiStorm java class to my NiFi instance. I get an exception error \"Could not find port with name portName for remote NiFi instance\". My NiFi instance is running with the port name = \"portName\" and Ambari reports that the DRPC server, nimbus, and the UI Server has started. It also reports that the Supervisor is live. I have Storm on my sandbox - HDP 3.2. What could be causing this unrecognized port name error?</p>","tags":["Nifi","Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-06 08:39:58.0","id":43642,"title":"SparkR - Error in socketConnection(port = monitorPort) in centos 6.6","body":"<p>I installed HDP 2.3.4 on mutinodes , through blueprint using Ambari, installed spark 1.5.2. </p><p>and install R 3.3.0.</p><p>When I run sparkR on one node, it show the error message. Following</p><p><em>Error in socketConnection(port = monitorPort) : cannot open the connection In addition: Warning message: In socketConnection(port = monitorPort) : localhost:32001 cannot be opened</em></p><p><em>so, I also follow </em>https://community.hortonworks.com/articles/21304/installing-spark-16-on-hdp-23x.html to install spark 1.6 \n</p><p>and run sparkR.</p><p>But, it also show the same error message.</p>","tags":["Spark","sparkr"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-07 10:13:19.0","id":43911,"title":"Should i need to cleaning up of tmp space in hadoop cluster on weekly basis ? if yes how can i do it? please suggest","body":"","tags":["hadoop-maintenance","Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-07 07:22:43.0","id":43885,"title":"NOT IN operator in hive with a subquery","body":"<p>I have a subquery like this:</p><pre>select SF.customer_email , SF.created_at\nFROM \nTable1 SF\nWHERE\nYEAR(TO_DATE(SF.created_at)) = '2016'\nAND \nMONTH(TO_DATE(SF.created_at)) = '6'\nAND \nSF.customer_email \nNOT IN (\nselect SFO.customer_email FROM Table1 SFO \nWHERE\nTO_DATE(SFO.created_at) &lt; '2016-05-31'\n)</pre><p>I have checked manually and I should get results for the query but it returns empty resultset.</p><p>Note: I am using same table in the subquery as well. Just a different condition on date column.</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-07 16:00:40.0","id":43972,"title":"Failure during the step \"Install, start and test\"","body":"<p><a href=\"/storage/attachments/5597-ambari-service-install-failure.png\">ambari-service-install-failure.png</a><a href=\"/storage/attachments/5598-ambari-services.txt\">ambari-services.txt</a>\n</p><p>Hi,</p><p>I'm trying to install HDP stack 2.3 in Ubuntu 14.04 . During the service installation at step 'Install, start and test' it is failing after some services got installed. Not sure which of the service installation is actually failing. Also I'm not aware of where to get the exact log. Attaching the services list and the screenshot.</p><p>Please help.</p>","tags":["hdp-2.3.0","error"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-13 21:11:50.0","id":45073,"title":"Confused about the Spark setup doc","body":"<p>Hi,</p><p>Looks like the highlighted parts below are conflicting. Both are trying to set SPARK_HOME variable (with different values). Do you have any ideas?</p><p>link: <a href=\"http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/\">http://hortonworks.com/hadoop-tutorial/a-lap-around-apache-spark/</a></p><p><strong>3. Set JAVA_HOME and SPARK_HOME:</strong></p><p>Make sure that you set JAVA_HOME before you launch the Spark Shell or thrift server.</p><p>export JAVA_HOME=&lt;path to JDK 1.8&gt;</p><p>The Spark install creates the directory where Spark binaries are unpacked to /usr/hdp/2.3.4.1-10/spark.</p><p>Set the SPARK_HOME variable to this directory:</p><p><strong>export SPARK_HOME=/usr/hdp/2.3.4.1-10/spark/</strong></p><p><strong>4. Create hive-site in the Spark conf directory:</strong></p><p>As user root, create the file SPARK_HOME/conf/hive-site.xml. Edit the file to contain only the following configuration setting:</p><p>&lt;configuration&gt;</p><p>&lt;property&gt;</p><p>&lt;name&gt;hive.metastore.uris&lt;/name&gt;</p><p>&lt;!--Make sure that &lt;value&gt; points to the Hive Metastore URI in your cluster --&gt;</p><p>&lt;value&gt;thrift://sandbox.hortonworks.com:9083&lt;/value&gt;</p><p>&lt;description&gt;URI for client to contact metastore server&lt;/description&gt;</p><p>&lt;/property&gt;&lt;/configuration&gt;</p><p><strong>Set SPARK_HOME</strong></p><p>If you haven’t already, make sure to set SPARK_HOME before proceeding:</p><p><strong>export SPARK_HOME=/usr/hdp/current/spark-client</strong></p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-13 23:04:47.0","id":45076,"title":"Data Tokenization","body":"<p>Hello, </p><p>Please suggest some data tokenization took which would be compatible with Hortonworks. Should be certified. </p><p>Thanks .  </p>","tags":["MapReduce","security"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-07-14 03:12:32.0","id":45099,"title":"Name node heap memory and yarn memory?","body":"<p>I see in ambari cluster: If yarn memory is increases for some processes at the same time namenode heap memory are also increases. What is the relation between them?</p>","tags":["YARN","namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-14 00:48:25.0","id":45084,"title":"Zeppelin installation via Ambari is failing on HDP version 2.4","body":"<p>Hi All,</p><p>Zeppelin installation via Ambari fails on HDP2.4. RHEL6.5 64bit.</p><p>Below are the installation logs from Ambari. The yum command in the log actually fails when run manually.</p><p>Manually installed pip as below but when uninstalled and installed again the same error.</p><pre>[root@ip-172-31-24-221 ~]# curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\"\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1488k  100 1488k    0     0  19.1M      0 --:--:-- --:--:-- --:--:-- 45.4M\n[root@ip-172-31-24-221 ~]# \n[root@ip-172-31-24-221 ~]# python get-pip.py\nDEPRECATION: Python 2.6 is no longer supported by the Python core team, please upgrade your Python. A future version of pip will drop support for Python 2.6\nCollecting pip\n/tmp/tmpfrM4hF/pip.zip/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.\n/tmp/tmpfrM4hF/pip.zip/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  Downloading pip-8.1.2-py2.py3-none-any.whl (1.2MB)\n    100% |████████████████████████████████| 1.2MB 978kB/s \nCollecting wheel\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\n    100% |████████████████████████████████| 71kB 9.6MB/s \nCollecting argparse; python_version == \"2.6\" (from wheel)\n  Downloading argparse-1.4.0-py2.py3-none-any.whl\nInstalling collected packages: pip, argparse, wheel\n  Found existing installation: argparse 1.2.1\n    Uninstalling argparse-1.2.1:\n      Successfully uninstalled argparse-1.2.1\nSuccessfully installed argparse-1.4.0 pip-8.1.2 wheel-0.29.0\n/tmp/tmpfrM4hF/pip.zip/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n[root@ip-172-31-24-221 ~]# pip -V\npip 8.1.2 from /usr/lib/python2.6/site-packages (python 2.6)</pre><p>How could I get around this problem and install Zeppelin please?</p><p>Plus, after this failure, starting all services in Ambari stopped working. I have to start/stop each service at a time. It is not clear whether Zeppelin install failure somehow affect this. </p><p>Please advise.</p><p>Thanks,</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py\", line 235, in &lt;module&gt;\n    Master().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py\", line 54, in install\n    self.install_packages(env)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 404, in install_packages\n    Package(name)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py\", line 49, in action_install\n    self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py\", line 49, in install_package\n    shell.checked_call(cmd, sudo=True, logoutput=self.get_logoutput())\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of '/usr/bin/yum -d 0 -e 0 -y install python-pip' returned 1. Error: Nothing to do\n\n</pre>\n<pre>2016-07-14 10:18:17,167 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.4.2.0-258\n2016-07-14 10:18:17,167 - Checking if need to create versioned conf dir /etc/hadoop/2.4.2.0-258/0\n2016-07-14 10:18:17,167 - call['conf-select create-conf-dir --package hadoop --stack-version 2.4.2.0-258 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-07-14 10:18:17,189 - call returned (1, '/etc/hadoop/2.4.2.0-258/0 exist already', '')\n2016-07-14 10:18:17,189 - checked_call['conf-select set-conf-dir --package hadoop --stack-version 2.4.2.0-258 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-07-14 10:18:17,210 - checked_call returned (0, '')\n2016-07-14 10:18:17,211 - Ensuring that hadoop has the correct symlink structure\n2016-07-14 10:18:17,211 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-07-14 10:18:17,212 - Group['spark'] {}\n2016-07-14 10:18:17,214 - Group['zeppelin'] {}\n2016-07-14 10:18:17,214 - Adding group Group['zeppelin']\n2016-07-14 10:18:17,238 - Group['hadoop'] {}\n2016-07-14 10:18:17,239 - Group['users'] {}\n2016-07-14 10:18:17,239 - Group['knox'] {}\n2016-07-14 10:18:17,239 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,240 - User['storm'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,241 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,241 - User['oozie'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-07-14 10:18:17,242 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,243 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,243 - User['falcon'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-07-14 10:18:17,244 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-07-14 10:18:17,244 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,245 - Adding user User['zeppelin']\n2016-07-14 10:18:17,296 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,297 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-07-14 10:18:17,297 - User['flume'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,298 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,299 - User['sqoop'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,299 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,300 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,301 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,301 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,302 - User['hcat'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-07-14 10:18:17,303 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-07-14 10:18:17,305 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2016-07-14 10:18:17,309 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2016-07-14 10:18:17,309 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'recursive': True, 'mode': 0775, 'cd_access': 'a'}\n2016-07-14 10:18:17,310 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-07-14 10:18:17,311 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}\n2016-07-14 10:18:17,314 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if\n2016-07-14 10:18:17,315 - Group['hdfs'] {}\n2016-07-14 10:18:17,315 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'hdfs']}\n2016-07-14 10:18:17,316 - Directory['/etc/hadoop'] {'mode': 0755}\n2016-07-14 10:18:17,331 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-07-14 10:18:17,332 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}\n2016-07-14 10:18:17,345 - Repository['HDP-2.4'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.2.0', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP', 'mirror_list': None}\n2016-07-14 10:18:17,354 - File['/etc/yum.repos.d/HDP.repo'] {'content': '[HDP-2.4]\\nname=HDP-2.4\\nbaseurl=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.2.0\\n\\npath=/\\nenabled=1\\ngpgcheck=0'}\n2016-07-14 10:18:17,354 - Repository['HDP-UTILS-1.1.0.20'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP-UTILS', 'mirror_list': None}\n2016-07-14 10:18:17,358 - File['/etc/yum.repos.d/HDP-UTILS.repo'] {'content': '[HDP-UTILS-1.1.0.20]\\nname=HDP-UTILS-1.1.0.20\\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6\\n\\npath=/\\nenabled=1\\ngpgcheck=0'}\n2016-07-14 10:18:17,358 - Package['unzip'] {}\n2016-07-14 10:18:17,460 - Skipping installation of existing package unzip\n2016-07-14 10:18:17,460 - Package['curl'] {}\n2016-07-14 10:18:17,499 - Skipping installation of existing package curl\n2016-07-14 10:18:17,499 - Package['hdp-select'] {}\n2016-07-14 10:18:17,537 - Skipping installation of existing package hdp-select\n2016-07-14 10:18:17,699 - Execute['find /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package -iname \"*.sh\" | xargs chmod +x'] {}\n2016-07-14 10:18:17,708 - Execute['echo platform.linux_distribution:Red Hat Enterprise Linux Server+6.5+Santiago'] {}\n2016-07-14 10:18:17,712 - Package['gcc-gfortran'] {}\n2016-07-14 10:18:17,812 - Installing package gcc-gfortran ('/usr/bin/yum -d 0 -e 0 -y install gcc-gfortran')\n2016-07-14 10:18:22,975 - Package['blas-devel'] {}\n2016-07-14 10:18:23,016 - Installing package blas-devel ('/usr/bin/yum -d 0 -e 0 -y install blas-devel')\n2016-07-14 10:18:25,378 - Package['lapack-devel'] {}\n2016-07-14 10:18:25,418 - Installing package lapack-devel ('/usr/bin/yum -d 0 -e 0 -y install lapack-devel')\n2016-07-14 10:18:29,415 - Package['python-devel'] {}\n2016-07-14 10:18:29,458 - Skipping installation of existing package python-devel\n2016-07-14 10:18:29,459 - Package['python-pip'] {}\n2016-07-14 10:18:29,497 - Installing package python-pip ('/usr/bin/yum -d 0 -e 0 -y install python-pip')</pre>","tags":["zeppelin-notebook"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-14 11:01:37.0","id":45155,"title":"I tired using non authorized user to publish data on kafaka getting following error","body":"<p>I tired using non authorized user to publish data and it threw me this error:</p><p>\n[2016-07-14 06:31:18,796] WARN Error while fetching metadata [{TopicMetadata for topic test_topic_to_delete -&gt;\nNo partition metadata for topic test_topic_to_delete due to kafka.common.TopicAuthorizationException}] for topic [test_topic_to_delete]: class kafka.common.TopicAuthorizationException  (kafka.producer.BrokerPartitionInfo)</p><p>\nHowever, we are getting this error when publishing data with authorized error. After kinit, I ran console producer to topic \"test_topic_to_delete\" using broker \"*.*.*.*:6667\". This is throwing</p><p> LeaderNotAvailableException exception and fails after 3 tries.\n[2016-07-14 06:38:35,277] WARN Error while fetching metadata [{TopicMetadata for topic test_topic_to_delete -&gt;No partition metadata for topic test_topic_to_delete due to kafka.common.LeaderNotAvailableException}] for topic [test_topic_to_delete]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo) </p><p>However, when I ran topic describe I can see the leader chosen for the topic. I ran below command,\nkafka-topics.sh --describe --zookeeper *.*.*.*:2181 --topic test_topic_to_delete\nIt gave me below result:\nTopic:test_topic_to_delete      PartitionCount:1        ReplicationFactor:2     Configs:        Topic: test_topic_to_delete     Partition: 0    Leader: 1002    Replicas: 1002,1001     Isr: 1002\nSo topic leader is clearly chosen but console producer is giving me this error.</p><p> \nPlease help me  to debug this issue further. Somehow, Leader is not getting mapped.</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-15 00:22:54.0","id":45306,"title":"HDPCD:Spark","body":"<p>Hi,</p><p> I have a question regarding the exam. If the question \ndoes not specify the output format (json, parquet, etc..), does it mean I\n can use any of the available \noptions in spark? For example, would the output (which I will export via my Spark code) in hdfs \n\"part0000-.....gz.parquet\" be valid (assuming the data inside complies \nwith the question conditions/criteria). </p><p>Also, may I used \nDataFrames & Spark SQL to process the datasets, instead of plain RDD\n if the question does not specify that as well?</p><p>Thanks</p>","tags":["hdpcd","Spark"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-15 20:55:13.0","id":45462,"title":"INSERT INTO Hive Storage Handlers","body":"<p>I have the following technical questions regarding INSERT INTO Hive storage handlers. </p><p>FYI here is how I define my OutputFormat: </p><pre> public class MyOutputFormat extends OutputFormat implements HiveOutputFormat { </pre><p>1) I notice that when hive.execution.engine=mr, the checkOutputSpecs(FileSystem fs, JobConf job) method in my OutputFormat is called once before executing the multi-threaded RecordWriter code.  However, when hive.execution.engine=tez, the method is not called at all.  Is this expected?  And if so is there a different method called instead? </p><p>2) When will the getOutputCommitter() method in my OutputFormat be called?  I can't seem to get it called during my INSERT query.</p><p>\n3) How is the parallelism determined for a INSERT INTO Hive storage handler query?  Does Hive automatically decide how many mappers/containers to use based on the size of the data to be inserted?  Do we have any control over that?\nThank you in advance for any help to any of the questions.  </p>","tags":["Hive","hive-serde"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-20 03:03:54.0","id":46008,"title":"Urgent..please help me...2 data nodes goes down.","body":"<p>i have added dfs.namenode.acls.enable =true in hdfs..i have 4 data node cluster with HDP 2.1.2..i restart the services</p>","tags":["hadoop-maintenance"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-07-20 20:57:05.0","id":46223,"title":"I am trying to install Sandbox on Azure and getting a did not deploy message. Has anyone else experienced this problem and possibly have a solution?","body":"","tags":["deploy","azure"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-22 08:49:56.0","id":46652,"title":"Create web application over Hive and Hbase","body":"<p>We want to develop a web application on top of Hive and Hbase. The application must communicate with HDP securely.What are the best practice to be followed in terms of security?</p>","tags":["security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-07-22 20:45:25.0","id":46772,"title":"How to save dataframe as text file","body":"<p>How to save the data inside a dataframe to text file in csv format in HDFS?</p><p>Tried the following but csv doesn't see to be a supported format</p><pre>df.write.format(\"csv\").save(\"/filepath\")</pre>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-25 08:19:52.0","id":46905,"title":"Best Practices to access HIVE from IBM SPSS","body":"<p>At Daimler , We extensively use IBM SPSS Modeller for Data Science activities and when it comes to access HIVE from IBM SPSS we use Hortonworks ODBC connection. Performance of accessing data is not good and takes huge amount of time.</p><p>Please suggest the most optimized way to access HIVE data from IBM SPSS and if someone has already done some benchmark that would be highly appreciated.</p><p>Scenarios :</p><p>We used small tables with joins IBM SPSS took a lot of time.</p><p>We tried to combine data on HIVE side and then do filter in IBM SPSS still the performance is not good.</p><p>Thank you for suggestion and help.</p>","tags":["ibm","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-26 04:38:23.0","id":47167,"title":"Whether Web UI configuration is supported for Hive Server2  @HDP 2.2.9, if so how to configure it  ?","body":"","tags":["hiveserver2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-26 20:31:40.0","id":47468,"title":"Beeline--Why the select query doesn't go to application log? Why create/insert appears in application logs?","body":"<p>Beeline--Why the select query doesn't go to application log? Why create/insert appears in application logs? any reason behind that?</p>","tags":["YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-26 23:50:34.0","id":47476,"title":"zeppelin won't start in hdp 2. 4 sandbox","body":"<p>just downloaded and started a fresh 2.4 sandbox image.  After starting the VM (looked normal), doing nothing else, and browsing to ambari, I noticed that Zeppelin wasn't running, and that's what I was about to use. The VM startup said Zeppelin was started.  Tried to start it with Ambari and it didn't start. I tried this a few times with the same result.</p><p>There is a stack trace, any ideas? thanks.</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py\", line 235, in &lt;module&gt;\n    Master().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py\", line 179, in start\n    self.update_zeppelin_interpreter()\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py\", line 196, in update_zeppelin_interpreter\n    data = json.load(urllib2.urlopen(zeppelin_int_url))\n  File \"/usr/lib64/python2.6/urllib2.py\", line 126, in urlopen\n    return _opener.open(url, data, timeout)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 391, in open\n    response = self._open(req, data)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 409, in _open\n    '_open', req)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 369, in _call_chain\n    result = func(*args)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 1190, in http_open\n    return self.do_open(httplib.HTTPConnection, req)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 1165, in do_open\n    raise URLError(err)\nurllib2.URLError: &lt;urlopen error [Errno 111] Connection refused&gt;</pre>","tags":["zeppelin-notebook","zeppelin","startup"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-27 02:12:08.0","id":47501,"title":"User Management Question","body":"<p>Trying to explore the best way to manage users in hadoop ecosystem. Basically I am going to provide 3 user interface to user community:</p><p>a.) EdgeNode - Linux machine. This is where users can use their Linux credentials and use command line to use hadoop clients (spark, sqoop, hdfs etc)</p><p>b.) Ambari web interface</p><p>c.) HUE interface</p><p>d.) Ranger - For Admins to control file and folder permissions</p><p>Question I have is is it possible to create account in Linux environment and let all other pull it from there and use the same credentials. I read about LDAP but it appears to be difficult and we don't currently have a working LDAP. </p><p>How can I centrally manage users without using LDAP ?</p><p>Thanks</p><p>Prakash</p>","tags":["users"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-07-27 04:01:33.0","id":47518,"title":"Nifi and Clustering","body":"<p>Hi,</p><p>I have two questions:</p><p>1. My understanding is that if I have two servers and I want to route files on a given condition so that only a certain node located on one of the servers will do processing on these specified files, I should use Site-2-Site/Remote Process Group. Is this correct?</p><p>2. I have tried to set up an RPG. First I tried from a Nifi Cluster Manager instance to another remote nifi instance. Then I tried from one NCM to another NCM. I got the same error on both: a java.net.ConnectException. I am able to set up the input port, on the remote instance and have both clusters run on the servers, but the first NCM is unable to pass files to the second. Could this be due to a nifi.properties port issue? I have followed the set up for nifi.properties instructions pretty closely. Also, just for clarification, in nifi.properties on the remote instance (node receiving data) , is the input.remote.socket port on the manager supposed to be different than the child node's? Any advice would be appreciated. </p><p>Thanks.</p><p>K</p>","tags":["Nifi","process-groups"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-07-27 04:56:59.0","id":47526,"title":"Not able to start HDFS service from ambari after installing HDP 2.4 in single node vm","body":"<h1></h1><p>Hi,</p><p>I am a beginner in HDP and Ambari. Recently, i have installed HDP 2.4 in my cent os 7 VM ( single node with 5 gb ram). Installation went fine. After installation, when I tried to start hdfs service from UI ( automated start got failed), I got below error:</p><p>################</p><p>resource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ; /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh --config /usr/hdp/current/hadoop-client/conf start namenode'' returned 1. /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh: line 76: [: <a href=\"http://centos-node2.com/\">centos-node2.com</a>: integer expression expected starting namenode, logging to /var/log/hadoop/hdfs/hadoop-hdfs-namenode-192.168.2.15 Error: Could not find or load main class <a href=\"http://centos-node2.com/\">centos-node2.com</a></p><p>#####################</p><p>Not sure how to fix this issue (:. If anyone can please help!!!!!</p><p>some more info:</p><p>################</p><p>[root@192 sbin]# hadoop classpath /usr/hdp/2.4.0.0-169/hadoop/conf:/usr/hdp/2.4.0.0-169/hadoop/lib/*:/usr/hdp/2.4.0.0-169/hadoop/.//*:/usr/hdp/2.4.0.0-169/hadoop-hdfs/./:/usr/hdp/2.4.0.0-169/hadoop-hdfs/lib/*:/usr/hdp/2.4.0.0-169/hadoop-hdfs/.//*:/usr/hdp/2.4.0.0-169/hadoop-yarn/lib/*:/usr/hdp/2.4.0.0-169/hadoop-yarn/.//*:/usr/hdp/2.4.0.0-169/hadoop-mapreduce/lib/*:/usr/hdp/2.4.0.0-169/hadoop-mapreduce/.//*:/bin:/usr/java/jdk1.8.0_51/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/java/jdk1.8.0_51/bin:/root/bin:mysql-connector-java.jar:postgresql-jdbc2ee.jar:postgresql-jdbc2.jar:postgresql-jdbc3.jar:postgresql-jdbc.jar:/usr/hdp/2.4.0.0-169/tez/*:/usr/hdp/2.4.0.0-169/tez/lib/*:/usr/hdp/2.4.0.0-169/tez/conf</p><p>##################################</p><p>/var/lib/ambari-agent/data/errors-257.txt :</p><p>Traceback (most recent call last): File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py\", line 401, in NameNode().execute() File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute method(env) File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py\", line 102, in start namenode(action=\"start\", hdfs_binary=hdfs_binary, upgrade_type=upgrade_type, env=env) File \"/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py\", line 89, in thunk return fn(*args, **kwargs) File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py\", line 146, in namenode create_log_dir=True File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/utils.py\", line 267, in service Execute(daemon_cmd, not_if=process_id_exists_command, environment=hadoop_env_exports) File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__ self.env.run() File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run self.run_action(resource, action) File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action provider_action() File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 238, in action_run tries=self.resource.tries, try_sleep=self.resource.try_sleep) File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner result = function(command, **kwargs) File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call tries=tries, try_sleep=try_sleep) File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper result = _call(command, **kwargs_copy) File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call raise Fail(err_msg) resource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ; /usr/hdp/current/hadoop-client/sbin/hadoop- daemon.sh --config /usr/hdp/current/hadoop-client/conf start namenode'' returned 1. /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh: line 76: [: <a href=\"http://centos-node2.com/\">centos-node2.com</a>: integer expression expected starting namenode, logging to /var/log/hadoop/hdfs/hadoop-hdfs-namenode-192.168.2.15 Error: Could not find or load main class <a href=\"http://centos-node2.com/\">centos-node2.com</a></p><p>Thanks,</p><p>Subhadeep</p>","tags":["issue-resolution"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-29 09:45:49.0","id":48197,"title":"distcp -append not working between clusters","body":"<p>We need to use distcp -append to append data to files between Prod and Non-Prod. But I find that distcp -append does not work as it is overwriting the data and it only works when distcp is performed on the same cluster. Is this a known bug? Both the clusters are HDP2.3.4. </p><p>There is no error while performing operation but it does not append the data, instead, the whole file is being overwritten. I tried to run it with -update command also, but it didn't work. If I do simple distcp between prod n non-prod it works. Just with -append it doesn't work.</p>","tags":["distcp"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-02 11:45:37.0","id":48707,"title":"HDP2.4 services are stuck after Kerberize...","body":"<p>Getting boot issue with HDP2.4 sandbox after Kerberize it. </p><p>Kerberose Installation was successful but at the end wizard unable to start all services therefore i restarted the machine but after boot system stuck on \"Starting Zepplin\"</p><p>Anybody have seen similar issue before...</p><p><img src=\"/storage/attachments/6229-untitled.png\"></p>","tags":["kerberos","hdp2.4"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-08-08 06:33:46.0","id":50285,"title":"How to drop delimiter in hive table","body":"<p>1. I download the file using download.file function in R </p><p>2. Save it locally </p><p>3. Move it to HDFS</p><p>\nMy hive table is mapped to this location of file in HDFS. Now, this file has `,` as the delimiter/separator which I cannot change using download.file function from R.</p><p>\nThere is a field that has `,` in its content. How do I tell hive to drop this delimiter if found anywhere within the field contents?\nI understand we can change the delimiter but is there really a way I can drop it like sqoop allows us to do?\nMy R script is as simple as</p><p> \n    download.file() </p><p>    hdfs.put </p><p>Is there a workaround?</p>","tags":["Hive","r"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-09 11:09:31.0","id":50495,"title":"How we can sync up the ambari agents in other nodes after ambari upgrade?","body":"<p>Hi Guys,</p><p>We are using 24 node cluster.</p><p>We have upgraded my Ambari-server and ambari agent in main server. How can i sync up the ambari agent in all other nodes.</p><p>Can someone please help us?</p>","tags":["Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-08-16 17:17:59.0","id":51849,"title":"Can ExecuteScript create multiple files (similar to splittext)","body":"<p>I was wondering if ExecuteScript could create multiple flow files similar to splitext?  </p><p>I am thinking about a script that will split and do some etl and checks on a csv, and then create multiple sql statements that I will then pass down the pipe to the ExecuteSQLcomponent. </p><p>I have everything working, except all the sql all comes out in one file, so I was wondering how I would go about creating multiple flowfiles?</p><p>Thanks!\n  Gerry.</p>","tags":["Nifi","sql"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-12 10:12:40.0","id":51234,"title":"ambari server installation is not installing default postgres and oracle","body":"<p>Hello All,</p><p>  I local yum repository , and trying to install ambari server but its failing with dependency of postgress . as per the installation document yum install should bring default postgres DB and oracle ?</p><p>[root@cloud1 ~]# yum install ambari-server\nLoaded plugins: product-id, refresh-packagekit, security, subscription-manager\nThis system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\nSetting up Install Process\nResolving Dependencies\n--&gt; Running transaction check\n---&gt; Package ambari-server.x86_64 0:2.1.0-1470 will be installed\n--&gt; Processing Dependency: postgresql-server &gt;= 8.1 for package: ambari-server-2.1.0-1470.x86_64\n--&gt; Finished Dependency Resolution\nError: Package: ambari-server-2.1.0-1470.x86_64 (Updates-ambari-2.1.0)\n           Requires: postgresql-server &gt;= 8.1\n You could try using --skip-broken to work around the problem\n You could try running: rpm -Va --nofiles --nodigest\n[root@cloud1 ~]#\n</p>","tags":["ambari-server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-18 20:42:01.0","id":52407,"title":"How to find the node where ambari server runs","body":"<p>How to find the node where ambari server runs</p>","tags":["ambari-service","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-22 16:09:45.0","id":52819,"title":"Nifi Pivot row to columns","body":"<p>I have a XML file that’s a natural fit for HBASE, unfortunately\nI need to pivot the data and insert it into an Oracle Table. I used the NiFi\nSplitXML to break the file into a key value pairs. I’m considering using Python\n(Pandas) to pivot the data, however my question\nis how do I in NiFi collect (or know I have) all the small pieces (queues\nfile and combine them) so that I can output the Pivot Table?</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-22 19:28:18.0","id":52850,"title":"Unable to load AWS credentials from any provider in the chain, when trying to read from S3 using s3a://. I have updated the core-site, hdfs-site and hive-site files.","body":"<p>Unable to load AWS credentials from any provider in the chain, when trying to read from S3 using s3a://. </p><p>I have updated the core-site, hdfs-site and hive-site files properties with fs.3sa, fs.s3n, and fs.s3.</p>","tags":["aws"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-23 01:29:46.0","id":52855,"title":"Data transfer between two clusters","body":"<p>What are the different options to transfer data from old cluster to new one. (HDFS/Hive/HBase) ? </p>","tags":["hdp-2.4.0"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-08-25 02:56:20.0","id":53288,"title":"run oozie job error","body":"<p>When I run the job,I got a error:</p><pre>2016-08-25 10:34:31,351  WARN ActionStartXCommand:523 - SERVER[hdp-m2] USER[oozie] GROUP[-] TOKEN[] APP[sqoop-wf] JOB[0000002-160825102604568-oozie-root-W] ACTION[0000002-160825102604568-oozie-root-W@sqoop-node] Error starting action [sqoop-node]. ErrorType [FAILED], ErrorCode [EJ001], Message [Could not locate Oozie sharelib]\norg.apache.oozie.action.ActionExecutorException: Could not locate Oozie sharelib\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.addSystemShareLibForAction(JavaActionExecutor.java:730)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.addAllShareLibs(JavaActionExecutor.java:825)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.setLibFilesArchives(JavaActionExecutor.java:816)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:1044)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:1293)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:250)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:64)\n\tat org.apache.oozie.command.XCommand.call(XCommand.java:286)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:321)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:250)\n\tat org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n</pre><p>then, I check the sharelib  like this</p><pre>[root@hdp-m2 bin]# oozie admin -oozie http://hdp-m2:11000/oozie -shareliblist\n[Available ShareLib]\n</pre><p>so, I create sharelib like this </p><pre>./oozie-setup.sh sharelib create -fs http://hdp-m2:8020 -locallib /usr/hdp/2.4.0.0-169/oozie/lib/*\n</pre><p>then, I got the other error</p><pre>log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nError: tar: This does not look like a tar archive\ntar: Skipping to next header\ntar: Exiting with failure status due to previous errors\n\nStack trace for the error was (for debug purposes):\n--------------------------------------\nExitCodeException exitCode=2: tar: This does not look like a tar archive\ntar: Skipping to next header\ntar: Exiting with failure status due to previous errors\n at org.apache.hadoop.util.Shell.runCommand(Shell.java:576)\n at org.apache.hadoop.util.Shell.run(Shell.java:487)\n at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753)\n at org.apache.hadoop.fs.FileUtil.unTarUsingTar(FileUtil.java:675)\n at org.apache.hadoop.fs.FileUtil.unTar(FileUtil.java:651)\n at org.apache.oozie.tools.OozieSharelibCLI.run(OozieSharelibCLI.java:131)\n at org.apache.oozie.tools.OozieSharelibCLI.main(OozieSharelibCLI.java:57)\n--------------------------------------</pre><p> I don't konw why, Can somebody help me please. </p><p>Thank you very much!</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-08-25 05:52:38.0","id":53277,"title":"Unable to start SinkRunner","body":"<p>2016-08-24 20:12:27,541 (lifecycleSupervisor-1-3) [<em><strong>ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7c564448 counterGroup:{ name:null counters:{} } } - Exception follows.</strong></em>\njava.lang.NoSuchMethodError: org.elasticsearch.common.transport.InetSocketTransportAddress.&lt;init&gt;(Ljava/lang/String;I)V\n   at org.apache.flume.sink.elasticsearch.client.ElasticSearchTransportClient.configureHostnames(ElasticSearchTransportClient.java:143)\n   at org.apache.flume.sink.elasticsearch.client.ElasticSearchTransportClient.&lt;init&gt;(ElasticSearchTransportClient.java:77)\n   at org.apache.flume.sink.elasticsearch.client.ElasticSearchClientFactory.getClient(ElasticSearchClientFactory.java:48)\n   at org.apache.flume.sink.elasticsearch.ElasticSearchSink.start(ElasticSearchSink.java:357)\n   at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)\n   at org.apache.flume.SinkRunner.start(SinkRunner.java:79)\n   at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)\n   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n   at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)</p>","tags":["Flume"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-26 10:45:32.0","id":53502,"title":"Hive Insert sql throws InvalidInputException,Hive Insert sql gives org.apache.hadoop.mapred.InvalidInputException","body":"<p>Hi All,</p><p>I am using Ambari 2.2.2 and HDP 2.4</p><p>I have enabled ACID property if HIve using Ambari.</p><p>I have created a database table using the hive command prompt here is the sql</p><p><em>CREATE TABLE variables_new ( id INT, a_cost INT , b_cost INT ) CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES(\"transactional\"=\"true\"); </em></p><p>I am trying to connect to hive from my php source code. I have used the library from GIT. Till now I am able to connect to hive, list the databases, list tables and run select * from commands. Now I have to insert, update and delete from my php source code.</p><p>While running Insert command I am getting following error</p><p>SQL: <em>INSERT INTO TABLE DB_NAME.variables_new VALUES (2, 10, 20)</em></p><p>Error: </p><p><em>Vertex failed, vertexName=Map 1, vertexId=vertex_1471845991138_0085_1_00, diagnostics=\n Vertex vertex_1471845991138_0085_1_00\n Map 1\nkilled/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: values__tmp__table__1 initializer failed, vertex=vertex_1471845991138_0085_1_00\n Map 1\n<strong>org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://y:8020/tmp/hive/hive/9a4f4ac7-be23-4df2-b524-d0a48ec9c076/_tmp_space.db/Values__Tmp__Table__1</strong>\nat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\nat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\nat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\nat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:307)\nat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:409)\nat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:273)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:266)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:266)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)</em></p><p><em>Vertex killed, vertexName=Reducer 2, vertexId=vertex_1471845991138_0085_1_01, diagnostics=\n Vertex received Kill in INITED state., Vertex vertex_1471845991138_0085_1_01\n Reducer 2\nkilled/failed due to:OTHER_VERTEX_FAILURE</em></p><p><em>DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1</em></p><p>Please advice, where I am making a mistake.</p><p>Thanks in advance,</p><p>Paresh Kendre (kendreparesh@gmail.com)</p><p>,\n</p><p>Hi All,</p><p>I have a database table, created with following command in hive</p><p><em>CREATE TABLE variables_new ( id INT, a_cost INT , b_cost INT ) CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES(\"transactional\"=\"true\"); </em></p><p>I am using Ambari 2.2.2 and HDP 2.4 and I have enabled the ACID property of Hive.</p><p>I am trying to connect and do some insert operations using PHP client. I have used php thrift library for this. I am able to list all databases, tables and able to run select commands. Next step is to inert data into table using php client.</p><p>While writing INSERT command ( <em>INSERT INTO TABLE att.variables_new2 VALUES (2, 10, 20) </em>)</p><p> I am getting following error</p><p><em>Vertex failed, </em></p><p><em>vertexName=Map 1, vertexId=vertex_1471845991138_0085_1_00, diagnostics=\n Vertex vertex_1471845991138_0085_1_00\n Map 1\nkilled/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: values__tmp__table__1 initializer failed, vertex=vertex_1471845991138_0085_1_00\n Map 1\no<strong>rg.apache.hadoop.mapred.InvalidInputException: Input path does not exist:</strong> hdfs://y:8020/tmp/hive/hive/9a4f4ac7-be23-4df2-b524-d0a48ec9c076/_tmp_space.db/Values__Tmp__Table__1\nat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\nat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\nat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\nat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:307)\nat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:409)\nat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:273)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:266)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:266)\nat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)</em></p><p><em>Vertex killed, vertexName=Reducer 2, vertexId=vertex_1471845991138_0085_1_01, diagnostics=\n Vertex received Kill in INITED state., Vertex vertex_1471845991138_0085_1_01\n Reducer 2\nkilled/failed due to:OTHER_VERTEX_FAILURE</em></p><p><em>DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1</em></p><p>Please help.</p><p>-Paresh</p>","tags":["hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-08-26 19:03:52.0","id":53599,"title":"hadoop no route  error","body":"<p>I am keep getting these errors , and most of the time the application succeeds still.  what is this error and how I can resolve this ? I have ssh setup between all hosts </p><pre>16/08/26 14:59:54 INFO mapreduce.Job: Job job_1472231356029_0008 failed with state FAILED due to: Application application_1472231356029_0008 failed 2 times due to Error launching appattempt_1472231356029_0008_000002. Got exception: java.net.NoRouteToHostException: No Route to Host from  hadoop2.tolls.dot.state.fl.us/10.100.44.16 to hadoop3.tolls.dot.state.fl.us:45454 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n</pre>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-26 14:51:57.0","id":53525,"title":"HDP 2.5 TP Sandbox Virtual Box image not working","body":"<p>I tried downloading the 2.5 sandbox here: </p><p><a target=\"_blank\" href=\"http://hortonworks.com/downloads/#tech-preview\">http://hortonworks.com/downloads/#tech-preview</a></p><p><a target=\"_blank\" href=\"http://hortonassets.s3.amazonaws.com/2.5/Hortonworks%20Sandbox%20with%20HDP%202.5%20Technical%20Preview.ova\">2.5 Sandbox Download</a></p><p>I keep getting an error and it fails to import. </p><p><img src=\"/storage/attachments/6993-screen-shot-2016-08-26-at-95221-am.png\"></p><p>Any idea where I can get a working version? I am using Google Chrome if that makes a difference. </p><p>Thanks, </p>","tags":["hdp-2.5.0","tech-preview"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-01 08:39:15.0","id":54414,"title":"major compaction hbase","body":"<p>I have a use case where i will bulk load a million of rows into a hbase table. After that i will be running map reduce or analytical queries on the data loaded. IS it a good practice or worth doing a major compaction after the data upload?. will it yield any performance benefits. As the default major compaction will happen only once in 7 days. Any thought s on this would be appreciated.</p>","tags":["Hbase","compaction"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-05 01:50:41.0","id":54858,"title":"How long/many of the flowfiles will be maintained in Data Provenance of nifi?","body":"<p>How long/many of the flowfiles will be maintained in Data Provenance of nifi?</p><p>(It seems not permanent)</p>","tags":["nifi-templates"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-06 07:35:18.0","id":54993,"title":"'Remote instance of NiFi is not configured to allow RAW Socket site-to-site communications'","body":"<p>In cluster mode of NiFi 1.0.0. I try to distribute the flowfiles from primary node to cluster nodes by using Remote Process Group.</p><p>The property 'Transport Protocol'  of Remote Process Group is 'RAW'. I get the error message:</p><p>'Remote instance of NiFi is not configured to allow RAW Socket site-to-site communications'</p><p>How to resolve it? </p><p>Thanks for your help.</p><p>David</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-06 10:22:44.0","id":55038,"title":"Not able to connect to jmx metric running on HDP sandbox using jconsole remotely","body":"<p>I added following properties to hbase-site.xml </p><p>    &lt;property&gt;</p><p style=\"margin-left: 20px;\"> &lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt;</p><p style=\"margin-left: 20px;\"> &lt;value&gt;org.apache.hadoop.hbase.JMXListener&lt;/value&gt;</p><p style=\"margin-left: 20px;\">&lt;/property&gt;</p><p>    &lt;property&gt;</p><p>     &lt;name&gt;regionserver.rmi.registry.port&lt;/name&gt; </p><p>     &lt;value&gt;61130&lt;/value&gt; </p><p>  &lt;/property&gt; </p><p>  &lt;property&gt;</p><p>   &lt;name&gt;regionserver.rmi.connector.port&lt;/name&gt;</p><p>    &lt;value&gt;61140&lt;/value&gt; </p><p>  &lt;/property&gt;</p><p>Added following config to hbase-env</p><p>  export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\" </p><p>export HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS $HBASE_JMX_BASE\"</p><p>export HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE\"</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-07 01:24:40.0","id":55223,"title":"Nifi 1.0.0 failed to create socket to coordinator due to ProtocolException","body":"<p>Encountering the following error when attempting to create a NiFi unsecured cluster.  </p><p>2016-09-06 21:17:48,002 INFO [Leader Election Notification Thread-1] o.apache.nifi.controller.FlowController This node elected Active Cluster Coordinator\n2016-09-06 21:17:48,479 INFO [main] o.a.n.c.c.n.LeaderElectionNodeProtocolSender Determined that Cluster Coordinator is located at server2.hdp  :9999; will use this address for sending heartbeat messages\n2016-09-06 21:17:48,481 WARN [main] o.a.nifi.controller.StandardFlowService Failed to connect to cluster due to: org.apache.nifi.cluster.protocol.ProtocolException: Failed to create socket to server2.hdp  :9999 due to: java.net.UnknownHostException: server2.hdp</p><p>Confirmed the following:</p><p>- On Server2 there is a java process listening to port 9999.</p><p>- Conducting a telnet server2.hdp 9999 from the other two nodes in the  shows that the connection should be possible</p><p>Some configuration background:</p><p>- Attempting to create a 3 node unsecured NiFi cluster using NiFi-1.0.0 </p><p>-     clusterProtocolPort: 9999 # nifi.cluster.node.protocol.port </p><p>- RemoteInputPort: 9998 # nifi.remote.input.socket.port </p><p>- httpWebPort: 9090</p><p>- Embedded zookeeper with 3 nodes all on port 2181</p><p>Any idea what could cause such a message?</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-12 18:29:12.0","id":56084,"title":"what does this mean","body":"<p>I see all the left hand side as green but on right hand side still it shows some red alerts ? </p><p>does it mean the application is up or down ?</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-20 16:20:32.0","id":57521,"title":"YARN NODEMANAGER  BIND TO INTERNAL IP","body":"<p>I am facing security harden request due to nodemanager expose the internal IP when I run below command</p><p>printf \"GET / HTTP/1.0\\n\\nHost:152.220.128.107:8042\\n\\nUser-Agent:Feedburner\\n\\n\" | nc 152.220.128.107 8042</p><p>and result is </p><p>Connection to 152.220.128.107 8042 port [tcp/fs-agent] succeeded! </p><p>HTTP/1.1 302 Found </p><p>Cache-Control: no-cache </p><p>Expires: Tue, 20 Sep 2016 16:04:02 GMT </p><p>Date: Tue, 20 Sep 2016 16:04:02 GMT</p><p>\nPragma: no-cache\nExpires: Tue, 20 Sep 2016 16:04:02 GMT </p><p>Date: Tue, 20 Sep 2016 16:04:02 GMT </p><p>Pragma: no-cache\nContent-Type: text/plain; charset=UTF-8 </p><p>Location: http://10.50.12.217:8042/node </p><p>Content-Length: 0\nServer: Jetty(6.1.26.hwx)</p><p>from above result it shows 10.50.12.217 which is the host private IP. meanwhile, I am using AWS EC2 which the server interface bind only for internal IP. </p><p>So my questions is . Is there any way I can get this internal gone? or make it as server FQDN.</p><p>I've checked some documents which said I have to set </p><p>yarn.nodemanager.hostname as FQDN?</p><p>Thanks in advance.</p>","tags":["YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-21 22:02:04.0","id":57790,"title":"how to change hive external table location.","body":"<p>I want to change my external table hdfs location to new path location which is Amazon S3 in my case.</p><p>I tried following query.</p><pre>ALTER TABLE table_name set location 's3n://bucket/path/to/data'</pre><p>But some how it is still pointing to old hdfs external path.</p><p>Is there any query I need to use in order to update hive metastore  with new external data path location. </p><p>Any kind of help would be greatly appreciated . </p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-23 10:19:56.0","id":58050,"title":"How to install hdp without yum ?","body":"<p>Is there any way by which i can install hdp without yum ?</p><p>any help will be apprecialbe.</p><p>@Neeraj Sabharwal</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-23 15:02:31.0","id":58101,"title":"What is the appropriate GC for Kafka?","body":"<p>G1 vs CMS?</p><p>Article from @<a href=\"https://community.hortonworks.com/users/420/vjain.html\">Vedant Jain</a> seems to indicate G1 in Java/JVM section:</p><p><a href=\"https://community.hortonworks.com/articles/49789/kafka-best-practices.html\">https://community.hortonworks.com/articles/49789/kafka-best-practices.html</a> (Java/JVM tuning section)</p><p>LinkedIn Engineering concludes that CMS: <a href=\"https://engineering.linkedin.com/garbage-collection/garbage-collection-optimization-high-throughput-and-low-latency-java-applications\">https://engineering.linkedin.com/garbage-collection/garbage-collection-optimization-high-throughput-and-low-latency-java-applications</a></p><p>Opinions and reasons? What is your field experience?</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-23 20:20:46.0","id":58162,"title":"Enabling oozie web console and I didn't find the property to enable oozie web console in oozie configuration. Is there any manual steps to enable this","body":"","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-09-25 00:21:27.0","id":58247,"title":"HDP 2.5 Sandbox(VM) commands/scripts are not found","body":"<p></p><p></p><p></p><p>I am running HDP 2.5 Sandbox in VM. I am able to lunch HDP 2.5 welcome page and Ambari. However when I ran commands like sandbox-version and ambari-admin-password-reset in VM Terminal, I get message like command not found. Did I miss any step in the tutorial ? These commands were running fine in HDP 2.4.</p><p><img src=\"/storage/attachments/7951-cuserskhycdesktopmy-docs1officemydevmybd-hadoophor.jpg\"></p>","tags":["hdp-2.5.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-24 12:20:28.0","id":58218,"title":"Can I type in Solution in my Machine and Later Save to Client Machine during Exam ?","body":"<p>During Exam If the lag is same as in practice exam or even worst as There will be added camera streaming. Can I save my solutions in local machine. as Editing is also a big pain when it lags. Keyboard goes non responsive !</p><p>Can one write solution in local and later push to client machine ??</p>","tags":["examlocal"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-08-14 21:00:51.0","id":51423,"title":"Ambari rest API to get host of particular components.","body":"<p>Here i just want to get my resource manager hostip and its port.</p><p>example : -:   http://IP:8088, http://IP:50070, http://IP:19888.  </p>","tags":["ambari-api","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-08-15 15:24:56.0","id":51530,"title":"Data Reporting with Excel","body":"<p>Hello, </p><p>I was doing the following tutorial</p><p><a href=\"https://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#step5.b.1\"> https://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#step5.b.1</a>\n<strong></strong></p><p>I configured the ODBC for windows 7 but i am not sure which username to use while configuring it.</p><p>After this when i click i am here:</p><p><strong>Open a new blank workbook. Select Data tab at the top then select “Get External Data” and then select “From Other Data Sources”. Then at the bottom select “From Microsoft Query”. Choose your data source and ours is called Hadoop and you will then see the Query Wizard. We will import the avg_mileage table.\n</strong></p><p>I do not see the database for hadoop. This is the step i am stuck. I see following in my excel.</p><p><img src=\"/storage/attachments/6647-excel.png\"></p><p><strong>Can anyone help me from this point.\n</strong></p><p><strong>TIA</strong></p>","tags":["data-processing"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-08-29 04:33:11.0","id":53715,"title":"How to start Zeppelin By IDE (Eclipse, IDEA)","body":"<p>How to run Zeppelin server  in development mode by IDE (Eclipse or IDEA.)</p>","tags":["zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-08-29 12:11:46.0","id":53776,"title":"WARN security.UserGroupInformation: Exception encountered while running the renewal command. Aborting renew thread  ExitCodeException exitCode=1: kinit: Ticket expired while renewing credentials","body":"<p>Hi Folks ,</p><p>Appreciate your help  here!!</p><p>I have a secured cluster with Kerberos where I am using root user and performing  TGT with hive keytab and hive principal .</p><p>even I am having the proper TGT session when I am doing hadoop fs -ls / , I am getting the WARN</p><p>WARN security.UserGroupInformation: Exception encountered while running the renewal command. Aborting renew thread. ExitCodeException exitCode=1: kinit: Ticket expired while renewing credentials</p><p>and resulting hadoop files .</p><p>I want to know why the warning is coming even TGT session exist.</p><p>Thanks,</p><p>JAvvaji</p>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-08-30 18:39:56.0","id":54080,"title":"NIFI for HDP","body":"<p>can someone give me install instructions for NIFI on HDP and not SANDBOX ? I see this link but Its confusing as it talks about SANDBOX and also HDP . </p><p><a href=\"https://community.hortonworks.com/questions/44042/adding-nifi-server-to-hdp-25-sandbox.html\">https://community.hortonworks.com/questions/44042/adding-nifi-server-to-hdp-25-sandbox.html</a></p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-08-31 19:26:16.0","id":54292,"title":"Can a custom controller service be referenced from more than one nifi processor","body":"<p>Greetings,</p><p>I have a custom controller service implemented that reads properties from different properties files for subsequent lookup from another custom nifi processor that I implemented. Till then everything worked fine and I was able to lookup properties. But then, I had to implement another custom nifi processor which needed the same lookup from the controller service. That's when my second processor started showing me errors something like \"&lt;UUID&gt; is not a valid Controller Service Identifier or does not reference the correct type of Controller Service\". Both the properties have the custom controller service added as the descriptor. I'm not sure if I'm not setting up my maven build configuration correctly. </p><p>I have the Controller Service Interface as a nar. Then I have it's implementation as a separate project. Then I have two processor nars referencing the controller service implementation projects.</p><p>I was wondering if I'm doing anything wrong or how to fix this problem.</p><p>Any help is appreciated</p><p>Thanks.</p>","tags":["nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-01 14:41:50.0","id":54487,"title":"NiFi 1.0.0 Can't Start","body":"<p>aused by: java.lang.IllegalArgumentException: No enum constant org.wali.UpdateType.\n       at java.lang.Enum.valueOf(Enum.java:238) ~[na:1.8.0_91]\n       at org.wali.UpdateType.valueOf(UpdateType.java:24) ~[nifi-write-ahead-log-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.StateMapSerDe.deserializeRecord(StateMapSerDe.java:76) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.StateMapSerDe.deserializeEdit(StateMapSerDe.java:69) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.StateMapSerDe.deserializeEdit(StateMapSerDe.java:30) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.wali.MinimalLockingWriteAheadLog$Partition.recoverNextTransaction(MinimalLockingWriteAheadLog.java:1028) ~[nifi-write-ahead-log-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.wali.MinimalLockingWriteAheadLog.recoverFromEdits(MinimalLockingWriteAheadLog.java:448) ~[nifi-write-ahead-log-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.wali.MinimalLockingWriteAheadLog.recoverRecords(MinimalLockingWriteAheadLog.java:293) ~[nifi-write-ahead-log-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.providers.local.WriteAheadLocalStateProvider.init(WriteAheadLocalStateProvider.java:99) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.providers.AbstractStateProvider.initialize(AbstractStateProvider.java:34) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.manager.StandardStateManagerProvider.createStateProvider(StandardStateManagerProvider.java:189) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.manager.StandardStateManagerProvider.createLocalStateProvider(StandardStateManagerProvider.java:81) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.state.manager.StandardStateManagerProvider.create(StandardStateManagerProvider.java:67) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.FlowController.&lt;init&gt;(FlowController.java:470) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.controller.FlowController.createStandaloneInstance(FlowController.java:381) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.apache.nifi.spring.FlowControllerFactoryBean.getObject(FlowControllerFactoryBean.java:74) ~[nifi-framework-core-1.0.0-BETA.jar:1.0.0-BETA]\n       at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:168) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]\n       ... 36 common frames omitted\n2016-09-01 14:46:23,006 INFO [main] /nifi-content-viewer No Spring WebApplicationInitializer types detected on classpath\n2016-09-01 14:46:23,024 INFO [main] o.e.jetty.server.handler.ContextHandler Started o.e.j.w.WebAppContext@3f465ae{/nifi-content-viewer,file:///opt/nifi-1.0.0-BETA/work/jetty/nifi-web-content-viewer-1.0.0-BETA.war/webapp/,AVAILABLE}{./work/nar/framework/nifi-framework-nar-1.0.0-BETA.nar-unpacked/META-INF/bundled-dependencies/nifi-web-content-viewer-1.0.0-BETA.war}\n2016-09-01 14:46:23,025 INFO [main] o.e.jetty.server.handler.ContextHandler Started o.e.j.s.h.ContextHandler@55731ccd{/nifi-docs,null,AVAILABLE}\n2016-09-01 14:46:23,070 INFO [main] /nifi-docs No Spring WebApplicationInitializer types detected on classpath\n2016-09-01 14:46:23,089 INFO [main] o.e.jetty.server.handler.ContextHandler Started o.e.j.w.WebAppContext@750adad8{/nifi-docs,file:///opt/nifi-1.0.0-BETA/work/jetty/nifi-web-docs-1.0.0-BETA.war/webapp/,AVAILABLE}{./work/nar/framework/nifi-framework-nar-1.0.0-BETA.nar-unpacked/META-INF/bundled-dependencies/nifi-web-docs-1.0.0-BETA.war}\n2016-09-01 14:46:23,146 INFO [main] / No Spring WebApplicationInitializer types detected on classpath\n2016-09-01 14:46:23,148 INFO [main] o.e.jetty.server.handler.ContextHandler Started o.e.j.w.WebAppContext@3fed0c04{/,file:///opt/nifi-1.0.0-BETA/work/jetty/nifi-web-error-1.0.0-BETA.war/webapp/,AVAILABLE}{./work/nar/framework/nifi-framework-nar-1.0.0-BETA.nar-unpacked/META-INF/bundled-dependencies/nifi-web-error-1.0.0-BETA.war}\n2016-09-01 14:46:23,186 INFO [main] o.eclipse.jetty.server.AbstractConnector Started ServerConnector@48224381{HTTP/1.1,[http/1.1]}{0.0.0.0:8090}\n2016-09-01 14:46:23,187 INFO [main] org.eclipse.jetty.server.Server Started @56596ms\n2016-09-01 14:46:23,190 WARN [main] org.apache.nifi.web.server.JettyServer Failed to start web server... shutting down.\norg.apache.nifi.web.NiFiCoreException: Unable to start Flow Controller.\n       at org.apache.nifi.web.contextlistener.ApplicationStartupContextListener.contextInitialized(ApplicationStartupContextListener.java:93) ~[na:na]\n       at org.eclipse.jetty.server.handler.ContextHandler.callContextInitialized(ContextHandler.java:837) ~[jetty-server-9.3.9.v20160517.jar:9.3.9.v20160517]\n       at org.eclipse.jetty.servlet.ServletContextHandler.callContextInitialized(ServletContextHandler.java:533) ~[jetty-servlet-9.3.9.v20160517.jar:9.3.9.v20160517]\n       at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:810) ~[jetty-server-9.3.9.v20160517.jar:9.3.9.v20160517]\n       at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:345) ~[jetty-servlet-9.3.9.v20160517.jar:9.3.9.v2016051</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-06 12:54:18.0","id":55073,"title":"Application initialization failed (exitCode=255) with  output: main : command provided 0,Application  initialization failed (exitCode=255) with  output: main : command provided 0 main : run as user is gobblin","body":"<p>\n\tHi, I'm having my gobblin run failed for some reason. After I've checked yarn application logs I've found the following lines.</p><p>\n\tWould you mind to advise what could be checked here please or possible error. Thanks</p><pre>2016-09-06 11:52:46,078 DEBUG [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned based on * match\n2016-09-06 11:52:46,078 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:8 ScheduledReds:0 AssignedMaps:7 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:8 ContRel:0 HostLoca\nl:0 RackLocal:0\n2016-09-06 11:52:46,080 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1472225108935_4765_m_000000_1000 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP\n2016-09-06 11:52:46,080 DEBUG [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Dispatching the event org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptDiagnosticsUpdateEvent.EventType: TA_DIAGNOSTICS_UPDATE\n2016-09-06 11:52:46,080 DEBUG [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Processing attempt_1472225108935_4765_m_000000_1000 of type TA_DIAGNOSTICS_UPDATE\n2016-09-06 11:52:46,080 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1472225108935_4765_m_000000_1000: Application application_1472225108935_4765 initialization failed (exitCode=255) with\n output: main : command provided 0\nmain : run as user is gobblin\nmain : requested yarn user is gobblin</pre>","tags":["MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-06 11:51:35.0","id":55051,"title":"No Hive 2.1 in HDP 2.5","body":"<p>Hi guys,</p><p>I just installed HDP 2.5 on my physical cluster (not Sandbox) and found that there is no Hive 2.1 as promised. I see both Spark 1.6.2 and Spark 2.0.0, but only one Hive.</p><p>What to do? I really want Hive 2.1...</p><p><img src=\"/storage/attachments/7358-nohive.png\"></p><p><img src=\"https://community.hortonworks.com/storage/attachments/7359-nohive2.png\"></p>","tags":["hdp-2.5.0","Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-06 10:12:24.0","id":55027,"title":"How can I optimse his query","body":"<p>Hi I have some code that runs in HIVE SQL and takes 4 hours. We only have the option of map reduce, any ideas on how to optimise would be very welcome?</p><p>Here is the code...</p><p></p><p>\n</p><p>select \t\t</p><p>\t\t\tsr.tech_end_date</p><p>\t\t\t,sr.tech_start_date</p><p>\t\t\t,sr.ca_ref\t</p><p>\t\t\t,sr.contract_account_document_id</p><p>\t\t\t,sr.document_date</p><p>\t\t\t,sr.due_date</p><p>\t\t\t,sr.document_type</p><p>\t\t\t,sr.docsval\t</p><p>\t\t\t,sr.clearing_document</p><p>\t\t\t,sr.clearing_date</p><p>\t\t\t,sr.Clearing_PST_date</p><p>\t\t\t,sr.clearing_reason_cd</p><p>\t\t\t,sr.clearing_status_cd</p><p>\t\t\t,sr.repetition_item_num</p><p>\t\t\t,sr.contract_account_sub_item_num</p><p>\t\t\t,sr.contract_account_item_num</p><p>\t\t\t,sr.line_item_type_cd</p><p>\t\t\t,sr.net_payment_due_dt</p><p>\t\t\t,sr.bldat</p><p>\t\t\t,sr.BUDAT</p><p>\t\t\t,sr.Act_clearing_date</p><p>\t\t\t,sr.herkf</p><p>\t\t\t,sr.blart</p><p>\t\t\t,sr.hvorg</p><p>\t\t\t,sr.tvorg</p><p>FROM\t\t(SELECT\t\tca_ref</p><p>\t\t\t           \t,Contract_Account_Document_Id</p><p>\t\t\t           \t,Repetition_Item_Num</p><p>\t\t\t           \t,Contract_Account_Sub_Item_Num</p><p>\t\t\t           \t,Contract_Account_Item_Num</p><p>\t\t\t\t\t\t,MAX(tech_start_date) AS tech_start_date</p><p>\t\t\t\t\t\t,MAX(tech_end_date) AS tech_end_date</p><p>\t\t\tFROM\t\tanalytics_pqacs1.DOCUMENT_DET_f3</p><p>\n</p><p>\t\t\twhere \t\tline_item_type_cd = ' '</p><p>\n</p><p>\t\t\tGROUP BY\tca_ref</p><p></p><p>\t\t\t           \t,Contract_Account_Document_Id</p><p>\t\t\t           \t,Repetition_Item_Num</p><p>\t\t\t           \t,Contract_Account_Sub_Item_Num</p><p>\t\t\t           \t,Contract_Account_Item_Num</p><p>\t\t\t           \t)  V</p><p>JOIN\t\tanalytics_pqacs1.DOCUMENT_DET_f3 SR ON SR.ca_ref = V.ca_ref</p><p>\t\t\t           \tAND SR.Contract_Account_Document_Id = V.Contract_Account_Document_Id</p><p>\t\t\t           \tAND SR.repetition_Item_Num = V.repetition_Item_Num</p><p>\t\t\t           \tAND SR.Contract_Account_Sub_Item_Num = V.Contract_Account_Sub_Item_Num</p><p>\t\t\t           \tAND SR.Contract_Account_Item_Num = V.Contract_Account_Item_Num</p><p>\t\t\t\t\t\tAND\tSR.tech_start_date = V.tech_start_date</p><p>\t\t\t\t\t\tAND SR.tech_end_date = V.tech_end_date\t</p>","tags":["optimization"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-06 14:10:05.0","id":55083,"title":"saveAsTextFile creates deflate file on the cluster, I would like to create pure text file ,how would I do that?","body":"<p>when I do the saveAsTextFile on the local machine , text files are created but when I do the same on cluster it creates .deflate files, how can I have text files on the cluster , please help.</p><p>sqlContext.sql(sourceQuery).map { row =&gt; FixedLengthParser.parseRecord(row) }.repartition(1).saveAsTextFile(s\"$outputLocalPath/$sourceId\")</p>","tags":["spark-sql"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-09-07 02:35:59.0","id":55226,"title":"Metron \"metron_elasticsearch_templates : Wait for Index to Become Available\" failure on physical deployment","body":"<p>I'm running Metron on an 8 node physical cluster (CentOS 6), with a virtual installer node (CentOS 7).  </p><p>I'm running into the following error after running `ansible-playbook -i /root/incubator-metron/metron-deployment/inventory/$INSTALL_CONF_DIRECTORY metron_install.yml --skip-tags=\"solr,sensors\"`:</p><blockquote><p>TASK [metron_elasticsearch_templates : Wait for Index to Become Available] *****</p><p>fatal: [server2]: FAILED! =&gt; {\"failed\": true, \"msg\": \"ERROR! The conditional check 'result.content.find(\\\"green\\\") != -1 or result.content.find(\\\"yellow\\\") != -1' failed. The error was: ERROR! error while evaluating conditional (result.content.find(\\\"green\\\") != -1 or result.content.find(\\\"yellow\\\") != -1): ERROR! 'dict object' has no attribute 'content'\"}</p><p>fatal: [server2]: FAILED! =&gt; {\"failed\": true, \"msg\": \"ERROR! The conditional check 'result.content.find(\\\"green\\\") != -1 or result.content.find(\\\"yellow\\\") != -1' failed. The error was: ERROR! error while evaluating conditional (result.content.find(\\\"green\\\") != -1 or result.content.find(\\\"yellow\\\") != -1): ERROR! 'dict object' has no attribute 'content'\"}</p></blockquote><p>However when I check the servers manually I see:</p><blockquote><p>[root@e104d0xxxxxx scripts]# curl server1:9200/_cluster/health</p><p>{\"cluster_name\":\"metron\",\"status\":\"green\",\"timed_out\":false,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":0,\"active_shards\":0,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}[root@e104d0553817 scripts]# </p><p>[root@e104d0xxxxxx scripts]# curl server2:9200/_cluster/health</p><p>{\"cluster_name\":\"metron\",\"status\":\"green\",\"timed_out\":false,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":0,\"active_shards\":0,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}[root@e104d0553817 scripts]# </p></blockquote><p>And when I check in my web browser, I get:</p><blockquote><p>1473215295 22:28:15 metron green 2 2 0 0 0 0 0 0 - 100.0%</p><p>1473215299 22:28:19 metron green 2 2 0 0 0 0 0 0 - 100.0%</p></blockquote><p>I've tried editing incubator-metron/metron-deployment/roles/metron_elasticsearch_templates/tasks/load_templates.yml and increase the retries (up to 600) and delays (up to 1000) with no success.  </p><p>Some details:</p><blockquote><p>Metron 0.2.0BETA</p><p>--</p><p>* (no branch)</p><p>--</p><p>commit 75642001803396e8884385b0fc297a2312ead3eb</p><p>Author: cstella &lt;cestella@gmail.com&gt;</p><p>Date:   Wed Jul 13 11:15:20 2016 -0400</p><p>    METRON-298 Remove the effective_tld_names.dat files. closes apache/incubator-metron#186</p><p>--</p><p> metron-deployment/playbooks/metron_install.yml     |   45 ++++++++++++++++++++</p><p> .../roles/ambari_common/tasks/main.yml             |    5 ++-</p><p> .../roles/elasticsearch/defaults/main.yml          |    2 +-</p><p> .../roles/elasticsearch/tasks/elasticsearch.yml    |    2 +-</p><p> metron-deployment/roles/httplib2/tasks/main.yml    |    3 +-</p><p> .../roles/kibana/tasks/elasticdump.yml             |    4 +-</p><p> .../roles/opentaxii/tasks/opentaxii.yml            |    2 +-</p><p> 7 files changed, 57 insertions(+), 6 deletions(-)</p><p>--</p><p>ansible 2.0.0.2</p><p>  config file = /root/ansible.cfg</p><p>  configured module search path = ../extra_modules</p><p>--</p><p>./platform-info.sh: line 52: vagrant: command not found</p><p>--</p><p>Python 2.6.6</p><p>--</p><p>Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)</p><p>Maven home: /opt/maven</p><p>Java version: 1.8.0_91, vendor: Oracle Corporation</p><p>Java home: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.91-1.b14.el6.x86_64/jre</p><p>Default locale: en_US, platform encoding: UTF-8</p><p>OS name: \"linux\", version: \"3.10.0-327.el7.x86_64\", arch: \"amd64\", family: \"unix\"</p><p>--</p><p>Linux e104d0553817 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux</p></blockquote>","tags":["Metron"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-09-07 08:57:00.0","id":55257,"title":"ambari error","body":"<p>when i installing the HDP, I have issuse,some body can help me?</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py\", line 133, in &lt;module&gt;\n    AmsCollector().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py\", line 34, in install\n    self.install_packages(env)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 404, in install_packages\n    Package(name)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py\", line 49, in action_install\n    self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py\", line 49, in install_package\n    shell.checked_call(cmd, sudo=True, logoutput=self.get_logoutput())\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of '/usr/bin/yum -d 0 -e 0 -y install ambari-metrics-collector' returned 1. Error: Nothing to do</pre>","tags":["Ambari"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-08 17:50:16.0","id":55547,"title":"How to retrieve entire row including all columns in HBase when queried by time range in the scan?","body":"<p>Currently, scan only returns the columns updated within the time range.  But I need the entire row with other columns as well.  How do I do that?  Here is the snippet of my code.  Please help!</p><pre>Scan scan = new Scan();\nscan.setTimeRange(1471710010773L, System.currentTimeMillis());</pre>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-14 10:43:40.0","id":56434,"title":"What happens to logs if there is no space on partition containing  /var/log/hadoop-yarn/container","body":"<p>What we get are alerts, but is it possible to control logs deletion when disk is full?</p><p>,\n</p><p>What happens to logs if there is no space on partition containing  /var/log/hadoop-yarn/container</p><p>what we get are alerts, but is it possible to control logs deletion when disk is full?</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-14 15:52:36.0","id":56542,"title":"REST API for starting Spark jobs?","body":"<p>Does Hortonworks provide\na REST API for starting Spark jobs?  Microsoft and Cloudera are both\nlooking at Livy.  Wondering if Hortonworks has similar plans to provide a tool to make it easy to start Spark jobs via REST.</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-09-16 16:10:37.0","id":56952,"title":"Can we install or integrate Application servers like tomcat, websphere with hadoop cluster? Is there is any possibility?","body":"<p>Hi,</p><p>I am just wondering if we can install/integrate the web application servers on hadoop cluster? </p><p>Thanks,</p><p>Rahul</p>","tags":["integration","installation","HDFS"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-09-16 20:23:54.0","id":57021,"title":"How to allocate components on cluster with Kafka and Yarn usage?","body":"<p>This use case has very heavy usage on Kafka (3/4 nodes are allocated for Kafka brokers), and some usage on hdfs, hive, and spark, 5 zookeeper nodes also enabled. All nodes are powerful, with 256G memory, and 32 cores enabled. We are separating the hdfs and Kafka nodes. Most advices point out that Kafka should be allocated on dedicated nodes. My question here is whether yarn nodes can be allocated with Kafka (Kafka may not be able to use all the memory and CPU)? </p>","tags":["YARN","Kafka"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-09-21 07:05:19.0","id":57621,"title":"Sqoop : Teradata to HDFS using PARQUET file format not working","body":"<p>I am trying to execute below command in sqoop</p><pre>sqoop import --connection-manager org.apache.sqoop.teradata.TeradataConnManager --connect jdbc:teradata://***.***.***.***/DATABASE=***** --username ***** --password **** --table mytable --target-dir /user/aps/test2 --as-parquetfile -m 1</pre><p>Output :</p><p>-rw-r--r--   3 ****** hdfs          0 2016-09-21 12:25 /user/aps/test2/_SUCCESS </p><p>-rw-r--r--   3 ****** hdfs         18 2016-09-21 12:25 /user/aps/test2/part-m-00000</p><p>Above output is not in parquet format. If I use com.teradata.jdbc.TeraDriver , it is working. But I have to use  org.apache.sqoop.teradata.TeradataConnManager  for connection. Please help.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-22 13:55:20.0","id":57869,"title":"Top-K Hadoop Streaming","body":"<p>I am attempting to find the top 3 words in a given document using Hadoop in order to gain more experience with Hadoop. My output gives me an unexpected output as opposed to simply being the top 3 words I am looking for. The primary document is in a .txt format.</p><pre>#!/usr/bin/env python\nimport sys, time\n\ndef parseRecords():\n    for line in sys.stdin:\n        line = line.strip('\\n')\n        yield line.split()\n\ndef mapper():\n    for words in parseRecords():\n        for w in words:\n            print '%s\\t%s' % (w,1)\n\nif __name__=='__main__':\n    mapper()\n\n#!/usr/bin/env python\nimport itertools, operator, sys\nfrom collections import Counter\n\ncnt = Counter()\n\ndef parsePairs():\n    for line in sys.stdin:\n        yield tuple(line.strip('\\n').split('\\t'))\n\ndef reducer():\n    for key, pairs in itertools.groupby(parsePairs(),\n                                    operator.itemgetter(0)):\n        count = sum(int(i[1]) for i in pairs)\n        cnt[key] += count\n        for x, y in cnt.most_common(3):\n            print '%s\\t%s' % (x, y)\n\nif __name__=='__main__':\n    reducer()\n\n\n</pre>","tags":["hadoop","python"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-23 14:27:14.0","id":58088,"title":"end users are trying to run Hive queries and they are are getting \"User not found\" in the yarn logs. These users are authenticating via the non-centrify AD groups listed in Cloudera Manager. They also don't have Linux user accounts on the cluster nodes.","body":"","tags":["permissions"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-30 08:57:31.0","id":42554,"title":"is there a way in hive to list all the table locations along with the partition location.","body":"","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-28 13:40:09.0","id":42103,"title":"lab 4.2 java.lang.NoClassDefFoundError: org/apache/tez/dag/api/SessionNotRunning","body":"<p>when i run \n</p><pre><code>valhiveContext=neworg.apache.spark.sql.hive.HiveContext(sc)\n</code></pre><p>i get this error </p><p>java.lang.NoClassDefFoundError: org/apache/tez/dag/api/SessionNotRunning</p>","tags":["Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-30 16:55:03.0","id":42700,"title":"Using KML Directly","body":"<p>Is there a geo library that can work with KML for Java and Hadoop and/or Spark?</p>","tags":["magellan","Spark"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-30 02:45:31.0","id":42507,"title":"Ambari API Node Additions - Missing Keytabs","body":"<p>Has anyone else had issues with Ambari consistently copying keytabs to new nodes?  We have seen issues intermittently when adding nodes via the API. Logs appear to be creating the principal so it seems it is just the copy failing.</p>","tags":["keytab","ambari-api"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-30 18:36:25.0","id":42720,"title":"Main class [org.apache.oozie.action.hadoop.HiveMain], exit code [12]","body":"<p> When I try to run a hive action I get error number E0729 Main class [org.apache.oozie.action.hadoop.HiveMain], exit code [12] with nothing more in log file. How fix this?</p><p>log:</p><pre>2016-06-30 18:33:37,930  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@:start:] Start action [0000011-160630143249353-oozie-oozi-W@:start:] with user-retry state : userRetryCount [0], userRetryMax [0], userRetryInterval [10]\n2016-06-30 18:33:37,934  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@:start:] [***0000011-160630143249353-oozie-oozi-W@:start:***]Action status=DONE\n2016-06-30 18:33:37,934  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@:start:] [***0000011-160630143249353-oozie-oozi-W@:start:***]Action updated in DB!\n2016-06-30 18:33:38,013  INFO WorkflowNotificationXCommand:520 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[] No Notification URL is defined. Therefore nothing to notify for job 0000011-160630143249353-oozie-oozi-W\n2016-06-30 18:33:38,014  INFO WorkflowNotificationXCommand:520 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@:start:] No Notification URL is defined. Therefore nothing to notify for job 0000011-160630143249353-oozie-oozi-W@:start:\n2016-06-30 18:33:38,042  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] Start action [0000011-160630143249353-oozie-oozi-W@hive-node] with user-retry state : userRetryCount [0], userRetryMax [0], userRetryInterval [10]\n2016-06-30 18:33:49,999  INFO HiveActionExecutor:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] checking action, hadoop job ID [job_1467297249897_0020] status [RUNNING]\n2016-06-30 18:33:50,002  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] [***0000011-160630143249353-oozie-oozi-W@hive-node***]Action status=RUNNING\n2016-06-30 18:33:50,002  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] [***0000011-160630143249353-oozie-oozi-W@hive-node***]Action updated in DB!\n2016-06-30 18:33:50,017  INFO WorkflowNotificationXCommand:520 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] No Notification URL is defined. Therefore nothing to notify for job 0000011-160630143249353-oozie-oozi-W@hive-node\n2016-06-30 18:38:09,496  INFO HiveActionExecutor:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] action completed, external ID [job_1467297249897_0020]\n2016-06-30 18:38:09,783  WARN HiveActionExecutor:523 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] Launcher ERROR, reason: Main class [org.apache.oozie.action.hadoop.HiveMain], exit code [12]\n2016-06-30 18:38:10,580  INFO ActionEndXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] ERROR is considered as FAILED for SLA\n2016-06-30 18:38:10,984  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@fail] Start action [0000011-160630143249353-oozie-oozi-W@fail] with user-retry state : userRetryCount [0], userRetryMax [0], userRetryInterval [10]\n2016-06-30 18:38:11,002  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@fail] [***0000011-160630143249353-oozie-oozi-W@fail***]Action status=DONE\n2016-06-30 18:38:11,003  INFO ActionStartXCommand:520 - SERVER[sandbox.hortonworks.com] USER[ambari-qa] GROUP[-] TOKEN[] APP[hive-wf] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@fail] [***0000011-160630143249353-oozie-oozi-W@fail***]Action updated in DB!\n2016-06-30 18:38:11,506  INFO WorkflowNotificationXCommand:520 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@fail] No Notification URL is defined. Therefore nothing to notify for job 0000011-160630143249353-oozie-oozi-W@fail\n2016-06-30 18:38:11,506  INFO WorkflowNotificationXCommand:520 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[] No Notification URL is defined. Therefore nothing to notify for job 0000011-160630143249353-oozie-oozi-W\n2016-06-30 18:38:11,506  INFO WorkflowNotificationXCommand:520 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000011-160630143249353-oozie-oozi-W] ACTION[0000011-160630143249353-oozie-oozi-W@hive-node] No Notification URL is defined. Therefore nothing to notify for job 0000011-160630143249353-oozie-oozi-W@hive-node</pre><p>job.properties</p><pre>nameNode=hdfs://sandbox.hortonworks.com:8020\njobTracker=sandbox.hortonworks.com:8050\nqueueName=default\nexamplesRoot=examples\noozie.use.system.libpath=true\noozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/hive</pre><p> my Workflow is </p><pre>&lt;workflow-app xmlns=\"uri:oozie:workflow:0.2\" name=\"hive-wf\"&gt;\n    &lt;start to=\"hive-node\"/&gt;\n\t &lt;action name=\"hive-node\"&gt;\n        &lt;hive xmlns=\"uri:oozie:hive-action:0.2\"&gt;   \n\t &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n           &lt;prepare&gt;\n                &lt;delete path=\"${nameNode}/user/${wf:user()}/${examplesRoot}/output-data/hive\"/&gt;\n                &lt;mkdir path=\"${nameNode}/user/${wf:user()}/${examplesRoot}/output-data\"/&gt;\n            &lt;/prepare&gt;\n \t    &lt;job-xml&gt;hive-site.xml&lt;/job-xml&gt;\n            &lt;configuration&gt;\n                &lt;property&gt;\n                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;\n                    &lt;value&gt;${queueName}&lt;/value&gt;\n                &lt;/property&gt;\n            &lt;/configuration&gt;\n            &lt;script&gt;script.q&lt;/script&gt;\n            &lt;param&gt;INPUT=/user/${wf:user()}/${examplesRoot}/input-data/table&lt;/param&gt;\n            &lt;param&gt;OUTPUT=/user/${wf:user()}/${examplesRoot}/output-data/hive&lt;/param&gt;\n        &lt;/hive&gt;\n        &lt;ok to=\"end\"/&gt;\n        &lt;error to=\"fail\"/&gt;\n    &lt;/action&gt;\n    &lt;kill name=\"fail\"&gt;\n        &lt;message&gt;Hive failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n    &lt;/kill&gt;\n    &lt;end name=\"end\"/&gt;\n&lt;/workflow-app&gt;\n</pre>","tags":["oozie-hive","Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-07-01 17:35:29.0","id":42935,"title":"how to add a journal node with ambari","body":"<p>there are 3 journal node   in our cluster,but some one crushed, how to add a new on other host?</p>","tags":["journalnode","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-01 19:29:39.0","id":42978,"title":"Sandbox Import into Virtualbox Failed (HDP 2.5 Tech Preview)","body":"<p>I downloaded an hour ago and tried to Add to the newest version of Virtual Box 5.0.24 on Mac El Capitain.</p><p><img src=\"/storage/attachments/5414-sandbox.png\"></p><p>An earlier version was working then wouldn't start all of the HDFS and YARN processes needed (Master Node, ...)</p><p>Do I need a different vbox?</p><p>I was running 2.4 with no issues.</p>","tags":["tech-preview","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-18 18:26:50.0","id":45719,"title":"Your query has the following error(s): Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask","body":"<p><strong>Your query has the following error(s):</strong>\nError while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</p> \n","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-19 08:47:02.0","id":45819,"title":"Vertical scaling of ambari cluster launched with cloudbreak","body":"<p>We have launched a cluster using cloudbreak on AWS, using one of the default templates that come with cloudbreak (minviable-aws)</p><p>My question is - After the cluster was launched, is there anyway to vertically scale the nodes running on the server?</p><p>Let's say the template is configured to run m3.large instances, and now I want to add m4.xlarge instances to the cluster (using cloudbreak), is that possible in any way?</p>","tags":["aws","Ambari","Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-19 17:39:48.0","id":45921,"title":"Places (besides user doc) that you frequently use or have bookmarked for Hive reference?","body":"<p>Trying to compile a list of frequently used links for Hive users (also can include listservs, etc). What are your top go-tos outside of Hortonworks when you have a question on Hive? </p><p>Here's a list of what we have so far: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_dataintegration/content/hive-doc.html</p><p>Thanks for any additions. </p>","tags":["documentation","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-19 15:52:03.0","id":45896,"title":"Problem executing Joins on Phoenix Query Server","body":"<p>I am trying to execute joins on Phoenix using query server with python library 'Phoenixdb'.</p><p>Below is the code:</p><pre>cursor.execute('select * from tbl1 inner join tbl2 on tbl1.id=tbl2.id where tbl1.id = ?',[1,])</pre><p>I'm getting error as <strong>'Parameter value unbound Parameter at index 1 is unbound'</strong></p><p>This is the stack trace:</p><pre>java.lang.RuntimeException: java.sql.SQLException: ERROR 2004 (INT05): Parameter value unbound Parameter at index 1 is unbound\n        at org.apache.calcite.avatica.jdbc.JdbcMeta.propagate(JdbcMeta.java:737)\n        at org.apache.calcite.avatica.jdbc.JdbcMeta.prepare(JdbcMeta.java:756)\n        at org.apache.calcite.avatica.remote.LocalService.apply(LocalService.java:127)\n        at org.apache.calcite.avatica.remote.Service$PrepareRequest.accept(Service.java:270)\n        at org.apache.calcite.avatica.remote.Service$PrepareRequest.accept(Service.java:254)\n        at org.apache.calcite.avatica.remote.JsonHandler.apply(JsonHandler.java:43)\n        at org.apache.calcite.avatica.server.AvaticaHandler.handle(AvaticaHandler.java:55)\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n        at org.eclipse.jetty.server.Server.handle(Server.java:497)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:245)\n        at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.sql.SQLException: ERROR 2004 (INT05): Parameter value unbound Parameter at index 1 is unbound\n        at org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newException(SQLExceptionCode.java:386)\n        at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQLExceptionInfo.java:145)\n        at org.apache.phoenix.jdbc.PhoenixParameterMetaData.getParam(PhoenixParameterMetaData.java:88)\n        at org.apache.phoenix.jdbc.PhoenixParameterMetaData.isSigned(PhoenixParameterMetaData.java:138)\n        at org.apache.calcite.avatica.jdbc.JdbcMeta.parameters(JdbcMeta.java:235)\n        at org.apache.calcite.avatica.jdbc.JdbcMeta.signature(JdbcMeta.java:246)\n        at org.apache.calcite.avatica.jdbc.JdbcMeta.prepare(JdbcMeta.java:748)\n\n</pre><p>Can you tell me what is wrong?</p><p>If this is the bug in Calcite which is been fixed in new version, how to make query-server use latest Clacite jar?</p><p>I'm using HDP 2.3 Phoenix-version:4.4 Calcite-version:1.2</p><p>@Josh Elser</p>","tags":["queryserver","Phoenix","phoenix4.4"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-19 20:40:59.0","id":45956,"title":"Reporting and Analysis:","body":"<p>Hi,</p><p>I am working on a RFP and looking for an answer to:</p><p>Ability to recalculate and alert when there are changes to historical data within a time period within your solution:</p><p>What I don't understand is we cannot modify the data in HDFS. Its immutable. So the change of historical data, does that applies?</p><p>Any help is highly appreciated.</p><p>Thanks,</p><p>Sujitha</p>","tags":["reports","analysis"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-07-20 09:15:48.0","id":46058,"title":"timeline server memory leak?","body":"<p>Hi</p><p>I use HDP 2.4.0, CentOS 6.7, jdk1.8.0_72</p><p>I suspect timeline server memory leak.</p><p>I filed jira(https://issues.apache.org/jira/browse/YARN-5368)</p><p>Because I use HDP, so I post here.</p><p>I set -Xmx1024m, but ps command shows 3.5GB of memory usage.</p><p>And then memory usage increases everyday.</p><p>I'm wondering.\nWhat is the reason?\nfor example, LevelDB JNI memory leak?</p><p>If there is a good metrics for monitoring, could you tell me?</p><pre># ps aux | grep timelineserver\nyarn       6163  2.7  5.3 6630548 3545856 ?     Sl   Jul13 288:20 /usr/java/jdk1.8.0_72/bin/java -Dproc_timelineserver -Xmx1024m -Dhdp.version=2.4.0.0-169 -Dhadoop.log.dir=/var/log/hadoop-yarn/yarn -Dyarn.log.dir=/var/log/hadoop-yarn/yarn -Dhadoop.log.file=yarn-yarn-timelineserver-myhost.log -Dyarn.log.file=yarn-yarn-timelineserver-myhost.log -Dyarn.home.dir= -Dyarn.id.str=yarn -Dhadoop.root.logger=INFO,EWMA,RFA -Dyarn.root.logger=INFO,EWMA,RFA -Djava.library.path=:/usr/hdp/2.4.0.0-169/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.0.0-169/hadoop/lib/native:/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir:/usr/hdp/2.4.0.0-169/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.0.0-169/hadoop/lib/native:/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir -Dyarn.policy.file=hadoop-policy.xml -Djava.io.tmpdir=/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir -Dhadoop.log.dir=/var/log/hadoop-yarn/yarn -Dyarn.log.dir=/var/log/hadoop-yarn/yarn -Dhadoop.log.file=yarn-yarn-timelineserver-myhost.log -Dyarn.log.file=yarn-yarn-timelineserver-myhost.log -Dyarn.home.dir=/usr/hdp/current/hadoop-yarn-timelineserver -Dhadoop.home.dir=/usr/hdp/2.4.0.0-169/hadoop -Dhadoop.root.logger=INFO,EWMA,RFA -Dyarn.root.logger=INFO,EWMA,RFA -Djava.library.path=:/usr/hdp/2.4.0.0-169/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.0.0-169/hadoop/lib/native:/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir:/usr/hdp/2.4.0.0-169/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.0.0-169/hadoop/lib/native:/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir -classpath /usr/hdp/current/hadoop-client/conf:/usr/hdp/current/hadoop-client/conf:/usr/hdp/current/hadoop-client/conf:/usr/hdp/2.4.0.0-169/hadoop/lib/*:/usr/hdp/2.4.0.0-169/hadoop/.//*:/usr/hdp/2.4.0.0-169/hadoop-hdfs/./:/usr/hdp/2.4.0.0-169/hadoop-hdfs/lib/*:/usr/hdp/2.4.0.0-169/hadoop-hdfs/.//*:/usr/hdp/2.4.0.0-169/hadoop-yarn/lib/*:/usr/hdp/2.4.0.0-169/hadoop-yarn/.//*:/usr/hdp/2.4.0.0-169/hadoop-mapreduce/lib/*:/usr/hdp/2.4.0.0-169/hadoop-mapreduce/.//*::/usr/hdp/2.4.0.0-169/tez/*:/usr/hdp/2.4.0.0-169/tez/lib/*:/usr/hdp/2.4.0.0-169/tez/conf:/usr/hdp/2.4.0.0-169/tez/*:/usr/hdp/2.4.0.0-169/tez/lib/*:/usr/hdp/2.4.0.0-169/tez/conf:/usr/hdp/current/hadoop-yarn-timelineserver/.//*:/usr/hdp/current/hadoop-yarn-timelineserver/lib/*:/usr/hdp/current/hadoop-client/conf/timelineserver-config/log4j.properties org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer\n</pre>","tags":["yarntimeline"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-20 18:23:27.0","id":46196,"title":"Why am I getting disk usage alert?","body":"<p>I created 7 AWS EC2 instances (1 for ambari server and 6 for ambari agents) and installed HDP 2.4 using ambari. Now, when I open the ambari dashboard, it shows me critical alerts related to disk usage. It shows that the hosts have less than 6GB disk size. For example, the first host in the attached image shows \"4.56 GB/5.63 (88.1% used)\". How can I solve this problem?</p><p><img src=\"/storage/attachments/5914-screen-shot-2016-07-20-at-20926-pm.png\"></p>","tags":["hdp-2.4.0","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-21 10:16:46.0","id":46385,"title":"Yarn Timeline db consuming 466GB space","body":"<p>I have a scenario where yarn timeline store db is increasing day by day. In April it was 346GB and now it increased to 466GB and occupying lot of space in /var/opt/hadoop/yarn/timeline.</p><p>#   df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda2        30G   11G   18G  37% /\nudev            126G  260K  126G   1% /dev\ntmpfs           126G     0  126G   0% /dev/shm\n/dev/sda5       5.0G  2.5G  2.2G  54% /var\n<em><strong>/dev/sda7       756G  698G   21G  98% /var/opt/</strong></em>\n/dev/sdb        2.5T  6.6G  2.3T   1% /data</p><p>when i checked disk usage on /var/opt/hadoop/yarn/timeline</p><p>du -sh *\n466G    leveldb-timeline-store.ldb\n40K     timeline-state-store.ldb</p><p>I don't know why it is occupying this much of space. What steps do i need to take to make it consume less space.</p>","tags":["yarntimeline"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-07-27 14:27:52.0","id":47673,"title":"Apache Zeppelin installation on RHEL 7.2","body":"<p>We have a cluster with RHEL 7.2 and we want to install Zeppelin with Ambari but we are facing problems. Is Zeppelin supported in RHEL 7.2? Has anyone installed it on RHEL 7.2?  <a target=\"_blank\" href=\"https://github.com/hortonworks-gallery/ambari-zeppelin-service#configure-zeppelin\">Here</a> it says that it is only tested on CentOS/RHEL 6 so far. We are using HDP 2.4.</p>","tags":["hdp-2.4.0","zeppelin-notebook"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-07-27 20:52:22.0","id":47777,"title":"I am looking to generate sequence no in a file .Used RANK, but it's failing for files > 10GB.  Here is the code ​temp = LOAD 'abc.txt' using PigStorage(';','-tagFile'); test = RANK temp; DUMP test;","body":"","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-07-27 17:25:55.0","id":47723,"title":"is JAVA knowldege is mandatory for HDPCD exam ?","body":"","tags":["hdpcd"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-07-28 20:30:12.0","id":48092,"title":"Where can I find list of enhancements (Release notes) on the latest release of cloudbreak?","body":"<p>Where can I find list of enhancements (Release notes) on the latest release of cloudbreak?</p>","tags":["Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-07-31 02:09:17.0","id":48343,"title":"Hive metastore database is not initialized","body":"<p></p><p>Hi </p><p>below error occurred when loging into HIVE via CLI. I have initialised the derby database using command</p><p>schematool -initSchema -dbType derby and successfully logged into HIVE and created databases and tables. </p><p>After I quit HIVE (QUIT command) and try to log in second time the same error occurs to have to reinitialise DB. </p><p>I think reinitialising the HIVE Metastore will erase the database. What should be done to stop this error permanently?</p><p>my jdbc connection is  \"jdbc:derby:;databaseName=/usr/local/hive/iotmp/metastore_db;create=true\" </p><p>Exception in thread \"main\" java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql)) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.ja</p>","tags":["Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-04 04:50:32.0","id":49802,"title":"Escaping double quotes in spark dataframe","body":"<p>I am reading a csv file into a spark dataframe. i have the double quotes (\"\") in some of the fields and i want to escape it. can anyone let me know how can i do this?. since double quotes is used in the parameter list for options method, i dont know how to escape double quotes in the data</p><p>val df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"delimiter\", \"|\"). option(\"escape\", -----</p>","tags":["dataframe","Spark","spark-sql"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-08-05 19:02:05.0","id":50156,"title":"ESRI Spatial Framework for Hadoop","body":"<p></p><p>ESRI\nSpatial Framework for Hadoop does not support a binary geometry data type for\nHive. What are the alternatives?</p>","tags":["esri","Hive"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-08-05 18:55:02.0","id":50153,"title":"ESRI spatial framework and Geometry API libraries for Hadoop","body":"<p>I run HDP 2.4.2. How do I build ESRI spatial framework and Geometry API libraries for Hadoop 2.7, Hive 1.2 and Java 1.8? ESRI currently provides only binaries for older versions of Hadoop and Hive.</p>","tags":["esri"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-08-09 19:01:24.0","id":50627,"title":"ListSftp works but FetchSftp doesn't work in Cluster mode","body":"<p>Hello,</p><p>I deployed a 3 nodes cluster in AWS. One of them is NCM.</p><p>The embedded zookeeper servers are set in the two work nodes.</p><p>The data flow is: ListSftp -&gt; FetchSftp -&gt; PutFile.</p><p>The ListSftp is scheduled in the Primary node.</p><p>The issue is:</p><p>ListSftp works well. The test files are queued before coming into FetchSftp.</p><p>The error in FetchSftp is:</p><p>18:36:05 UTCERROR5cdfac90-2d07-443e-97b6-b06a1a883a22\n172.31.48.155:8080FetchSFTP[id=5cdfac90-2d07-443e-97b6-b06a1a883a22] FetchSFTP[id=5cdfac90-2d07-443e-97b6-b06a1a883a22] failed to process due to org.apache.nifi.processor.exception.ProcessException: IOException thrown from FetchSFTP[id=5cdfac90-2d07-443e-97b6-b06a1a883a22]: java.io.IOException: error; rolling back session: org.apache.nifi.processor.exception.ProcessException: IOException thrown from FetchSFTP[id=5cdfac90-2d07-443e-97b6-b06a1a883a22]: java.io.IOException: error</p><p>I tried GetSftp -&gt; PutFile with the same sftp setting. It works well.</p><p>I was wondering whether the issue is related with zookeeper or primary node talking with the other work node.</p><p>I didn't setup site-to-site property in nifi.properties.</p><p>Didn't setup distributed cache service.</p><p>How could I get more log details about this processor IOException?</p><p>Thanks.</p>","tags":["Nifi","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-10-03 19:37:07.0","id":59612,"title":"Error Running MapReduce2 job using YARN","body":"<p>We installed  and configured HDP-2.5.0.0. When we run a mapreduce job it fails with the diagnostics message \"We crashed durring a commit\". The status on Map and Reduce tasks indicate that they were successful. Following is the error  observed in syslog output. How do we troubleshoot this error?Any tips will be appreciated.</p><pre>2016-10-03 15:19:20,017 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster\njava.io.IOException: Was asked to shut down.\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$5.run(MRAppMaster.java:1559)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1553)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1486)\n2016-10-03 15:19:20,021 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1\n\n</pre>","tags":["YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-10-03 20:11:03.0","id":59620,"title":"h2o hadoop nodes cluster IP","body":"<p>Waiting for H2O cluster to come up...</p><p>H2O node 10.135.132.98:54321 requested flatfile\nSending flatfiles to nodes...\n [Sending flatfile to node 10.135.132.98:54321]\nH2O node 10.135.132.98:54321 reports H2O cluster size 1\nH2O cluster (1 nodes) is up\n(Note: Use the -disown option to exit the driver after cluster formation)</p><p>Open H2O Flow in your web browser: <a href=\"http://10.135.132.98:54321/\">http://10.135.132.98:54321</a></p><p>Ran the below command and the output is as expected in the document. However the IP address 10.135.132.98 is not the local IP, not sure from where its picking it up. Hence not able to open the H2O in browser</p><p>hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output /apps/fmw/BIGDATA/H2O/hdfs_out</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-19 07:55:19.0","id":10496,"title":"ranger/ambari/hue How to use the LDAP setup role","body":"<p>Now I can use the ldap sync users and groups, but I need to set the admin role in ldap, allow the user to log on to the ranger or ambari is the admin role.</p>","tags":["Ambari","ranger-usersync"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-16 16:32:16.0","id":6840,"title":"Ambari metrics using 100% cpu","body":"<p>I have a HDP 2.3.0 cluster with 4 nodes. I noticed this process was consuming 100% cpu on my NameNode:</p><pre>ams       5386  223  5.0 3596120 1666616 ?     Sl   13:50  46:56 /usr/jdk64/jdk1.8.0_40/bin/java -Dproc_master -XX:OnOutOfMemoryError=kill -9 %p -Xmx1536m -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/ambari-metrics-collector/hs_err_pid%p.log -Djava.io.tmpdir=/opt/var/lib/ambari-metrics-collector/hbase-tmp -Djava.library.path=/usr/lib/ams-hbase/lib/hadoop-native/ -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/ambari-metrics-collector/gc.log-201512161350 -Xms1536m -Xmx1536m -Xmn256m -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Dhbase.log.dir=/var/log/ambari-metrics-collector -Dhbase.log.file=hbase-ams-master-NPAA1809.petrobras.biz.log -Dhbase.home.dir=/usr/lib/ams-hbase/bin/.. -Dhbase.id.str=ams -Dhbase.root.logger=INFO,RFA -Dhbase.security.logger=INFO,RFAS org.apache.hadoop.hbase.master.HMaster start</pre><p>But my cluster does not have Hbase installed. Then I simply killed this process, but Ambari metrics went down together. After I restarted Ambari metrics, this process continues trying to executing and always consuming a lot of CPU. Here is a print of Ambari Metrics durint time of my actions.</p><p><img src=\"/storage/attachments/882-ambari.png\"></p><p>How can I configure AMS to stop trying to monitor HBase?</p>","tags":["Hbase","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-19 13:02:45.0","id":10546,"title":"Amount of data storage : HDFS vs NoSQL","body":"<p>In several sources on internet, they explain that HDFS is build to handle more amount of data than nosql solutions(cassandra for ex). in general when we go further than 1To we must start thinking Hadoop(HDFS) and not NoSQL.</p><p>Beside the architecture and the fact that HDFS performs in batch and that most of noSQL (ex : cassandra) perform in random I/O, and beside the schema design differences, why NoSQL Solutions cassandra for example can't handle the same amount of data like HDFS ?</p><p>Why can't we use those solutions as datalake, why we only use them as hot storage solutions in a big data architecture.</p><p>thanks a lot</p>","tags":["hadoop","HDFS","nosql","cassandra"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-19 18:41:59.0","id":10617,"title":"after decommissioning nodes (delete host), we see new conf created which are identical to old conf","body":"<p>Ambari 1.7, after decommissioning nodes (delete host), we see new version of conf created to HDFS, Yarn, HBase, Hive... but they are identical to previous conf. </p><p>Is it a known issue? can we make the previous version as current to avoid \"restart required labels\"?</p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-19 19:25:14.0","id":10636,"title":"Can I place the OS for the nodes Hadoop Cluster on a SAN while the Local Disk and Hadoop components/ bits reside on Local disks","body":"<p>I have a Hadoop cluster, each node on a 2 X 8GB fabric interconnect (48 Port on one RACK) , each server has a dedicated 10 GB NIC for each one.</p><p>To save space on each node, I would like to put the OS on a SAN backed by this CICSO UCS Interconnect.\nAll Hadoop data would be stored on locally on DAS on Data Nodes (JBOD)</p><p>All Master nodes (and Edge Node) disks would be RAID and contain the Master components.</p><p>Only the OS would be on a SAN instead of locally.</p><p>Are there any issues with this?</p>","tags":["best-practices","design","san","HDFS"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-20 00:22:42.0","id":10689,"title":"AvroStorage - output file name definition","body":"<p>\n\n        <a href=\"http://stackoverflow.com/questions/34880880/avrostorage-output-file-name-definition#\">favorite</a>\n        \n\n\n    </p><p>I use AvroStorage to store result set from the pig. Is there a way \nhow can I store data into one specified avro file...e.g  OutputFileGen1?\n Pig is storing data into the directory named OutpuFileGen1 with \nstructure as listed below:</p>\n\n<pre><code> ls -al  OutputFileGen1/\ntotal 20\ndrwxr-xr-x 2 root root 4096 2016-01-18 14:35 .\ndrwxr-xr-x 6 root root 4096 2016-01-19 10:27 ..\n-rw-r--r-- 1 root root 4083 2016-01-18 14:35 part-m-00000.avro\n-rw-r--r-- 1 root root   40 2016-01-18 14:35 .part-m-00000.avro.crc\n-rw-r--r-- 1 root root    0 2016-01-18 14:35 _SUCCESS\n-rw-r--r-- 1 root root    8 2016-01-18 14:35 ._SUCCESS.crc\n</code></pre><p>Thank you</p><p>http://stackoverflow.com/questions/34880880/avrostorage-output-file-name-definition</p>","tags":["avrostorage","Pig","avro"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-19 21:26:01.0","id":10627,"title":"Hue Install Fails on RHEL 7  .. expecting python 2.6 .. rhel 7 defaults to phython 2.7","body":"<p>getting the following error message</p><p>Running transaction</p><p>  Installing : hue-common-2.6.1.2.3.4.0-3485.el6.x86_64   1/7</p><p>  Installing : hue-beeswax-2.6.1.2.3.4.0-3485.el6.x86_64  2/7</p><p>/usr/lib/hue/build/env/bin/python: error while loading shared libraries: libpython2.6.so.1.0: cannot open shared object file: No such file or directory</p><p>  Installing : hue-hcatalog-2.6.1.2.3.4.0-3485.el6.x86_64</p>","tags":["hue"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-16 23:21:10.0","id":10192,"title":"Failed to Establish Connection with ODBC connection","body":"<pre>Driver Version: V2.0.5.1005\nRunning connectivity tests...\nAttempting connection\nFailed to establish connection\nSQLSTATE: HY000[Hortonworks][HiveODBC] (34) Error from server: connect() failed: errno = 10060\nTESTS COMPLETED WITH ERROR.</pre>","tags":["Sandbox","Hive","odbc"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-18 23:20:52.0","id":10467,"title":"How to restart (unpause a downgrade)","body":"<p>While attempting to downgrade a cluster,the process becomes paused. How do you reinitiate this process?</p><p><img src=\"/storage/attachments/1440-dg.png\"></p><p>This is an Isilon backed cluster, but the logs show no issues with the component. </p><p><img src=\"/storage/attachments/1451-dg2.png\"></p><p>Is there anyway to restart/unpause a downgrade?</p><p>thanks</p>","tags":["downgrade","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-19 09:25:25.0","id":10524,"title":"HDP Windows services continuous restarting","body":"<p>Hi All,</p><p>I installed HDP 2.3.0.0 for Windows on Win Srv 2012.</p><p>I followed installation guide for a single node: </p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4-Win/bk_QuickStart_HDPWin/content/ch_qinst.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4-Win/bk_QuickStart_HDPWin/content/ch_qinst.html</a></p><p>After intallation I restarted the server and I checked system env variables.</p><p>When I start the services with the command start_local_hdp_services, I notice that some services continuous restarting.</p><p>In example derby, master data node, thrieft, oozie. This make HDP unstable and not usable.</p><p>During installation I tried other combination of password and DB flavours, without any fortune.</p><p>I tried to not install addictional component also but nothing changed.</p><p>Any ideas?</p><p>Thank you in advance.</p><p>Andrea</p>","tags":["hdp-2.3.4","windows","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-20 03:17:08.0","id":10684,"title":"Falcon Tutorial : EL function issue in Oozie via Falcon - HDP 2.3.2","body":"<p>Unable to use expression language functions in oozie workflow with Falcon. It seems some jar files are missing but unsure what. </p><p>Here is the exception when using - http://hortonworks.com/hadoop-tutorial/defining-processing-data-end-end-data-pipeline-apache-falcon/</p><p>The error occurs at this step - </p><pre>falcon entity -type process -schedule -name rawEmailIngestProcess</pre><pre>2016-01-19 21:08:31,969 ERROR CoordSubmitXCommand:517 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] XException,\norg.apache.oozie.command.CommandException: E1004: Expression language evaluation error, Unable to evaluate :${now(0,0)}:\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.submitJob(CoordSubmitXCommand.java:259)\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.submit(CoordSubmitXCommand.java:203)\n        at org.apache.oozie.command.SubmitTransitionXCommand.execute(SubmitTransitionXCommand.java:82)\n        at org.apache.oozie.command.SubmitTransitionXCommand.execute(SubmitTransitionXCommand.java:30)\n        at org.apache.oozie.command.XCommand.call(XCommand.java:286)\n        at org.apache.oozie.CoordinatorEngine.dryRunSubmit(CoordinatorEngine.java:561)\n        at org.apache.oozie.servlet.V1JobsServlet.submitCoordinatorJob(V1JobsServlet.java:228)\n        at org.apache.oozie.servlet.V1JobsServlet.submitJob(V1JobsServlet.java:95)\n        at org.apache.oozie.servlet.BaseJobsServlet.doPost(BaseJobsServlet.java:102)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\n        at org.apache.oozie.servlet.JsonRestServlet.service(JsonRestServlet.java:304)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.oozie.servlet.AuthFilter$2.doFilter(AuthFilter.java:171)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:595)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:554)\n        at org.apache.oozie.servlet.AuthFilter.doFilter(AuthFilter.java:176)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.oozie.servlet.HostnameFilter.doFilter(HostnameFilter.java:86)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:620)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.oozie.coord.CoordinatorJobException: E1004: Expression language evaluation error, Unable to evaluate :${now(0,0)}:\n\n\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.resolveTagContents(CoordSubmitXCommand.java:1003)\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.resolveIOEvents(CoordSubmitXCommand.java:889)\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.resolveInitial(CoordSubmitXCommand.java:797)\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.basicResolveAndIncludeDS(CoordSubmitXCommand.java:606)\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.submitJob(CoordSubmitXCommand.java:229)\n        ... 32 more\nCaused by: java.lang.Exception: Unable to evaluate :${now(0,0)}:\n\n\n        at org.apache.oozie.coord.CoordELFunctions.evalAndWrap(CoordELFunctions.java:723)\n        at org.apache.oozie.command.coord.CoordSubmitXCommand.resolveTagContents(CoordSubmitXCommand.java:999)\n        ... 36 more\nCaused by: javax.servlet.jsp.el.ELException: No function is mapped to the name \"now\"\n        at org.apache.commons.el.Logger.logError(Logger.java:481)\n        at org.apache.commons.el.Logger.logError(Logger.java:498)\n        at org.apache.commons.el.Logger.logError(Logger.java:525)\n        at org.apache.commons.el.FunctionInvocation.evaluate(FunctionInvocation.java:150)\n        at org.apache.commons.el.ExpressionEvaluatorImpl.evaluate(ExpressionEvaluatorImpl.java:263)\n        at org.apache.commons.el.ExpressionEvaluatorImpl.evaluate(ExpressionEvaluatorImpl.java:190)\n        at org.apache.oozie.util.ELEvaluator.evaluate(ELEvaluator.java:204)\n        at org.apache.oozie.coord.CoordELFunctions.evalAndWrap(CoordELFunctions.java:714)\n        ... 37 more\n\n</pre>","tags":["Oozie","Falcon","expression-language","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-20 11:24:41.0","id":10765,"title":"AvroStorage","body":"<p>Hi,</p><p>I use AvroStorage to store relation into avro file, when i dont specify any path inside the STORE all works fine, but when path is explicitly specified then im occurring strange error related to the permissions.... any idea? I run pig in local mode, why is there hdfs issue?</p><p>using stamement without specifiyng the path, all works fine. </p><pre>STORE A INTO 'xyz-CustomerData-20160120-1101174-ttp'</pre><p>using statement below i got error: </p><pre>STORE A INTO '/root/xyz-CustomerData-20160120-1101174-ttp'</pre>\n<pre>java.lang.Exception: org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode=\"/root/xyz-CustomerData-20160120-1101174-ttp/_temporary/0/_temporary/attempt_local1434837835_0012_m_000000_0/part-m-00000.avro\":hdfs:hdfs:drwxr-xr-x\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)\n  at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:300)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1771)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1755)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1738)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2509)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2444)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2328)\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:624)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)\n  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2137)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2133)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2131)</pre>\n<pre>  at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode=\"/root/xyz-CustomerData-20160120-1101174-ttp/_temporary/0/_temporary/attempt_local1434837835_0012_m_000000_0/part-m-00000.avro\":hdfs:hdfs:drwxr-xr-x\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)\n  at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:300)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1771)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1755)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1738)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2509)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2444)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2328)\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:624)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)\n  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2137)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2133)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2131)</pre>\n<pre>  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n  at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n  at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n  at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1697)\n  at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1703)\n  at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1638)\n  at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)\n  at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)\n  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n  at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:444)\n  at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)\n  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)\n  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)\n  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:776)\n  at org.apache.pig.piggybank.storage.avro.PigAvroOutputFormat.getRecordWriter(PigAvroOutputFormat.java:109)\n  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.getRecordWriter(PigOutputFormat.java:81)\n  at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.&lt;init&gt;(MapTask.java:647)\n  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:767)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode=\"/root/xyz-CustomerData-20160120-1101174-ttp/_temporary/0/_temporary/attempt_local1434837835_0012_m_000000_0/part-m-00000.avro\":hdfs:hdfs:drwxr-xr-x\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)\n  at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:300)\n  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1771)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1755)\n  at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1738)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2509)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2444)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2328)\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:624)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)\n  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2137)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2133)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2131)</pre><p>  at org.apache.hadoop.ipc.Client.call(Client.java:1427)\n  at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n  at com.sun.proxy.$Proxy14.create(Unknown Source)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:606)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n  at com.sun.proxy.$Proxy15.create(Unknown Source)\n  at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1692)\n  ... 22 more\n2016-01-20 11:17:46,729 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local1434837835_0012\n2016-01-20 11:17:46,729 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases sensitiveSet\n2016-01-20 11:17:46,729 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: sensitiveSet[5,15] C:  R:\n2016-01-20 11:17:46,733 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Ooops! Some job has failed! Specify -stop_on_failure if you want Pig to stop immediately on failure.\n2016-01-20 11:17:46,734 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - job job_local1434837835_0012 has failed! Stop running all dependent jobs\n2016-01-20 11:17:46,734 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n2016-01-20 11:17:46,736 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2016-01-20 11:17:46,738 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n2016-01-20 11:17:46,739 [main] ERROR org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil - 1 map reduce job(s) failed!\n2016-01-20 11:17:46,739 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics:</p>","tags":["piggybank","avro","Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-20 14:20:58.0","id":10809,"title":"Spark streaming generating error when reading kafka topic","body":"<p>I am completely blank on this error. its been 2 days i am struggling with this error.    my kafka producer is working fine emitting ClickEvent every second. confirmed that events are being posed to kafka topic in Avro format.</p><p>Now i want to read back in spark streaming.   code is at end.       notice  i got this error  \"java.lang.NoClassDefFoundError: org/apache/spark/streaming/util/WriteAheadLogUtils$\"      now i tried to find every where WriteAheadLogUtils and found nothing.    Please help me resolve this issues </p><pre>16/01/20 14:17:37 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@sandbox.hortonworks.com:38373/user/HeartbeatReceiver\n16/01/20 14:17:37 INFO NettyBlockTransferService: Server created on 60697\n16/01/20 14:17:37 INFO BlockManagerMaster: Trying to register BlockManager\n16/01/20 14:17:37 INFO BlockManagerMasterActor: Registering block manager localhost:60697 with 265.4 MB RAM, BlockManagerId(&lt;driver&gt;, localhost, 60697)\n16/01/20 14:17:37 INFO BlockManagerMaster: Registered BlockManager\n16/01/20 14:17:39 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\nError: application failed with exception\njava.lang.NoClassDefFoundError: org/apache/spark/streaming/util/WriteAheadLogUtils$\n        at org.apache.spark.streaming.kafka.KafkaUtils$.createStream(KafkaUtils.scala:84)\n        at streamingAvroConsumer$delayedInit$body.apply(streamingAvroConsumer.scala:44)\n        at scala.Function0$class.apply$mcV$sp(Function0.scala:40)\n        at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\n        at scala.App$$anonfun$main$1.apply(App.scala:71)\n        at scala.App$$anonfun$main$1.apply(App.scala:71)\n        at scala.collection.immutable.List.foreach(List.scala:318)\n        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)\n        at scala.App$class.main(App.scala:71)\n        at streamingAvroConsumer$.main(streamingAvroConsumer.scala:18)\n        at streamingAvroConsumer.main(streamingAvroConsumer.scala)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:367)\n        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:77)\n        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.streaming.util.WriteAheadLogUtils$\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n        ... 18 more\n\n\n\n</pre><p>Code is below</p><pre>import events.avro.ClickEvent\nimport kafka.serializer.DefaultDecoder\nimport org.apache.avro.io.DecoderFactory\nimport org.apache.avro.specific.SpecificDatumReader\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.{Minutes, Seconds, StreamingContext}\n\n\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.SparkConf\n\n\n\n\n\n\nobject streamingAvroConsumer extends App \n{\n  println(\"Initializing App\")\n\n\n\n\n  \tval numThreads = \"1\" \n\tval topics = \"testing2\"\n\n\n  val sparkConf = new SparkConf().setAppName(\"WindowClickCount\").setMaster(\"local[2]\")\n\n\n  // Slide duration of ReduceWindowedDStream must be multiple of the parent DStream, and we chose 2 seconds for the reduced\n  // window stream\n  val ssc = new StreamingContext(sparkConf, Seconds(2))\n\n\n  // Because we're using .reduceByKeyAndWindow, we need to persist it to disk\n  ssc.checkpoint(\"./checkpointDir\")\n\n\n  val kafkaConf = Map(\n    \"metadata.broker.list\" -&gt; \"sandbox.hortonworks.com:6667\", \n    \"zookeeper.connect\" -&gt; \"sandbox.hortonworks.com:2181\", \n    \"group.id\" -&gt; \"kafka-spark-streaming-example\",\n    \"zookeeper.connection.timeout.ms\" -&gt; \"1000\")\n\n\n  val topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\n\n\n  // Create a new stream which can decode byte arrays.  For this exercise, the incoming stream only contain user and product Ids\n  val lines = KafkaUtils.createStream[String, Array[Byte], DefaultDecoder, DefaultDecoder](ssc, kafkaConf, topicMap, StorageLevel.MEMORY_ONLY_SER).map(_._2)\n\n\n\n\n\tval mappedUserName = lines.transform\n\t{   rdd =&gt;\n\t\t\trdd.map { bytes =&gt; AvroUtil.clickEventDecode(bytes) }.map \n\t\t\t{ clickEvent =&gt;\n\t\t\t\tprintln(clickEvent.time);\n\t\t\t}\n\t}\n\n\n\n\n  ssc.start()\n  ssc.awaitTermination()\n}\n\n\n\n\n\n\n\tobject AvroUtil \n\t{\n\t\tval reader = new SpecificDatumReader[ClickEvent](ClickEvent.getClassSchema)\n\t\tdef clickEventDecode(bytes: Array[Byte]): ClickEvent = {\n\t\t\tval decoder = DecoderFactory.get.binaryDecoder(bytes, null)\n\t\t\treader.read(null, decoder)\n\t    }\n\t}\n\n\n</pre>","tags":["Kafka","Spark","streaming"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-20 16:16:36.0","id":10845,"title":"I need to send data JMS-->kafka and kafka to HDFS using flume can you advise me what is the best option and conf file looks like?","body":"<p>I can able to send data JMS to hdfs succcessfully. but I want to send it to above flow if anybody guide me with flume.conf looks like and what sort of properties needs to be changed. I really apreciated  </p>","tags":["Flume","HDFS","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-20 18:12:48.0","id":10909,"title":"HDP Rolling Upgrade using dzdo","body":"<p>Upgrading HDP from 2.2.0.0-2041 to 2.3 we do not have root access. Instead we are using dzdo.  Prior to upgrade and to avoid pain during upgrade, I am asking our community to share any insights I should know of prior to running upgrade using dzdo.  </p><ul>\n<li>Which rights should I validate dzdo has?</li><li>Any actions I should be performed to validate I have the correct rights</li><li>Any insights or gotcha you can share if running upgrade outside of root</li></ul>","tags":["upgrade","hdp-2.3.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-21 11:56:02.0","id":11025,"title":"Hive query issue","body":"<p>Goodmorning everyone. </p><p>A customer has a problem with a query on hive (with Tez). </p><p>The query is as follows:</p><pre>select a.hashedaddress from x a join x b on (a.hashedaddress = b.hashedaddress) where a.guid != b.guid;</pre><p>The process is stopped at the first step of mapping:</p><pre>VERTICES\tSTATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\n\nMap 1           FAILED     62          0        0       62      43       7 \n\nMap 3           KILLED     62          0        0       62       0       0 \n\nReducer 2       KILLED   1009          0        0     1009       0       0 \n\nVERTICES: 00/03  [&gt;&gt;--------------------------] 0%    ELAPSED TIME: 26.69 s</pre><p>Log error is this: <a href=\"/storage/attachments/1483-vertexerror.txt\">vertexerror.txt</a></p><p>The table schema is as follows:</p><pre>CREATE TABLE `x`(\n  `guid` string,\n  `brandname` string, `hashedaddress` string, [and more]) PARTITIONED BY (\n  `calendardate` date) CLUSTERED BY (\n  brandname) \n\nINTO 5 BUCKETS ROW FORMAT SERDE\n  'org.apache.hadoop.hive.ql.io.orc.OrcSerde' \n\nSTORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' \n\nOUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' \n\nLOCATION\n  '*****' \n\nTBLPROPERTIES (\n  'transient_lastDdlTime'='1445504632')</pre><p>Table contains some 20 million records, to do the tests we tried to apply a limit to the tables:</p><pre>select a.hashedaddress from (select * from x limit 10000) a join (select * from x limit 10000) b on (a.hashedaddress = b.hashedaddress) where a.guid != b.guid;</pre><p>The query runs without problems</p><pre>VERTICES      \tSTATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\n\nMap 1 ..........SUCCEEDED     62         62        0        0       0       0 \n\nMap 3 ..........SUCCEEDED     62         62        0        0       0       0 \n\nReducer 2 ......SUCCEEDED      1          1        0        0       0       0 \n\nReducer 4 ......SUCCEEDED      1          1        0        0       0       0 \n\nVERTICES: 04/04  [==========================&gt;&gt;] 100%  ELAPSED TIME: 55.13 s</pre>Then we have increased the limits up to the cap (100000. 1000000, 10000000) and it works correctly,\nAfter this changing the query removing the limitations and it works:<pre>select a.hashedaddress from (select * from x) a join (select * from x) b on (a.hashedaddress = b.hashedaddress) where a.guid != b.guid;</pre><pre>VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\nMap 1 ..........SUCCEEDED     62         62        0        0       0       0 \n\nMap 3 ..........SUCCEEDED     62         62        0        0       0       0 \n\nReducer 2 ......SUCCEEDED     253        253       0        0       0       0 \n\nVERTICES: 03/03  [==========================&gt;&gt;] 100%  ELAPSED TIME: \n</pre><p>Can you help me understand why not work the first query? The join does it with the same data. Thanks</p>","tags":["Hive","query"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-12 14:51:58.0","id":3317,"title":"Ambari LDAP Sync to AD using SSL returns HTTP Error 503","body":"<p>I'm trying to sync users and groups from an Active Directory LDAP which is using SSL.  I'm using Ambari 2.1.0</p><p>I have used the following information on setup-ldap:</p><pre>At the Primary URL* prompt, enter the server URL and port you collected above. Prompts marked with an asterisk are required values.\nldaps://&lt;ADServerName&gt;:636 (note, I have tried this without the ldaps prefix, but got the same error)\nAt the Secondary URL* prompt, enter the secondary server URL and port. This value is optional. \n\nAt the Use SSL* prompt, enter your selection. If using LDAPS, enter true. \ntrue\nAt the User object class* prompt, enter the object class that is used for users. \ninetOrgPerson\nAt the User name attribute* prompt, enter your selection. The default value is uid. \n(default)\nAt the Group object class* prompt, enter the object class that is used for groups. \ngroup\nAt the Group name attribute* prompt, enter the attribute for group name. \ncn (default)\nAt the Group member attribute* prompt, enter the attribute for group membership. \nmember\nAt the Distinguished name attribute* prompt, enter the attribute that is used for the distinguished name. \ndn (default)\nAt the Base DN* prompt, enter your selection. \n&lt;BaseDN&gt;\nAt the Referral method* prompt, enter to follow or ignore LDAP referrals. \nfollow\nAt the Bind anonymously* prompt, enter your selection. \nfalse\nAt the Manager DN* prompt, enter your selection if you have set bind.Anonymously to false. \n&lt;BindDN&gt;\nAt the Enter the Manager Password* prompt, enter the password for your LDAP manager DN. \n&lt;BindDNpw&gt;\nDo you want to provide custom TrustStore for Amabari\ny\nTrustStoretype [jks/jceks/pkcs12] (jks)\njks (default)\nPath to TrustStore file:\n/etc/ambari-server/keys/ldaps-keystore.jks\nPassword for TrustStore\nRe-enter password:\n====================\nReview Settings\n====================\nauthentication.ldap.managerDn: &lt;BindDN&gt;\nauthentication.ldap.managerPassword: &lt;BindDNpw&gt;\nssl.trustStore.type: jks\nssl.trustStore.path: /etc/ambari-server/keys/ldaps-keystore.jks\nssl.trustStore.password: \nSave settings [y/n] (y)? y\nSaving...done\nAmbari Server 'setup-ldap' completed successfully.\n</pre><p>I have tried this originally without the TrustStore set up (originally), then I tried again with a self-signed certificate using the instructions in Section 3.3.2.3. Create and Import Self-Signed Certificate in Hadoop Security Guide (May 26, 2015)</p><p>When I try to do an LDAP Sync, I get this error</p><pre>ambari-server sync-ldap --users users.txt --groups groups.txt\nUsing python  /usr/bin/python2.6\nSyncing with LDAP...\nEnter Ambari Admin login: admin\nEnter Ambari Admin password:\nSyncing specified users and groups.ERROR: Exiting with exit code 1.\nREASON: Sync event creation failed. Error details: HTTP Error 503: Service Unavailable</pre><p>I have successfully used the same BindDN info, etc using the ldapsearch command on the command line, and using that, I can list the users and groups that I am trying sync.</p><p>Any suggestions on how I can figure out why I'm getting this error?</p>","tags":["active-directory","ambari-ldap-sync","ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-22 16:39:35.0","id":11285,"title":"Yarn jobs fails with \"Not able to initialize user directories in any of the configured local directories\"","body":"<p>I am trying to run a benchmark job, with the following command :\nyarn jar /path/to/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 1000 -resFile /tmp/TESTDFSio.txt\nbut my job fails with following error messages : </p><p>16/01/22 15:08:47 INFO mapreduce.Job: Task Id : attempt_1453395961197_0017_m_000008_2, Status : FAILED\nApplication application_1453395961197_0017 initialization failed (exitCode=255) with output: main : command provided 0 </p><p>main : user is foo </p><p>main : requested yarn user is foo </p><p>Path /mnt/sdb1/yarn/local/usercache/foo/appcache/application_1453395961197_0017 has permission 700 but needs permission 750. </p><p>Path /var/hadoop/yarn/local/usercache/foo/appcache/application_1453395961197_0017 has permission 700 but needs permission 750.\nDid not create any app directories </p><p>Even when I change these directories permission to 750, I get errors.\nAlso these caches dont get cleaned off, after one job'and create collisons when running the next job.\nAny insights ?</p>","tags":["MapReduce","YARN","hadoop","yarn-node-labels"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-24 20:51:18.0","id":11467,"title":"Backing up hive database","body":"<p>Hi,I am trying to setup Hortonworks Data Platform. I would want to setup Hive in high availability mode (both metastore and as well as HiveServer2). Along with that, Hortonworks recommendation is to backup the RDBMS behind Hive service.\nCan anyone please let me know what is the best practice around this? As, by default, Hive uses MySQL, please advise me on the best way to achieve high availability for all the Hive components.</p>","tags":["Hive","mysql","high-availability"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-25 10:23:07.0","id":11534,"title":"Can \"javax.jdo.option.ConnectionPassword\" be encrypted ?","body":"<p>Hive users can read the hive-site.xml and javax.jdo.option.ConnectionPassword because the password is stored in plane text.</p><p>Can I make it encrypted ?</p>","tags":["security","Hive","hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-27 09:11:49.0","id":11929,"title":"Upgrading JDK on HDP Cluster in a Rolling Upgrade Fashion","body":"<p>Let's say I want to upgrade JDK in my HDP cluster without stopping it.\nI can start by upgrading masters one by one and restarting services, and then I can upgrade workers \nin groups of 2 (HDFS replication factor is 3). HA components (NN, RM etc) will have no down time, but non-HA (History Server, SHS) will have some down time.\nHowever, midway through the process some processes will run on old Java version and some on the new version. Is this acceptable? Of course we'd like to avoid\ntroubles. Has anyone tried this? A concrete example: HDP-2.2.6, Ambari-2.0.1 and upgrading Java from\n1.7.0.79 to 1.7.0.91. Any comments will be appreciated. Thanks!</p>","tags":["hdp-2.3.4","upgrade","Ambari","java","hdp-2.2.6"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-31 09:19:51.0","id":12670,"title":"Cannot start data node - did the troubleshooting and find something wired: could you help!!!","body":"<p>I installed Hadoop 2.7.2 (1 master NN -1 second NN-3 datanodes) and tried to start my datanodes !!! Get stuck! </p><p>After trouble shouting the logs (see below), the fatal error is due to ClusterID mismatch... easy! just change the IDs. <strong>WRONG</strong>... when I checked my VERSION files on the NameNode and the DataNodes they are identical.(see by yourself below)</p><p>So the question is simple: INTO the log file --&gt; Where the ClusterID of the NameNode is coming From????</p><p>LOG FILE:</p>\n<hr><pre>WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException:Incompatible clusterIDs in/home/hduser/mydata/hdfs/datanode: namenode clusterID =**CID-8e09ff25-80fb-4834-878b-f23b3deb62d0**; datanode clusterID =**CID-cd85e59a-ed4a-4516-b2ef-67e213cfa2a1**\norg.apache.hadoop.hdfs.server.datanode.DataNode:Initialization failed forBlock pool &lt;registering&gt;(DatanodeUuid unassigned) service to master/172.XX.XX.XX:9000.Exiting.\njava.io.IOException:All specified directories are failed to load.\natorg.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:478)\natorg.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1358)\natorg.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1323)\natorg.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:317)\natorg.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)\natorg.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:802)\nat java.lang.Thread.run(Thread.java:745)\nWARN org.apache.hadoop.hdfs.server.datanode.DataNode:Ending block pool service for:Block pool &lt;registering&gt;(DatanodeUuid unassigned) service to master/172.XX.XX.XX:9000\nINFO org.apache.hadoop.hdfs.server.datanode.DataNode:RemovedBlock pool &lt;registering&gt;(DatanodeUuid unassigned)\nWARN org.apache.hadoop.hdfs.server.datanode.DataNode:ExitingDatanode</pre><p>COPY of THE VERSION FILE</p><hr>\n<p><strong>the master</strong></p><pre>storageID=DS-f72f5710-a869-489d-9f52-40dadc659937\nclusterID=CID-cd85e59a-ed4a-4516-b2ef-67e213cfa2a1\ncTime=0\ndatanodeUuid=54bc8b80-b84f-4893-8b96-36568acc5d4b\nstorageType=DATA_NODE\nlayoutVersion=-56</pre><p><strong>THE DataNode</strong></p><pre>storageID=DS-f72f5710-a869-489d-9f52-40dadc659937\nclusterID=CID-cd85e59a-ed4a-4516-b2ef-67e213cfa2a1\ncTime=0\ndatanodeUuid=54bc8b80-b84f-4893-8b96-36568acc5d4b\nstorageType=DATA_NODE\nlayoutVersion=-56</pre>","tags":["HDFS","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-31 22:36:16.0","id":12710,"title":"I'm unable to view the databases inside Mysqlworkbench. Instead the following databases are listed : information_schema, hive, mysql, ranger, ranger_audit,test","body":"<p>Whenever I execute the following query : sqoop list-databases --connect jdbc:mysql://localhost:3306 --username xxx -password xxx . Only the following tables are displayed information_schema, hive, mysql, ranger, ranger_audit,test, but not the databases which I created inside mysql workbench. Your guidance will be appreciated.</p>","tags":["mysql","Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-01 14:20:54.0","id":12852,"title":"How can we isolate the jobs running in the same queue?","body":"<p>I created four node labels, assigned 4 nodes to each node label. Now, the jobs are running fine in the specified queues and on assigned nodes through node labels. Let us consider a case, two jobs are running in the same queue. Does one job which is acquiring more cluster resources will kill another job in the same queue? How can we isolate two jobs without impacting each other? Is there any solution?</p>","tags":["yarn-scheduler","jobs","queue","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-07 21:25:54.0","id":15095,"title":"Hive  Compaction for ACID Transactions","body":"<p>Hello Hive SME's, </p><p>I am setting up Hive tables for ACID Transactions. There are fair(not high) number of inserts/ updates expected on each tables.</p><p>Should the compactions be scheduled every day? or let Hive manage compaction? Are there any pros/cons on hive managed compactions?</p><p>Hive Version 0.14</p><p>Thank You</p><p>Pranay Vyas </p>","tags":["Hive","compaction"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-09 14:41:12.0","id":15642,"title":"How to connect informatica with hadoop","body":"<p>Please suggest me approach to integrate informatica with Hadoop? Also suggest the versions that support as i come to know Informatica have this feature now and how to integrate earlier versions.</p><p>Also let me know most cost effective distributor and solution</p>","tags":["integration"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-09 15:25:39.0","id":15656,"title":"How to switch off the log rotation in hadoop","body":"<p>How to switch off the log rotation in hadoop. </p><p>The log rotation  is currently done by the log4j. I am trying to collect individual log file details and also SEV levels</p>","tags":["logs","licensing","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-09 14:59:35.0","id":15645,"title":"What are \"clients\" shown under the components list?","body":"<p>On the host details page, I can see various clients listed as \"Client / HBase Client, HCat Client...\". I would like to understand what are these clients and what is their use?. I also see that some of the clients have a red rectangle against them.</p>","tags":["ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-09 16:02:43.0","id":15667,"title":"Windows -  Hive connection issue through ODBC using Hive ODBC Driver for HDP 2.2","body":"<p>\n\tI installed <b>Hive ODBC Driver for HDP 2.2 </b>on my windows 7 machine and trying to connect to hive through ODBC(hadoop istalled on CENTOS).I encoutered with following error. configs are all default. For example authentication for hiveserver2 is \"none\"(default).Is anything i missed out.I followed the document of hortoworks.I gave the server ip and port is 10000.I assumed hiveserver2 is running because beeline command line is working for following command</p>\n<pre>beeline -u jdbc:hive2://ip:10000\n</pre><p><img src=\"/storage/attachments/1953-fireshot-capture-8-drafts-35-bsuresh.png\"></p>","tags":["hiveserver2","odbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-09 18:36:09.0","id":15735,"title":"Spark and scala","body":"<p>Hi </p><p>Need to know if there are build tools for spark with scala on HDP VM. I do not see one. I  need to build a program using scala over spark and need to know to how to build the same</p><p>Regards</p><p>Sreeni</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-09 17:25:44.0","id":15695,"title":"NFSGateway","body":"<p>Hi:</p><p>I need to know in which cases install NFSGateways into de nodes to the cluster.</p><p>Thanks</p>","tags":["nfs"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-09 18:12:20.0","id":15722,"title":"When enable ranger solr audit with solrcloud, do not see the access audit in the ranger audit. How to view the audit records/logs?","body":"<p>When enable ranger solr audit with solrcloud and enable hdfs audit also, do not see the access audit in the ranger audit tab.  How to view the audit records/logs?  </p><p>I see that the logs are written to hdfs. But not able to view them. </p><p>Same thing with the solr audit logs also. Does this need creation of any policies for Solr in Ranger, just to view the audit logs? </p>","tags":["solrcloud","audit"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-10 09:06:42.0","id":15868,"title":"Hive - TBLPROPERTIES","body":"<p>Hello, DW guy learning hadoop.</p><p>I wanted to understand what are the full list of TBLPROPERTIES that are predefined in Hive and available for use?\nCouldnt find it in the Hive Manual.\n\nThanks for the help</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-10 08:35:08.0","id":15856,"title":"setpgid(2296, 0) failed - [Errno 13] Permission denied issue, server getting stopped after some time","body":"<p>ambari-server start\nUsing python  /usr/bin/python2\nStarting ambari-server\nAmbari Server running with administrator privileges.\nOrganizing resource files at /var/lib/ambari-server/resources... </p><p>WARNING: setpgid(2296, 0) failed - [Errno 13] Permission denied\nServer PID at: /var/run/ambari-server/ambari-server.pid\nServer out at: /var/log/ambari-server/ambari-server.out\nServer log at: /var/log/ambari-server/ambari-server.log\nWaiting for server start....................</p><p>\nAmbari Server 'start' completed successfully.</p><p>But server getting stopped automatically.</p><p>I can refer to https://community.hortonworks.com/questions/10044/ambari-server-start-warning-setpgid4340-0-failed-e.html but didn't make any sense on how to apply the patch.</p><p>Any help would be great....! Any clear steps to resolve the issue and apply patch.</p><p>Thanks</p>","tags":["ambari-server","stops after some time"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-10 21:38:36.0","id":16170,"title":"How to handle the connection error to server when running PIg script?","body":"<p>@<a href=\"https://community.hortonworks.com/users/393/aervits.html\">Artem Ervits</a>    </p><p>Can you help me resolve this error, ? Iam not getting this because there are no alerts popping up also my internet connection is also up.</p>","tags":["Pig","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-11 04:55:28.0","id":16254,"title":"Create DB in Hive","body":"<p>When I am trying to create DB with hIVE VIEW editor getting below error</p><p>org.apache.ambari.view.utils.hdfs.HdfsApiException: HDFS030 Error in creation /user/admin/hive/jobs/hive-job-2-2016-02-10_09-55...</p>","tags":["Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-11 03:37:16.0","id":16258,"title":"How many NameNodes needed on Backup Cluster","body":"<p>The customer wants to setup a backup to a backup cluster.  They want to know how many NameNodes are required on the Backup Cluster.  They want to know if there is some rule of thumb.  The constraints for the backup cluster is below</p><ul></ul><ul>\n<li>Constraints<ul>\n<li>Backup window: 1am-5am ET (12am-4am CT)</li><li>Want to have a yarn job that runs Falcon backup jobs in another queue when the cluster isn’t busy - that way it doesn't pile up for the night.</li><li>The clusters will be in Amazon, so they want the backup cluster to be backed by Amazon S3 storage (being used as HDFS)</li></ul></li></ul>","tags":["distcp","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-02-11 12:28:59.0","id":16347,"title":"Adding multiple host while on install options step of ambari server","body":"<p>I am setting up a 3 node cluster using HDP 2.3 stack and ambari 2.2.0. While, on the step to install options. As I understand i need to state the FQDNS of all the 3 nodes (FQDNs of 3 vsphere VMs in my case). In the Target Hosts text box I am entering I 3 nodes FQDNs each on a new line, But what do I do about the key? I mean, how do I paste or select the SSH-private key for all the 3 hosts. I only get an option to select one, because of which one gets successfully installed, and the other 2 fail.</p>","tags":["hadoop","installation","Ambari"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-11 19:06:48.0","id":16432,"title":"cant start sqoop at /usr/bin/ and sqoop directory have file link errors","body":"<p>as root, su - sqoop, I am in /usr/bin, but I saw some link with sqoop directory problem here. </p><p><a href=\"/storage/attachments/2049-4-neeraj-2-11.png\">4-neeraj-2-11.png</a></p>","tags":["Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-11 15:07:12.0","id":16391,"title":"Flume data ingestion - Twitter data having non-printable characters","body":"<p>Hello</p><p>I had recently used Flume (on the Hortonworks Practice environment on AWS) and ingested Twitter data into HDFS. Though I was able to import the data into HDFS, it looks like some of the tweets had foreign language characters (French etc) and they were not readable when I opened the file on HDFS.</p><p>Is there a way we could view the non-printable characters properly on HDFS ? Do we need to set the codepage to Unicode / some thing else ? It would be great if you could help me fix this so that I can successfully load that twitter data into Hive.</p><p>Thanks!</p>","tags":["Flume","twitter"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-11 19:45:55.0","id":16427,"title":"Connection refused while accessing HDFS Files in Ambari","body":"<p>While following Sandbox Tutorial, I am trying to access HDFS Files in Ambari to upload sensor data in to HDFS. Getting <strong>500 </strong><strong>Connection refused</strong> error. </p><p>Any suggestion ?</p>","tags":["error"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-12 01:10:23.0","id":16502,"title":"where can i find sample csv files for Hadoop developer HDPCD exam","body":"<p>where can i find sample csv files for Hadoop developer HDPCD exam ? can you send AWS location for dowloading these sample files ?</p>","tags":["examples"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-12 11:31:08.0","id":16592,"title":"How to redirect the access logs and other logs to external file system?","body":"<p>Hi folks,</p><p>One of the requirements is to redirect the all kind of logs/ ranger logs/access logs and also other component's logs to external file system. Probably on NFS and not on HDFS or in the DB. Does HDP provide out of the box solution for this?</p><p>One workaround I can think of using Flume but wanted to know other approaches?</p><p>Regards,</p><p>DP</p>","tags":["Ranger","hdp-2.3.4","hortonwork"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-12 14:38:59.0","id":16625,"title":"Basic Question: Install Spark on HDP 2.3 using Ambari","body":"<p>I have a 9-node (6 slaves, 2 masters and 1 edge node) cluster of HDP 2.3 with Ambari running. Currently only HDFS, YARN, Zookeeper, and Ambari metrics are running. </p><p>I'd like to install Spark.\nWhen I did an install of Spark 1.4.1 via Ambari, it installed a Spark history server on one node and spark client on 2 nodes.\nI don't see spark on the other nodes. Do I have to install Spark client on every node and set the master and slaves configuration and start spark manually? </p><p>I am not connected to the Internet and there are no proxy servers.</p>","tags":["installation","spark-1.4.1"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-12 17:56:25.0","id":16691,"title":"Can Phoenix leverage Knox for JDBC Gateway?","body":"<p>Simply curious if Phoenix JDBC connections can go through Knox for security purposes? If so, does anyone have any tutorials/examples of this?</p><p>Thanks,</p>","tags":["Knox","Phoenix","jdbc"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-13 16:50:58.0","id":16766,"title":"Hbase-Hive Integration accessing Timestamp/versioned data","body":"<p>How can I access Timestamp data of Hbase in Hive.Is there any way to see different version of Hbase data from Hive.</p><p>Does the patch Hive-2828 work for Hortonworks(https://issues.apache.org/jira/browse/HIVE-2828)</p><p>Thanks,</p><p>Dijin</p>","tags":["Hbase","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-04 21:09:17.0","id":14542,"title":"How can I create a simple topology? I want to write this program using python on the hortonworks shell.","body":"<p>I'm new to Storm so, please suggest me where to start typing this piece of code and methods to save and execute it on the hortonworks shell. Any suggestions to get started will be helpful. Thanks in advance for your time. </p>","tags":["Storm","python"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-15 03:48:46.0","id":16958,"title":"MapReduce timeout","body":"<p>2016-02-14 22:40:05,909 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf</p><pre>2016-02-14 22:40:06,618 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-02-14 22:40:06,760 - HdfsResource['/user/ambari-qa/mapredsmokeoutput'] {'security_enabled': True, 'hadoop_bin_dir': '/usr/hdp/current/hadoop-client/bin', 'keytab': '/etc/security/keytabs/hdfs.headless.keytab', 'default_fs': 'hdfs://HDPCA', 'hdfs_site': ..., 'kinit_path_local': '/usr/bin/kinit', 'principal_name': 'hdfs-HDPCA@EXAMPLE.COM', 'user': 'hdfs', 'action': ['delete_on_execute'], 'hadoop_conf_dir': '/usr/hdp/current/hadoop-client/conf', 'type': 'directory'}\n2016-02-14 22:40:06,860 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs-HDPCA@EXAMPLE.COM'] {'user': 'hdfs'}\n2016-02-14 22:40:11,788 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx0.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmpZmSrzL 2&gt;/tmp/tmpxqBP9F''] {'quiet': False}\n2016-02-14 22:40:15,601 - call returned (0, '')\n2016-02-14 22:40:15,603 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx1.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmpWcmtSE 2&gt;/tmp/tmp8ueZHF''] {'quiet': False}\n2016-02-14 22:40:19,015 - call returned (0, '')\n2016-02-14 22:40:19,017 - NameNode HA states: active_namenodes = [(u'nn1', 'lnx0.localdomain.com:50070')], standby_namenodes = [(u'nn2', 'lnx1.localdomain.com:50070')], unknown_namenodes = []\n2016-02-14 22:40:19,018 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx0.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmpEDIPa1 2&gt;/tmp/tmp1Xt3Yx''] {'quiet': False}\n2016-02-14 22:40:22,856 - call returned (0, '')\n2016-02-14 22:40:22,858 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx1.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmpKQmmZw 2&gt;/tmp/tmpJ6YFEL''] {'quiet': False}\n2016-02-14 22:40:26,162 - call returned (0, '')\n2016-02-14 22:40:26,164 - NameNode HA states: active_namenodes = [(u'nn1', 'lnx0.localdomain.com:50070')], standby_namenodes = [(u'nn2', 'lnx1.localdomain.com:50070')], unknown_namenodes = []\n2016-02-14 22:40:26,167 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X GET --negotiate -u : '\"'\"'http://lnx0.localdomain.com:50070/webhdfs/v1/user/ambari-qa/mapredsmokeoutput?op=GETFILESTATUS&user.name=hdfs'\"'\"' 1&gt;/tmp/tmpC3nS44 2&gt;/tmp/tmpYhq8nH''] {'logoutput': None, 'quiet': False}\n2016-02-14 22:40:29,885 - call returned (0, '')\n2016-02-14 22:40:30,159 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X DELETE --negotiate -u : '\"'\"'http://lnx0.localdomain.com:50070/webhdfs/v1/user/ambari-qa/mapredsmokeoutput?op=DELETE&user.name=hdfs&recursive=True'\"'\"' 1&gt;/tmp/tmpKxdOxa 2&gt;/tmp/tmpNflsmo''] {'logoutput': None, 'quiet': False}\n2016-02-14 22:40:34,695 - call returned (0, '')\n2016-02-14 22:40:34,697 - HdfsResource['/user/ambari-qa/mapredsmokeinput'] {'security_enabled': True, 'hadoop_bin_dir': '/usr/hdp/current/hadoop-client/bin', 'keytab': '/etc/security/keytabs/hdfs.headless.keytab', 'source': '/etc/passwd', 'default_fs': 'hdfs://HDPCA', 'hdfs_site': ..., 'kinit_path_local': '/usr/bin/kinit', 'principal_name': 'hdfs-HDPCA@EXAMPLE.COM', 'user': 'hdfs', 'action': ['create_on_execute'], 'hadoop_conf_dir': '/usr/hdp/current/hadoop-client/conf', 'type': 'file'}\n2016-02-14 22:40:34,699 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs-HDPCA@EXAMPLE.COM'] {'user': 'hdfs'}\n2016-02-14 22:40:36,030 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx0.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmpWi075d 2&gt;/tmp/tmpfGORFu''] {'quiet': False}\n2016-02-14 22:40:39,946 - call returned (0, '')\n2016-02-14 22:40:39,948 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx1.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmpVeHNPf 2&gt;/tmp/tmpXbZTBA''] {'quiet': False}\n2016-02-14 22:40:41,300 - call returned (0, '')\n2016-02-14 22:40:41,302 - NameNode HA states: active_namenodes = [(u'nn1', 'lnx0.localdomain.com:50070')], standby_namenodes = [(u'nn2', 'lnx1.localdomain.com:50070')], unknown_namenodes = []\n2016-02-14 22:40:41,303 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx0.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmp_2srqJ 2&gt;/tmp/tmpaBMZT6''] {'quiet': False}\n2016-02-14 22:40:41,588 - call returned (0, '')\n2016-02-14 22:40:41,591 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '\"'\"'http://lnx1.localdomain.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'\"'\"' 1&gt;/tmp/tmpzAgUPP 2&gt;/tmp/tmp_JVFtg''] {'quiet': False}\n2016-02-14 22:40:41,859 - call returned (0, '')\n2016-02-14 22:40:41,861 - NameNode HA states: active_namenodes = [(u'nn1', 'lnx0.localdomain.com:50070')], standby_namenodes = [(u'nn2', 'lnx1.localdomain.com:50070')], unknown_namenodes = []\n2016-02-14 22:40:41,867 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '\"'\"'%{http_code}'\"'\"' -X GET --negotiate -u : '\"'\"'http://lnx0.localdomain.com:50070/webhdfs/v1/user/ambari-qa/mapredsmokeinput?op=GETFILESTATUS&user.name=hdfs'\"'\"' 1&gt;/tmp/tmp1NN7xj 2&gt;/tmp/tmpdtdIhc''] {'logoutput': None, 'quiet': False}\n2016-02-14 22:40:42,493 - call returned (0, '')\n2016-02-14 22:40:42,494 - DFS file /user/ambari-qa/mapredsmokeinput is identical to /etc/passwd, skipping the copying\n2016-02-14 22:40:42,495 - HdfsResource[None] {'security_enabled': True, 'hadoop_bin_dir': '/usr/hdp/current/hadoop-client/bin', 'keytab': '/etc/security/keytabs/hdfs.headless.keytab', 'default_fs': 'hdfs://HDPCA', 'hdfs_site': ..., 'kinit_path_local': '/usr/bin/kinit', 'principal_name': 'hdfs-HDPCA@EXAMPLE.COM', 'user': 'hdfs', 'action': ['execute'], 'hadoop_conf_dir': '/usr/hdp/current/hadoop-client/conf'}\n2016-02-14 22:40:42,495 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-HDPCA@EXAMPLE.COM;'] {'user': 'ambari-qa'}\n2016-02-14 22:40:42,778 - ExecuteHadoop['jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples-2.*.jar wordcount /user/ambari-qa/mapredsmokeinput /user/ambari-qa/mapredsmokeoutput'] {'bin_dir': '/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/var/lib/ambari-agent:/usr/hdp/current/hadoop-client/bin:/usr/hdp/current/hadoop-yarn-client/bin', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'logoutput': True, 'try_sleep': 5, 'tries': 1, 'user': 'ambari-qa'}\n2016-02-14 22:40:42,881 - Execute['hadoop --config /usr/hdp/current/hadoop-client/conf jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples-2.*.jar wordcount /user/ambari-qa/mapredsmokeinput /user/ambari-qa/mapredsmokeoutput'] {'logoutput': True, 'try_sleep': 5, 'environment': {}, 'tries': 1, 'user': 'ambari-qa', 'path': ['/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/var/lib/ambari-agent:/usr/hdp/current/hadoop-client/bin:/usr/hdp/current/hadoop-yarn-client/bin']}\nWARNING: Use \"yarn jar\" to launch YARN applications.\n16/02/14 22:42:50 INFO impl.TimelineClientImpl: Timeline service address: http://lnx1.localdomain.com:8188/ws/v1/timeline/\n16/02/14 22:42:51 INFO client.RMProxy: Connecting to ResourceManager at Lnx1.localdomain.com/192.168.122.40:8050\n16/02/14 22:42:53 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 599 for ambari-qa on ha-hdfs:HDPCA\n16/02/14 22:42:54 INFO security.TokenCache: Got dt for hdfs://HDPCA; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:HDPCA, Ident: (HDFS_DELEGATION_TOKEN token 599 for ambari-qa)\n16/02/14 22:42:54 WARN token.Token: Cannot find class for token kind kms-dt\n16/02/14 22:42:54 INFO security.TokenCache: Got dt for hdfs://HDPCA; Kind: kms-dt, Service: 192.168.0.102:9292, Ident: 00 0f 61 6d 62 61 72 69 2d 71 61 2d 48 44 50 43 41 02 72 6d 00 8a 01 52 e3 06 19 54 8a 01 53 07 12 9d 54 04 02\n16/02/14 22:43:03 INFO input.FileInputFormat: Total input paths to process : 1\n16/02/14 22:43:06 INFO mapreduce.JobSubmitter: number of splits:1\n16/02/14 22:43:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1455503320604_0004\n16/02/14 22:43:10 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:HDPCA, Ident: (HDFS_DELEGATION_TOKEN token 599 for ambari-qa)\n16/02/14 22:43:10 WARN token.Token: Cannot find class for token kind kms-dt\n16/02/14 22:43:10 WARN token.Token: Cannot find class for token kind kms-dt\nKind: kms-dt, Service: 192.168.0.102:9292, Ident: 00 0f 61 6d 62 61 72 69 2d 71 61 2d 48 44 50 43 41 02 72 6d 00 8a 01 52 e3 06 19 54 8a 01 53 07 12 9d 54 04 02\n16/02/14 22:43:19 INFO impl.YarnClientImpl: Application submission is not finished, submitted application application_1455503320604_0004 is still in NEW\n16/02/14 22:43:20 INFO impl.YarnClientImpl: Submitted application application_1455503320604_0004\n16/02/14 22:43:21 INFO mapreduce.Job: The url to track the job: http://Lnx1.localdomain.com:8088/proxy/application_1455503320604_0004/\n16/02/14 22:43:21 INFO mapreduce.Job: Running job: job_1455503320604_0004</pre>","tags":["MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-16 14:01:46.0","id":17278,"title":"wordcount not completed with Yarn exception ...","body":"<p>I am trying to perform a simple wordcount on my single node server (Ubuntu-Hadoop 2.7.2 - not HDP) and as soon as the map reduce process start I receive the below error in the log...</p><p>Sounds this is due to some memory issue but not sure what kind of issue and less sure how to fix it...</p><p>Could someone help?</p><p>thanks!</p><p>******************************</p><pre>2016-02-16 13:24:00,063 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1455610525380_0004_01_000005\n2016-02-16 13:24:00,094 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 14725 for container-id container_1455610525380_0004_01_000001: 117.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used\n2016-02-16 13:24:00,111 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 14955 for container-id container_1455610525380_0004_01_000005: 78.8 MB of 1 GB physical memory used; 746.3 MB of 2.1 GB virtual memory used\n2016-02-16 13:24:01,933 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1455610525380_0004_01_000005 is : 1\n2016-02-16 13:24:01,933 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exception from container-launch with container ID: container_1455610525380_0004_01_000005 and exit code: 1\nExitCodeException exitCode=1: \nat org.apache.hadoop.util.Shell.runCommand(Shell.java:545)\nat org.apache.hadoop.util.Shell.run(Shell.java:456)\nat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)\nat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\n************************************</pre>","tags":["hadoop","yarn-container","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-16 13:42:52.0","id":17265,"title":"i am unable to login in ambari with username:admin and password :admin","body":"<p>i am running sandbox on vmware worsktation . the moment i enter url 127.0.0.1:8080 ,the login page comes where by entering username as admin and password as admin ,it is showing the following message,\n\"Unable to connect to Ambari Server. Confirm Ambari Server is running and you can reach Ambari Server from this machine.\"</p><p>HELP</p>","tags":["login"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-16 13:17:19.0","id":17253,"title":"Error in Beeline","body":"<p>We have set up HDP 2.3.4.0 and over that we have set up Apache Hue successfully. Hue works fine when running any query in Beeswax but we are facing issues while running any query in Beeline. Following error is observed when we are trying to run in any query in beeline prompt after jdbc connectivity.</p><p><strong>Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask (state=08S01,code=1)</strong></p><p>Need your help!</p>","tags":["beeline"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-16 16:57:41.0","id":17338,"title":"Ambari host registration failed - I have updated openssl also, ambari-agent running successfully","body":"<pre>16 Feb 2016 21:57:05,190  INFO [qtp-ambari-agent-92] HostImpl:285 - Received hos                                                                             t registration, host=[hostname=www,fqdn=www.tx.parahost.com,domain=tx.parahost.c                                                                             om,architecture=x86_64,processorcount=4,physicalprocessorcount=4,osname=centos,o                                                                             sversion=7.2.1511,osfamily=redhat,memory=16164224,uptime_hours=3,mounts=(availab                                                                             le=301718120,mountpoint=/,used=12701080,percent=5%,size=314419200,device=/dev/ma                                                                             pper/centos-root,type=xfs)(available=8066400,mountpoint=/dev,used=0,percent=0%,s                                                                             ize=8066400,device=devtmpfs,type=devtmpfs)(available=8082028,mountpoint=/dev/shm                                                                             ,used=84,percent=1%,size=8082112,device=tmpfs,type=tmpfs)(available=8073128,moun                                                                             tpoint=/run,used=8984,percent=1%,size=8082112,device=tmpfs,type=tmpfs)(available                                                                             =9379640,mountpoint=/boot,used=269788,percent=3%,size=10190100,device=/dev/sda1,                                                                             type=ext4)(available=616797016,mountpoint=/home,used=1568932,percent=1%,size=618                                                                             365948,device=/dev/mapper/centos-home,type=xfs)(available=1616408,mountpoint=/ru                                                                             n/user/42,used=16,percent=1%,size=1616424,device=tmpfs,type=tmpfs)(available=161                                                                             6424,mountpoint=/run/user/0,used=0,percent=0%,size=1616424,device=tmpfs,type=tmp                                                                             fs)]\n, registrationTime=1455640025179, agentVersion=2.2.0.0\n16 Feb 2016 21:57:05,271  INFO [qtp-ambari-agent-92] TopologyManager:617 - Topol                                                                             ogyManager.replayRequests: Entering\n16 Feb 2016 21:57:05,271  INFO [qtp-ambari-agent-92] TopologyManager:313 - Topol                                                                             ogyManager.onHostRegistered: Entering\n16 Feb 2016 21:57:05,271  INFO [qtp-ambari-agent-92] TopologyManager:374 - Topol                                                                             ogyManager: Queueing available host www.tx.parahost.com\n16 Feb 2016 21:57:05,291  INFO [qtp-ambari-agent-92] HeartBeatHandler:994 - Reco                                                                             very configuration set to RecoveryConfig{, type=AUTO_START, maxCount=6, windowIn                                                                             Minutes=60, retryGap=5, maxLifetimeCount=1024, disabledComponents=, enabledCompo                                                                             nents=METRICS_COLLECTOR}\n16 Feb 2016 22:09:16,232  WARN [qtp-ambari-agent-106] SecurityFilter:103 - Reque                                                                             st https://localhost:8440/ca doesn't match any pattern.\n16 Feb 2016 22:09:16,233  WARN [qtp-ambari-agent-106] SecurityFilter:62 - This r                                                                             equest is not allowed on this port: https://localhost:8440/ca\n16 Feb 2016 22:09:19,822  INFO [qtp-ambari-agent-106] HeartBeatHandler:927 - age                                                                             ntOsType = centos7\n16 Feb 2016 22:09:19,825  INFO [qtp-ambari-agent-106] HostImpl:285 - Received ho                                                                             st registration, host=[hostname=www,fqdn=www.se.parahost.com,domain=se.parahost.                                                                             com,architecture=x86_64,processorcount=4,physicalprocessorcount=4,osname=centos,                                                                             osversion=7.2.1511,osfamily=redhat,memory=16164224,uptime_hours=3,mounts=(availa                                                                             ble=301717984,mountpoint=/,used=12701216,percent=5%,size=314419200,device=/dev/m                                                                             apper/centos-root,type=xfs)(available=8066400,mountpoint=/dev,used=0,percent=0%,                                                                             size=8066400,device=devtmpfs,type=devtmpfs)(available=8082028,mountpoint=/dev/sh                                                                             m,used=84,percent=1%,size=8082112,device=tmpfs,type=tmpfs)(available=8073128,mou                                                                             ntpoint=/run,used=8984,percent=1%,size=8082112,device=tmpfs,type=tmpfs)(availabl                                                                             e=9379640,mountpoint=/boot,used=269788,percent=3%,size=10190100,device=/dev/sda1                                                                             ,type=ext4)(available=616797016,mountpoint=/home,used=1568932,percent=1%,size=61                                                                             8365948,device=/dev/mapper/centos-home,type=xfs)(available=1616408,mountpoint=/r                                                                             un/user/42,used=16,percent=1%,size=1616424,device=tmpfs,type=tmpfs)(available=16                                                                             16424,mountpoint=/run/user/0,used=0,percent=0%,size=1616424,device=tmpfs,type=tm                                                                             pfs)]\nI have the above lines in my log file.\nAny help will be appreciated?</pre>","tags":["Ambari","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-19 08:00:10.0","id":18094,"title":"Looking for a better explanation for \"orc.row.index.stride\" property in ORC","body":"<p>The default value is set to 10,000 and should be &gt; 100, as per the docs.</p><p>How should this value be changed or altered? Need some guidance.</p><p>If I have a large table of billion rows should we increase the value? Will this be affected by?</p><p>I am assuming also that the \"orc.bloom.filter.columns\" will be the list of columns on which the indexes will be created?</p>","tags":["orc","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-19 15:02:57.0","id":18226,"title":"Zookeeper Smoke test failing when on the 9th step of Install, start and test ambari setup","body":"<p>I get the following screen while installing because of the error</p><p><img src=\"/storage/attachments/2264-1583a432aa7fcefcb18894f98d4a593c26ff45e3efdcd72b80.png\"></p><p>The installation has not completed because of the zookeeper failing to pass the smoke test thus halting the processes further and giving 100% complete and with warning error on the installation page  I have attached the <a href=\"/storage/attachments/2262-zookeeper.txt\">zookeeper.txt</a> file which shows the error. </p>","tags":["installation","Ambari","hadoop"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-22 07:46:20.0","id":18617,"title":"I am not able to download sandbox 2.3.2 as automatically cancelled after download 2 GB","body":"","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-22 12:10:54.0","id":18656,"title":"How to get specified datatype in union type in hive???","body":"<p>I have column in hive table like </p><pre>UNIONTYPE&lt;int, double, array&lt;string&gt;, struct&lt;a:int,b:string&gt;</pre><p>This one will results when select that column like following.</p><p>1)How to get particular tag/column type eg: struct (tag no: 3)</p><p>2)If the selected type is struct ,then how to access fields(filelds of struct) in it</p><pre>{0:1}\n{1:2.0}\n{2:[\"three\",\"four\"]}\n{3:{\"a\":5,\"b\":\"five\"}}\n{2:[\"six\",\"seven\"]}\n{3:{\"a\":8,\"b\":\"eight\"}}\n{0:9}\n{1:10.0}\n</pre>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-22 14:31:30.0","id":18681,"title":"Apache Phoenix on Opentsdb","body":"<p>Hi, </p><p>is it possible to connect Opentsdb on top of Phoenix ? </p>","tags":["opentsdb","Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-22 15:19:43.0","id":18698,"title":"Ranger policy sync doesn't work in some cases","body":"<p>If I add a new policy in Ranger for HDFS, sometimes it doesn't sync. Other times it will sync properly. If I delete my new policy, sync starts working again. </p><p>I am checking timestamps on policies in policy sync directory.</p>","tags":["ambari-ldap-sync","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-24 01:44:15.0","id":19054,"title":"hiveserver2 alway shut down","body":"<p>The hiveserver2 service alway shut down after i start it. The errors follows:</p><pre>2016-02-24 09:31:57,949 INFO  [main]: service.AbstractService (AbstractService.java:stop(125)) - Service:CLIService is stopped. \n2016-02-24 09:31:57,950 INFO  [main]: service.AbstractService (AbstractService.java:stop(125)) - Service:HiveServer2 is stopped.\n2016-02-24 09:31:57,965 INFO  [main]: zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x3530737a5b706d0 closed \n2016-02-24 09:31:57,965 INFO  [main-EventThread]: zookeeper.ClientCnxn (ClientCnxn.java:run(524)) - EventThread shut down \n2016-02-24 09:31:57,965 INFO  [main]: server.HiveServer2 (HiveServer2.java:removeServerInstanceFromZooKeeper(279)) - Server instance removed from ZooKeeper. \n2016-02-24 09:31:57,967 WARN  [main]: server.HiveServer2 (HiveServer2.java:startHiveServer2(376)) - Error starting HiveServer2 on attempt 1, will retry in 60 seconds\njava.lang.NoSuchMethodError: org.apache.curator.utils.PathUtils.validatePath(Ljava/lang/String;)Ljava/lang/String;\n        at org.apache.curator.framework.recipes.nodes.PersistentEphemeralNode.&lt;init&gt;(PersistentEphemeralNode.java:194)\n        at org.apache.hive.service.server.HiveServer2.addServerInstanceToZooKeeper(HiveServer2.java:194)\n        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)\n        at org.apache.hive.service.server.HiveServer2.access$700(HiveServer2.java:74)\n        at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:588)\n        at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:461)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136) \n2016-02-24 09:32:09,219 INFO  [HiveServer2-Handler-Pool: Thread-53]: thrift.ThriftCLIService (ThriftCLIService.java:OpenSession(294)) - Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8 \n2016-02-24 09:32:09,266 INFO  [HiveServer2-Handler-Pool: Thread-53]: session.SessionState (SessionState.java:createPath(641)) - Created local directory: /tmp/6cb7c752-3742-4a66-8f5a-ecb87abd78b9_resources\n2016-02-24 09:32:09,282 INFO  [HiveServer2-Handler-Pool: Thread-53]: session.SessionState (SessionState.java:createPath(641)) - Created HDFS directory: /tmp/hive/hive/6cb7c752-3742-4a66-8f5a-ecb87abd78b9\n2016-02-24 09:32:09,284 INFO  [HiveServer2-Handler-Pool: Thread-53]: session.SessionState (SessionState.java:createPath(641)) - Created local directory: /tmp/hive/6cb7c752-3742-4a66-8f5a-ecb87abd78b9 \n2016-02-24 09:32:09,290 INFO  [HiveServer2-Handler-Pool: Thread-53]: session.SessionState (SessionState.java:createPath(641)) - Created HDFS directory: /tmp/hive/hive/6cb7c752-3742-4a66-8f5a-ecb87abd78b9/_tmp_space.db \n2016-02-24 09:32:09,294 WARN  [HiveServer2-Handler-Pool: Thread-53]: session.HiveSessionImpl (HiveSessionImpl.java:setOperationLogSessionDir(230)) - Unable to create operation log session directory: /tmp/hive/operation_logs/6cb7c752-3742-4a66-8f5a-ecb87abd78b9 \n2016-02-24 09:32:09,569 INFO  [HiveServer2-Handler-Pool: Thread-53]: service.CompositeService (SessionManager.java:closeSession(300)) - This instance of HiveServer2 has been removed from the list of server instances available for dynamic service discovery. The last client session has ended - will shutdown now. \n2016-02-24 09:32:09,570 INFO  [Thread-23]: server.HiveServer2 (HiveServer2.java:stop(305)) - Shutting down HiveServer2\n2016-02-24 09:32:09,570 INFO  [Thread-23]: server.HiveServer2 (HiveServer2.java:removeServerInstanceFromZooKeeper(279)) - Server instance removed from ZooKeeper.</pre>","tags":["hadoop","hdp-2.3.4","hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-24 12:16:47.0","id":19192,"title":"How to Transform Hive Table using SerDe","body":"<p>I have a table created in Hive with the following struture:</p><pre><code>Table 1:\nField_A String,\nField_B String,\nField_C String\n</code></pre><p>Using SerDe how can I get the following schema:</p><pre><code>Table 1:\nField_A Int,\nField_B Date,\nField_C TimeStamp\n</code></pre><p>I'm getting confused about this question, because I don't know If I need to create a SerDe Class in Java to achieve this...</p><p>Thanks!</p>","tags":["Hive","hive-serde"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-24 11:03:50.0","id":19160,"title":"How to set up the popular EclipseBIRT to develop JavaEE in HDP","body":"","tags":["hortonwork","java"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-24 15:23:45.0","id":19272,"title":"Suggested Maximum auth_to_local Mappings","body":"<p>What is the suggested maximum number of hadoop.security.auth_to_local mappings in a cluster? Would several thousand mappings be unreal?</p>","tags":["hdfs-permissions","security","hdfs-policies","kerberos"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-26 01:24:40.0","id":19614,"title":"Sqoop connection to SAP HANA giving SQLException in nextKeyValue","body":"<p>Sqoop to fetch data from SAP HANA is giving following error.</p><pre>Error: java.io.IOException: SQLException in nextKeyValue\n        at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)</pre><p>Here is my sqoop statement</p><pre>sqoop import --connect \"jdbc:sap://&lt;server&gt;:30015\" --driver \"com.sap.db.jdbc.Driver\" --username admin --password **** --table ABC.TABLE1 -m 1</pre>","tags":["import","Sqoop","sap-hana"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-26 01:49:56.0","id":19627,"title":"How to create ambari alert?","body":"<p>How does one go about creating a custom ambari alert?  I am trying to find turtoral or any support documetation on how to?</p>","tags":["How-To/Tutorial","Ambari","ambari-alerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-26 05:31:35.0","id":19659,"title":"Information about  Namenode and Datanode on Isilon node","body":"<p>Hi , </p><p>I just configured setting up a cluster on Isilon Node (which will have the datanode and namenode )+ Other Compute Nodes(which will have other master services) with HDP 2.2 with Ambari 2.1 and everything is running fine. I also performed smoke tests and ran sample map-reduce Jobs. </p><p>I wanted to know more in terms of storage in Isilon nodes . How can I access the data stored in the Datanodes and the Metadata of the Namenode ? Isilon nodes have the OneFS File System . How is it different from HDFS ?  Also , where can I see the Namenode and the Datanode logs ?   </p><p>Mangesh </p>","tags":["query","hdp-2.2.0","Ambari"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-26 11:02:04.0","id":19691,"title":"how to select the multiple hive partitioned based on where conditions","body":"<p>Hi,</p><p>On the staging database we are getting updated data from relational database. As HDFS/Hive supports append only. So we need to delete the partitions where data is modified and replace with updated data partitions.</p><p>Lets say we have a T_UpdatedData table which gets populated by Sqoop per day. If on any data we have updated data from multiple partitions, we need to backup, delete those partitions from the T_ProductionData.  Partitions are created on Year, Month, Account columns. Backup should be saved on the T_HistoryData</p><p>How to can select the different partitions from the T_UpdatedData & copy those partitions from T_ProductionData to T_HistoryData ?</p><p>Inserting dynamic partitions can be done with following syntax.</p><p>INSERT OVERWRITE TABLE tablename1 PARTITION (partcol1=val1, partcol2=val2...)</p><p>Two ways to select the distinct partition data could be</p><p>1. SELECT * FROM T_ProductionData WHERE (year,month,account) IN (SELECT distinct year, month, account FROM T_UpdatedData)   -- Supported by Oracle.</p><p>2. Using joins. Using joins could be more costly as it will try to perform join in all T_ProductionData.</p><p>What is the best practice to backup the partitions and insert the modified partitions ?</p><p>Thanks</p>","tags":["Hive","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-26 03:09:57.0","id":19643,"title":"Nifi with Isilon?","body":"<p>Would it ever make sense to put any of the NiFi Cluster repositories (Flow File, Content, Provenance) in a NAS like Isilon?  I know disk can be the bottleneck but you also want these repositories on drives with a strong RAID, thus my question.</p>","tags":["Nifi","isilon"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-26 12:05:46.0","id":19694,"title":"Help with exception from HDFS balancer","body":"<p>We recently tried to run the HDFS balancer for the first time. </p><p>(Somehow we've been using HDP for almost 2 years and never knew that we should be doing this)</p><p>After about an hour, it showed \"5.56 GB moved / 12.72 TB left / 40 GB being processed\". </p><p>Now (10 hours later), it still says the same thing. \n\nDoes anyone know what the issue or solution here is?</p><p>We're on 2.2.8 running in HA.\n\nThere's nothing in stderr. </p><p>stdout is below. \n\n[balancer] 16/02/24 21:11:51 WARN balancer.Dispatcher: Failed to move blk_1083404960_9672884 with size=134217728 from 10.22.4.44:50010:DISK to 10.22.6.22:50010:DISK through 10.22.4.46:50010: block move is failed: Not able to receive block 1083404960 from /10.22.4.64:38809 because threads quota is exceeded. \n[balancer] 16/02/24 21:11:51 INFO balancer.Dispatcher: DDatanode:10.22.4.46:50010 activateDelay 10.0 seconds \n16/02/24 21:11:51 WARN balancer.Dispatcher: Failed to move blk_1083404871_9672795 with size=134217728 from 10.22.4.44:50010:DISK to 10.22.6.22:50010:DISK through 10.22.4.44:50010: block move is failed: Not able to receive block 1083404871 from /10.22.4.64:38810 because threads quota is exceeded. \n16/02/24 21:11:51 INFO balancer.Dispatcher: DDatanode:10.22.6.22:50010 activateDelay 10.0 seconds \n[balancer] 16/02/24 21:11:51 INFO balancer.Dispatcher: DDatanode:10.22.4.44:50010 activateDelay 10.0 seconds \n16/02/24 21:11:51 INFO balancer.Dispatcher: DDatanode:10.22.6.22:50010 activateDelay 10.0 seconds \n[balancer] 16/02/24 21:11:56 INFO balancer.Dispatcher: Failed to find a pending move 5 times. Skipping 10.22.4.46:50010:DISK \n[balancer] 16/02/24 21:11:56 INFO balancer.Dispatcher: Failed to find a pending move 5 times. Skipping 10.22.4.44:50010:DISK \n[balancer] 16/02/24 21:11:57 INFO balancer.Dispatcher: Failed to find a pending move 5 times. Skipping 10.22.4.43:50010:DISK \n[balancer] 16/02/24 21:11:57 INFO balancer.Dispatcher: Failed to find a pending move 5 times. Skipping 10.22.4.41:50010:DISK</p>","tags":["balancer","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-26 19:03:57.0","id":19785,"title":"Help after moving Ambari metrics server","body":"<p>I moved our ambari metrics server from a data node to an edge node.</p><p>I followed all of the directions outlined here: </p><p>http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_ambari_reference_guide/content/ch_moving_the_ambari_metrics_collector.html</p><p>Now, ambari metrics kind of 'works', but not completely.</p><p>The main Ambari dashboard looks good - all of the metrics are populated.</p><p>But if I drill into HDFS, YARN, HBase, etc - only some of the metrics are there. </p><p>Many say \"No data available\".</p><p>I tried re-starting ambari server, but no luck. </p><p>I tried ctrl+shift+r to force a clean refresh of the GUI, and that didn't help either.</p><p>I know that all the nodes are pointing to the right place in /etc/ambari-metrics-monitor/conf/metric_monitor.ini</p><p>Where can I look to figure this out?</p>","tags":["notify-docs","ambari-metrics","Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-27 21:28:43.0","id":19924,"title":"Database restriction in Ambari","body":"<p>How to restrict database access from user to another. Currently one user is able to see tables created by others. What's the best way to restrict this access in Ambari views.</p>","tags":["database"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-28 13:19:42.0","id":19984,"title":"How spark works to analyze huge databases","body":"<p>Hi,</p><p>Im studing spark, because I read some studies about it and it seems amazing to process large volumes of data. So I was thinking to expriment this, generating 100gb of data with some benchmark like tpc and execute the queries with spark using 2 nodes, but Im with some doubts how to do this. </p><p>I need to install hadoop two hadoop nodes to store the tpc tables? And then execute the queries with spark against hdfs? But how we can create the tpc schema and store the tables in hadoop hdfs?Is it possible? Or it´s not necessary install hadoop and we need to use hive instead? I reading some articles about this but but I m getting a bit confused. Thanks for your attention!</p>","tags":["Spark","HDFS","hadoop"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-28 22:36:56.0","id":20038,"title":"Hive View not populating databases","body":"<p><a rel=\"user\" href=\"/users/140/nsabharwal.html\" nodeid=\"140\">@Neeraj Sabharwal</a></p><p>Not sure what happened but all of a sudden, hive view is not showing any databases. Earlier Hive shell wasn't working from console but then I created a folder /user/root, then I could see the Hive prompt. On hive prompt I could see all the databases and tables but not via Ambari view (screen shot). </p><p><img src=\"/storage/attachments/2449-hive.jpg\"></p><pre>hive&gt; show tables punjtest;\nOK\nTime taken: 0.278 seconds\nhive&gt; show tables default;\nOK\nTime taken: 0.275 seconds\nhive&gt; show tables;\nOK\navg_mileage\ndrivermileage\nfinalresults\ngeolocation\ngeolocation_stage\nnrel_onshore_twh\nriskfactor\ntruck_mileage\ntrucks\ntrucks_stage\nTime taken: 0.273 seconds, Fetched: 10 row(s)\nhive&gt; show databases;\nOK\narvindtest\ndbforall\ndefault\nkemr_fst\nkemrfst\npunj\npunjtest\nTime taken: 0.025 seconds, Fetched: 7 row(s)\nhive&gt; show tables from default;\nOK\navg_mileage\ndrivermileage\nfinalresults\ngeolocation\ngeolocation_stage\nnrel_onshore_twh\nriskfactor\ntruck_mileage\ntrucks\ntrucks_stage\nTime taken: 0.377 seconds, Fetched: 10 row(s)\nhive&gt; show tables from punjtest;\nOK\nbatting_data\nmaster_data\nTime taken: 0.401 seconds, Fetched: 2 row(s)\nhive&gt;</pre>","tags":["Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-28 21:02:15.0","id":20031,"title":"Spark 1.5.1 Tech Preview Broke my Sandbox","body":"<p>I began with a working Centos HDP 2.3.2 Sandbox running in VirtualBox on a Win 10 PC, and installed the Apache Spark 1.5.1 Technical Preview.  I followed the steps<a href=\"http://hortonworks.com/hadoop-tutorial/apache-spark-1-5-1-technical-preview-with-hdp-2-3/\"> here</a>, and was able to reproduce the results.</p><p>However, the next time I rebooted my sandbox, HDFS failed to run.  It will start but all of the processes die quickly, and I have blocks under-replicated errors.  Most Ambari dashboard widgets show no data, and most processes are stopped with red (but unhelpful) warning icons next to them. Start All produces the same result.  The Ambari widgets light up for a few seconds then die.  If I try to just start HDFS, rather than all services, the same thing happens.</p><p>I'm relatively new to this and not sure where to even start to debug this. </p><p>Any pointers? </p><p>Thanks.</p>","tags":["Sandbox","Spark"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-01 20:52:43.0","id":20618,"title":"Ambari, set  \"auto.offset.reset\" property for kafka?","body":"<p>Hi,</p><p>I wanted to set  \"auto.offset.reset\" to smallest in kafka consumer, as i my kafka consumer (spark-scala program) is not reading data from beginning currently.</p><p><a href=\"https://www.google.com/url?q=https%3A%2F%2Fkafka.apache.org%2F08%2Fconfiguration.html&sa=D&sntz=1&usg=AFQjCNFwsc-UGuOTYf-wtykogvXfKeWq_A\">https://kafka.apache.org/08/configuration.html</a></p><p>* smallest : automatically reset the offset to the smallest offset</p><p>\nThanks & Regards,</p><p>Vinti</p>","tags":["Spark","spark-streaming","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-01 20:48:39.0","id":20611,"title":"Upgrade failed, reverted, now ambari-server won't start","body":"<p>I'm seeing this in the log</p><pre>[root@bodcdevvhdp104 tmp]# !cat\ncat /var/log/ambari-server/ambari-server.out \n[EL Warning]: metadata: 2016-03-01 15:47:35.985--ServerSession(1327476372)--The reference column name [resource_type_id] mapped on the element [field permissions] does not correspond to a valid id or basic field/column on the mapping reference. Will use referenced column name as provided.\n[EL Info]: 2016-03-01 15:47:38.254--ServerSession(1327476372)--EclipseLink, version: Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd\n[EL Info]: connection: 2016-03-01 15:47:38.597--ServerSession(1327476372)--file:/usr/lib/ambari-server/ambari-server-2.1.0.1470.jar_ambari-server_url=jdbc:postgresql://localhost/ambari_user=ambari login successful\n[EL Warning]: 2016-03-01 15:47:41.688--ServerSession(1327476372)--Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException\nInternal Exception: org.postgresql.util.PSQLException: ERROR: column \"upgrade_package\" does not exist\n  Position: 53\nError Code: 0\nCall: SELECT repo_version_id, display_name, repositories, upgrade_package, version, stack_id FROM repo_version WHERE (repo_version_id = ?)\nbind =&gt; [1 parameter bound]\nQuery: ReadObjectQuery(name=\"repositoryVersion\" referenceClass=RepositoryVersionEntity )\n[root@bodcdevvhdp104 tmp]#</pre>","tags":["ambari-2.1.2"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-02 02:55:26.0","id":20680,"title":"Kafka Version on HDP-2.3.2?","body":"<p>Hi ALL!</p><p>We upgraded our cluster from HDP-2.2.0 to HDP-2.3.2</p><p>From HDP site, HDP-2.3.2 includes Kafka 0.8.2</p><p>But from our ambari-2.2 (hdp 2.3.2), kafka version show 0.9.0.2.</p><p>When I check kafka lib folder from broker kafka lib show 0.8.2 kafka version.</p><p>Which config setup should I follow, 0.9.x or 0.8.2?</p><p>Thank you!</p>","tags":["Kafka","upgrade","hdp-2.3.2"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-02 07:14:06.0","id":20677,"title":"Name Node UI not accessible","body":"<p>Namenode UI not accessible but Ambari is accessible.</p><p>In the server system localhost.localdomain:50070 is running and able to access. But not accessible within the Lan from the other system.</p><p>See the screen shots as follows.</p><p><img src=\"/storage/attachments/2500-ambari-running.jpg\"></p><p><img src=\"/storage/attachments/2521-name-and-data-node-config.jpg\"></p><p>Any Idea how to resolve this?</p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-02 09:40:01.0","id":20738,"title":"Can we upgrade Spark in HDP-2.3.2","body":"<p>HI,</p><p>Recently we install HDP-2.3.2.0-2950, in that we can see Spark-1.3.1.2.3</p><p>Can we upgrade Spark to 1.6 in same Hadoop version&gt; And How?</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-02 10:48:11.0","id":20732,"title":"cant install Spark 1.6 – with HDP 2.4","body":"<p>Hi all, i got issue when i want install spark 1.6 on hdp 2.4</p><p>http://hortonworks.com/hadoop/spark/#section_6</p><p>at that link it said</p><p>\"<strong>Download the Spark 1.6 RPM repository:\"</strong></p><p><strong>wget -nv http://</strong><strong>private-repo-1.hortonworks.staging.wpengine.com</strong><strong>/HDP/centos6/2.x/updates/2.3.4.1-10/hdp.repo -O /etc/yum.repos.d/HDP-TP.repo</strong></p><p><strong>\n</strong></p><p><strong>but i cant resolve the host address </strong><strong>private-repo-1.hortonworks.staging.wpengine.com</strong></p><p><strong>\n</strong></p><p><strong>\n</strong></p><p>wget: unable to resolve host address “private-repo-1.hortonworks.staging.wpengine.com”</p><p>anyone can help me ?</p>","tags":["upgrade"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-02 14:13:11.0","id":20765,"title":"RPMs for HDP 2.4","body":"<p>Where can I go to get the RPMs to install HDP 2.4 on CentOS 7. While I like the idea of the Sandbox (and have used it for previous releases), I would like to build from RPMs myself. My intent is to build a multi-node VM environment -- one VM for management and one (or more) VMs for datanodes. </p>","tags":["hdp-3.0.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-03 14:41:53.0","id":21015,"title":"Zeppelin security","body":"<p>How can I secure access to Zeppelin. Also it stores name and password in interpreter and is visible to all. How can I protect that information.</p><p>Thanks</p>","tags":["zeppelin-notebook"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-03 11:54:23.0","id":20982,"title":"is it possible that hdfs utilization could be different in two different cluster.","body":"<p>Actually I just copied some data from one cluster A to another cluster B through distcp and used for hive.It copied successfully as well because I have verified row counts and number of tables and all are same in both cluster. </p><p>But when I do du -h to that dir it show diffrence in size like below :</p><p>[aman@clustera ~]$  hadoop fs -du -h -s /sample/data/datasets/files/sitecatdatapipeline/</p><p>60.1 G  /sample/data/datasets/files/sitecatdatapipeline</p><p>[aman@clusterb]$ hadoop fs -du -h -s /sample/data/datasets/files/sitecatdatapipeline/</p><p>33.0 G  /sample/data/datasets/files/sitecatdatapipeline</p><p>So I just wanted to know is there any possibility of such kind of issue. </p><p>Also note that there is no compression configured and used. </p>","tags":["clustering"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-03 15:52:55.0","id":21035,"title":"Questions regarding Phoenix with bulk CSV data loading and possible index population?","body":"<p>I have a few questions regarding Phoenix and CSV data loading.  Currently on HDP 2.3.4.</p><p>1.  Does \"Bulk CSV Data Loading\" support local index creation?</p><p>2.  Does \"Asynchronous Index Population\" support local index creation?</p><p>OR</p><p>3.  Is there any way to create a local index manually rather than automatic as when we are doing bulk upload in existing table, automatic local index becomes inconsistent. FYI - We are only using hbase bulk upload to upload data into hbase.</p>","tags":["bulk","Phoenix","csv"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-03 18:03:13.0","id":21057,"title":"curl with hive on secured cluster","body":"<p>How can I run curl with hive command in secured cluster?</p><pre>curl -s -d execute=\"select+*+from+pokes;\" \\\n       -d statusdir=\"pokes.output\" \\\n       'http://localhost:50111/templeton/v1/hive?user.name=ekoifman'</pre><p>in user.name - I tried to pass Hive Principal and keytab didnt work and tried with user principal didnt work. can you please provide some example?</p>","tags":["webhcat","hiveserver2","curl"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-04 02:29:37.0","id":21124,"title":"Kafka HDP-2.3.2 Broker Config for Listeners=?","body":"<p>Hi ALL!</p><p>After upgrading to HDP-2.3.2, we're having problems on Kafka.</p><p>I have 3 brokers. My question is, what should I put on Broker Config \"Listeners=?\"</p><p>If I put PLAINTEXT://0.0.0.0:6667 - hostname is not registered to ZK.\nIf I put PLAINTEXT://host3:6667,PLAINTEXT://host2:6667,PLAINTEXT://host1:6667 - Kafka won't start because it wont access port 6667 being used by other brokers (conflict).</p><p>The problem, I can't produce and consume messages from kafka. I can create topics and list them okay.</p>","tags":["Kafka"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-04 10:18:50.0","id":21150,"title":"When installing H2O told me my YARN memory exceed, and I went to Ambari to adjust, it didn't work...","body":"<p>I have received the following error when trying to install H2O to my single node Hortonworks sandbox. My input is \"sudo -u hdfs hadoop jar h2odriver.jar -nodes 1 -mapperXmx 2g -output hdfsOutputH2O2g2\"</p><p>\nQueue 'default' approximate utilization: 0.2 / 2.2 GB used, 1 / 8 vcores used</p><p>----------------------------------------------------------------------</p><p>ERROR:   Job container memory request (2.2 GB) does not fit on any YARN cluster node\nERROR:   Job memory request (2.2 GB) exceeds available YARN cluster memory (2.2 GB)\nWARNING: Job memory request (2.2 GB) exceeds queue available memory capacity (2.0 GB)</p><p>----------------------------------------------------------------------</p><p>For YARN users, logs command is 'yarn logs -applicationId application_1456805979592_0007'</p><p>[analytics@sandbox h2o-3.8.0.6-hdp2.3]$</p><p>----------------------------------------------------------------------</p><p>I have tried, but there seem no way I can modified the YARN value. Ambari asked me to restart, but never able to restart it.\n</p><p><a href=\"http://i.imgur.com/8tSOUL5.jpg\">http://i.imgur.com/8tSOUL5.jpg</a></p><p>Please kindly let me know if you have any idea</p><p><img src=\"/storage/attachments/2595-clipboard03.jpg\"></p>","tags":["Ambari"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-04 02:45:41.0","id":21135,"title":"Creating Ranger Child Policy","body":"<p>How do I create a child policy using delegated admin policy?</p><p>Ranger Guide on Delegated Admin - When a policy is assigned to a user or a group of users those users become the delegated admin.The delegated admin can update, delete the policies. <strong>It can also create child policies based on the original policy (base policy).</strong></p><p><strong> \n</strong></p><p>I have viewed the following and the instructions were not clear:</p><p><a href=\"https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5+-+User+Guide\">https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5+-+User+Guide</a></p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Ranger_User_Guide\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Ranger_User_Guide</a></p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-04 05:02:50.0","id":21143,"title":"With Oracle 12c upgrade , Is there a dependency in Ambari/HDP so we have to install Oracle jdbc jars in all slave node","body":"<p>Is there a dependency in Ambari/HDP so we have to install Oracle jdbc jars in all slave node . </p><p>Hadoop uses jdbc jars coming with instant client to connect Oracle backend databases, and as Hadoop administrators use sqlplus to verify the database and do trouble-shooting.</p>","tags":["Ambari","oracle"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-04 06:54:31.0","id":21175,"title":"h2o hadoop nodes cluster","body":"<p>Hi:\nI am trying to up 5 h2o hadoop nodes, bu i am receiving this error, but with just 1 node is working\n[</p><pre>[root@lnxbig01 hadoop]# hadoop jar h2odriver_hdp2.1.jar water.hadoop.h2odriver -libjars ../h2o.jar -mapperXmx 2g -nodes 2 -output /tmp/h2o\nWARNING: Use \"yarn jar\" to launch YARN applications.\nDetermining driver host interface for mapper-&gt;driver callback...\n    [Possible callback IP address: 10.1.246.15]\n    [Possible callback IP address: 127.0.0.1]\nUsing mapper-&gt;driver callback IP address and port: 10.1.246.15:36832\n(You can override these with -driverif and -driverport.)\nDriver program compiled with MapReduce V1 (Classic)\nMemory Settings:\n    mapred.child.java.opts:      -Xms2g -Xmx2g\n    mapred.map.child.java.opts:  -Xms2g -Xmx2g\n    Extra memory percent:        10\n    mapreduce.map.memory.mb:     2252\n16/03/04 07:53:32 INFO impl.TimelineClientImpl: Timeline service address: http://lnxbig06.cajarural.gcr:8188/ws/v1/timeline/\n16/03/04 07:53:32 INFO client.RMProxy: Connecting to ResourceManager at lnxbig05.cajarural.gcr/10.1.246.19:8050\n16/03/04 07:53:34 INFO mapreduce.JobSubmitter: number of splits:2\n16/03/04 07:53:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1456130415890_1037\n16/03/04 07:53:34 INFO impl.YarnClientImpl: Submitted application application_1456130415890_1037\n16/03/04 07:53:34 INFO mapreduce.Job: The url to track the job: http://lnxbig05.cajarural.gcr:8088/proxy/application_1456130415890_1037/\nJob name 'H2O_64985' submitted\nJobTracker job ID is 'job_1456130415890_1037'\nFor YARN users, logs command is 'yarn logs -applicationId application_1456130415890_1037'\nWaiting for H2O cluster to come up...\nH2O node 10.1.246.18:54323 requested flatfile\nH2O node 10.1.246.15:54321 requested flatfile\nSending flatfiles to nodes...\n    [Sending flatfile to node 10.1.246.18:54323]\n    [Sending flatfile to node 10.1.246.15:54321]\nH2O node 10.1.246.18:54323 reports H2O cluster size 1\nH2O node 10.1.246.15:54321 reports H2O cluster size 1\nH2O node 10.1.246.15:54321 on host 10.1.246.15 exited with status -1\nERROR: At least one node failed to come up during cluster formation\nERROR: H2O cluster failed to come up\nAttempting to clean up hadoop job...\n16/03/04 07:53:51 INFO impl.YarnClientImpl: Killed application application_1456130415890_1037\nKilled.\n\n\n</pre><p>Please any suggestions why with more than one node it fail??</p><p>Many thanks</p>","tags":["hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-04 10:35:18.0","id":21217,"title":"Dynamic oozie action name?","body":"<p>Hi,</p><p>Is it possible to pass oozie action name from property file instead of giving it in xml itself? Let me know if anyone tried this?</p>","tags":["Oozie"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-03-04 12:22:40.0","id":21228,"title":"Restrict the command.","body":"<p>I Have one cluster and I want to restrict the user to run rm command to delete the data from directory and also want to restrict the user to drop the table from hive . Any one have idea?</p>","tags":["hdp-2.3.0"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-04 12:47:36.0","id":21233,"title":"how to check on which host HiveServer2 is running?","body":"<p>I need to apply authorization using Sentry. I need to create the local groups on the host on which HiveServer2 is running. How to know on which host HiveServer2 running ??</p>","tags":["Hive","hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-04 15:36:25.0","id":21259,"title":"Ranger Usersync ascii codec problem","body":"<p>Hi everyone! </p><p>My Ranger Usersync does not working, it fails during start with error:</p><p><em>ascii codec cant encode characters in position 0-9: ordinal not in ranger (128)</em></p><p>Has anyone faced this problem?</p><p>Thanks</p>","tags":["ranger-0.5.0","ranger-usersync"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-04 21:42:53.0","id":21309,"title":"Install HPD 2.4.0.0 on windows 2012 R2","body":"<p>\n\tI  keep getting errors installing HDP 2.4.0.0 on windows 2012 . I have followed the install guide .http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0-Win/bk_QuickStart_HDPWin/content/inst_HDPWin.html</p><pre>CustomAction GUI returned actual error code -1073740771 (note this may not be 100% accurate if translation happened inside sandbox)\nMSI (c) (9C:84) [15:42:44:935]: Note: 1: 1722 2: GUI 3: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\MSI230D.tmp 4: run \"C:\\installs\\hdp-2.4.0.0\\hdp-2.4.0.0\\\" \nMSI (c) (9C:84) [15:42:44:935]: Note: 1: 2205 2:  3: Error \nMSI (c) (9C:84) [15:42:44:935]: Note: 1: 2228 2:  3: Error 4: SELECT `Message` FROM `Error` WHERE `Error` = 1722 \nMSI (c) (9C:90) [15:42:44:950]: Font created.  Charset: Req=0, Ret=0, Font: Req=MS Shell Dlg, Ret=MS Shell Dlg\nMSI (c) (9C:84) [15:42:47:279]: Note: 1: 2205 2:  3: Error \nMSI (c) (9C:84) [15:42:47:279]: Note: 1: 2228 2:  3: Error 4: SELECT `Message` FROM `Error` WHERE `Error` = 1709 \nMSI (c) (9C:84) [15:42:47:279]: Product: Hortonworks Data Platform 2.4.0.0 for Windows -- Error 1722. There is a problem with this Windows Installer package. A program run as part of the setup did not finish as expected. Contact your support personnel or package vendor.  Action GUI, location: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\MSI230D.tmp, command: run \"C:\\installs\\hdp-2.4.0.0\\hdp-2.4.0.0\\\" \nAction ended 15:42:47: GUI. Return value 3.\nMSI (c) (9C:84) [15:42:47:279]: Doing action: INSTFAILED\nMSI (c) (9C:84) [15:42:47:279]: Note: 1: 2205 2:  3: ActionText \nAction start 15:42:47: INSTFAILED.\nMSI (c) (9C:90) [15:42:47:279]: Font created.  Charset: Req=0, Ret=0, Font: Req=MS Shell Dlg, Ret=MS Shell Dlg\nMSI (c) (9C:84) [15:42:48:247]: Note: 1: 2205 2:  3: Error \nMSI (c) (9C:84) [15:42:48:247]: Note: 1: 2228 2:  3: Error 4: SELECT `Message` FROM `Error` WHERE `Error` = 1709 \nMSI (c) (9C:84) [15:42:48:247]: Product: Hortonworks Data Platform 2.4.0.0 for Windows -- Installation failed. Please see installation log for details: HadoopInstallFiles\\HadoopSetupTools\\hdp-2.4.0.0.winpkg.install.log\nAction ended 15:42:48: INSTFAILED. Return value 3.\nAction ended 15:42:48: INSTALL. Return value 3.\n=== Logging stopped: 3/4/2016  15:42:48 ===\nMSI (c) (9C:84) [15:42:48:247]: Windows Installer installed the product. Product Name: Hortonworks Data Platform 2.4.0.0 for Windows. Product Version: 2.4.0.0. Product Language: 1033. Manufacturer: Hortonworks. Installation success or error status: 1603.\nMSI (c) (9C:84) [15:42:48:247]: Grabbed execution mutex.\nMSI (c) (9C:84) [15:42:48:247]: Cleaning up uninstalled install packages, if any exist\nMSI (c) (9C:84) [15:42:48:247]: MainEngineThread is returning 1603\n=== Verbose logging stopped: 3/4/2016  15:42:48 ===</pre>","tags":["windows"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-04 14:44:46.0","id":21207,"title":"Can we have user logins for ATLAS service","body":"<p>Hello I have ATLAS 0.5.0 version deployed . I want to enable user login to the ATLAS web portal . Is there a way I can enable this ?</p>","tags":["Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-03-05 18:14:42.0","id":21405,"title":"Where to write fsimage files when running QJM NN HA","body":"<p>I am trying to determine and plan the best disk layout for my active/standby NNs for a new production rollout that is going to run with QJM NN HA. I plan to have three servers, each running an instance of ZK and JN. The plan, per recommendations against another <a href=\"https://community.hortonworks.com/questions/16465/understanding-check-pointing-with-namenode-ha.html\">question</a> I asked on this forum, is to have a dedicated RAID-1 disk on each server for the JNs to use for edit logs. I expect that array to use 256-512 GB sized disks. Each server will also have a dedicated disk for the OS, logs, tmp, etc, also RAID-1 using two 1.2TB drives. Each ZK instance will also have dedicated disks (spindles) per recommendations <a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html\">here</a>. </p><p>I am having a hard time answering this question...</p><p><strong><em>Where to store the fsimage files?</em></strong> Could I, for example, store them on the same RAID-1 disk that the JNs are using? I do plan to collocate two of the three JNs on the same two servers running the NNs and the third JN on a third server so, the NN would have access to the same drives used by the JNs. This collocation seems to be a commonly recommend arrangement. Or, should the fsimage files be pointed to a separate RAID-1 disk array just for that purpose? Another option would be to point the fsimage files to a separately size partition on the OS RAID-1 disk array.</p><p>These questions do NOT come from a sizing perspective, but more from a workload perspective. Fitting the files somewhere is easy to figure out. The <strong><em>real</em></strong> question is about performance impacts of mixing, for example, the background checkpointing operations done by the standby namenode with the work being done by the JNs to save edits and putting all that onto the same spindle. I see clearly that ZK should be kept on it's own spindle due to how it using a write ahead log and how latency is a huge concern in that case to impacting ZK performance. I just don't have a good feel for mixing the two work loads of checkpointing and edit log updates. Can someone please make some recommendation here?</p>","tags":["journalnode","namenode-ha"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-07 09:10:32.0","id":21644,"title":"Release of Hive 2.0 with HDP","body":"<p>Hi,</p><p>Hive 2.0 (with LLAP!) was released at 16 Feb. 2016, when is it expected to arrive to HDP?</p><p>Thanks</p>","tags":["llap","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-23 03:42:17.0","id":18760,"title":"Can not invoke hdfs command after invoking webhdfs operation in secure cluster","body":"<p>Here's what I did in my secure cluster:</p><p>1. Invokding webhdfs command: \"curl -s -i --negotiate -u:anyUser http://sandbox.hortonworks.com:50070/webhdfs/v1/?op=LISTSTATUS\"   (this works)</p><p>2. Invoking hdfs command: \"hadoop fs -ls /\" </p><p>But get the following error. (I need to kdestroy and kinti again to renew the ticket to make it work). This is weird, how can I make the webhdfs command not affect the hdfs command ?</p><p>16/02/23 03:38:41 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\nls: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"sandbox.hortonworks.com/10.0.2.15\"; destination host is: \"sandbox.hortonworks.com\":8020;</p><p>The only difference after step 1 in klist is that I have 2 extra HTTP principal, but I still have my principal jeff@EXAMPLE.COM</p><p>Default principal: jeff@EXAMPLE.COM\nValid starting     Expires            Service principal\n02/23/16 03:20:05  02/24/16 03:20:05  krbtgt/EXAMPLE.COM@EXAMPLE.COM\nrenew until 02/23/16 03:20:05\n02/23/16 03:20:10  02/24/16 03:20:05  HTTP/sandbox.hortonworks.com@\nrenew until 02/23/16 03:20:05\n02/23/16 03:20:10  02/24/16 03:20:05  HTTP/sandbox.hortonworks.com@EXAMPLE.COM\nrenew until 02/23/16 03:20:05</p>","tags":["kerberos","webhdfs"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-22 16:57:54.0","id":18708,"title":"Are Spark RDD really immutable?","body":"<p>Hi folks,</p><p>All the way, I have been reading that RDD are immutable but to my surprise today I found different result. I would like to know the reason and supporting documentation if possible.</p><pre>scala&gt; val m = Array.fill(2, 2)(5)\nm: Array[Array[Int]] = Array(Array(5, 5), Array(5, 5))\n\nscala&gt; val rdd = sc.parallelize(m)\n\nscala&gt; rdd.collect()\nres6: Array[Array[Int]] = Array(Array(5, 5), Array(5, 5))\n\n// Interesting here.\nscala&gt; m(0)(1) = 99\n\nscala&gt; rdd.collect()\nres8: Array[Array[Int]] = Array(Array(5, 99), Array(5, 5)) \n</pre><p>Thanks </p>","tags":["scala","Spark","rdd"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-23 08:05:38.0","id":18792,"title":"pig and json","body":"<p>Hi:</p><p>I need to reed this json but <strong>i cant print or access to the multilevel field</strong> just I can read the fisrt level, look:</p><p>for example i need to read \"VARIABLE\":{\"TIPINC-F\")</p><p>my json</p><pre>{\"NUM-PARTICION-F\":\"001\",\"NOMBRE-REGLA-F\":\"SAI_TIP_INC_TRN\",\"FECHA-OPRCN-F\":\"2015-12-06 00:00:01\",\"COD-NRBE-EN-F\":\"9998\",\"COD-NRBE-EN-FSC-F\":\"9998\",\"COD-INTERNO-UO-F\":\"0001\",\"COD-INTERNO-UO-FSC-F\":\"0001\",\"COD-CSB-OF-F\":\"0001\",\"COD-CENT-UO-F\":\"\",\"ID-INTERNO-TERM-TN-F\":\"A0299989\",\"ID-INTERNO-EMPL-EP-F\":\"99999989\",\"CANAL\":\"01\",\"NUM-SEC-F\":\"764\",\"COD-TX-F\":\"SAI01COU\",\"COD-TX-DI-F\":\"TUX\",\"ID-EMPL-AUT-F\":\"U028765\",\"FECHA-CTBLE-F\":\"2015-12-07\",\"COD-IDENTIFICACION-F\":\"\",\"IDENTIFICACION-F\":\"\",\"VALOR-IMP-F\":\"0.00\",\"VARIABLE\":{\"TIPINC-F\":\"0\",\"PERFIL-CAJ-F\":\"0\",\"PERFIL-COM-F\":\"0\",\"PERFIL-TAR-F\":\"0\",\"RESPONSABLE-F\":\"0\",\"RESP-EXCEP-F\":\"0\",\"EXCEPCION-F\":\"0\",\"STD-CHAR-01-F\":\"1\",\"STD-DEC-1-F\":\"0\",\"STD-DEC-2-F\":\"0\"} }</pre><p>my code:</p><pre>A = LOAD '/RSI/staging/input/logs/log.json' USING com.twitter.elephantbird.pig.load.JsonLoader('-nestedLoad');\n\nB = FOREACH A GENERATE (CHARARRAY) $0#'FECHA-OPRCN-F' AS\nfecha, (CHARARRAY) $0#'COD-NRBE-EN-F' AS entidad, (CHARARRAY) $0#'COD-INTERNO-UO-FSC-F' AS ofi\n, (CHARARRAY) $0#'COD-TX-F' AS ope;\n\n\n</pre><p>my output</p><pre>(2015-12-06 00:06:40,9998,0001,DVI82OOU,)\n(2015-12-06 00:06:42,9998,0001,DVI95COU,)\n(2015-12-06 00:06:49,3191,9204,BDPPM1ZJ,)\n(2015-12-06 00:06:49,3076,9554,STR03CON,)\n(2015-12-06 00:06:53,3008,9521,BDPPM1RJ,)\n</pre>","tags":["json","Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-23 09:12:37.0","id":18819,"title":"How to fix missing and under replicated blocks?","body":"<p>In my HDFS status summary, I see the following messages about missing and under-replicated blocks:</p><p>2,114 missing blocks in the cluster. 5,114,551 total blocks in the cluster. Percentage missing blocks: 0.04%. Critical threshold: any.</p><p>On executing the command : hdfs fsck -list-corruptfileblocks</p><p>I got following output : The filesystem under path '/' has 2114 CORRUPT files</p><p> What is the best way to fix these corrupt files and also fix the underreplicated block problem?</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-23 20:42:28.0","id":19018,"title":"Sqoop Import data limit","body":"<p>What is the sqoop import data limit from Teradata onto local file system instead of HDFS.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-23 19:41:51.0","id":18994,"title":"SPARKR can I??","body":"<p>HI:</p><p>Could i connect from RStudio to Spark HDP using SparkR¿¿¿¿</p><p>Thanks</p>","tags":["sparkr","Spark","r"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-23 16:28:40.0","id":18960,"title":"Business Days in Hive","body":"<p>Hi,</p><p>I want to calculate no. of business days between two dates using hive queries. How can I do it??</p><p>Thanks</p><p>Mamta</p>","tags":["hive-udf"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-23 23:22:16.0","id":19033,"title":"Running spark beeline on Yarn","body":"<p>The default bin/beeline does not seem show up in resource manager for Spark History server for some reason on HDP 2.3.4. How to make it run on Yarn?</p>","tags":["Spark","spark-sql"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-24 07:52:12.0","id":19104,"title":"Not able to restart services in Ambari even servers and agents are running","body":"<p>I am facing this problem after I rebooted my sandbox, I restarted ambari-agent and server also but no luck!</p><p><img src=\"/storage/attachments/2383-services.jpg\"></p><p>Please help.</p>","tags":["ambari-service","Sandbox"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-24 08:45:05.0","id":19107,"title":"Hive Downgrade","body":"<p>Hi all.</p><p>We would downgrad Hive 1.2.1 to 0.13.0 ( I mean, after an HDP 2.3 installation we have Hive 1.2.1).</p><p>Bacause , before with HDP 2.1 our job run fine and now, with HDP 2.3 we have errors.</p><p>We are on Productions and the fast way imho is a simple downgrade only for HIVE.</p><p>It's a good idea ? In order to downgrade I read :</p><h3>Downgrade Hive </h3><p>The Hive downgrade process reverses the upgrade process:</p><ul><li>Downgrade WebHCat </li><li>Start the previous Hive Server 2 instance</li><li>Deregister the new Hive Server 2 instance</li><li>Downgrade Hive Metastore servers </li></ul><p>When finished, validate the downgrade process.</p><p>It's correct ? Someone already tested ? </p><p>Thanks</p>","tags":["downgrade","Hive","hiveserver2"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-24 13:12:35.0","id":19204,"title":"Do we have any script which we can use to clean /tmp/hive/ dir frequently on hdfs. Because it is consuming space in TB.","body":"<p>Do we have any script which we can use to clean /tmp/hive/ dir frequently on hdfs. Because it is consuming space in TB.</p><p>I have gone through below one but I am looking for any shell script.</p><p>https://github.com/nmilford/clean-hadoop-tmp/blob/master/clean-hadoop-tmp</p>","tags":["Hive","HDFS","MapReduce"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-24 14:05:49.0","id":19220,"title":"Nifi twitter extract; is it possible to pull the tweets between 2 dates?","body":"<p>Hi,</p><p>I 've been following the tutorial to pull tweets from tweeter. Do you know if i can pull every tweets made by a specific account between 2 dates and how i could achieve that?</p>","tags":["twitter","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-24 14:29:54.0","id":19243,"title":"Filter match not doesnt work","body":"<p>Hi:</p><p>Iam using pig to Filter a file, but it doesnt work, after type this result is ok</p><pre>W = FILTER B BY ($4 matches '.*c.*') OR ($4 matches '.*e.*') OR ($4 matches '.*f.*') OR ($4 matches '.*b.*');\n(2015-12-06 00:00:48,9998,0001,DVI82OOU,9c000)\n(2015-12-06 00:01:01,9998,0001,DVI82OOU,26e2)\n(2015-12-06 00:01:16,9998,0001,DVI82OOU,9c000)\n(2015-12-06 00:01:33,3187,0913,BDP00SMU,f8000)\n(2015-12-06 00:02:01,9998,0001,DVI82OOU,82b00)\n(2015-12-06 00:02:09,9998,0001,DVI82OOU,99e6)\n(2015-12-06 00:02:10,9998,0001,DVI82OOU,7f880)</pre><p>but the NOT doesnt work like this, i try it:</p><pre>W = FILTER B BY NOT($4 matches '.*c.*') OR NOT($4 matches '.*e.*') OR NOT($4 matches '.*f.*') OR NOT($4 matches '.*b.*');</pre><p>but it doesnt work.</p><p>Please help!</p><p>thanks</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-24 20:43:55.0","id":19334,"title":"Oozie Installation Issue","body":"<p>I am attempting to install HDP 2.3.4 for Windows and have encountered the following error:</p><p><em>\"OOZIE: Configuring service config d:\\hdp\\oozie-4.2.0.2.3.4.0-3485\\Service\\oozieservice.xml </em></p><p><em>OOZIE: Updating file d:\\hdp\\oozie-4.2.0.2.3.4.0-3485\\Service\\oozieservice.xml to point to executable d:\\hdp\\oozie-4.2.0.2.3.4.0-3485\\oozie-win-distro\\bin\\oozied.cmd </em></p><p><em>OOZIE: Copy Oozie additional Jars </em></p><p><em>OOZIE: Calling Setup script to add Hadoop libs to the generated oozie.war file Creating OOZIE_TEMP directory 'D:\\hdp\\oozie-4.2.0.2.3.4.0-3485\\oozie-win-distro\\temp' 'D:\\Java\\bin\\jar.exe' is not recognized as an internal or external command, operable program or batch file.\"  </em></p><p>I'm not a Java expert, but I did a fairly standard Java install per the installation instructions and there is no \"jar.exe\" file anywhere in my installation.  I have checked other machines and I don't believe jar.exe is part of the current build.  I have double checked my Path variable and it has a reference to \"d\\java\" and \"d\\java\\bin\".  I'm running out of ideas.</p><p>Any suggestions how to resolve this issue would be greatly appreciated.  </p>","tags":["installation","java","Oozie"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-24 22:44:27.0","id":19354,"title":"Allow users to create Hive external tables but not Hive managed tables with Ranger","body":"<p>Is it possible to allow users to create Hive managed tables on top of existing data in their user directories but then prevent them from creating Hive managed tables? This is specific to Ranger 0.5.0. From reading this seems like it might be achievable with Ranger 0.6.0 and deny policies but I'm looking for a 0.5.0 approach. If that isn't possible feedback on if the deny policies would make this possible would also be helpful.</p>","tags":["Hive","ranger-0.5.0"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-24 22:50:26.0","id":19359,"title":"Any known issues on Hive 1.2.1 performance with date and timestamp data types?","body":"<p>Any known issues on Hive 1.2.1 performance with date and timestamp data types?  Any issues?  Any guidance anyone can provide?</p>","tags":["Hive","date","timestamp"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-25 03:38:43.0","id":19421,"title":"Question - Oozie sharelib","body":"<p>Hey Guys,</p><p>I have a question related to oozie sharelib, suppose I'm running any simple hive action via oozie workflow. I know that we have hive related jars present on hdfs in oozie sharelib. When nodemanagers are executing map/reduce attempts, oozie launcher will make sure to get these jars available to those tasks. </p><p>Please correct if my above understanding is incorrect!</p><p>Now the question -</p><p>If above understanding is correct then do we need hive client to be present on all the nodemanagers? if yes then why? do we also need hive jars to be present on local storage on each nodemanager(/usr/hdp/current/&lt;hive-home&gt;/lib/)?</p>","tags":["oozie-sharelib","Oozie","oozie-hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-25 07:53:59.0","id":19460,"title":"Text file to Parquet file conversion using Pig","body":"<p>Hi ,</p><p>Please help on the text file to parquet file conversion  using pig. </p><p>Got this syntax.</p><pre>store A into '/test-warehouse/tinytable' USING parquet.pig.ParquetStorer;\n\nusing this am getting error.\n\nwhether text file has to be loaded first into alias A. and then run the above store command.\n\n\"/test-warehouse/tinytable\" - this path is parquet file put path ?\n\n</pre>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-25 11:45:17.0","id":19491,"title":"Increasing the HDFS Disk size in HDP 2.3 3 node cluster","body":"<p>In the 3 node cluster installation for POC, My 3rd note is datanode, it has a disk space of about 200 GB.</p><p>As per the widget, my current HDFS Usage is as follows:</p><p>DFS Used: 512.8 MB (1.02%); non DFS used 8.1 GB (16.52%); remaining 40.4GB (82.46 %)</p><p>When I do df -h to check the disk size i can see a lot of space is taken by tmpfs as shown in the following screenshot:</p><p><img src=\"/storage/attachments/2403-disk-size.png\"></p><p>How can I increase my HDFS disk size?</p>","tags":["HDFS","hadoop"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-25 06:47:34.0","id":19449,"title":"Hadoop LocalFileSystem Checksum calculation","body":"<p>Hello Everyone, Kindly help me on these queries  ( Reference book O'reilly )</p><p>First of all Im not sure abt this word \"LocalFileSystem\" , </p><p>a) Does this mean it is machines file system on which hadoop is installed example : ext2, ext3, NTFS etc.....</p><p>b) Also there are CheckSumFileSystem  etccc. Why hadoop has multiple filesystems , i thought it has only HDFS apart from local machines filesystem.</p><p>Questions :</p><p>Can someone explain me this statement , very confusing to me right now. </p><p><strong></strong></p><p><strong>1. The Hadoop LocalFileSystem performs client-side checksumming. </strong></p><p>So if im correct, without this filesystem earlier client used to calculate the checksum for each chunk of data and pass it to datanodes for storage? correct me if my understanding is wrong . </p><p><strong>\n</strong></p><p><b>2. This means that\nwhen you write a file called filename, the filesystem client transparently creates a hidden\nfile, .filename.crc, in the same directory containing the checksums for each chunk of the\nfile. </b></p><p>Where is this filestystem client at the client layer or at the hdfs layer </p><p><b>The chunk size is controlled by the file.bytes-per-checksum property, which\ndefaults to 512 bytes. The chunk size is stored as metadata in the .crc file, so the file can\nbe read back correctly even if the setting for the chunk size has changed. Checksums\nare verified when the file is read, and if an error is detected, LocalFileSystem throws\na ChecksumException.</b></p><p>How does this FileSystem differs from HDFS in terms of Checksum ???????</p>","tags":["hadoop","filesystem"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-25 16:45:07.0","id":19534,"title":"Location of pig libraries","body":"<p>On HDP I am looking for where parquet-pig-1.4.1.jar is located.  I checked /usr/hdp/current/pig-client/lib and did not find anything.  </p><p>Also where are 3rd party or custom libraries stored for pig?  is it /usr/hdp/current/pig-client/lib ?</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-25 17:37:58.0","id":19553,"title":"CloudBreak install on Amazon AWS Failure : MyBatis Migrations FAILURE : uaabd : relation \"groups\" does not exist","body":"<p>I´m trying CloudBreak on Amazon AWS t2.micro instance to launch new clusters within EC2 (r2 instances) e I´m getting the following error after <strong><em>cbd start</em></strong>. I´m following the instructions on <a target=\"_blank\" href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_cldbrk_install/bk_CLBK_IAG/bk_CLBK_IAG-20150721.pdf\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_cldbrk_install/bk_CLBK_IAG/bk_CLBK_IAG-20150721.pdf</a>.</p><pre>[root@ip-172-31-17-119 cloudbreak]# cbd start\ngenerating docker-compose.yml\ngenerating uaa.yml\nCreating cbreak_cbdb_1...\nPulling image sequenceiq/cbdb:1.1.0-rc.28...\n1.1.0-rc.28: Pulling from sequenceiq/cbdb\n193224d99eda: Already exists\na3ed95caeb02: Pull complete\ne17b9bab0b51: Already exists\ncebac4ac10ae: Already exists\nd2cf979f7728: Already exists\ned097438f485: Already exists\n887fe6d60a09: Already exists\n0a29f8781259: Already exists\n966ac5df714c: Already exists\nf5f8a69cf357: Already exists\n21986ed0c2a0: Already exists\nb69dc2bc622c: Already exists\n58f25ac44eea: Pull complete\n9684816e42d9: Pull complete\nDigest: sha256:120d468fe085873f6848435aa39aa40de64f3136406829092e254c0d59b1ea5e\nStatus: Downloaded newer image for sequenceiq/cbdb:1.1.0-rc.28\nCreating cbreak_pcdb_1...\nPulling image sequenceiq/pcdb:1.0.0-rc.3...\n1.0.0-rc.3: Pulling from sequenceiq/pcdb\n193224d99eda: Already exists\na3ed95caeb02: Pull complete\ne17b9bab0b51: Already exists\ncebac4ac10ae: Already exists\nd2cf979f7728: Already exists\ned097438f485: Already exists\n887fe6d60a09: Already exists\n0a29f8781259: Already exists\n966ac5df714c: Already exists\nf5f8a69cf357: Already exists\n21986ed0c2a0: Already exists\nb69dc2bc622c: Already exists\n1d1622f430a4: Pull complete\n9a960fd777c6: Pull complete\nDigest: sha256:02e8782f9c7ba00d62e7223291b29621bd5b040e1bb0eb9fa7a101351fbd7f3d\nStatus: Downloaded newer image for sequenceiq/pcdb:1.0.0-rc.3\nCreating cbreak_uaadb_1...\nPulling image sequenceiq/uaadb:v2.7.1...\nv2.7.1: Pulling from sequenceiq/uaadb\nf8efbffe7b95: Pull complete\na3ed95caeb02: Pull complete\n64bb2775d20d: Pull complete\n112aae0540bb: Pull complete\n96d2a7a4fcb3: Pull complete\n8d43a1a2d52b: Pull complete\n1eba7ded6f54: Pull complete\n72377f1d0630: Pull complete\n5b95ba5f224f: Pull complete\nb91c5ed5aea8: Pull complete\n1c9d3fc5a296: Pull complete\n793d7d756766: Pull complete\nc357a7b255ac: Pull complete\n0db0a73a83ca: Pull complete\nDigest: sha256:270e87a90add32c69d8cb848c7455256f4c0a73e14a9ba2c9335b11853f688a6\nStatus: Downloaded newer image for sequenceiq/uaadb:v2.7.1\nMigration SUCCESS: cbdb up\nMigration SUCCESS: cbdb pending\nMigration SUCCESS: pcdb up\nMigration SUCCESS: pcdb pending\n[ERROR] Migration failed: uaadb up\n[ERROR] See logs in: db_migration.log\n[ERROR] Migration failed: uaadb pending\n[ERROR] See logs in: db_migration.log\n-- MyBatis Migrations FAILURE\n-- MyBatis Migrations FAILURE\n[ERROR] Migration is failed, please check the log: db_migration.log\n[root@ip-172-31-17-119 cloudbreak]# cat db_migration.log\n[MIGRATE] Script location: container\n[MIGRATE] Docker image name: sequenceiq/cloudbreak:1.1.0-rc.47\n[MIGRATE] Migration command on cbdb with params: 'container up' will be executed on container: da0171af836c430c45a96b3ea2658a7f64c50c7a6e50ba1f602b38861ed40410\n[MIGRATE] Scripts location:  container\n[MIGRATE] Schema will be extracted from image:  sequenceiq/cloudbreak:1.1.0-rc.47\n[MIGRATE] Scripts location:  /home/ec2-user/cloudbreak/.schema/cbdb\n------------------------------------------------------------------------\n-- MyBatis Migrations - up\n------------------------------------------------------------------------\n------------------------------------------------------------------------\n-- MyBatis Migrations SUCCESS\n-- Total time: 0s\n-- Finished at: Thu Feb 25 12:23:40 UTC 2016\n-- Final Memory: 10M/483M\n------------------------------------------------------------------------\n[MIGRATE] Script location: container\n[MIGRATE] Docker image name: sequenceiq/cloudbreak:1.1.0-rc.47\n[MIGRATE] Migration command on cbdb with params: 'container pending' will be executed on container: da0171af836c430c45a96b3ea2658a7f64c50c7a6e50ba1f602b38861ed40410\n[MIGRATE] Scripts location:  container\n[MIGRATE] Schema will be extracted from image:  sequenceiq/cloudbreak:1.1.0-rc.47\n[MIGRATE] Scripts location:  /home/ec2-user/cloudbreak/.schema/cbdb\n------------------------------------------------------------------------\n-- MyBatis Migrations - pending\n------------------------------------------------------------------------\nWARNING: Running pending migrations out of order can create unexpected results.\n------------------------------------------------------------------------\n-- MyBatis Migrations SUCCESS\n-- Total time: 0s\n-- Finished at: Thu Feb 25 12:23:42 UTC 2016\n-- Final Memory: 10M/483M\n------------------------------------------------------------------------\n[MIGRATE] Script location: container\n[MIGRATE] Docker image name: sequenceiq/periscope:1.0.0-rc.3\n[MIGRATE] Migration command on pcdb with params: 'container up' will be executed on container: 4fd1f384f534ea6ff9a2b1178e9c1c240c491b87261844195235fd96a18a8719\n[MIGRATE] Scripts location:  container\n[MIGRATE] Schema will be extracted from image:  sequenceiq/periscope:1.0.0-rc.3\n[MIGRATE] Scripts location:  /home/ec2-user/cloudbreak/.schema/pcdb\n------------------------------------------------------------------------\n-- MyBatis Migrations - up\n------------------------------------------------------------------------\n------------------------------------------------------------------------\n-- MyBatis Migrations SUCCESS\n-- Total time: 0s\n-- Finished at: Thu Feb 25 12:23:44 UTC 2016\n-- Final Memory: 10M/483M\n------------------------------------------------------------------------\n[MIGRATE] Script location: container\n[MIGRATE] Docker image name: sequenceiq/periscope:1.0.0-rc.3\n[MIGRATE] Migration command on pcdb with params: 'container pending' will be executed on container: 4fd1f384f534ea6ff9a2b1178e9c1c240c491b87261844195235fd96a18a8719\n[MIGRATE] Scripts location:  container\n[MIGRATE] Schema will be extracted from image:  sequenceiq/periscope:1.0.0-rc.3\n[MIGRATE] Scripts location:  /home/ec2-user/cloudbreak/.schema/pcdb\n------------------------------------------------------------------------\n-- MyBatis Migrations - pending\n------------------------------------------------------------------------\nWARNING: Running pending migrations out of order can create unexpected results.\n------------------------------------------------------------------------\n-- MyBatis Migrations SUCCESS\n-- Total time: 0s\n-- Finished at: Thu Feb 25 12:23:46 UTC 2016\n-- Final Memory: 10M/483M\n------------------------------------------------------------------------\n[MIGRATE] Script location: container\n[MIGRATE] Docker image name: sequenceiq/sultans-bin:1.1.0-rc.5\n[MIGRATE] Migration command on uaadb with params: 'container up' will be executed on container: 6399ceb17cf3217b9a2cd7bbe84dbd08e61bcd7bc6b17d16996d7c8ba6c5366e\n[MIGRATE] Scripts location:  container\n[MIGRATE] Schema will be extracted from image:  sequenceiq/sultans-bin:1.1.0-rc.5\n[MIGRATE] Scripts location:  /home/ec2-user/cloudbreak/.schema/uaadb\n------------------------------------------------------------------------\n-- MyBatis Migrations - up\n------------------------------------------------------------------------\n========== Applying: 20150918124357_add_new_read_scopes.sql ====================\nError executing: --  add new read scopes\n-- Migration SQL that makes the change goes here.\nINSERT INTO groups (id, displayname) VALUES ('2a4f837e-2ab0-4663-8644-de23047d040d', 'cloudbreak.blueprints.read');\nINSERT into groups (id, displayname) VALUES ('9fc82f95-0318-4dda-af65-ea32e2f00497', 'cloudbreak.templates.read');\nINSERT into groups (id, displayname) VALUES ('8cbffdab-560d-4e14-9d1e-d5ff3f43bec6', 'cloudbreak.credentials.read');\nINSERT into groups (id, displayname) VALUES ('7f85eaad-4dac-4f74-9238-aef0e585df07', 'cloudbreak.recipes.read');\nINSERT into groups (id, displayname) VALUES ('aac2dad1-6a7b-4ae2-9c58-e71d367d0e2d', 'cloudbreak.networks.read');\nINSERT into groups (id, displayname) VALUES ('6674545b-295c-4f23-b1b4-953447dc5f40', 'cloudbreak.securitygroups.read');\nINSERT into groups (id, displayname) VALUES ('f07b098a-2700-471a-9f0d-f7dd730db95d', 'cloudbreak.stacks.read');\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.blueprints.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.templates.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.credentials.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.recipes.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.networks.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.securitygroups.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.stacks.read';\n.  Cause: org.postgresql.util.PSQLException: ERROR: relation \"groups\" does not exist\n  Position: 88\n------------------------------------------------------------------------\n-- MyBatis Migrations FAILURE\n-- Total time: 0s\n-- Finished at: Thu Feb 25 12:23:48 UTC 2016\n-- Final Memory: 10M/483M\n------------------------------------------------------------------------\nERROR: Error executing command.  Cause: org.apache.ibatis.jdbc.RuntimeSqlException: Error executing: --  add new read scopes\n-- Migration SQL that makes the change goes here.\nINSERT INTO groups (id, displayname) VALUES ('2a4f837e-2ab0-4663-8644-de23047d040d', 'cloudbreak.blueprints.read');\nINSERT into groups (id, displayname) VALUES ('9fc82f95-0318-4dda-af65-ea32e2f00497', 'cloudbreak.templates.read');\nINSERT into groups (id, displayname) VALUES ('8cbffdab-560d-4e14-9d1e-d5ff3f43bec6', 'cloudbreak.credentials.read');\nINSERT into groups (id, displayname) VALUES ('7f85eaad-4dac-4f74-9238-aef0e585df07', 'cloudbreak.recipes.read');\nINSERT into groups (id, displayname) VALUES ('aac2dad1-6a7b-4ae2-9c58-e71d367d0e2d', 'cloudbreak.networks.read');\nINSERT into groups (id, displayname) VALUES ('6674545b-295c-4f23-b1b4-953447dc5f40', 'cloudbreak.securitygroups.read');\nINSERT into groups (id, displayname) VALUES ('f07b098a-2700-471a-9f0d-f7dd730db95d', 'cloudbreak.stacks.read');\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.blueprints.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.templates.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.credentials.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.recipes.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.networks.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.securitygroups.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.stacks.read';\n.  Cause: org.postgresql.util.PSQLException: ERROR: relation \"groups\" does not exist\n  Position: 88[MIGRATE] Script location: container\n[MIGRATE] Docker image name: sequenceiq/sultans-bin:1.1.0-rc.5\n[MIGRATE] Migration command on uaadb with params: 'container pending' will be executed on container: 6399ceb17cf3217b9a2cd7bbe84dbd08e61bcd7bc6b17d16996d7c8ba6c5366e\n[MIGRATE] Scripts location:  container\n[MIGRATE] Schema will be extracted from image:  sequenceiq/sultans-bin:1.1.0-rc.5\n[MIGRATE] Scripts location:  /home/ec2-user/cloudbreak/.schema/uaadb\n------------------------------------------------------------------------\n-- MyBatis Migrations - pending\n------------------------------------------------------------------------\nWARNING: Running pending migrations out of order can create unexpected results.\n========== Applying: 20150918124357_add_new_read_scopes.sql ====================\nError executing: --  add new read scopes\n-- Migration SQL that makes the change goes here.\nINSERT INTO groups (id, displayname) VALUES ('2a4f837e-2ab0-4663-8644-de23047d040d', 'cloudbreak.blueprints.read');\nINSERT into groups (id, displayname) VALUES ('9fc82f95-0318-4dda-af65-ea32e2f00497', 'cloudbreak.templates.read');\nINSERT into groups (id, displayname) VALUES ('8cbffdab-560d-4e14-9d1e-d5ff3f43bec6', 'cloudbreak.credentials.read');\nINSERT into groups (id, displayname) VALUES ('7f85eaad-4dac-4f74-9238-aef0e585df07', 'cloudbreak.recipes.read');\nINSERT into groups (id, displayname) VALUES ('aac2dad1-6a7b-4ae2-9c58-e71d367d0e2d', 'cloudbreak.networks.read');\nINSERT into groups (id, displayname) VALUES ('6674545b-295c-4f23-b1b4-953447dc5f40', 'cloudbreak.securitygroups.read');\nINSERT into groups (id, displayname) VALUES ('f07b098a-2700-471a-9f0d-f7dd730db95d', 'cloudbreak.stacks.read');\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.blueprints.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.templates.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.credentials.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.recipes.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.networks.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.securitygroups.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.stacks.read';\n.  Cause: org.postgresql.util.PSQLException: ERROR: relation \"groups\" does not exist\n  Position: 88\n------------------------------------------------------------------------\n-- MyBatis Migrations FAILURE\n-- Total time: 0s\n-- Finished at: Thu Feb 25 12:23:49 UTC 2016\n-- Final Memory: 10M/483M\n------------------------------------------------------------------------\nERROR: Error executing command.  Cause: org.apache.ibatis.jdbc.RuntimeSqlException: Error executing: --  add new read scopes\n-- Migration SQL that makes the change goes here.\nINSERT INTO groups (id, displayname) VALUES ('2a4f837e-2ab0-4663-8644-de23047d040d', 'cloudbreak.blueprints.read');\nINSERT into groups (id, displayname) VALUES ('9fc82f95-0318-4dda-af65-ea32e2f00497', 'cloudbreak.templates.read');\nINSERT into groups (id, displayname) VALUES ('8cbffdab-560d-4e14-9d1e-d5ff3f43bec6', 'cloudbreak.credentials.read');\nINSERT into groups (id, displayname) VALUES ('7f85eaad-4dac-4f74-9238-aef0e585df07', 'cloudbreak.recipes.read');\nINSERT into groups (id, displayname) VALUES ('aac2dad1-6a7b-4ae2-9c58-e71d367d0e2d', 'cloudbreak.networks.read');\nINSERT into groups (id, displayname) VALUES ('6674545b-295c-4f23-b1b4-953447dc5f40', 'cloudbreak.securitygroups.read');\nINSERT into groups (id, displayname) VALUES ('f07b098a-2700-471a-9f0d-f7dd730db95d', 'cloudbreak.stacks.read');\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.blueprints.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.templates.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.credentials.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.recipes.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.networks.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.securitygroups.read';\nINSERT into group_membership SELECT gr.id AS group_id, usr.id AS member_id, 'USER', 'MEMBER' FROM users usr, groups gr WHERE gr.displayname='cloudbreak.stacks.read';\n.  Cause: org.postgresql.util.PSQLException: ERROR: relation \"groups\" does not exist\n  Position: 88[root@ip-172-31-17-119 cloudbreak]#\n[root@ip-172-31-17-119 cloudbreak]#</pre>","tags":["help","Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-25 13:07:15.0","id":19504,"title":"can not connect to zeppelin via 9995 port (zeppelin-deamon.sh statuts returns: \"Zeppelin running but process is dead\" ); sandbox","body":"<p>Hi</p><p>This refers to Hortonworks sandbox. After installing Anaconda Python and successfully changed $PATH to point to python 3.5 Zeppelin died :(\nPyspark from interactive console works properly saying python version 3.5.</p><p>I tired to change:\nexport PYSPARK_PYTHON=python\nexport PYSPARK_DRIVER_PYTHON=python</p><p>in zeppelin-env template via Ambari hoping this may force zeppelin to use proper python version (3.5) but still not working</p><p>Any idea how to make process working and finally zeppelin?</p>","tags":["zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-28 18:02:36.0","id":20018,"title":"I am unable to open link in browser","body":"<p>I have installed HDP sandbox 2.2.4, on VMware. I am able to login in HDP command prompt using username and password.</p><p>But I am not able to open the ip in any browser. I tried to ping that ip but getting request timeout.</p><p>Please help me how to solve this issue </p>","tags":["installation","login","vmware","hdp-2.2.4"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-29 22:18:29.0","id":20309,"title":"/usr/lib/hadoop-mapreduce/hadoop-streamingxxxx.jar not found","body":"<p>Hi all,</p><p>I was following this tutorial: </p><p>http://hortonworks.com/blog/using-r-and-other-non-java-languages-in-mapreduce-and-hive/</p><p>and I couldn't find hadoop-streamingxxxx.jar. I'm using a cluster with hdp-2.3.4.0-3485.</p><p>Does any know where to find it or how to add it ?</p><p>Thanks :)</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-01 00:16:25.0","id":20319,"title":"Installing Ambari and HDP for Debian 8.","body":"<p>Hi, I'm trying to make ambari work for debian 8. I want to know how the HDP stack and the repository info's are mapped and how this data is populated? I'm able to get past the registration process by adding the entry in os_family.json. But during the deployment process. The error gets thrown again. I wanted to know how the data is populated which maps the repository version and the OS version.</p><p>Thanks,</p><p>Sriganesh</p>","tags":["Ambari","hdp-2.3.0","debian7"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-03-01 03:55:24.0","id":20384,"title":"Hive query running for infinite time","body":"<p>http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-hive/</p><p>I am loading data from above exercise. but the query \"SELECT year, max(runs) FROM batting GROUP BY year;\"  running for infinite time. any suggestion?</p>","tags":["hive-streaming"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-01 03:57:02.0","id":20385,"title":"Pig, Hive View in Ambari for HDPCD Practice","body":"<p>In the exam are we allowetd to use Ambari's Pig, Hive Views to write our scripts? Or must it be via the command line?    The practice test only allows via command line so just want to double check. Thank you.</p>","tags":["Hive","Pig","hdpcd","practice"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-01 09:24:33.0","id":20424,"title":"add Certificates Facebook to apache nifi","body":"<p>i try to use the plain HTTP endpoint of api open graph of facebook,\n but it support HTTPS endpoint ( authentication with access_token) , \nso i obliged to add certificate facebook to nifi and create a ssl context, i upload the different certificates (file PEM) that facebook use but i don't know how to configure nifi to know it( how i add to keystore and trustore), any help is appreciate.</p>","tags":["ssl","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-01 13:20:30.0","id":20456,"title":"Have two Flume sources point to same channel","body":"<p>Is it possible to have two sources ( syslog udp and tcp  ) point to the same channel? </p><pre>a.sources = s t\n\na.channels = c\n\na.sources.s.channels = c\n\na.sources.t.channels = c</pre><p>The output of both sources has the same format so I would hate to configure two channels and two sinks. But it didn't seem to work.</p><p>ITs weirdly hard to google something as basic as this.</p>","tags":["Flume"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-01 12:49:08.0","id":20453,"title":"Integrating HDP with A/D without writing anything to A/D? How?","body":"<p>Hi Guys,</p><p>I am trying to Kerberizing the cluster and want to integrate with A/D for user authentication. Earlier I've done it using MIT KDC in the HDP cluster and setting bi-directional trust with A/D. But as I remember, the previous step adds couple of entries in the A/D. However, customer does not want to give write access to the A/D. How to proceed in this scenario?</p><p>Thanks,</p><p>SS.</p>","tags":["kerberos","security","active-directory"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-01 16:56:55.0","id":20476,"title":"We need to set up hortonworks HDP for three servers. Can we set up Sandbox in one server and later on add other two servers or do we need to first get the all prerequisites software in all server and then establish the setup for HDP?","body":"<p>We need to set up hortonworks HDP for three\nservers. Can we set up Sandbox in one server and later on add other two\nservers or do we need to first get the all prerequisites software in all\nserver and then establish the setup for HDP?</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-02 19:17:59.0","id":20840,"title":"Hive:  F060 Couldn't open connection to HDFS","body":"<a href=\"/storage/attachments/2546-hive-hdfs-error.txt\"></a><p>Earlier today I got an error message stating the \"HDFS070 fs.defaultFS is not configured\" after upgrading the RAM in my cluster.  I don't believe it has anything to do with the RAM but no other changes were made to the cluster and it was working fine previously.  I get the error when I go to Hive view in Ambari.  I am unable to see any of the databases in the cluster.</p><p><a href=\"/storage/attachments/2546-hive-hdfs-error.txt\">hive-hdfs-error.txt</a></p>","tags":["ambari-views","Hive","error","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-03 06:07:14.0","id":20924,"title":"How to resolve 'Could not connect to sandbox.hortonworks.com:9083'?","body":"<p>I am new to Haddop and I am trying the tutorials in the VM(in HDP 2.0.). After uploading the NYS file via File browser, when I go to HCatalog I am getting error  'Could not connect to sandbox.hortonworks.com:9083'. </p>","tags":["hcatalog","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-06 03:27:57.0","id":21449,"title":"question on NIFI Putsql processor configuration","body":"<p>Hi ,</p><p>I am trying to use put sql processor with \nfollowing connection in HDP sandbox,where i configured \ndbcpconnectionpool  with configuration Database connection url as </p><p>jdbc:mysql://sandbox.hortonworks.com/hive?createDatabaseIfNotExist=true</p><p>Database Driver class name as </p><p>com.mysql.jdbc.Driver</p><p>I made it as default for other settings. </p><p>when i tried to run the put sql processor it says</p><p>Cannot load jdbc driver . Please let me know, if i need to configure more on it?</p>","tags":["mysql","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-07 03:24:21.0","id":21536,"title":"How can i change the ip address for hortonworks VM.","body":"<p>Hi, How can I change the IP address for Hortonworks VM. I am trying to connect Hortonworks to MicroStrategy which is installed on a different virtual machine and I can not change ip of MicroStrategy machine as it is on the same network as my physical machine where everything else is installed. Now I am trying to change Hortonworks vm id, so that everything is on the same network. Thanks</p>","tags":["network"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-07 11:36:39.0","id":21687,"title":"How to increase the capacity of HDFS?","body":"<p>I am posting this answer after searching in the internet for a good explanation. Currently the total physical hard disk space (4 nodes) is 720 GB.  The dashboard currently shows that only 119 GB is configured for DFS.  I want to increase this space to at last 300 GB.  I didn't find anything staright forward on Ambari dashboard to do this.  The only information I found on the internet is to modifify core-site.xml file to hav a property hadoop.tmp.dir pr that points to another directory. I do not want to blankly do it, without understanding what it means to be expanding HDFS capacity and how to do it through Ambari Dashboard.</p>","tags":["ambari-2.1.2","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-07 10:55:04.0","id":21682,"title":"delete and update hive","body":"<p>Hi:</p><p>Please how can i delete some row in hive from beeline or hue or sqldeveloper?</p><pre>CREATE EXTERNAL TABLE roberto_delete( \n  WORD string\n  ) \nROW FORMAT DELIMITED \nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nstored as ORC\nLOCATION '/RSI/tables/logs/roberto_delete'\nTBLPROPERTIES (\"immutable\"=\"false\");\n</pre><p>I set this atributes:</p><pre>0: jdbc:hive2://lnxbig05.cajarural.gcr:10000&gt; delete from roberto_delete where word='1';\nError: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294)\n\n\n</pre><p>after activate ACID y see this error:</p><pre>Error: Error while compiling statement: FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.roberto_delete that does not use an AcidOutputFormat or is not bucketed (state=42000,code=10297)\n\n\n</pre><p>Please how can i delete one row??</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-08 00:35:19.0","id":21834,"title":"Hive Error in  Sqoop Action using Oozie worlflow being submitted via the HUE interface error","body":"<p>I am trying to sqoop data into hive table from oracle db using a Oozie workflow submitted via the HUE interface.</p><p>import --connect jdbc:postgresql://localhost/test --username admin --password admin --query 'SELECT s.sale_id,c.cust_id,c.state,s.qty FROM customer c JOIN sales s on c.cust_id = s.cust_id WHERE s.fch_insercion&lt;TO_DATE(20160211) and $CONDITIONS'  --hive-import --hive-table sales --target-dir /user/cloudera/sales_products --hive-delims-replacement '/t' --hive-partition-key FCH_INSERCION --hive-partition-value 20160212 -m 1</p><p>But finished with the following error</p><p>ERROR [main] tool.ImportTool (ImportTool.java:run(609)) - Encountered IOException running import job: java.io.IOException: Hive exited with status 1</p><p>This the stack trace of the error </p><pre>56789 [Thread-31] INFO  org.apache.sqoop.hive.HiveImport  - Loading data to table sales partition (fch_insercion=20160212)\n2016-03-07 12:33:48,666 INFO  [Thread-31] hive.HiveImport (LoggingAsyncSink.java:run(85)) - Loading data to table sales partition (fch_insercion=20160212)\n57097 [Thread-31] INFO  org.apache.sqoop.hive.HiveImport  - Failed with exception copyFiles: error while checking/creating destination directory!!!\n2016-03-07 12:33:48,974 INFO  [Thread-31] hive.HiveImport (LoggingAsyncSink.java:run(85)) - Failed with exception copyFiles: error while checking/creating destination directory!!!\n57099 [Thread-31] INFO  org.apache.sqoop.hive.HiveImport  - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask\n2016-03-07 12:33:48,976 INFO  [Thread-31] hive.HiveImport (LoggingAsyncSink.java:run(85)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask\n57721 [main] ERROR org.apache.sqoop.tool.ImportTool  - Encountered IOException running import job: java.io.IOException: Hive exited with status 1\nat org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:385)\nat org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:335)\nat org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:239)\nat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:511)\nat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)\nat org.apache.sqoop.Sqoop.run(Sqoop.java:143)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)\nat org.apache.sqoop.Sqoop.main(Sqoop.java:236)\nat org.apache.oozie.action.hadoop.SqoopMain.runSqoopJob(SqoopMain.java:206)\nat org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:174)\nat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:39)\nat org.apache.oozie.action.hadoop.SqoopMain.main(SqoopMain.java:45)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:226)\nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2016-03-07 12:33:49,598 ERROR [main] tool.ImportTool (ImportTool.java:run(609)) - Encountered IOException running import job: java.io.IOException: Hive exited with status 1\nat org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:385)\nat org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:335)\nat org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:239)\nat org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:511)\nat org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)\nat org.apache.sqoop.Sqoop.run(Sqoop.java:143)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)\nat org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)\nat org.apache.sqoop.Sqoop.main(Sqoop.java:236)\nat org.apache.oozie.action.hadoop.SqoopMain.runSqoopJob(SqoopMain.java:206)\nat org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:174)\nat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:39)\nat org.apache.oozie.action.hadoop.SqoopMain.main(SqoopMain.java:45)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:226)\nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n</pre><p>Thank you </p><p><a rel=\"user\" href=\"/users/393/aervits.html\" nodeid=\"393\">@Artem Ervits</a> </p>","tags":["Sqoop","Oozie","Hive"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-07 20:12:24.0","id":21800,"title":"can not login to ranger using LDAP user after user sync","body":"<p>\n\tHi,</p><p>\n\tI successfully configured and sync the users/groups with Ranger from LDAP. I see the users and groups successfully on the Ranger UI. </p><p>\n\tHowever, when I try to login to ranger UI using the LDAP user, it fails saying \"invalid user credentials.\" There is no other information or exception. </p><p>In the audit log see the record of \"wrong password\" under login. However, validated that the user's password is correct by logging in as that user from console. </p><p>\n\tI followed the instructions as mentioned in --</p><p>\n\thttps://community.hortonworks.com/articles/16696/ranger-ldap-integration.html</p><p>\n\tAssigned the LDAP user in the Ranger UI as \"admin\" role. Still does not work. </p><p>\n\tThe configuration that I have is: </p><p style=\"margin-left: 40px;\">\n\tEnable User Sync --&gt; Yes\n        LDAP/AD URL --&gt; ldap://ldap-server.com:389\n\t\n\n        Sync Source --&gt; LDAP/AD\n\t\n\n\tBind Anonymous --&gt; No\n\t\n\n\tBind User --&gt; cn=root\n\t\n\n\tBind User Password --&gt; password for root \n\t\n\n\tUsername Attribute\n\t--&gt; uid\n\t\n\n\tUser Object Class\n\t--&gt; posixAccount\n\t\n\n\tUser Search Base\n\t--&gt; dc=hadoop,dc=com\n\t\n\n\tUser Search Filter\n\t--&gt; (uid=*)\n\t\n\n\tUser Search Scope\n\t--&gt; blank\n\n\tUser Group Name Attribute\n\t--&gt; cn\n\t\n\n\tGroup User Map Sync\n\t--&gt; Yes </p><p style=\"margin-left: 40px;\">Enable Group Sync --&gt; Yes \nGroup Member Attribute --&gt; member\nGroup Name Attribute --&gt; cn\nGroup Object Class --&gt; posixGroup\nGroup Search Base --&gt;dc=hadoop,dc=com\nGroup Search Filter --&gt; (member=*)</p><p>I am expecting to be able to login into Ranger UI using the LDAP user, as mentioned in the article: </p><p>https://community.hortonworks.com/articles/16696/ranger-ldap-integration.html</p><p>Regards, </p><p>Madhavi. </p>","tags":["ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-08 10:45:57.0","id":21863,"title":"How to create phoenix table using existing hbase table ??","body":"<p>my use case i need to create phoenix table by using existing hbase table. I have got only one way to map hbase table i,e creating view table. is there any other way to create phoenix table by using existing hbase table ??</p>","tags":["Hbase","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-08 09:11:37.0","id":21876,"title":"How to remove an old HDP version","body":"<p>How are you supposed to remove an old version of HDP once you have successfully upgraded to a new version?</p><p>The old version is still listed as installed and the \"Deregister\" button is disabled because \"it is installed\".</p><p>It would be easy to delete the /usr/hdp/[old version] folder but all the packages would still be considered installed by the OS. On the other hand trying to remove the old packages manually on each node is cumbersome and risky.</p><p>Leaving behing GBs of data and lots of packages will easily buildup as you upgrade to new versions over the years.</p>","tags":["Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-09-02 03:30:37.0","id":54620,"title":"MiNiFi usage as a test harness framework","body":"<p>hello, I am trying to figure out how to best test the dataflow created using NiFi. My data flow consists of many process groups within process groups with each process group performing multiple tasks of transformation and enrichment both based on rules and from data fetched from other systems like databases or Web services, before the flow file exits the system onto JMS to be consumed by different system.</p><p>There were multiple options suggested however each requiring dedicated creation of full fledged test harness.</p><p>My question was, is there a way I can use MiNiFi and feed it a part of my dataflow under test as yml file and feed the dataflow variety of test data and as output capture the attributes to assert my test outcome. This can function as somewhat in container testing and verification of my data flow  before actually promoting any changes to Nifi dataflow. </p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-02 07:07:22.0","id":54645,"title":"please help me out how to find the alerts coming from which hosts in the hadoop cluster","body":"","tags":["hadoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-04 07:27:46.0","id":25742,"title":"I have setup Sanbox VM and it is working fine, but how can i connect and explore HDP, like copying files etc, like HDFS explorer","body":"<p>I have setup Sanbox VM and it is working fine, but how can i connect and explore HDP, like copying files etc, like HDFS explorer</p><p>I would like to connect to and see sandbox HDFS/HDP files system, Please provide details of how can i do it.</p>","tags":["HDFS"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-04 15:40:41.0","id":25828,"title":"UDF reflect is not allowed - beeline","body":"<p>not able to use refect function using beeline, same query just works fine with hivecli.</p><p>both in hivecli & beeline when i search for show functions i could see reflect .. I donot see any denials in ranger</p><p> SELECT reflect(\"java.net.URLDecoder\",\"decode\",search_query) FROM jlp_endeca_searches limit 5;\nError: Error while compiling statement: FAILED: SemanticException UDF reflect is not allowed (state=42000,code=40000)</p>","tags":["beeline","udf","hiveserver2"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-05 12:38:35.0","id":25944,"title":"while saving  cleansedEmailFeed  . Error is generated as org.xml.sax.saxparseexception cvc-complex-type.4 attribute . could you please help me out?","body":"","tags":["hdp-2.4.0","tutorial-580"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-06 20:15:57.0","id":26269,"title":"How many salt buckets should I use for my Phoenix tables?","body":"<p>How do upserts of new records impact the number of pre-split regions?</p><p>How do updates of existing records impact the number of pre-split regions?</p>","tags":["Hbase","Phoenix"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-07 21:31:42.0","id":26526,"title":"Falcon: When is instance created?","body":"<p>I have a question on when an instance is created. Following is my scenario. </p><p>If the frequency for a process entity is 1 day and start time let us say 2016-04-07 9am. I submit and schedule this entity let us say 2016-04-07 at 6am. The following are my questions:</p><p>1. When will an instance of this entity is created for today?</p><p>2. When will an instance be created for tomorrow and so on?</p><p>3. Is there a configurable parameter to adjust the timing like 2 hours before the frequency is approaching etc?</p><p>Thanks</p><p>Nagaraju</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-04-08 09:44:25.0","id":26584,"title":"Develop a RESTful API for a Front End","body":"<p>Hi all,</p><p>I'd like to develop a front end to run various algorithms I developed and visualise results. I need my Front End to be personalised that's why I didn't use Hue. To achieve this I thought about developing a RESTful API using Java Jersey and Hive JDBC to be called from AngularJS.</p><p>Is this a good choice ? or I've other alternatives (suggestions are welcome)?</p><p>Does Hive JDBC support concurrency and simultaneous queries ?  </p>","tags":["api","hiveserver2","hive-jdbc"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-09 11:24:24.0","id":26750,"title":"Hello..I am installing Hortonworks sandbox on oracle VM virtual box. after I click start it gives an error VT-x is disabled in bios..how can I resolve this error?","body":"","tags":["Sandbox","installation"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-04-12 02:35:21.0","id":27067,"title":"VD: error VERR_VD_VMDK_INVALID_HEADER opening image file  , Says unable to  open file Hortonworks_sanbox_with_hdp_2_4_virtualbox-disk1.vmdk","body":"<p>my macbook shut down forcefully while virtual box was open , after I restarted Macbook , I couldn't open the virtual box and it throws the above error. Kindly suggest on what file to be copied and from where I can find the correct version of file to repair this error. </p>","tags":["virtualbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-12 16:05:22.0","id":27192,"title":"Getting Kafka Consumer offsets - getting null for both kafka-consumer-groups.sh and kafka-consumer-offset-checker.sh","body":"<p>Hi,</p><p>When trying to get the current offset of a consumer group for a topic I'm getting null. I have tried this on a kerberised environment in 2.3.4. Has anyone else seen this? Is it a misconfiguration or am I not understanding how offset checking is supposed to work?</p><pre>./kafka-consumer-groups.sh -zookeeper zk1:2181 -describe -group $group --security-protocol PLAINTEXTSASL\nGROUP, TOPIC, PARTITION, CURRENT OFFSET, LOG END OFFSET, LAG, OWNER\nError while executing consumer group command null\njava.nio.channels.ClosedChannelException\n        at kafka.network.BlockingChannel.send(BlockingChannel.scala:122)\n        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:114)\n        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:99)\n        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:165)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService$$anonfun$getLogEndOffset$1.apply(ConsumerGroupCommand.scala:205)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService$$anonfun$getLogEndOffset$1.apply(ConsumerGroupCommand.scala:202)\n        at scala.Option.map(Option.scala:145)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService.getLogEndOffset(ConsumerGroupCommand.scala:202)\n        at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$class.kafka$admin$ConsumerGroupCommand$ConsumerGroupService$$describePartition(ConsumerGroupCommand.scala:133)\n        at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$$anonfun$describeTopicPartition$2.apply(ConsumerGroupCommand.scala:114)\n        at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$$anonfun$describeTopicPartition$2.apply(ConsumerGroupCommand.scala:113)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$class.describeTopicPartition(ConsumerGroupCommand.scala:113)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService.describeTopicPartition(ConsumerGroupCommand.scala:142)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService.kafka$admin$ConsumerGroupCommand$ZkConsumerGroupService$$describeTopic(ConsumerGroupCommand.scala:189)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService$$anonfun$describeGroup$1.apply(ConsumerGroupCommand.scala:174)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService$$anonfun$describeGroup$1.apply(ConsumerGroupCommand.scala:174)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService.describeGroup(ConsumerGroupCommand.scala:174)\n        at kafka.admin.ConsumerGroupCommand$ConsumerGroupService$class.describe(ConsumerGroupCommand.scala:96)\n        at kafka.admin.ConsumerGroupCommand$ZkConsumerGroupService.describe(ConsumerGroupCommand.scala:142)\n        at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:75)\n        at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)\n</pre>\n<pre>./kafka-consumer-offset-checker.sh --zookeeper zk1:2181 --group $group --security-protocol PLAINTEXTSASL\n[2016-04-12 16:56:13,227] WARN WARNING: ConsumerOffsetChecker is deprecated and will be dropped in releases following 0.9.0. Use ConsumerGroupCommand instead. (kafka.tools.ConsumerOffsetChecker$)\n[2016-04-12 16:56:13,403] ERROR The TGT cannot be renewed beyond the next expiry date: Wed Apr 13 02:30:33 BST 2016.This process will not be able to authenticate new SASL connections after that time (for example, it will not be able to authenticate a new connection with a Kafka Broker).  Ask your system administrator to either increase the 'renew until' time by doing : 'modprinc -maxrenewlife null ' within kadmin, or instead, to generate a keytab for null. Because the TGT's expiry cannot be further extended by refreshing, exiting refresh thread now. (org.apache.kafka.common.security.kerberos.Login)\nGroup           Topic                          Pid Offset          logSize         Lag             Owner\nExiting due to: null.\n</pre><p>Alternatively, is there another reliable way to get consumer group offsets?</p>","tags":["offset","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-13 10:38:14.0","id":27314,"title":"how to apply sentiment analysis algorithms on data stored in hive by using spark ?","body":"<p>Hello everyone,\nI'm streaming data from twitter  in json format by using apache flume and storing it then in HDFS, then i load json files from hdfs into an external hive table, but the  problem is that tweets texts aren't classified, i need to extract only profils who are against the issue i'm working on , and i was wondering how can i use apache spark to extract these data from my hive table and then apply sentiment analysis algorithms on extraxted tweets texts.</p><p>Thanks. </p>","tags":["hdp-2.4.0","tutorial-400"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-14 12:49:08.0","id":27582,"title":"After upgrading to Ambari .2.2.1 container-executor.cfg is always overwritten by a template container-executor.cfg.j2","body":"<p>After upgrading to Ambari 2.2.1 When restarting NodeManager ambari overwrites the container-executor.cfg with a template in file container-executor.cfg.j2 .</p><p>This template is wrong and causing the yarn to failed any job because the min user is 1000.</p>","tags":["yarn-scheduler","ambari-server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-15 15:39:09.0","id":27791,"title":"Unable to open Ranger Admin UI after enabling https (6182)","body":"<p>After Enabling https Ranger Admin UL with 6182 not working</p>","tags":["ranger-admin"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-18 02:35:21.0","id":27963,"title":"policy composition","body":"<p>If there are multiple policies have conflict results (these policies may defined in the same or different magnitude ), how to handle the conflictions?</p><p>Also, is there possible to compose two policies together as one?</p>","tags":["policy"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-18 06:59:29.0","id":27957,"title":"Does Ranger 0.5 work without Solr?","body":"<p>Hi, Currently I am working with HDP 2.3 and Ranger 0.5. I want to install hdfs plugin on the namenode so that I can see the audit logs in Ranger UI. I also see this message:\"Ranger Plugin for hadoop has been enabled. Please restart hadoop to ensure that changes are effective.\" But, I am not able to see any plugin in audits-&gt;plugins. I am not able to see any logs also in the UI. What can be the issue?  Is it necessary to have Solr? Or the audits can also be saved in the DB as in Ranger 0.4?</p>","tags":["HDFS","audit","plugins"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-18 15:04:37.0","id":28060,"title":"Can sqoop be used to directly import data into an ORC table?","body":"<p>Right now, we use a 2 step process to import data from sqoop to ORC tables. </p><p>Step 1: Use sqoop to import raw text (in text format) into Hive tables. </p><p>Step 2: Use insert overwrite as select to write this into a hive table that is of type ORC. </p><p>Now, with this approach, we have to manually create ORC backed tables that Step 2 writes into. This also ends up with raw data in text format that we don't really need. Is there a way to directly write into hive tables as ORC format? Also, is there a way to not manually create ORC backed tables from text file backed tables? </p>","tags":["Sqoop","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-19 05:47:11.0","id":28224,"title":"Strange issue with beeline","body":"<p>I was working with beeline for quite a long time and this morning I got a strange issue. When I executed the below simple beeline command I didn't get any output.</p><pre>beeline -u jdbc:hive2://10.241.1.8:10000 -f testarea/hive.hql</pre><p>here is what I get</p><pre>$ beeline -u jdbc:hive2://10.241.1.8:10000 -f testarea/hive.hql \nscan complete in 3ms \nConnecting to jdbc:hive2://10.241.1.8:10000 \nConnected to: Apache Hive (version 0.13.1-cdh5.3.6) \nDriver: Hive JDBC (version 0.13.1-cdh5.3.6) \nTransaction isolation: TRANSACTION_REPEATABLE_READ \n0: jdbc:hive2://10.241.1.8:10000&gt; select count (*) from active_139;\nClosing: 0: jdbc:hive2://10.241.1.8:10000</pre><p>the hive.hql contains the below hive query</p><pre>select count (*) from active_139;</pre><p>However if I execute the same hive query from beeline it works.</p><pre>$ beeline -u jdbc:hive2://10.241.1.8:10000 -e 'select count (*) from active_139;'\nscan complete in 3ms\nConnecting to jdbc:hive2://10.241.1.8:10000\nConnected to: Apache Hive (version 0.13.1-cdh5.3.6)\nDriver: Hive JDBC (version 0.13.1-cdh5.3.6)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n+---------+--+\n|   _c0   |\n+---------+--+\n| 121837  |\n+---------+--+\n1 row selected (23.827 seconds)\nBeeline version 0.13.1-cdh5.3.6 by Apache Hive\nClosing: 0: jdbc:hive2://10.241.1.8:10000</pre><p>Any body got a clue on this?</p>","tags":["beeline"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-19 16:46:07.0","id":28303,"title":"Was the Hex Viewer for application/octet-stream removed and if so, why?","body":"<p>It seems like the Hex viewer is not available for unknown data types anymore, but it seems that you should be able to hex view ANY data.</p><p>So, I just upgraded Nifi from 0.4.1 to 0.6.0. In 0.4.1 I could view Avro files coming out of an ExecuteSQL processor by selecting the Hex viewer option. It wasn't a perfect viewer, but it was usually enough for my purposes. With both versions the viewer page defaults to \"No viewer is registered for this content type\". In 0.4.1 the the \"View as\" combo box still has the Hex viewer option but in 0.6.0 there is no Hex option (it has no options at all). I can still download and view the content externally, but shouldn't the Hex viewer be available for all data/content types, even if they are application/octet-stream? Just wondering if this was an inadvertent break or if this was intentional or am I missing something? </p><p>I don't know what version changed this.</p><p>Thanks,\nJim</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-21 17:29:02.0","id":28769,"title":"Mobius (Spark API for C#) compatibility with HDP","body":"<p>Has anyone tried using Mobius with HDP 2.4? I am curious to know if it is a smooth ride or not... I assume it would require to deploy mono and Mobius to all the boxes but I am wondering what else there would be required to make these components work together.</p>","tags":["development","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-22 10:00:32.0","id":28910,"title":"Error while creating a table with 'LIKE' clause","body":"<p>I am trying this </p><p>create table emp1 LIKE emp CLUSTERED BY () INTO 4 BUCKETS STORED AS ORC;</p><p>Its giving a parse exception : </p>\n<pre></pre><ol><li>rg.apache.ambari.view.hive.client.HiveInvalidQueryException:Errorwhile compiling statement: FAILED:ParseException line 4:42 missing EOF at 'CLUSTERED' near 'emp'[ERROR_STATUS]</li><li>org.apache.ambari.view.hive.client.HiveInvalidQueryException:Errorwhile compiling statement: FAILED:ParseException line 4:42 missing EOF at 'CLUSTERED' near 'emp'[ERROR_STATUS]</li><li>\tat org.apache.ambari.view.hive.client.Utils.verifySuccess(Utils.java:46)</li></ol>","tags":["Hive","data-migration"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-22 20:38:36.0","id":29074,"title":"Publish JMS processor failing","body":"<p>MQ provider: IBM Websphere 7.5</p><p>HDF version: 1.2</p><p>Trying to consume from A.INBOUND queue using <strong>ConsumeJMS</strong> processor and post a message to another Queue : A.OUTBOUND using <strong>PublishJMS</strong> processor. Both processors use the same JMS connection service.</p><p>ConsumeJMS works fine, but PublishJMS fails with the following error:</p><pre>16:36:52 EDT ERROR 8bd6ca6e-6cc6-4553-9435-08481dffbe2b PublishJMS - JMSPublisher[destination:A.OUTBOUND; pub-sub:false;] Failed while sending message to JMS via JMSPublisher[destination:A.OUTBOUND; pub-sub:false;]</pre><p>MQ server is running locally.</p><p>I can confirm the connection info is correct and the queues exist.</p><p>Any help is appreciated.</p>","tags":["Nifi","jms","ibm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-27 03:45:59.0","id":29785,"title":"How to enable internet connection through company LAN and proxy for Hortonworks Sandbox for window using VMWare","body":"<p>Hi,</p><p>I've recently downloaded Hortonworks Sandbox for window for VMWare and able to open Hortonworks Sandbox for Window using VMWare successfully.  However, I only able to launch sandbox browser when the network connection to wireless and LAN is unplug and disabled.  Will not able to launch sandbox browser once it's connected to LAN/wireless networks.</p><p>May I check how to enable \"Internet Connection\" for Hortonworks Sandbox.  I'll like to be able to work on Hortonworks browser ambari and be able to access internet at the same time.</p><p>Is there any specific setting for network adaptor in VMWare, in Sandbox, and in window host?</p><p>Thank you.</p><p>Regards,</p><p>Kelly</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-28 18:25:40.0","id":30245,"title":"how to set up ambari metrics from ambari?","body":"<p>how to set up Ambari Metrics?</p>","tags":["ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-29 12:41:54.0","id":30405,"title":"Node Manager not configured at the starting , how to add it , I want to add them on the existings servers from the cluster","body":"","tags":["Ambari","nodemanager"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-30 02:00:50.0","id":30536,"title":"Where can i run the hdfs commands","body":"<p>I started my HDP2.4 in vmware, & i could able to launch the ambari browser & i can see my hdfs files there..</p><p>But if i want to run the HDFS commands,, where can i run them...?</p>","tags":["hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-30 07:13:16.0","id":30564,"title":"Too many open files in region server logs","body":"<p>I have 3 region servers and their total size on HDFS is ~50G only. I have ulimit set to unlimited and for the hbase user also the value is very high (32K +). I am noticing following in my logs very often after which I start getting HFile corruption exceptions:</p><p>2016-04-27 16:44:46,845 WARN \n[StoreFileOpenerThread-g-1] hdfs.DFSClient: Failed to connect to\n/10.45.0.51:50010 for block, add to deadNodes and continue. java.net.SocketException:\n<strong>Too many open files</strong>\njava.net.SocketException: Too many open files\n       at\nsun.nio.ch.Net.socket0(Native Method)</p><p>After many of these open files issues, I get a barrage of HFile corrupt issues too and hbase fails to come up:</p><p>2016-04-27 16:44:46,313 ERROR\n[RS_OPEN_REGION-secas01aplpd:44461-1] handler.OpenRegionHandler: Failed open of\nregion=lm:DS_326_A_stage,\\x7F\\xFF\\xFF\\xF8,1460147940285.1a764b8679b8565c5d6d63e349212cbf.,\nstarting to roll back the global memstore size.</p><p>java.io.IOException: java.io.IOException:\norg.apache.hadoop.hbase.io.hfile.<strong>CorruptHFileException</strong>: <strong>Problem reading HFile\nTrailer from file</strong> hdfs://mycluster/MA/hbase/data/lm/DS_326_A_stage/1a764b8679b8565c5d6d63e349212cbf/e/63083720d739491eb97544e16969ffc7</p><p>     at\norg.apache.hadoop.hbase.regionserver.HRegion.initializeRegionStores(HRegion.java:836)\n     at\norg.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:747)\n     at\norg.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:718)</p><p>My questions are two:</p><p>1. No other process on this node shows up too many open files issue. Even data node seems to not show this error in logs. Not sure, why then this error should be reported.</p><p>2. Would an OfflineMetaRepair following by hbck -fixMeta and hbck -fixAssignments solve the issue?</p>","tags":["files","regionserver","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-02 19:51:08.0","id":30889,"title":"Cloudbreak - Tagging EC2 Instances?","body":"<p>Is there a way to tag EC2 instances created by cloudbreak?</p>","tags":["Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-02 13:39:28.0","id":30772,"title":"I'm getting a \"start token not found where expected\" when selecting data from a table using the native JSON serde.","body":"<p>I created a table based off the Hive native JSON SerDe. My intention is to create a table for tweets. I've attached <a href=\"/storage/attachments/3917-tweet-table.txt\">tweet-table.txt</a>.  The table is created without errors. I then load the data using the LOAD DATA INPATH command and I get no errors. When I try to select data from the table, I get the following error:</p><p>{\"message\":\"H170 Unable to fetch results. java.io.IOException: org.apache.hadoop.hive.serde2.SerDeException: java.io.IOException: Start token not found where expected [ERROR_STATUS]\",\"status\":500,\"trace\":\"org.apache.ambari.view.hive.client.HiveErrorStatusException: H170 Unable to fetch results. java.io.IOException: org.apache.hadoop.hive.serde2.SerDeException: java.io.IOException: Start token not found where expected [ERROR_STATUS]\\n\\norg.apache.ambari.view.hive.client.HiveErrorStatusException: H170 Unable to fetch results. java.io.IOException: org.apache.hadoop.hive.serde2.SerDeException: java.io.IOException: Start token not found where expected [ERROR_STATUS</p><p>Any suggestions?</p>","tags":["hive-serde","tweets","json"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-03 17:26:28.0","id":31173,"title":"Hive table via HCat question","body":"<p>What is the nature of write operation using HCatOutputFormatWriter APIs. is it completely atomic(either completes or no write)? </p><p>What will happen when job gets killed after all the Tasks have committed but before the Job commit?</p>","tags":["Hive","webhcat"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-04 02:48:11.0","id":31277,"title":"Not able to retrieve average load Ambari metric using API","body":"<p>I am trying to run some test API calls on HDP Sandbox 2.4 with Ambari 2.2.1.0 and facing some issues. Here is the call API call I am making.</p><pre>http://localhost:8080/api/v1/clusters/Sandbox?fields=metrics/load[1462233600,1462406399]&_=1462328977\n</pre><p>and all I am getting in return is</p><pre>{\n  \"href\" : \"http://localhost:8080/api/v1/clusters/Sandbox?fields=metrics/load[1462233600,1462406399]&_=1462328977\",\n  \"Clusters\" : {\n    \"cluster_name\" : \"Sandbox\",\n    \"version\" : \"HDP-2.4\"\n  }\n</pre><p>Would appreciate any help here.</p>","tags":["ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-04 13:46:26.0","id":31438,"title":"Getting all NULLS when selecting from a Hive  JSON table","body":"<p>I’m trying to get some twitter JSON to show up in Hive and I’m not having any luck. All I get are NULLS returned and no errors. I've tried the native JSON serde as well as the openx serde but get the same results.</p><p><em>LOAD DATA INPATH '/tmp/tweets_staging/‘ OVERWRITE INTO tweets;</em></p><p><em>ADD JAR /hadoop/json-serde-1.3.8-SNAPSHOT-jar-with-dependencies.jar;</em></p><p><em>SELECT * FROM tweets LIMIT 100;</em></p><p><img src=\"/storage/attachments/4001-9688f87e-f96b-4330-833c-c2e59ab2e9c9.png\"></p><p><em><a href=\"/storage/attachments/4002-tweet-table.txt\">tweet-table.txt</a></em></p>","tags":["hive-serde","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-04 15:43:41.0","id":31472,"title":"Ambari DB migration from postgres to Oracle","body":"<p>Tried https://hortonworks.my.salesforce.com/kA0E0000000fy7d.But when I source /tmp/ambari.sql , Its failing with syntax errors . Tried using Mysql workbench But 19 tables got affected.</p>","tags":["ambari-server"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-11 21:03:41.0","id":32822,"title":"Spark Kafa createDirectStream failed while createStream successful","body":"<p>I'm testing spark streaming from Kafka code from <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\">Spark example</a> in HDP 2.4 and Spark 1.6.</p><p>I tried <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\">KafkaWordCount.scala</a> example successful with </p><p>`$ bin/run-example org.apache.spark.examples.streaming.KafkaWordCount zoo01,zoo02,zoo03  my-consumer-group topic1,topic2 1`</p><p>But <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\">DirectKafkaWordCount.scala</a> failed with below query and error</p><p>bin/run-example streaming.DirectKafkaWordCount broker1-host:port,broker2-host:port topic1,topic2</p><pre>16/05/11 13:52:02 INFO SimpleConsumer: Reconnect due to socket error: java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.\norg.apache.spark.SparkException: java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.\n        at org.apache.spark.streaming.kafka.KafkaCluster$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)\n        at org.apache.spark.streaming.kafka.KafkaCluster$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)</pre>","tags":["spark-streaming","Kafka","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-11 22:17:40.0","id":32833,"title":"Encryption Overhead Metrics","body":"<p>Are there any available metrics for the overhead associated with enabling the various types of encryption (RPC, HTTPS, etc...) across the cluster?</p>","tags":["network","wire-encryption","encryption"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-12 09:42:06.0","id":32907,"title":"error while starting namenode (HA)","body":"<p>Incompatible namespaceID for journal Storage Directory</p>","tags":["namenode-ha"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-12 13:56:28.0","id":32964,"title":"Hive CLI throwing ClientHandlerException on Sandbox 2.4","body":"<p>I just downloaded the latest sandbox image (2.4) </p><p>I brought up ambari and started all the appropriate services and made everything green.</p><p>But something seems to be wrong with Hive.</p><p>I tried to open a Hive CLI instance via SSH, and I get the stack trace below whenever I perform any kind of query.</p><p>Does anyone know the solution for this?</p><p>[root@sandbox parser]# hive </p><p>WARNING: Use \"yarn jar\" to launch YARN applications.\nLogging initialized using configuration in file:/etc/hive/2.4.0.0-169/0/hive-log4j.properties </p><p>hive&gt; show databases; </p><p>FAILED: Hive Internal Error: com.sun.jersey.api.client.ClientHandlerException(java.io.IOException: java.net.ConnectException: Connection refused)\ncom.sun.jersey.api.client.ClientHandlerException: java.io.IOException: java.net.ConnectException: Connection refused\n        at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149)\n        at com.sun.jersey.api.client.Client.handle(Client.java:648)\n        at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670)\n        at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)\n        at com.sun.jersey.api.client.WebResource$Builder.method(WebResource.java:623)\n        at org.apache.atlas.AtlasClient.callAPIWithResource(AtlasClient.java:351)\n        at org.apache.atlas.AtlasClient.callAPIWithResource(AtlasClient.java:346)\n        at org.apache.atlas.AtlasClient.getType(AtlasClient.java:191)\n        at org.apache.atlas.hive.bridge.HiveMetaStoreBridge.registerHiveDataModel(HiveMetaStoreBridge.java:496)\n        at org.apache.atlas.hive.hook.HiveHook.fireAndForget(HiveHook.java:194)\n        at org.apache.atlas.hive.hook.HiveHook.run(HiveHook.java:172)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1585)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1254)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1118)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1108)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.io.IOException: java.net.ConnectException: Connection refused\n        at org.apache.atlas.security.SecureClientUtils$1$1.run(SecureClientUtils.java:107)\n        at org.apache.atlas.security.SecureClientUtils$1$1.run(SecureClientUtils.java:99)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.atlas.security.SecureClientUtils$1.getHttpURLConnection(SecureClientUtils.java:99)\n        at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:159)\n        at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:147)\n        ... 26 more\nCaused by: java.net.ConnectException: Connection refused\n        at java.net.PlainSocketImpl.socketConnect(Native Method)\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n        at java.net.Socket.connect(Socket.java:579)\n        at sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\n        at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)\n        at sun.net.www.http.HttpClient.New(HttpClient.java:308)\n        at sun.net.www.http.HttpClient.New(HttpClient.java:326)\n        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:998)\n        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:934)\n        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:852)\n        at org.apache.hadoop.security.authentication.client.PseudoAuthenticator.authenticate(PseudoAuthenticator.java:76)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:128)\n        at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:215)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:322)\n        at org.apache.atlas.security.SecureClientUtils$1$1.run(SecureClientUtils.java:103)\n        ... 33 more</p>","tags":["Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-12 12:13:31.0","id":32948,"title":"Malformed ORC file Invalid postscript","body":"<p>Stack : Installed HDP-2.3.2.0-2950 using Ambari 2.1</p><p>sqoop import</p><pre>\nsqoop import --connect 'jdbc:sqlserver://dbserver;database=dbname' --username username --password password --as-textfile --fields-terminated-by '|'  --table DimECU  --warehouse-dir /dataload/tohdfs/reio/odpdw/may2016 --verbose</pre><p>create external table</p><pre>CREATE EXTERNAL TABLE IF NOT EXISTS DimECU (`ECU_ID` int,`ECU_Name` varchar(15),`ECU_FAMILY_NAME` varchar(15),`INSERTED_BY`varchar(64),`INSERTION_DATE` timestamp) ROW FORMAT DELIMITED\n   FIELDS TERMINATED BY '|' STORED AS ORC LOCATION '/dataload/tohdfs/reio/odpdw/may2016/DimECU';</pre><p>Can't select the data :</p><pre>hive (odp_dw_may2016_orc)&gt;\n                         &gt;\n                         &gt; select * from DimECU limit 5;\nOK\ndimecu.ecu_id   dimecu.ecu_name dimecu.ecu_family_name  dimecu.inserted_by      dimecu.insertion_date\nFailed with exception java.io.IOException:org.apache.hadoop.hive.ql.io.FileFormatException: Malformed ORC file hdfs://l1031lab.sss.se.scania.com:8020/dataload/tohdfs/reio/odpdw/may2016/DimECU/part-m-00000. Invalid postscript.\nTime taken: 0.074 seconds</pre><p>The exception is :</p><pre>2016-05-12 13:17:26,334 ERROR [main]: CliDriver (SessionState.java:printError(960)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.io.FileFormatException: Malformed ORC file hdfs://l1031lab.sss.se.scania.com:8020/dataload/tohdfs/reio/odpdw/may2016/DimECU/part-m-00000. Invalid postscript.\njava.io.IOException: org.apache.hadoop.hive.ql.io.FileFormatException: Malformed ORC file hdfs://l1031lab.sss.se.scania.com:8020/dataload/tohdfs/reio/odpdw/may2016/DimECU/part-m-00000. Invalid postscript.\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)\nat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)\nat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: org.apache.hadoop.hive.ql.io.FileFormatException: Malformed ORC file hdfs://l1031lab.sss.se.scania.com:8020/dataload/tohdfs/reio/odpdw/may2016/DimECU/part-m-00000. Invalid postscript.\nat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.ensureOrcFooter(ReaderImpl.java:251)\nat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:376)\nat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.&lt;init&gt;(ReaderImpl.java:317)\nat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:237)\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1208)\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1117)\nat org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:324)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:446)\n... 15 more\n2016-05-12 13:17:26,334 INFO  [main]: exec.TableScanOperator (Operator.java:close(613)) - 0 finished. closing... \n2016-05-12 13:17:26,334 INFO  [main]: exec.SelectOperator (Operator.java:close(613)) - 1 finished. closing... \n2016-05-12 13:17:26,334 INFO  [main]: exec.LimitOperator (Operator.java:close(613)) - 2 finished. closing... \n2016-05-12 13:17:26,334 INFO  [main]: exec.ListSinkOperator (Operator.java:close(613)) - 4 finished. closing... \n2016-05-12 13:17:26,334 INFO  [main]: exec.ListSinkOperator (Operator.java:close(635)) - 4 Close done\n2016-05-12 13:17:26,334 INFO  [main]: exec.LimitOperator (Operator.java:close(635)) - 2 Close done\n2016-05-12 13:17:26,335 INFO  [main]: exec.SelectOperator (Operator.java:close(635)) - 1 Close done\n2016-05-12 13:17:26,335 INFO  [main]: exec.TableScanOperator (Operator.java:close(635)) - 0 Close done\n2016-05-12 13:17:26,352 INFO  [Atlas Logger 0]: security.SecureClientUtils (SecureClientUtils.java:getClientConnectionHandler(91)) - Real User: hive (auth:SIMPLE), is from ticket cache? false\n2016-05-12 13:17:26,353 INFO  [Atlas Logger 0]: security.SecureClientUtils (SecureClientUtils.java:getClientConnectionHandler(94)) - doAsUser: hive\n2016-05-12 13:17:26,356 INFO  [main]: CliDriver (SessionState.java:printInfo(951)) - Time taken: 0.065 seconds\n2016-05-12 13:17:26,356 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - &lt;PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver&gt;\n2016-05-12 13:17:26,356 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - &lt;/PERFLOG method=releaseLocks start=1463051846356 end=1463051846356 duration=0 from=org.apache.hadoop.hive.ql.Driver&gt;\n2016-05-12 13:17:26,989 INFO  [Atlas Logger 0]: hook.HiveHook (HiveHook.java:run(168)) - Atlas hook failed\norg.apache.atlas.AtlasServiceException: Metadata service API SEARCH_GREMLIN failed with status 400(Bad Request) Response Body ({\"error\":\"javax.script.ScriptException: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\",\"stackTrace\":\"org.apache.atlas.discovery.DiscoveryException: javax.script.ScriptException: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat org.apache.atlas.discovery.graph.GraphBackedDiscoveryService.searchByGremlin(GraphBackedDiscoveryService.java:175)\\n\\tat org.apache.atlas.GraphTransactionInterceptor.invoke(GraphTransactionInterceptor.java:41)\\n\\tat org.apache.atlas.web.resources.MetadataDiscoveryResource.searchUsingGremlinQuery(MetadataDiscoveryResource.java:155)\\n\\tat sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:606)\\n\\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\\n\\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\\n\\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\\n\\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\\n\\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\\n\\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\\n\\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\\n\\tat org.apache.atlas.web.filters.AuditFilter.doFilter(AuditFilter.java:67)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\\n\\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\\n\\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\\n\\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\\n\\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\\n\\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\\n\\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\\n\\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\\n\\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\\n\\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\\n\\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\\n\\tat org.mortbay.jetty.Server.handle(Server.java:326)\\n\\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\\n\\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\\n\\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\\n\\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\\n\\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\\n\\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\\n\\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\\nCaused by: javax.script.ScriptException: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:94)\\n\\tat javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:233)\\n\\tat org.apache.atlas.discovery.graph.GraphBackedDiscoveryService.searchByGremlin(GraphBackedDiscoveryService.java:172)\\n\\t... 48 more\\nCaused by: javax.script.ScriptException: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:221)\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:90)\\n\\t... 50 more\\nCaused by: com.thinkaurelius.titan.core.TitanException: Could not start new transaction\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.newTransaction(StandardTitanGraph.java:276)\\n\\tat com.thinkaurelius.titan.graphdb.transaction.StandardTransactionBuilder.start(StandardTransactionBuilder.java:220)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.newThreadBoundTransaction(StandardTitanGraph.java:265)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.getAutoStartTx(TitanBlueprintsGraph.java:104)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.query(TitanBlueprintsGraph.java:225)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.query(TitanBlueprintsGraph.java:27)\\n\\tat com.tinkerpop.pipes.transform.GraphQueryPipe.processNextStart(GraphQueryPipe.java:34)\\n\\tat com.tinkerpop.pipes.transform.GraphQueryPipe.processNextStart(GraphQueryPipe.java:17)\\n\\tat com.tinkerpop.pipes.AbstractPipe.next(AbstractPipe.java:89)\\n\\tat com.tinkerpop.pipes.IdentityPipe.processNextStart(IdentityPipe.java:19)\\n\\tat com.tinkerpop.pipes.AbstractPipe.next(AbstractPipe.java:89)\\n\\tat com.tinkerpop.pipes.IdentityPipe.processNextStart(IdentityPipe.java:19)\\n\\tat com.tinkerpop.pipes.AbstractPipe.hasNext(AbstractPipe.java:98)\\n\\tat com.tinkerpop.pipes.util.Pipeline.hasNext(Pipeline.java:105)\\n\\tat org.codehaus.groovy.runtime.DefaultGroovyMethods.toList(DefaultGroovyMethods.java:1946)\\n\\tat org.codehaus.groovy.runtime.dgm$836.invoke(Unknown Source)\\n\\tat org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:271)\\n\\tat org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:53)\\n\\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)\\n\\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)\\n\\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)\\n\\tat Script180.run(Script180.groovy:1)\\n\\tat com.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:219)\\n\\t... 51 more\\nCaused by: com.thinkaurelius.titan.diskstorage.PermanentBackendException: Could not start BerkeleyJE transaction\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJEStoreManager.beginTransaction(BerkeleyJEStoreManager.java:144)\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJEStoreManager.beginTransaction(BerkeleyJEStoreManager.java:34)\\n\\tat com.thinkaurelius.titan.diskstorage.keycolumnvalue.keyvalue.OrderedKeyValueStoreManagerAdapter.beginTransaction(OrderedKeyValueStoreManagerAdapter.java:52)\\n\\tat com.thinkaurelius.titan.diskstorage.Backend.beginTransaction(Backend.java:465)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.openBackendTransaction(StandardTitanGraph.java:282)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.newTransaction(StandardTitanGraph.java:272)\\n\\t... 73 more\\nCaused by: com.sleepycat.je.LogWriteException: (JE 5.0.73) Environment must be closed, caused by: com.sleepycat.je.LogWriteException: Environment invalid because of previous exception: (JE 5.0.73) \\/var\\/lib\\/atlas\\/data\\/berkeley java.io.IOException: No space left on device LOG_WRITE: IOException on write, log is likely incomplete. Environment is invalid and must be closed.\\n\\tat com.sleepycat.je.LogWriteException.wrapSelf(LogWriteException.java:72)\\n\\tat com.sleepycat.je.dbi.EnvironmentImpl.checkIfInvalid(EnvironmentImpl.java:1512)\\n\\tat com.sleepycat.je.Environment.checkEnv(Environment.java:2185)\\n\\tat com.sleepycat.je.Environment.beginTransactionInternal(Environment.java:1313)\\n\\tat com.sleepycat.je.Environment.beginTransaction(Environment.java:1284)\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJEStoreManager.beginTransaction(BerkeleyJEStoreManager.java:134)\\n\\t... 78 more\\nCaused by: com.sleepycat.je.LogWriteException: Environment invalid because of previous exception: (JE 5.0.73) \\/var\\/lib\\/atlas\\/data\\/berkeley java.io.IOException: No space left on device LOG_WRITE: IOException on write, log is likely incomplete. Environment is invalid and must be closed.\\n\\tat com.sleepycat.je.log.FileManager.writeLogBuffer(FileManager.java:1652)\\n\\tat com.sleepycat.je.log.LogBufferPool.writeBufferToFile(LogBufferPool.java:260)\\n\\tat com.sleepycat.je.log.LogBufferPool.writeCompleted(LogBufferPool.java:345)\\n\\tat com.sleepycat.je.log.LogManager.serialLogWork(LogManager.java:716)\\n\\tat com.sleepycat.je.log.LogManager.serialLogInternal(LogManager.java:493)\\n\\tat com.sleepycat.je.log.SyncedLogManager.serialLog(SyncedLogManager.java:42)\\n\\tat com.sleepycat.je.log.LogManager.multiLog(LogManager.java:395)\\n\\tat com.sleepycat.je.log.LogManager.log(LogManager.java:335)\\n\\tat com.sleepycat.je.txn.Txn.logCommitEntry(Txn.java:957)\\n\\tat com.sleepycat.je.txn.Txn.commit(Txn.java:719)\\n\\tat com.sleepycat.je.txn.Txn.commit(Txn.java:584)\\n\\tat com.sleepycat.je.Transaction.commit(Transaction.java:317)\\n\\tat com.thinkaurelius.titan.diskstorage.berkeleyje.BerkeleyJETx.commit(BerkeleyJETx.java:81)\\n\\tat com.thinkaurelius.titan.diskstorage.keycolumnvalue.cache.CacheTransaction.commit(CacheTransaction.java:198)\\n\\tat com.thinkaurelius.titan.diskstorage.BackendTransaction.commitStorage(BackendTransaction.java:117)\\n\\tat com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.commit(StandardTitanGraph.java:670)\\n\\tat com.thinkaurelius.titan.graphdb.transaction.StandardTitanTx.commit(StandardTitanTx.java:1337)\\n\\tat com.thinkaurelius.titan.graphdb.blueprints.TitanBlueprintsGraph.commit(TitanBlueprintsGraph.java:60)\\n\\tat org.apache.atlas.GraphTransactionInterceptor.invoke(GraphTransactionInterceptor.java:42)\\n\\tat org.apache.atlas.services.DefaultMetadataService.createEntity(DefaultMetadataService.java:231)\\n\\tat org.apache.atlas.web.resources.EntityResource.submit(EntityResource.java:96)\\n\\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:606)\\n\\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\\n\\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\\n\\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\\n\\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\\n\\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\\n\\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\\n\\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\\n\\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\\n\\tat org.apache.atlas.web.filters.AuditFilter.doFilter(AuditFilter.java:67)\\n\\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\\n\\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\\n\\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\\n\\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\\n\\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\\n\\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\\n\\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\\n\\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\\n\\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\\n\\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\\n\\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\\n\\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\\n\\tat org.mortbay.jetty.Server.handle(Server.java:326)\\n\\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\\n\\tat org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\\n\\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\\n\\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\\n\\t... 3 more\\nCaused by: java.io.IOException: No space left on device\\n\\tat java.io.RandomAccessFile.writeBytes0(Native Method)\\n\\tat java.io.RandomAccessFile.writeBytes(RandomAccessFile.java:520)\\n\\tat java.io.RandomAccessFile.write(RandomAccessFile.java:550)\\n\\tat com.sleepycat.je.log.FileManager.writeToFile(FileManager.java:1757)\\n\\tat com.sleepycat.je.log.FileManager.writeLogBuffer(FileManager.java:1637)\\n\\t... 65 more\\n\"})\nat org.apache.atlas.AtlasClient.callAPIWithResource(AtlasClient.java:365)\nat org.apache.atlas.AtlasClient.callAPIWithResource(AtlasClient.java:346)\nat org.apache.atlas.AtlasClient.searchByGremlin(AtlasClient.java:294)\nat org.apache.atlas.hive.bridge.HiveMetaStoreBridge.getEntityReferenceFromGremlin(HiveMetaStoreBridge.java:227)\nat org.apache.atlas.hive.bridge.HiveMetaStoreBridge.getProcessReference(HiveMetaStoreBridge.java:183)\nat org.apache.atlas.hive.hook.HiveHook.registerProcess(HiveHook.java:297)\nat org.apache.atlas.hive.hook.HiveHook.fireAndForget(HiveHook.java:202)\nat org.apache.atlas.hive.hook.HiveHook.access$200(HiveHook.java:54)\nat org.apache.atlas.hive.hook.HiveHook$2.run(HiveHook.java:166)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)</pre>","tags":["Sqoop","Hive","invalid-format"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-12 18:16:40.0","id":33027,"title":"Knox 0.6 log file issue","body":"<p>Hi, the gateway.log & gateway-audit.log is not getting updated after a while, however the log file timestamp is updated. </p><p>I had to manually start the gateway and run the following command to get the logging, starting from Ambari didn't work.</p><pre><code>java -jar bin/gateway.jar &gt; gateway.log</code></pre><p>Let me know what is causing this issue.</p>","tags":["Knox","knox-gateway"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-12 19:03:25.0","id":33055,"title":"Porting HUE Database over to Ambari Views","body":"<p>Hi All,</p><p>I am looking to move all my Hive and Pig scripts written in HUE over to Ambari Views. Is there a script that I can use to extract the data from the HUE RDBMS and import it into the appropriate Ambari Views?</p><p>Thanks,</p>","tags":["hue","ambari-views"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-13 09:30:36.0","id":33123,"title":"Adding Node to cluster - Assign Slaves and Clients window doesn't go forward.","body":"<p>I have registered a host successfully. Put tick marks on the slaves and client boxes, however, unable to go forward after that.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-31 18:48:15.0","id":25414,"title":"HDPCA Exam - Configure NameNode HA - Can you do the configuration using ambari or do you have to do it manually?","body":"<p>For the exam can you do the configurations using ambari or do i have to do it manually , the instructions link to the manual way (http://hortonworks.com/training/class/hdp-certified-administrator-hdpca-exam/) </p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_hadoop-ha/content/ch_HA-NameNode.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_hadoop-ha/content/ch_HA-NameNode.html</a></p>","tags":["hdpca"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-22 19:48:22.0","id":24135,"title":"Falcon UI issues","body":"<p><a href=\"/storage/attachments/2941-falconui.png\">falconui.png</a></p><p>In the attached image for Falcon UI, have two questions:</p><p>(1) In the entity tree, why black-ed out boxes?\n(2) In the lineage, why no directional arrows?</p><p>PS- Tutorial steps followed from link- http://hortonworks.com/hadoop-tutorial/defining-processing-data-end-end-data-pipeline-apache-falcon</p><p>TIA</p>","tags":["ui","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-04-01 11:28:19.0","id":25508,"title":"CSV Load into Phoenix Error","body":"<p>This is my script</p><p>REGISTER /usr/local/etlload/piggybank.jar; </p><p>REGISTER /usr/hdp/2.3.4.0-3485/phoenix/phoenix-4.4.0.2.3.4.0-3485-client.jar; </p><p>a = load </p><p>'/data/HDFS_TEST_DATA.txt' USING PigStorage('\\u0001') AS (DATA_KEY:chararray, EKEY:chararray, ETYPE:chararray, E_ID:chararray, E_VALUE:chararray, P_KEY:chararray); </p><p>STORE a into 'hbase://EQUIP_TEST' using org.apache.phoenix.pig.PhoenixHBaseStorage(hostname,'-batchSize 5000');</p><p>I am getting the following error</p><p>Pig Stack Trace\n---------------\nERROR 2244: Job failed, hadoop does not return any error message\norg.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message\n        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:179)\n        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:234)\n        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\n        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\n        at org.apache.pig.Main.run(Main.java:631)\n        at org.apache.pig.Main.main(Main.java:177)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p>","tags":["phoenix4.4"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-01 21:15:39.0","id":25584,"title":"Datanode bindexception after upgrade","body":"<p>I upgraded my cluster from 2.2 to 2.3 and now the datanodes aren't able to bind to the specified port.  Netstat shows the ports are not in use.</p><p>Has anyone seen this before?</p><p>ava.net.BindException: Port in use: localhost:0\nat org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:919)\nat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:856)\nat org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.&lt;init&gt;(DatanodeHttpServer.java:107)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:779)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1135)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.&lt;init&gt;(DataNode.java:430)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2411)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2345)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2526)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2550)\nCaused by: java.net.BindException: Cannot assign requested address\nat sun.nio.ch.Net.bind0(Native Method)\nat sun.nio.ch.Net.bind(Net.java:463)\nat sun.nio.ch.Net.bind(Net.java:455)\nat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\nat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\nat org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)\nat org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:914)\n... 10 more</p>","tags":["datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-01 18:13:43.0","id":25565,"title":"How to resolve JA009: Could not load history file hdfs://sandbox.hortonworks.com:8020/mr-history/tmp/hue/job","body":"<p>I am new in oozie. I have a java program which produce data into kafka topic(it is not map reduce job). I am trying to schedule it with ozzie. How ever, I am getting this error:</p><p>JA009: Could not load history file hdfs://sandbox.hortonworks.com:8020/mr-history/tmp/hue/job_1459358290769_0012-1459533575025-hue-oozie%3Alauncher%3AT%3Djava%3AW%3DData+Producer%3AA%3DproduceDat-1459533591693-1-0-SUCCEEDED-default-1459533581542.jhist at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:349) at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.&lt;init&gt;(CompletedJob.java:101) at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.</p><p>I read it can be permission or owner problem so, I changed the owner to mapred and give 777 permission. But I still I get the same error. I am using java action to schedule my jar file.</p>","tags":["spark-history-server","Oozie","mr-history"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-04-03 12:14:43.0","id":25689,"title":"Error when run command 'wget -nv http://private-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.1-10/hdp.repo -O /etc/yum.repos.d/HDP-TP.repo'","body":"<p>when i run command '</p><p>wget -nv http://private-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.1-10/hdp.repo -O /etc/yum.repos.d/HDP-TP.repo'</p><p>command in Zeppelin, getting error as </p><p>&lt;console&gt;:2: error: ';' expected but '(' found.\n       print(\"\")</p><p>Ps help. </p><p>Sandbox HDP version is 2.3.2</p>","tags":["spark-shell"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-04 09:03:23.0","id":25764,"title":"Capacity Scheduler does not use all the resources available","body":"<p>\n\tI created two queues (it, price). I expect that when a user runs a job on a cluster he gets all the free resources of the cluster (77 containers in our case). However, the ituser1 uses only the resources available to its queue (32 containers). Is it possible to allow the ituser1 to use all available resources of the cluster?</p><p>Total number of containers in cluster - 77</p><pre>yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\nyarn.scheduler.capacity.maximum-am-resource-percent=0.2\nyarn.scheduler.capacity.maximum-applications=10000\nyarn.scheduler.capacity.node-locality-delay=40\nyarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator\nyarn.scheduler.capacity.root.accessible-node-labels=*\nyarn.scheduler.capacity.root.accessible-node-labels.default.capacity=-1\nyarn.scheduler.capacity.root.accessible-node-labels.default.maximum-capacity=-1\nyarn.scheduler.capacity.root.acl_administer_queue=*\nyarn.scheduler.capacity.root.capacity=100\nyarn.scheduler.capacity.root.default-node-label-expression= \nyarn.scheduler.capacity.root.it.user-limit-factor=1\nyarn.scheduler.capacity.root.price.user-limit-factor=1\nyarn.scheduler.capacity.root.it.state=RUNNING\nyarn.scheduler.capacity.root.price.state=RUNNING\nyarn.scheduler.capacity.root.it.capacity=40\nyarn.scheduler.capacity.root.price.capacity=60\nyarn.scheduler.capacity.root.it.maximum-capacity=100\nyarn.scheduler.capacity.root.price.maximum-capacity=100\nyarn.scheduler.capacity.queue-mappings=u:ituser1:it,u:ituser2:it,u:ituser3:it,u:priceuser1:price,u:priceuser2:price,u:priceuser3:price\nyarn.scheduler.capacity.root.it.minimum-userlimit-Percent=50\nyarn.scheduler.capacity.root.price.minimum-userlimit-Percent=30\nyarn.scheduler.capacity.root.price.default.ordering-policy=fair\nyarn.scheduler.capacity.root.it.default.ordering-policy=fair\nyarn.scheduler.capacity.root.it.acl_administer_jobs=*\nyarn.scheduler.capacity.root.it.acl_submit_applications=*\nyarn.scheduler.capacity.root.price.acl_administer_jobs=*\nyarn.scheduler.capacity.root.price.acl_submit_applications=*\nyarn.scheduler.capacity.root.queues=it,price\n</pre>","tags":["yarn-scheduler","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-04 12:49:41.0","id":25794,"title":"Resume copy of tables in Sqoop","body":"<p>Hi Team,</p><p>I am importing tables from oracle database to hive via sqoop. Due to network disconnection few tables are unable to copy. How can I continue to copy remaining tables from where it stopped? </p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-04 16:50:38.0","id":25836,"title":"Upgrading HDP from 2.2.8 (without Ambari) to 2.4.0 (using Ambari)","body":"<p>We have a running HDP 2.2.8 cluster (in H.A.) which was installed manually (without Ambari). Today, we plan to move to 2.4 using Ambari. This assumes that I will be installing a new cluster with Ambari, while in realiy it is a kind of upgrade, since hdp-select is installed and /usr/hdp/current exists... During the install, the earlier KAFKA and ZOOKEEPER should never be stopped, while the other services I can stop them. Any recommendation on how to do that ?</p>","tags":["Ambari","hdp-2.4.0","upgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-27 09:34:40.0","id":11917,"title":"Hi , I need to integrate data coming from a WSDL soap webservice into KAFKA.Please guide me the way forward.","body":"<p>Fill in the details...</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-21 09:57:39.0","id":11017,"title":"processing the AVRO file using metadata","body":"<p>Hello,</p><p>I have the AVRO file with multiple fields. The fields are described by JSON file, e.g. ValidityDate, ValidityOption [delete, replace]. </p><p>What is the best approach / design to process that avro file according to the JSON file, eg. if validity option for selected field is \"delete\" i have to delete that field from input file etc. </p><p>Does it make sense to create java program that generates Pig script for that task or its better to store data somewhere else/into different format and other tool can handle it?</p><p>Thank you</p>","tags":["Falcon","governance","hadoop","Pig","Atlas"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-27 11:30:31.0","id":11946,"title":"HIVE / SparkSQL '.parquet not a SequenceFile '","body":"<p>Using the sandbox I have saved a parquet file as a table with:</p><p>df.write.format('parquet').mode('overwrite').saveAsTable(myfile)</p><p>followed by:</p><p>sqlContext.refreshTable(myfile)</p><p>when I attempt to query the file with SparkSQL or Hive I get the error:</p><p>{\"message\":\"H170 Unable to fetch results. java.io.IOException: java.io.IOException: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/myfile/part-r-00000-5dc24bf0-23ef-4f3c-a1fc-42928761592d.gz.parquet not a SequenceFile [ERROR_STATUS]\",\"status\":500,\"trace\":\"org.apache.ambari.view.hive.client.HiveErrorStatusException: H170 Unable to fetch results. java.io.IOException: java.io.IOException: </p><p>....</p><p>This issue started after I had first replaced the parquet file underlying the original df and attempted to rebuild.</p><p>When I run df.head(10) I can see the dataframe.</p><p>I have attempted manually deleting the parquet and the Hive files under the warehouse, even after they are deleted when I resave the table the issue occurs.</p><p>I have sqlContext.setConf(\"spark.sql.hive.convertMetastoreParquet\", \"false\")</p><p>I have tried os.environ[\"HADOOP_USER_NAME\"] = 'hdfs'</p><p>I have tried unpersisting the dataframe</p><p>I have tried changing the permissions with os.system('hdfs fs -chmod -R 777 hdfs://apps/hive/warehouse')</p><p>I can't seem to clear out this issue. I have seen resolutions with the above but none have helped me. I can't seem to get back to being able to access the data via Hive or SparkSQL.</p>","tags":["Hive","spark-sql","parquet"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-26 20:20:19.0","id":11843,"title":"Issues with HDP 2.3.4 Install with Redhat Satellite Server","body":"<p>\n\tHi,</p><p>\n\tI am setting up HDP 2.3.4, where the machines are not internet facing, with the HDP repos were made available on a Redhat Satellite server. During the setup, I am facing some issues and resolving one of the after. I just want to validate and confirm whether this is being done correctly or not.</p><p>\n\tAs the repos are available on RHS server, I have made the following change in the /var/lib/ambari-server/resources/stacks/HDP/2.0.6/configuration/cluster-env.xml file under the xml property &lt;name&gt;repo_suse_rhel_template&lt;/name&gt;</p>\n<pre>&lt;value&gt;[{{repo_id}}-DISABLED]\nname={{repo_id}}-DISABLED\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\npath=/\nenabled=0\ngpgcheck=0&lt;/value&gt;\n</pre><p>\n\tAlso, I didnt download the ambari.repo, HDP.repo and HDP-UTILS.repo files to /etc/yum.repos.d files initially thinking that they are already available on RHS server and Ambari would pick them appropriately during installation.</p><p>\n\tHowever, during installation, Ambari threw an error</p><pre>/etc/yum.repos.d/ambari.repo: No such file or directory \nscp /etc/yum.repos.d/ambari.repo</pre><p>So, in order to get around with the issue, I created empty / dummy repo files under /etc/yum.repos.d with </p><pre>touch ambari.repo \n\ntouch HDP.repo touch \n\nHDP-UTILS.repo</pre><p>After this, Ambari issue got resolved, but HDP services were not able to get installed. So, I have updated HDP.repo and HDP-UTILS.repo with the below content</p><pre>HDP-UTILS.repo \n\n\n[HDP-UTILS-1.1.0.20] \nname=HDP Utils Version - HDP-UTILS-1.1.0.20 \nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6 \ngpgcheck=1 \ngpgkey=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 \npriority=1 \n\n\nHDP.repo\n\n\n#VERSION_NUMBER=2.3.4.0-3485 \n[HDP-2.3.4.0] name=HDP Version - HDP-2.3.4.0 \nbaseurl=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0 \ngpgcheck=1 \ngpgkey=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 \npriority=1 </pre><p>with these updated contents, I am able to install most of the services. However, Oozie-Client seems to be failing while installation. The error it logs is as below:</p><pre>resource_management.core.exceptions.Fail: Execution of '/usr/bin/yum -d 0 -e 0 -y install extjs' returned 1. Error: Nothing to do</pre><p>The HDP-UTILS repo on the RHS server have been named as \"HDP Utils Version - HDP-UTILS-1.1.0.20\". So just wondering whether the naming convention could be the source of the issue to get Oozie-Client installed or some other issue? The documentation around  naming wasnt clear and hence the ambiguity,.</p><p>Please let me know if it is done wrongly in any of the above steps.</p><p>Many Thanks</p><p>Vijay</p>","tags":["installation","hdp-2.3.4","Ambari","Oozie"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-27 12:31:54.0","id":11965,"title":"Ambari-agent logs not generating","body":"<p>what are the major causes of down ambari-agent. Does restarting of ambari-server affects ambari-agent status. How to find root cause for ambari-agent not generating any logs.</p>","tags":["ambari-agent","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-27 11:47:05.0","id":11955,"title":"how to use select query or DML queries in WebHCat ?","body":"<p>Hi,</p><p>Can we use select query in webhcat ?</p><p>When I was trying DDL like show database/show tables using webhcat,i can get the output in json format</p><p>but when i use select query ,am getting error with exit code-64..Here is my query:</p><p>\"<em>curl -s -d user.name=root -d 'exec=select * from tweets_clean;' 'http://localhost:50111/templeton/v1/ddl'</em> \"</p><p>Can anyone pls suggest any idea how to use so because I need to fetch hive table data and visualize that in my web application</p><p>Thanks in advance</p>","tags":["Hive","webhcat","ddl"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-24 15:30:15.0","id":4694,"title":"Possible scenarios for 'online' backups of HDFS metadata?","body":"<p>I'm looking into the possibility of performing an 'online' backup of HDFS metadata without having to take down HDFS or NameNodes and wanted to find out if the following plan is doable or not. </p><p>General assumptions:</p><ul>\n<li>Regardless of the solution, we'll never have a full, up-to-date continuous backup of the namespace – we’ll always loose some of the most recent data. It’s not an OLTP system, most of the data can be easily recreated (re-run ETL or processing jobs).</li></ul><ul>\n<li>Normal NN failures are handled by the Standby NN. The goal here is to have a procedure in place for a very unlikely case where both master nodes fail.</li></ul><ul>\n<li>In the case of both NN failures,  the NN service can be started up with the most recent image of the namespace we have.</li></ul><p> The understanding of how the Name Nodes maintain the namespace, in short, is:</p><ul>\n<li>Standby NN keeps a namespace image in memory based on edits available in a storage ensemble in Journal Nodes.</li></ul><ul>\n<li>Based on pre-conditions (No of transactions or period), standby NN makes a namespace checkpoint and saves a “fsimage_*” to disk.</li></ul><ul>\n<li>Standby NN transfers the fsimage to the primary NN over http.</li></ul><p>The understanding is that both NN write fsimages to disk in the following sequence:</p><ul>\n<li>NN writes the namespace to a file “fsimage.ckpt_*” on disk</li></ul><ul>\n<li>NN creates a “fsimage_*.md5” file</li></ul><ul>\n<li>NN moves the file “fsimage.ckpt_*” to “fsimage_.*”</li></ul><p>The above means that:</p><ul>\n<li>The most recent namespace image on disk in in a “fsimage_*” file is on the Standby NN.</li></ul><ul>\n<li>Any  “fsimage_*” file on disk is finalized and won’t receive more updates.</li></ul><p>Based on the above, a proposed, simple procedure what won’t affect the availability of NN is as follows:</p><ul>\n<li>Make sure the Standby NN checkpoints the namespace to “fsimage_” once per hour.</li></ul><ul>\n<li>Backup the most recent “fsimage_*” and “fsimage_*.md5” from the Standby NN periodically. We can try to keep the latest version of the file on another machine in the cluster.</li></ul><p>Are there any issues or potential pitfalls with this approach that anyone can see? </p>","tags":["namenode","namenode-ha","HDFS","operations","backup"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-29 03:24:40.0","id":432,"title":"Permissions necessary for “the user” or Ranger Config user required to configure an HDFS Repository in the Sandbox","body":"<p>It appears that it should be an O/S user. In addition, if you use kerberos, this needs to be a kerberos user with password.</p><p>But with what rights?</p><p>What purpose does this user get used for?</p><p>The <a target=\"_blank\" href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_Ranger_User_Guide/content/hdfs_repository.html\">documentation</a> says that it gets \"used for connection\". Connection to what exactly?</p><p>I'm trying to figure it out from looking at the Hortonworks 2.3 sandbox VM - it comes pre-configured with Ranger  and an HDFS policy store.  This policy store is pre-configured with run-as user of 'xapolicymgr' who belongs to one group called 'xapolicymanager'. It's not the O/S user that the 'ranger-admin' nor 'ranger-usersync' services are started with (they get started under the \"ranger\" user on the sandbox)</p><p>Does this user/group need to own any files or directories?</p>","tags":["ranger-0.5.0","configuration","hdp-2.3.0","Sandbox","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-27 18:09:27.0","id":12039,"title":"Ranger UI for Hive Plug in Auto complete of Tables and Columns does not work.","body":"<p>I read the following  https://community.hortonworks.com/questions/432/permissions-necessary-for-the-user-required-to-con.html.</p><p>However for the Hive Plug in, in the Ranger UI the Autocomplete does not work.\n\nWhich user handles the lookup for databases and tables for the Hive Plug in, in the Ranger UI?  Note, the Hive Plug in, and the authorization works, but the autocomplete of database and tables and columns does not work....</p><p><img src=\"/storage/attachments/1598-screen-shot-2016-01-27-at-125904-pm.png\"></p><p>Is it the Ranger repository config user the one that does the lookup for tables and columns to get the autocomplete to work in Ranger UI?</p><p>Is the Ranger repository config user only a database user? </p><p>What if someone used cluster prefixes for non-local system users upon creation of cluster e.g. &lt;cluster&gt;-ambari-qa and &lt;cluster&lt;-hive?\n\nShould the Ranger repository config user still be \"hive\" as it is not an actual system account BUT the Policy User for Hive be &lt;cluster&gt;-ambari-qa?\n\nTrying to make sense of all of this and why the Hive autocomplete does not work.</p>","tags":["ranger-0.5.0","Ranger","Hive"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-27 18:11:33.0","id":12051,"title":"I am working with Falcon, while creating the entity for cluster i got the error for the location of staging directory does not exists. here is my xml file","body":"<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;clustername=\"primaryCluster\"description=\"this is primary cluster\"colo=\"primaryColo\"xmlns=\"uri:falcon:cluster:0.1\"&gt;&lt;tags&gt;primaryKey=primaryValue&lt;/tags&gt;&lt;interfaces&gt;&lt;interfacetype=\"readonly\"endpoint=\"hftp://sandbox.hortonworks.com:50070\"version=\"2.2.0\"/&gt;&lt;interfacetype=\"write\"endpoint=\"hdfs://sandbox.hortonworks.com:8020\"version=\"2.2.0\"/&gt;&lt;interfacetype=\"execute\"endpoint=\"sandbox.hortonworks.com:8050\"version=\"2.2.0\"/&gt;&lt;interfacetype=\"workflow\"endpoint=\"http://sandbox.hortonworks.com:11000/oozie/\"version=\"4.0.0\"/&gt;&lt;interfacetype=\"messaging\"endpoint=\"tcp://sandbox.hortonworks.com:61616?daemon=true\"version=\"5.1.6\"/&gt;&lt;/interfaces&gt;&lt;locations&gt;&lt;locationname=\"staging\"path=\"/apps/falcon/primaryCluster/staging\"/&gt;&lt;locationname=\"temp\"path=\"/tmp\"/&gt;&lt;locationname=\"working\"path=\"/apps/falcon/primaryCluster/working\"/&gt;&lt;/locations&gt;&lt;ACLowner=\"ambari-qa\"group=\"users\"permission=\"0x755\"/&gt;&lt;properties&gt;&lt;propertyname=\"test\"value=\"value1\"/&gt;&lt;/properties&gt;&lt;/cluster&gt;</code></pre>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-28 00:48:44.0","id":12100,"title":"Spark Streaming Kafka dependency issue on Zeppelin","body":"<p>Anyone has any idea why I get the following error when I execute KafkaUtils.createStream on Zeppelin? However it works fine on spark-shell yarn-client</p><pre>error: bad symbolic reference. A signature in KafkaUtils.class refers to term kafka\nin package &lt;root&gt; which is not available.\nIt may be completely missing from the current classpath, or the version on\nthe classpath might be incompatible with the version used when compiling KafkaUtils.class.\nerror: bad symbolic reference. A signature in KafkaUtils.class refers to term serializer\nin value kafka which is not available.\nIt may be completely missing from the current classpath, or the version on\nthe classpath might be incompatible with the version used when compiling KafkaUtils.class.</pre>","tags":["zeppelin","Kafka","hdp-2.3.2","spark-streaming"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-27 22:08:38.0","id":12074,"title":"Ranger usersync with AD fails with https://server1:6182/service/users/default returned a response status of 403 Forbidden","body":"<p>Hi,</p><p>Ranger usersync process not synching the AD users and throwing below error when a new user is added</p><pre>27 Jan 2016 15:48:15 ERROR LdapUserGroupBuilder [UnixUserSyncThread] - sink.addOrUpdateUser failed with exception: POST https://server1:6182/service/users/default returned a response status of 403 Forbidden, for user: mthal, groups:</pre><p>It has picked the newly added user but could not add it to ranger users list.</p><p>I have enabled the SSL for Ranger.</p><p>Please advice</p><p>Thanks,</p><p>Venkat</p>","tags":["ranger-usersync"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-27 18:53:57.0","id":12053,"title":"Ambari  Metrics Collector Start Failed on 3 Node AWS Cluster","body":"<p>I am on Ambari Version 2.2.0.0 and did a fresh install of hadoop.</p><p>Whenever I start my hadoop services I get the following warnings and my metrics collector is shutting down (all other hadoop services are fine):</p><p><img src=\"/storage/attachments/1602-bildschirmfoto-2016-01-27-um-193216.png\"></p><p>I attached all the logs from the ambari-metrics-collector directory in the attached zip.</p><p>Any help is appreciated!</p><p><a href=\"/storage/attachments/1603-ambari-metrics-collector.zip\">ambari-metrics-collector.zip</a></p><p>br,</p><p>Rainer</p>","tags":["ambari-service","Ambari","ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-28 01:02:46.0","id":12087,"title":"Connection failed  :  Secondary Name Node service","body":"<p>Hi,</p><p>I had set a multi-node HDP cluster, with name node and secondary name node running on two different machines. Cluster was working very fine at this stage.</p><p>Later, I enabled high availability for the name node using the Ambari HA wizard. I could complete the high availability setup successfully and didn’t see any errors during the setup process.</p><p>However, post installation, when I navigated to Hive View or even the Namenode UI running on 50070, I saw an error message appearing on the screen  (I couldn’t save the error text and couldn’t recollect now)</p><p>Later, we have decided to rollback the cluster and move back to the earlier successfully running state of Namenode / Sec Name node setting as it was earlier. I could perform all the steps in the HDP documentation listed (<a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_Ambari_Users_Guide/content/_how_to_roll_back_namenode_ha.html\">http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_Ambari_Users_Guide/content/_how_to_roll_back_namenode_ha.html</a>)</p><p>However, I am facing an issue (Connection failed &lt;urlopen error [Errno 111] Connection refused&gt;</p><p>) , for the SNN service, which comes as an alert on the machine running the secondary name node and informs that SNN service is the one having this issue.</p><p>Rest of the cluster services are all running fine, except this one with the urlopen error.</p><p>Can anyone let me know what would be the cause of this issue and the subsequent remedial actions needed.</p><p>Thanks</p>","tags":["Ambari","namenode","high-availability"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-21 23:27:41.0","id":11189,"title":"Oozie work flow - PIG action failed","body":"<p>Hi I am trying to run a PIG action in oozie workflow and getting this error:</p><pre>Pig Stack Trace\n---------------\nERROR 2998: Unhandled internal error. org/apache/hadoop/hive/shims/ShimLoader\n\njava.lang.NoClassDefFoundError: org/apache/hadoop/hive/shims/ShimLoader\n\tat org.apache.hadoop.hive.conf.HiveConf$ConfVars.&lt;clinit&gt;(HiveConf.java:368)\n\tat org.apache.hive.hcatalog.pig.PigHCatUtil.getHCatServerUri(PigHCatUtil.java:134)\n\tat org.apache.hive.hcatalog.pig.HCatLoader.getSchema(HCatLoader.java:217)\n\tat org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:175)\n\tat org.apache.pig.newplan.logical.relational.LOLoad.&lt;init&gt;(LOLoad.java:89)\n\tat org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:901)\n\tat org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3568)\n\tat org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1625)\n\tat org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)\n\tat org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)\n\tat org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)\n\tat org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)\n\tat org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1735)\n\tat org.apache.pig.PigServer$Graph.access$000(PigServer.java:1443)\n\tat org.apache.pig.PigServer.parseAndBuild(PigServer.java:387)\n\tat org.apache.pig.PigServer.executeBatch(PigServer.java:412)\n\tat org.apache.pig.PigServer.executeBatch(PigServer.java:398)\n\tat org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:171)\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:234)\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\n\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\n\tat org.apache.pig.Main.run(Main.java:502)\n\tat org.apache.pig.PigRunner.run(PigRunner.java:49)\n\tat org.apache.oozie.action.hadoop.PigMain.runPigJob(PigMain.java:288)\n\tat org.apache.oozie.action.hadoop.PigMain.run(PigMain.java:231)\n\tat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:47)\n\tat org.apache.oozie.action.hadoop.PigMain.main(PigMain.java:76)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:236)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.shims.ShimLoader\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\t... 40 more\n================================================================================\nFailing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.PigMain], exit code [2]</pre>","tags":["hcatalog","Falcon","Pig","Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-27 13:40:58.0","id":11991,"title":"Error while executing hive query on Spark as execution engine","body":"<p>I am using HDP 2.3.2</p><p>I used following commands to set spark as hive's execution engine</p><p>set hive.execution.engine=spark;</p><p>and executed query as </p><p>select count(*) from tablename;</p><p>I got following error,</p><pre>java.lang.NoSuchMethodError: com.fasterxml.jackson.module.scala.deser.BigDecimalDeserializer$.handledType()Ljava/lang/Class;\n        at com.fasterxml.jackson.module.scala.deser.NumberDeserializers$.&lt;init&gt;(ScalaNumberDeserializersModule.scala:49)\n        at com.fasterxml.jackson.module.scala.deser.NumberDeserializers$.&lt;clinit&gt;(ScalaNumberDeserializersModule.scala)\n        at com.fasterxml.jackson.module.scala.deser.ScalaNumberDeserializersModule$class.$init$(ScalaNumberDeserializersModule.scala:61)\n        at com.fasterxml.jackson.module.scala.DefaultScalaModule.&lt;init&gt;(DefaultScalaModule.scala:19)\n        at com.fasterxml.jackson.module.scala.DefaultScalaModule$.&lt;init&gt;(DefaultScalaModule.scala:35)\n        at com.fasterxml.jackson.module.scala.DefaultScalaModule$.&lt;clinit&gt;(DefaultScalaModule.scala)\n        at org.apache.spark.rdd.RDDOperationScope$.&lt;init&gt;(RDDOperationScope.scala:78)\n        at org.apache.spark.rdd.RDDOperationScope$.&lt;clinit&gt;(RDDOperationScope.scala)\n        at org.apache.spark.SparkContext.withScope(SparkContext.scala:681)\n        at org.apache.spark.SparkContext.hadoopRDD(SparkContext.scala:956)\n        at org.apache.spark.api.java.JavaSparkContext.hadoopRDD(JavaSparkContext.scala:428)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateMapInput(SparkPlanGenerator.java:188)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateParentTran(SparkPlanGenerator.java:134)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:106)\n        at org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.execute(LocalHiveSparkClient.java:130)\n        at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:64)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:107)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nFAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. com.fasterxml.jackson.module.scala.deser.BigDecimalDeserializer$.handledType()Ljava/lang/Class;</pre><p>Please suggest some fixes. thanks</p>","tags":["error","Spark","hiveserver2","hdp-2.3.2","Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-26 13:23:00.0","id":11774,"title":"Cannot run MapReduce with azure blob storage (WASB) - UPDATE","body":"<p>We are trying to run smoke test mapreduce job from hadoop examples (terasort):</p><pre>[hdfs@xxxxx 2.2.0.0-2041]$ hadoop jar /usr/hdp/2.2.0.0-2041/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0.2.2.0.0-2041.jar teragen 100 /test/10gsort/input\n</pre><p>We are dealing with following problems:</p><pre>[hdfs@xxxxx bduser]$ hadoop jar /usr/hdp/2.2.0.0-2041/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0.2.2.0.0-2041.jar teragen 100 /test/10gsort/input\n16/01/28 09:44:55 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n16/01/28 09:44:56 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 60 second(s).\n16/01/28 09:44:56 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n16/01/28 09:44:57 INFO client.RMProxy: Connecting to ResourceManager at xxxxx/10.0.0.8:8050\n16/01/28 09:44:58 INFO mapreduce.Cluster: Failed to use org.apache.hadoop.mapred.YarnClientProtocolProvider due to error: java.lang.reflect.InvocationTargetException\njava.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.\n        at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:120)\n        at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:82)\n        at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:75)\n        at org.apache.hadoop.mapreduce.Job$9.run(Job.java:1266)\n        at org.apache.hadoop.mapreduce.Job$9.run(Job.java:1262)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.mapreduce.Job.connect(Job.java:1261)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1290)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1314)\n        at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:305)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:309)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n16/01/28 09:44:58 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n16/01/28 09:44:58 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n16/01/28 09:44:58 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n\n\n\n</pre>","tags":["MapReduce","azure","wasb","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-28 19:17:20.0","id":12235,"title":"Ambari is taking long time to restart slave services on a particular server in the cluster","body":"<p>Hi,</p><p>We have a cluster of 10 servers.</p><p>One worker server among them has the problem while starting/stopping services from Ambari.</p><p>When I try to invoke any operations on any service (HDFS/HBASE/METRICS) from Ambari, the command is taking very long time to execute.</p><p>I searched Ambari logs, Servcie logs but could not find any error.</p><p>I tried to restart Ambari server and Ambari agent but still no luck.</p><p>I had the same problem earlier but reinstalling ambari-agent fixed the issue but no luck now.</p><p>I deleted host from cluster, cleaned total server and added back to server but still the same issue.</p><p>Please advice.</p><p>Thanks,</p><p>Venkat</p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-29 06:29:40.0","id":12351,"title":"sqoop import string delimiter","body":"<p>hi experts,</p><p>I am trying to import data from Oracle DB. I just wanted to check if we can use a string field delimiter instead of comma(default delimiter for columns)</p><p>Thanks.</p>","tags":["import","Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-29 15:41:30.0","id":12468,"title":"HBase REST API: can not get scanner, can not get by row key","body":"<p>I have HDP 2.3 and hbase 1.1.2 on a cluster.</p><p>I have started HBase rest server/daemon. </p><p>The following restful API works for me:</p><p>curl -X GET http://&lt;myhost&gt;:9080/ </p><p>//To get table schema\ncurl -X GET http://&lt;myhost&gt;/&lt;mytable&gt;/schema</p><p>//To get a single column of a row from a table\ncurl -X GET http://&lt;myhost&gt;:9080/&lt;mytable&gt;/&lt;rowkey1&gt;/&lt;cf:q&gt;/</p><p>However, the following restful API does not work for me:</p><p>//To get all columns by row key.</p><p>curl -X GET http://&lt;myhost&gt;:9080/&lt;mytable&gt;/&lt;rowkey1&gt;</p><p>HTTP/1.1 400 Bad Request\nContent-Length: 57\nContent-Type: text/plain\n\nBad request: Either 0 or more than 1 columns specified.</p>//To get a scanner of a table:<p>curl -i -H \"Content-Type: text/xml\" -d '&lt;Scanner \nbatch=\"1\"/&gt;' \"http://&lt;myhost&gt;:9080/&lt;mytable&gt;/scanner\"</p><p>HTTP/1.1 400 Bad Request\nContent-Type: text/html; charset=iso-8859-1\nCache-Control: must-revalidate,no-cache,no-store\nContent-Length: 1399</p><p>Per Hbase document, these should all be allowed. Any idea why it is not working? What configuration in Hbase I should be checking? Any help is greatly appreciated.</p>","tags":["api","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-29 18:47:09.0","id":12484,"title":"How can we setup customized cgroups for Centos 7?","body":"<p>I came to know that HDP is not supporting cgroups for CentOS 7. We are running our cluster on Centos 7. How can we enable CPU scheduling and Isolation? What is the process of defining our own cgroups for centos 7?</p>","tags":["scheduling","centos","cgroups","YARN","cpu"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-01 09:32:37.0","id":12798,"title":"Deploying hadoop cluster","body":"<p>I want to set up hadoop cluster where i want master node and few slave node to be in some country and remaining slave nodes sitting in different country. Is it possible to deploy this? Is there any article or tutorial which might help me to get the solution.</p>","tags":["HDFS","master","hadoop","slave"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-01 13:50:56.0","id":12840,"title":"Intergrating Ambari and Ranger with LDAP - from scratch manual steps","body":"<p>I want to integrate Ambari and Ranger with LDAP/AD . The infra team has just shared ip and port for LDAP/AD server. </p><p>I am looking into the Hortonworks docs but not able to find how I can integrate. If some one can provide the manual steps for that it will be handy. I have got docs but seems bit confusing to me. </p>","tags":["integration","Ambari","Ranger"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-01 14:26:34.0","id":12855,"title":"Does Cgroups implementation require kerberos enabled cluster? Apache documentation says security is not needed. Hortonworks says it needs kerberos enabled cluster? Which is correct?","body":"<p>We are running our cluster on CentOS 7. We have a plan to implement yarn new features such as CPU isolation. CPU isolation needs cgroups. May i implement cgroups without implementing Kerberos?</p>","tags":["cpu","kerberos","centos"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-01 14:20:23.0","id":12837,"title":"i have accidentally changed the hortonworks IP address in etc/hosts , so could someone update me the default Ip address that come withthe sandbox. Thanks in advace :)","body":"","tags":["Sandbox","hadoop","how-to-tutorial"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-01 17:25:38.0","id":12902,"title":"Phoenix - Query Timeout","body":"<p>I have a table in HBase created via Phoenix.  The table has approxmiately 20 million records.  I'm connecting to Phoenix via:</p><pre>phoenix-sqlline.py hbasemaster:2181:/hbase-unsecure</pre><p>I'm trying to run a count as follows:</p><pre>select count(columnname) from tablename;</pre><p>When I run that SQL, Phoenix reports a timeout</p><pre>org.apache.phoenix.exception.PhoenixIOException: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:\n\njava.net.SocketTimeoutException: callTimeout=60000, callDuration=60317: row ....</pre><p>I've tried changing the hbase.rpc.timeout via Ambari, but that doesn't seem to be the issue.  The default timeout in Ambari was set to 1m30s and I changed it to 2m.  The timeout reported by Phoenix is 60s before and after the change, so I don't think that's the culprit anyway.</p><p>What setting do I need to change to allow for longer running queries?  Is there something else that I should be looking at?</p>","tags":["Hbase","sql","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-01 20:52:19.0","id":12952,"title":"Best API to pull Flume Metrics from Ambari","body":"<p>\n\tWhat is the best way to access the Flume metrics data via REST API which is shown in the Ambari Flume service page (image attached).</p><p>\n\tTried to access this information via the standard <a href=\"https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md\">Ambari REST API,</a> however it only gave me high level information about the Flume service:</p><p>curl --user admin:admin http://sandbox234:8080/api/v1/clusters/Sandbox/services/FLUME</p><p>curl --user admin:admin http://sandbox234:8080/api/v1/clusters/Sandbox/hosts/sandbox.hortonworks.com/host_components/FLUME_HANDLER</p><p>Should this information be available via the <a href=\"https://cwiki.apache.org/confluence/display/AMBARI/Ambari+Metrics+API+specification\">Ambari Metrics REST API</a> instead?</p><p>Thank you</p>","tags":["ambari-metrics","Ambari","Flume"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-01 17:07:31.0","id":12892,"title":"Salesforce integration with Hortonworks Data Flow (Apache Nifi)","body":"<p>The requirement is to extract data from Salesforce and to ingest into Hive.\nIs this a good use case for HDF?</p><p>The main requirement is  to pull data from Salesforce. What processors are appropriate here?  </p><p>The invokeHttp processor and/or ExtractText processor?</p>","tags":["nifi-processor","Nifi","salesforce"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-01 23:14:35.0","id":12978,"title":"Spark 1.3.1 not pulling all hive metadata when executing query - header row of CSV datafile not ignored","body":"<p>I am using Spark 1.3.1 to create a hive table from a CSV file (in which the first row is the header row).  I have set the hive table property to skip the header row:</p><p> \n               TBLPROPERTIES (\"skip.header.line.count\"=\"1\") </p><p>I validated with a \"show create table BOP\" that the table property is set to ignore the header row. But when i execute \"select count(*) from mytable\" i get the correct count from HUE/beeswax/beeline, but if i execute the same query via Spark i get a result that is count+1 (i.e. it counts the header row as a data row). Why is Spark reading the hive metadata and still not ignoring the header row?</p>","tags":["metadata","Hive","csv","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-02 07:49:19.0","id":13053,"title":"Enable ranger for solr","body":"<p>In HDP2.3.4 environment, solr does not show up in Ambari UI,therefore, how do I enable ranger for solr?</p><p>thanks.</p>","tags":["ranger-0.5.0","Ranger","SOLR"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-02 13:41:59.0","id":13132,"title":"Best practice to import data from SQL server to Hive through Sqoop","body":"<p>Hi,</p><p>We are working on to import data from MS SQL Server to hive through Sqoop. If we use the incremental & append mode which is the requirement then we need to specify the --last-value of the row id which we inserted last time. </p><p>I have to update about 100 tables into Hive. </p><p>1. What is the practice to save the value of row id for all tables and specify in the sqoop --last-value command ? </p><p>2. Why does not Sqoop itself check the row id of the source & destination table, finally update the rows onwards the last row id value of the destination table? </p><p>3. If i save the last value of row id for all tables in a hive table and want to use those values in Sqoop job then how it's possible?</p><p>All and above, i want to automate the data importing job so that i do not have to provide the value manually for each table data import per day</p><p>Any pointers ? </p><p>Thanks </p>","tags":["Sqoop","sql","cloudera","import","Hive"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-02 05:21:20.0","id":54624,"title":"Hbase Master","body":"<p>I have a 5 node HDP cluster and i want to add Hbase on to it. On how many nodes should i install HMaster. By default one node is enough i guess. Any thoughts on it. Also do i need to install phoenix query server, if so on how many nodes.</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-09-05 08:53:05.0","id":54875,"title":"Oozie job hovers in PREP state","body":"<p>I've been truing to run Oozie job for one of examples from  doc/examples/apps/map-reduce  just to be sure my cluster installation is fine but see all my oozie jobs hovering in PREP state.</p><p>Use 8020 port for NameNode </p><p>and  8050  for jobTracker</p><p>and  the command to kick a job is :</p><p>oozie job -oozie http://#######:11000/oozie/ -config  ./job.properties -run</p><p><img src=\"/storage/attachments/7339-screen.png\"></p><p>I've been truing to do this both under ambari-qa and root user but result is negative.</p><p>At the same time, I can see the same task has finished successfully  onse during HW installation somehow :</p><p><img src=\"/storage/attachments/7340-screen.png\"></p><p>I fill like I  screwed sonthing out or missed somthing. </p><p>My question here what the correct way to run Ooozie job  ?</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-09-07 14:44:56.0","id":55345,"title":"hdfs balancer","body":"<div>Hello,</div><ul><li>I started my balancer job as below:</li></ul><p>nohup hdfs balancer -threshold 40 -include hostfile &gt;rebal.log 2&gt;rebal.err &</p><ul><li>The rebal.err is showing files being moved:</li></ul><p>16/09/07 13:47:42 INFO net.NetworkTopology: Adding a new node: /default-rack/10.12.217.87:50010 </p><p>16/09/07 13:47:42 INFO net.NetworkTopology: Adding a new node: /default-rack/10.12.217.96:50010 </p><p>16/09/07 13:47:42 INFO net.NetworkTopology: Adding a new node: /default-rack/10.12.217.94:50010 </p><p>16/09/07 13:47:42 INFO net.NetworkTopology: Adding a new node: /default-rack/10.12.217.95:50010 </p><p>16/09/07 13:47:42 INFO balancer.Balancer: 1 over-utilized: [Source[10.12.217.87:50010, utilization=94.3875712244642]]\n16/09/07 13:47:42 INFO balancer.Balancer: 0 underutilized: []\n16/09/07 13:47:42 INFO balancer.Balancer: Need to move 2.08 TB to make the cluster balanced.\n16/09/07 13:47:42 INFO balancer.Balancer: Decided to move 10 GB bytes from 10.12.217.87:50010 to 10.12.217.96:50010\n16/09/07 13:47:42 INFO balancer.Balancer: Will move 10 GB in this iteration\n16/09/07 13:47:43 INFO balancer.Balancer: Successfully moved blk_1121226541_47486631 with size=21089 from 10.12.217.87:50010 to 10.12.217.96:5001\n0 through 10.12.217.95:50010</p><ul><li>but the rebal.log shows 'Bytes Already Moved' remaining at 0B after 46 iterations</li></ul><p>Time Stamp                 Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved </p><p>07-Sep-2016 13:44:36              0                0 B                             2.08 TB                             10 GB\n</p><p>...</p><p>07-Sep-2016 14:13:05  46              0 B                            2.08 TB                              10 GB</p><ul><li>It has been running for hours, but i don't see changes in 'df' on the over-utilized hosts. It looks like the rebalancer is not actually moving blocks. Is there any way i can see more verbose output or am i missing something?</li></ul><p>Thanks in advance for any help.</p>","tags":["HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-09-07 18:41:50.0","id":55409,"title":"public-repo-1.hortonworks.com 404 for packages","body":"<p>I'm in the middle of installing Ambari 2.4 and running into issues pulling packages from the hortonworks repo.  </p><div><div><div><div><pre>2016-09-07 14:28:24,972 - Installing package hdp-select ('/usr/bin/yum -d 0 -e 0 -y install hdp-select')</pre></div></div></div></div><p>However, if I do a yum list hdp-select it says it has exactly what I'm looking for.  </p><blockquote>hdp-select.noarch                                                                                  2.4.2.0-258.el6                                                                                   HDP-2.4</blockquote><p>http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.0.0/hdp-select/hdp-select-2.4.2.0-258.el6.noarch.rpm gives me a 404 on the server and in my browser I get a NoSuchKey.  </p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-09 18:16:58.0","id":55803,"title":"HDPCD Practice Exam-  VNC viewer Connection timeout","body":"<p>Hi,</p><p>I am not able to connect EC2 instance via VNC viewer. I am getting connection timeout error.</p><p>My VNC viewer version is 5.3.2. It shows field for VNC server where I put public DNS name of my EC2 instance followed by:5901 and for Encryption, I choose all the options but result is same. In practice guide it shows field for Address and picture quality field only.</p><p>I mentioned the port details under security group as mentioned in setup guide.</p><p>Thanks,</p>","tags":["error"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-14 14:15:46.0","id":56502,"title":"Purpose of cbgateway node in cloudbreak","body":"<p>Hello,</p><p>Can someone briefly explain the purpose of the cbgateway node when using cloudbreak?</p><p>- Any recommendations around minimum aws instance sizes to satisfy its requirements? </p><p>- Can this node be turned into an edge node with client libraries? </p><p>Thanks,</p>","tags":["Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-15 13:23:01.0","id":56749,"title":"HDF 2.0 on HDP 2.5 Installation Issue","body":"<p>I installed the HDF 2.0 management pack on HDP 2.5 ambari server and after restart it dies.  Is there a different pack for 2.5?</p><p>http://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.0.0/bk_ambari-installation/content/set_up_the_ambari_server.html</p><p>2016-09-15 13:18:31,098  INFO - ******************************* Check database started *******************************\n2016-09-15 13:18:35,606  INFO - Checking for configs not mapped to any cluster\n2016-09-15 13:18:35,630  INFO - Checking for configs selected more than once\n2016-09-15 13:18:35,633  INFO - Checking for hosts without state\n2016-09-15 13:18:35,634  INFO - Checking host component states count equals host component desired states count\n2016-09-15 13:18:35,637  INFO - Checking services and their configs\n2016-09-15 13:18:37,207 ERROR - Exception occurred during complex service check procedure:\norg.apache.ambari.server.ParentObjectNotFoundException: Parent Stack Version resource doesn't exist.  Stack data, Stack HDP 2.5 is not found in Ambari metainfo.  Stack data, Stack HDP 2.5 is not found in Ambari metainfo\nat org.apache.ambari.server.api.services.AmbariMetaInfo.getServices(AmbariMetaInfo.java:520)\nat org.apache.ambari.server.checks.DatabaseConsistencyCheckHelper.checkServiceConfigs(DatabaseConsistencyCheckHelper.java:540)\nat org.apache.ambari.server.checks.DatabaseConsistencyChecker.main(DatabaseConsistencyChecker.java:115)\nCaused by: org.apache.ambari.server.StackAccessException: Stack data, Stack HDP 2.5 is not found in Ambari metainfo\nat org.apache.ambari.server.api.services.AmbariMetaInfo.getStack(AmbariMetaInfo.java:618)\nat org.apache.ambari.server.api.services.AmbariMetaInfo.getServices(AmbariMetaInfo.java:518)\n... 2 more\n2016-09-15 13:18:37,212  INFO - ******************************* Check database completed *******************************\n2016-09-15 13:18:42,433  INFO - Checking DB store version\n2016-09-15 13:18:43,193  INFO - DB store version is compatible</p>","tags":["hdf-2.0.0","Ambari","hdp-2.5.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-09-14 21:46:12.0","id":56628,"title":"Falcon cluster onboarding & oozie URLs","body":"<p>I'm trying to onboard a cluster entity to Falcon.  I've successfully followed the tutorials for doing clusters/feeds/process in the sandbox.  But for some reason not in our HA environment.</p><p>falcon-cluster.xml</p><pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;cluster name=\"primaryCluster\" description=\"this is primary cluster\" colo=\"primaryColo\" xmlns=\"uri:falcon:cluster:0.1\"&gt;\n    &lt;tags&gt;primaryKey=primaryCluster&lt;/tags&gt;\n    &lt;interfaces&gt;\n        &lt;interface type=\"readonly\" endpoint=\"hdfs://ip-xx-xx-xx-xx.ec2.internal:50070\" version=\"2.2.0\"/&gt;\n        &lt;interface type=\"write\" endpoint=\"hdfs://ip-xx-xx-xx-xx.ec2.internal:8020\" version=\"2.2.0\"/&gt;\n        &lt;interface type=\"execute\" endpoint=\"ip-xx-xx-xx-xx.ec2.internal:8050\" version=\"2.2.0\"/&gt;\n        &lt;interface type=\"workflow\" endpoint=\"http://oozie.hdp.server:11000/oozie\" version=\"4.0.0\"/&gt;\n        &lt;interface type=\"messaging\" endpoint=\"tcp://ip-xx-xx-xx-xx.ec2.internal:61616?daemon=true\" version=\"5.1.6\"/&gt;\n    &lt;/interfaces&gt;\n    &lt;locations&gt;\n        &lt;location name=\"staging\" path=\"/apps/falcon/primaryCluster/staging\"/&gt;\n        &lt;location name=\"temp\" path=\"/tmp\"/&gt;\n        &lt;location name=\"working\" path=\"/apps/falcon/primaryCluster/working\"/&gt;\n    &lt;/locations&gt;\n    &lt;ACL owner=\"ambari-qa\" group=\"users\" permission=\"755\"/&gt;\n    &lt;properties&gt;\n        &lt;property name=\"dfs.namenode.kerberos.principal\" value=\"nn/_HOST@HDP.FOO.LOCAL\"/&gt;\n    &lt;/properties&gt;\n&lt;/cluster&gt;\n</pre><p>error:</p><p>ERROR: Bad Request;default/org.apache.falcon.FalconWebException::org.apache.falcon.FalconException: Invalid Workflow server or port: http://oozie.hdp.server:11000/oozie</p><p>cause:</p><p>Caused by: E1400 : User [falcon/ip-xx-xx-xx-xx.ec2.internal@HDP.FOO.LOCAL] not defined as proxyuser</p><p>new to falcon.  any suggestions?</p>","tags":["Oozie","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-09-16 18:14:29.0","id":56966,"title":"Is there a way to use a basic centos image rather than the Amazon Linux?","body":"<p>I am trying to add some other software to the cluster with quite a few requirements and the amazon versions cause it to fail out.   I am trying to work around it, but was wondering if there was a way just to substitute a different base image.</p>","tags":["hortonworks-cloud","cloud"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-19 09:12:37.0","id":57153,"title":"Sqoop import job is failing with java.lang.OutOfMemoryError: unable to create new native thread","body":"<p>Team,</p><p>Earlier the same jobs were running without any issue but since last week it started failing with following error. </p><p>java.lang.OutOfMemoryError: unable to create new native thread. </p><p>I checked ulimit -a and it is following, so is it good to increase ulimit. </p><p>core file size          (blocks, -c) 0</p><p>data seg size           (kbytes, -d) unlimited</p><p>scheduling priority             (-e) 0</p><p>file size               (blocks, -f) unlimited</p><p>pending signals                 (-i) 515981</p><p>max locked memory       (kbytes, -l) 64</p><p>max memory size         (kbytes, -m) unlimited</p><p>open files                      (-n) 1024</p><p>pipe size            (512 bytes, -p) 8</p><p>POSIX message queues     (bytes, -q) 819200</p><p>real-time priority              (-r) 0</p><p>stack size              (kbytes, -s) 10240</p><p>cpu time               (seconds, -t) unlimited</p><p>max user processes              (-u) 1024</p><p>virtual memory          (kbytes, -v) unlimited</p><p>file locks                      (-x) unlimited</p>","tags":["Sqoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-09-19 12:21:46.0","id":57186,"title":"Error when creating table truck_mileage in hive","body":"<p>Hi, </p><p>I follow the Hadoop tutorial using Hortonworks Sandbox HDP 2.4 on Azure. When I get to the step create table truck_mileage I try to exectue the provided query but get an error:  </p><pre>org.apache.ambari.view.hive.client.HiveClientException: H130 Unable to fetch operation status: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out</pre>","tags":["tutorial-100","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-09-22 17:01:32.0","id":57914,"title":"spark-streaming kafka in a kerberized cluster - security question.","body":"<p>I am using spark streaming to access kafka in a kerberized hadoop-spark and kafka environment.</p><p>According to </p><p>https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_spark-guide/content/spark-streaming-kafka-kerb.html</p><p>I pass the \"keytab\" using the --files option to the spark job. This copies the keytab to the spark executor nodes.</p><p>As per my understanding there is no security risk here because the keytab can only be read by the \"user' that is running the spark job and the keytab is deleted after the job is complete.</p><p>Please confirm if this is correct ? Passsing keytabs to the executors is a bit concerning, however I dont see a way around this.\n</p>","tags":["spark-streaming","Kafka","kerberos"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-02-24 14:45:10.0","id":19248,"title":"How to set up secure HDP production cluster on Azure?","body":"<p>Hi All,</p><p>We are looking forward to setup HDP on Azure to start with 18 node cluster. Mostly with A-8 to A-11 type instances. </p><p>I am looking for setup instruction and best practices. </p><p>I went through the following documents but I see they are a bit out dated and most of them tackle with sandbox. </p><p><a href=\"http://hortonworks.com/blog/hortonworks-sandbox-azure/\">Azure Sandbox - Hortonworks Blog</a></p><p><a href=\"http://hortonworks.com/hadoop-tutorial/deploying-hortonworks-sandbox-on-microsoft-azure/\">Deploying Sandbox in Azure - Hortonworks Tutorial</a></p><p><a href=\"http://hortonworks.com/blog/easy-steps-to-create-hadoop-cluster-on-microsoft-azure/\">Azure Using Cloud break</a> (We are not looking forward to use CloudBreak and Ambari Blue Prints)</p><p><a href=\"https://channel9.msdn.com/Shows/Data-Exposed/Deploying-Hortonworks-HDP-on-Microsoft-Azure\">Deploying Hortonworks HDP on Microsoft Azure Video</a></p><p><a href=\"http://blogs.technet.com/b/oliviaklose/archive/2014/06/18/hadoop-on-linux-on-azure-step-by-step-build-the-infrastructure-2.aspx\">Step by step guide - TechNet blog</a></p><p><strong>\n</strong></p><p><strong>Few specific questions :</strong></p><p>- Should we place each of the node in a separate cloud service or all nodes in a single cloud service?</p><p>- Should we setup HDP on all the nodes (18 nodes) and setup a Windows Server with Active directory for name resolution as show in the TechNet blog? Can we not have HDP cluster running without jumpbox? </p><p>- Do we need to build a separate VPC?</p><p>- Is there any documentation in place which talks about step by step setting up?</p><p>- What are the best practices around? </p>","tags":["azure","hdp-2.3.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-29 14:37:20.0","id":20188,"title":"How to build hive source code from hortonworks github repo","body":"<p>I have downloaded the source code for hive version <em>0.13.0.2.1.2.0-402</em> and when I am trying to build it with the command </p><pre>mvn clean package -DskipTests</pre><p>its not getting build and throwing the following error:</p><pre><em>[ERROR] Failed to execute goal on project hive-shims-0.23: Could not resolve dependencies for project org.apache.hive.shims:hive-shims-0.23:jar:0.13.0: The following artifacts could not be resolved: org.apache.hadoop:hadoop-common:jar:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-hdfs:jar:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-hdfs:jar:tests:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:tests:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-yarn-api:jar:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-yarn-common:jar:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-yarn-client:jar:2.4.1-SNAPSHOT, org.apache.hadoop:hadoop-yarn-server-tests:jar:tests:2.4.1-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-common:jar:2.4.1-SNAPSHOT in conjars (http://conjars.org/repo) -&gt; [Help 1]</em></pre><p></p><p>My first question is how to get this build successful?</p><p>second one is why there is SNAPSHOT coming in one of the releases of hive?</p><p></p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-29 14:36:06.0","id":20185,"title":"What is the recommended method for changing hostnames in an Ambari managed cluster? And what other considerations should be taken in to account?","body":"<p>The cluster is not Kerberized but we are migrating a cluster to a different domain along with with increased security. \n\nMy questions: \n\n1) What's the recommended method for changing the cluster hostnames and IPs so that Ambari/HDP is aware. \n</p><p>2) There will be new restrictions regarding iptables/ports and root access. This cluster was set up with no firewalls and password-less root access with Ambari using the root user. Aside from making sure the correct ports are open, Are there more considerations here?</p>","tags":["security","Ambari","hostname","ambari-2.1.2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-29 18:53:58.0","id":20248,"title":"Ranger Usersync - Unix Source","body":"<p>HDP 2.3, Ambari 2.2</p><p>I didn't get success in integrating LDAPS groups and users (Uses within few Groups) due LDAP/AD Directory Structure.</p><p>Now I am fall back to UNIX source. My question is:  </p><p>In Usersync, how I can get users within specific Unix Groups, so that I should have in Usersync UI only users list belong to Specific Groups. </p><p>For example, I have -</p><p>/ect/group : test-group:x:19024:abc,def   and many more</p><p>/etc/passwd:  abc:x:100509398:6347:abc@zz.com,100509398,AB C,LDAP-Sync:/home/abc:/bin/bash</p><p>                      abc:x:100509398:6347:def@zz.com,100509398,DE F,LDAP-Sync:/home/def:/bin/bash</p><p>I want only test-group and its member: abc, def to be listed in the UI of usersync. I don't want all users and groups should get listed in the Usersync UI.</p><p>I may request you to please help me exact configuration changes as per my environment and not the link to the ranger/usesync pages from hortonworks (Which I read many times).</p>","tags":["ranger-usersync","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-29 21:43:05.0","id":20298,"title":"SQL on HBase or Analytical approaches for HBase tables with unknown schema","body":"<p>I have a HBase table with column names are not known.  Column names varies from Rowkey to Rowkey.   What are the best approaches for SQL on HBase or Analytical approaches in this case?</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-29 16:01:34.0","id":20209,"title":"Impact of growing a Datanode Volume","body":"<p>I think I am asking a slightly different question than is here </p><p>https://community.hortonworks.com/questions/6796/how-to-increase-datanode-filesystem-size.html</p><p>but a solution should help both.</p><p>SAN issues aside!</p><p>Is there a method to expand the volume under a datanode directory and have HDFS recognize the new allocated space?  For instance if we were to mount a virtual file system, say netapp, in Centos and then expand that filesystem:  How would one make the change known to HDFS?</p>","tags":["HDFS","hadoop","filesystem","virtualization"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-29 17:16:21.0","id":20219,"title":"I got 500 server error when i tried to start the host component using REST API.what is the reason? Can anyone explain?","body":"<p>I tried to start hdfs client component on a host. Initially, i changed state to installed using REST API. Now, i am unable to put the state to STARTED. These are the REST APIs i used.</p><pre>curl -u admin:password-i -H 'X-Requested-By:ambari'-X PUT -d '{\"HostRoles\":\n{\"state\": \"INSTALLED\"}}'\nhttp://localhost:8080/api/v1/clusters/et_cluster/hosts/serf010ext.etops.tllsc.net/host_components/HDFS_CLIENT</pre><p>Status check</p><pre>curl\n-u admin:password -H \"X-Requested-By:ambari\" -i -X GET http://localhost:8080/api/v1/clusters/et_cluster/hosts/serf010ext.etops.tllsc.net/host_components/HDFS_CLIENT </pre><p>To start the host component :</p><pre>curl -u admin:password -H 'X-Requested-By:ambari'-X PUT -d '{\"HostRoles\":\n{\"state\": \"STARTED\"}}'\nhttp://localhost:8080/api/v1/clusters/et_cluster/hosts/ serf010ext.etops.tllsc.net/host_components/HDFS_CLIENT</pre><pre>Error :\nHTTP/1.1 500 Server Error \nUser: admin\nSet-Cookie: AMBARISESSIONID=i3xktymirjhbjtrenyi7gcvs;Path=/;HttpOnly\nContent-Type: text/plain;charset=ISO-8859-1\nContent-Length: 48\nServer: Jetty(8.1.17.v20150415)\n{\n  \"status\": 500,\n  \"message\": \"Server Error\"\n}</pre>","tags":["api","Ambari","service","start"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-29 18:21:46.0","id":20224,"title":"HA for NameNode doesn't work","body":"<p>We are trying to launch HA for NameNode and ResourceManager by using blueprint.  We are using HDP 2.3, Ambari 2.1 and 2.2. There are two issues:</p><p>1. Namenodes and DataNodes fail to start because of \"too small initial heap size\" exception. After some research we figure out that several default configs in hadoop-env.sh file don't have 'm' letter (which means 'megabyte'), namely: XX:NewSize={{namenode_opt_newsize}}, XX:MaxNewSize={{namenode_opt_maxnewsize}},  Xms{{namenode_heapsize}}, Xmx{{namenode_heapsize}}, Xmx{{dtnode_heapsize}}. This problem occurs only when we enable HA. Without HA all these configs have 'm' and all works fine.</p><p>2. We explicitly setup wrong default configs that pointed above. But we get this situation: NameNodes and ZKFCs are started successfully but after this they are suddenly stopped. In logs we have \"NameNode is not formatted\" exception.</p>","tags":["ambari-2.2.0","hdp-2.3.0","namenode-ha","ambari-2.1.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-01 00:39:39.0","id":20325,"title":"TempletonControllerJob stuck in HDP 2.3 (hue issue?)","body":"<p>Hi there, I've got what looks like a newbie question, however I haven't been able to find the answer using this forum or google.</p><p>So I've installed HDP 2.3.4, using Ambari, as a single-machine cluster on a SLES11 SP3 with 16Gb of memory and 4 CPUs. </p><p>Everything seems to be up, and I don't see any alerts.\nWhen using Hive View in Ambari, I can upload a file into a Hive table.</p><p>\nThen I installed Hue 2.6.1-3485 and tried to upload a file into a table using Hue's HCatalog view. Then in the JobBrowser I see two jobs submitted: a TempletonControllerJob and a Hive-somethingsomething job. The latter never starts. The former gets stuck \"running\" at 5% for both maps and reduces, and its log has a huge number of entries like </p><pre>2016-03-01 09:30:20,070 INFO [pool-5-thread-3] org.apache.hive.hcatalog.templeton.tool.LaunchMapper: KeepAlive Heart beat... </pre><p>(where the number of dots grows). </p><p>Googling for solution got me to believe it's related to memory allocation to YARN containers.</p><p>Question #1: How can I confirm the issue is with Yarn, not Hue?</p><p>The general recommendation was to either reduce the amount of memory allocated to YARN containers or increase the amount of memory on the server. After the memory on the server has been increased to 32Gb, the issue didn't disappear. I tried to reduce the amount of memory allocated to Yarn containers using <a href=\"http://hortonworks.com/blog/how-to-plan-and-configure-yarn-in-hdp-2-0/\">this document</a>, and I've got the TempletonControllerJob finishing with this error message: </p><pre>2016-02-29 16:48:44,128 INFO [main] org.apache.zookeeper.ZooKeeper: Session: 0x1532b18956c02e1 closed \n\n2016-02-29 16:48:44,128 INFO [main] org.apache.hive.hcatalog.templeton.tool.LaunchMapper: templeton: job failed with exit code 1</pre><p>Question #2: Did that^ happen because the allocated memory was not sufficient?</p><p>Question #3: Before I dive into <a href=\"https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_installing_manually_book/content/determine-hdp-memory-config.html#ref-cb07873e-edba-4c77-bd34-edbcefcbfc16\">manual configuration</a> of Yarn memory allocation, is there anything more obvious that I'm missing to do?</p><p>Currently, \"free -m\" command returns 24 Gb used and 8 Gb free, plus 2 Gb swap. Yarn is configured for 8Gb of total containers memory (nodemanager.resource.memory-mb), 2.5 Gb minimum allocation and 4Gb max allocation for a container. During the mentioned jobs execution Ambari shows that Yarn memory is 64% used.</p>","tags":["YARN","hue"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-01 04:04:21.0","id":20376,"title":"Oozie - Is there a way to capture output of Sqoop eval node in other oozie nodes ?","body":"<p>I have an Oozie job that is performing sqoop import action. I want to add another sqoop eval node to this job to run sqoop eval query on source database to get control totals and insert control totals to another Hive table. Is there a way to capture output of sqoop eval node in Oozie ?</p>","tags":["Oozie","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-29 23:35:46.0","id":20313,"title":"Can we move file from TDE encrypt zone?","body":"<p>I am new to TDE and one of our customers would like to know the following:</p><p>What happens when an encrypted file is moved from encrypted zone to another location on HDFS? can we still decrypt and re-encrypt that file using the same key? or we can't decrypt that file once it is moved from its encrypted zone location.</p>","tags":["encryption","ranger-kms"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-01 03:03:34.0","id":20367,"title":"Check hdfs failed when using Ambari UI to run check hdfs service","body":"<p>Hi,</p><p>I'm trying to check Hdfs Service using Ambari UI. And I can run 'java -version' as any user in linux.</p><p><img src=\"/storage/attachments/2475-1.jpg\"></p><p><img src=\"https://community.hortonworks.com/storage/attachments/2476-2.jpg\"><img src=\"https://community.hortonworks.com/storage/attachments/2477-3.jpg\"></p><p>I got the following error with <strong>check Hdfs</strong>:</p>stderr:  /var/lib/ambari-agent/data/errors-2113.txt<pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/service_check.py\", line 146, in &lt;module&gt;\n    HdfsServiceCheck().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/service_check.py\", line 52, in service_check\n    bin_dir=params.hadoop_bin_dir\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/providers/execute_hadoop.py\", line 55, in action_run\n    environment = self.resource.environment,\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 238, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'hadoop --config /usr/hdp/current/hadoop-client/conf dfsadmin -fs hdfs://bigdata -safemode get | grep OFF' returned 1. DEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\n/usr/hdp/2.3.2.0-2950//hadoop-hdfs/bin/hdfs.distro: line 308: /usr/java/default/bin/java: No such file or directory \n/usr/hdp/2.3.2.0-2950//hadoop-hdfs/bin/hdfs.distro: line 308: exec: /usr/java/default/bin/java: cannot execute: No such file or directory\n</pre>","tags":["service","Ambari","HDFS","fail-over"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-03-01 08:53:00.0","id":20418,"title":"How to load a bag from a file","body":"<p>Hi,</p><p>I was trying to load a file in Pig which contains data like :</p><p>{(3),(mary),(19)}</p><p>{(1),(john),(18)}</p><p>{(2),(joe),(18)}</p><p>Following command is falling :</p><p>A = LOAD 'data3' AS (B: bag {T: tuple(t1:int), F:tuple(f1:chararray), G:tuple(g1:int)});</p><p>How to do it in correct way ?</p><p>Thanks,</p><p>Soumya</p>","tags":["Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-01 02:18:20.0","id":20362,"title":"Unable to create directory in HDFS","body":"<p>HI, I am not able to create directory into the root/user folder into the HDFS. I tried creating hdfs user with admin privileges but no luck. </p>","tags":["hdfs-permissions"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-01 20:13:10.0","id":20601,"title":"subproduct files into HDFS","body":"<p>Hi:</p><p>I have a question about layers on HDFS.</p><p>If i need to make subproducts, is better proccess with pig, spark or R the Virgin files and convert it into transformed files and insert in hive, o better attack the virgin files and show with any analitic sofware??</p><p>thanks</p>","tags":["files","HDFS"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-01 16:36:25.0","id":20523,"title":"Can any one explain, what does \"hostname -f\" say in curl command?","body":"<p>curl -u admin:$PASSWORD -i -H 'X-Requested-By: ambari'-X PUT -d  '{\"RequestInfo\": {\"context\" :\"Start HDFS via REST\"}, \"Body\": {\"ServiceInfo\": {\"state\": \"STARTED\"}}}'  http://`hostname -f`:8080/api/v1/clusters/$CLUSTER_NAME/services/HDFS  In the above curl command, what is the meaning for \"hostname -f\". I am new to usage of REST APIs, it may be a stupid question. I would like to get clear idea for the above command.</p>","tags":["Ambari","api","service","curl"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-03-01 20:22:04.0","id":20606,"title":"how to delete table \"_Vname\" in hive?","body":"<p>I am getting error with this table name in hive I tried to drop but I can not. can you please let me know how to drop it?</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-01 21:46:38.0","id":20635,"title":"Hortonworks ODBC Driver failure on _HOST kerberos host name","body":"<p>Hello, </p><p>I was able to setup 2nd Hiveserver2 and able to connect to Hive through beeline and zookeeper. </p><p>I am able to connect using kerbfqdh as _HOST through Beeline. </p><p>But for the some reason, Hortonworks ODBC driver is giving error 34 if I provide _HOST in krbhostfqdn field.</p><p>I am using </p><p>service discovery mode = Zookeeper, </p><p>hosts =&lt;zookeeper quorom&gt;, </p><p>authentication method = kerberos </p><p>service name = Hive </p><p>kerb hostfqdn = _HOST</p><p>Regards</p><p>Pranay Vyas</p>","tags":["odbc","zookeeper","kerberos","hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-01 21:51:51.0","id":20636,"title":"Using Sqoop Netezza to hive import what if I want to changed some data type ?","body":"<p>I am using sqoop netezza database to Hive import.Successfully, hive-import when I check data type so some of the data type changed as Netezza data type:</p><p>Netezza----&gt; HIVE</p><p>BIGINT---BIGINT</p><p>DATE---STRING</p><p>INT------INT</p><p>CHARACTER---STRING</p><p>NUMERIC----DOUBLE</p><p>TIMESTAMP----STRING</p><p>BUT I want (1) Timestamp in hive instead of String (2) numeric--double...this is right??</p><p>I might transfer more tables so what Can i do to keep my data type in hive instead of sqoop automatically transfer..</p><p>what is best solution for this???</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-02 00:44:10.0","id":20658,"title":"unexpected error, closing socket connection and attempting reconnect..","body":"<p>Hi,</p><p>It was running fine, but suddenly, I started getting below error, Does anyone know what can be the issue?</p><p>I restarted zookeepr from ambari as well.</p><pre>[root@ttsv-lab-vmdb-01 ~]# /usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --zookeepelocalhost:2081 --topic 56d4a92bb0073e537755b461 --from-beginning\n[2016-03-01 16:37:13,249] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\njava.net.ConnectException: Connection refused\n   at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n   at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n   at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)\n   at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)\n[2016-03-01 16:37:13,354] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\njava.net.ConnectException: Connection refused</pre>","tags":["Ambari","Kafka","zookeeper"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-02 01:05:32.0","id":20652,"title":"After upgrading from HDP 2.2 to HDP 2.3, all kafka topics are unaccessible by producers/consumers.","body":"<p>We upgraded our cluster from HDP 2.2.0 to HDP 2.3.</p><p>After the upgrade, we check the topics and all topics are still there.</p><p>But when trying to produce or even consume messages on all of the topics, we're having errors.</p><p>What should I do to fix my issue? I also notice on broker side that the broker.id also changed form id=0 to id=1001.</p><p>Please help. </p>","tags":["upgrade","hdp-2.3.2","Kafka","ambari-2.2.0"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-02 21:07:23.0","id":20863,"title":"Sqoop Incremental scenairo","body":"<p>Hi, I have table Service Requests (SRs) which is having 10k records as of today trying to implement incremental logic so that when i run tomorrow need to capture only newly inserted and updated records</p><p><b>Table structure </b>: row_id (unique), SR# , Sr_owner,sr_group,sr_last_update_date (timestamp) </p><p><b>Approache :</b></p><p>1) created sqoop job as below</p><pre>sqoop job --create sr_table -- import --connect \"jdbc:sqlserver://localhost:1431;database=test;username=root;password=welcome1\" --query 'select * from  sr_table where $CONDITIONS' --target-dir /data/sr/sr_table --append --check-column SR_LAST_UPD --incremental lastmodified --last-value '1900-01-01' --split-by ROW_ID</pre><p>2) While running this for the first time its working fine (Full data or full load)</p><p>3) If I re-execute it again its not comparing he Upper Bound value with last run time (SR_LAST_UPD)</p><p>Was able to follow this (https://github.com/abajwa-hw/single-view-demo/blob/master/singleview-mysql-advanced-23.md)</p><p>I'm looking for more dynamic way of <strong>comparing the sqoop job execution time to compare with sr_last_upd value</strong> </p><p>Thanks -</p>","tags":["Hive","append","Sqoop","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-03 06:01:05.0","id":20937,"title":"what is the difference between exec and the run command? As I know if we use run, the statements from the script are available in the command history.Is there any more ans?","body":"","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-03 09:18:59.0","id":20963,"title":"JobTracker job.properties Setting Not Being respected in Oozie","body":"<p>I'm an issue with Oozie, whereby I set the jobTracker setting to one value (port 8050), but the error logs show the workflow failing as it is trying to use another value (8032). The jobTracker port in my workflow/job.properties is set to 8050 (to match the yarn setting) and I can see in the oozie UI (click on job &gt; action &gt; action configuration) that 8050 is being used:</p><p><strong>job.properties</strong></p><pre>nameNode=hdfs://myDomain:8020\njobTracker=myOtherDomain:8050\nqueueName=default\nmaster=yarn # have also tried yarn-cluster and yarn-client\n\noozie.use.system.libpath=true\noozie.wf.application.path=${nameNode}/bmp/\noozie.action.sharelib.for.spark=spark2 # I've added the updated spark libs I need in here\n\n\n</pre><p>workflow</p><pre>&lt;workflow-app xmlns='uri:oozie:workflow:0.5' name='MyWorkflow'&gt;\n    &lt;start to='spark-node' /&gt;\n    &lt;action name='spark-node'&gt;\n        &lt;spark xmlns=\"uri:oozie:spark-action:0.1\"&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n            &lt;prepare&gt;\n                &lt;delete path=\"${nameNode}/bmp/output\"/&gt;\n            &lt;/prepare&gt;\n            &lt;master&gt;${master}&lt;/master&gt;\n            &lt;name&gt;My Workflow&lt;/name&gt;\n            &lt;class&gt;uk.co.bmp.drivers.MyDriver&lt;/class&gt;\n            &lt;jar&gt;${nameNode}/bmp/lib/bmp.spark-assembly-1.0.jar&lt;/jar&gt;\n            &lt;spark-opts&gt;--conf spark.yarn.historyServer.address=http://myDomain:18088 --conf spark.eventLog.dir=hdfs://myDomain/user/spark/applicationHistory --conf spark.eventLog.enabled=true&lt;/spark-opts&gt;\n            &lt;arg&gt;${nameNode}/bmp/input/input_file.csv&lt;/arg&gt;\n        &lt;/spark&gt;\n        &lt;ok to=\"end\" /&gt;\n        &lt;error to=\"fail\" /&gt;\n    &lt;/action&gt;\n    &lt;kill name=\"fail\"&gt;\n        &lt;message&gt;Workflow failed, error\n            message[${wf:errorMessage(wf:lastErrorNode())}]\n        &lt;/message&gt;\n    &lt;/kill&gt;\n    &lt;end name='end' /&gt;\n&lt;/workflow-app&gt;\n</pre><p>But when I drill down into the hadoop job history logs I see the error:</p><pre>Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.SparkMain], main() threw exception,Call From myDomain/ipAddress to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: Connection refused. For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</pre><p>Where is it pulling 8032 from? Why does it not use the port configured in the job.properties?</p><p>\"The workflow only started to work once I changed both of these to 8032\"</p><p>I'd rather not do this, given it is more of a workaround rather than fixing/understanding the issue and the change could have repercussions on other tools. Is there a way to configure it just for oozie and get it to respect the port in the job.properties?</p><p><strong>Related Questions</strong></p><ul>\n<li>https://community.hortonworks.com/questions/11599/how-to-change-resourcemanager-port-on-oozie.html</li><li>https://community.hortonworks.com/questions/7014/oozie-sparkaction-throwing-javalangnosuchmethoderr.html</li></ul>","tags":["YARN","Ambari","Spark","Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-03 04:48:15.0","id":20922,"title":"Scalable HDP cluster on AWS","body":"<p>I want to setup an elastic cluster using AWS EC2 and install HDP on it. How can i do it. What are the options available.</p><p>I dont want to use AWS EMR. Is it possible to bring up and down datnodes with HDP stack installed on it automatically.</p><p>Any suggestions would be great.</p>","tags":["aws","cloud"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-03 14:10:26.0","id":21011,"title":"how i extract attribute from json file using nifi","body":"<p>I use  gethttp processor to stream data using http request, so I need to get attribute from file json that I get from the first request and use it for another request, how I can do this with the existing nife processor.any help is appreciated </p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-03 21:58:16.0","id":21106,"title":"Ambari Install of HDP 2.4 Continually Failing","body":"<p>I am upgrading from HDP 2.3.4.0-3485 to HDP 2.4 and Ambari is able to install all of the HDP 2.4 packages on every node in our cluster except for the node running Ambari. It fails when it trys to \"yum install apache-maven-3.2*\"</p><p>\nI have tried replacing repositories, deleting HDP repositories and letting Ambari reload them in from the base urls, but it never seems to want to work. I also tried manually doing the yum install and still to no such avail. Stderr is located below, from what I can see it looks like that package doesn't even exist within that repository.</p><p>I am wondering if anyone else has run into a similar problem or if someone has seen this and been able to overcome this roadblock. Any help is appreciated or if someone could point me in the right direction I would also be extremely grateful! Thank you in advance.</p><p>  2016-03-03 15:45:31,408 - Package Manager failed to install packages. Error: Execution of '/usr/bin/yum -d 0 -e 0 -y install '--disablerepo=HDP-*' --enablerepo=HDP-2.4.0.0-169,HDP-UTILS-2.4.0.0-169 'apache-maven-3.2*'' returned 1. Error: Nothing to do</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py\", line 376, in install_packages\n    skip_repos=[self.REPO_FILE_NAME_PREFIX + \"*\"] if OSCheck.is_redhat_family() else [])\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py\", line 49, in action_install\n    self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py\", line 49, in install_package\n    shell.checked_call(cmd, sudo=True, logoutput=self.get_logoutput())\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nFail: Execution of '/usr/bin/yum -d 0 -e 0 -y install '--disablerepo=HDP-*' --enablerepo=HDP-2.4.0.0-169,HDP-UTILS-2.4.0.0-169 'apache-maven-3.2*'' returned 1. Error: Nothing to do\nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py\", line 497, in &lt;module&gt;\n    InstallPackages().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py\", line 163, in actionexecute\n    raise Fail(\"Failed to distribute repositories/install packages\")\nresource_management.core.exceptions.Fail: Failed to distribute repositories/install packages</pre>","tags":["hdp-2.3.4","upgrade","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-03 20:58:12.0","id":21098,"title":"Manual install for Ranger Repo Missing Error 404","body":"<p>I am following the manual installation guide for Ranger <a href=\"http://private-repo-1.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.4/bk_installing_manually_book/content/install-the-ranger-policy-manager-man-install.html\">here</a> (tried though Ambari already but am getting errors).  The first step on \"3.1 Install Ranger Policy Manager\" asks to add the repository to yum but the command given in the installation guide does not work.  I get an \"Error 404: Not Found.\"  I don't think I'm looking at an old guide.  Any assistance would be much appreciated.  I have been struggling to get Ranger installed for a couple weeks now.</p>","tags":["repository","installation","Ranger","error"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-04 06:43:41.0","id":21166,"title":"HDP 2.4 upgrade - Found service components in INSTALL_FAILED state","body":"<p>\n\tDuring upgrade to 2.4 I get </p><pre>Reason: Found service components in INSTALL_FAILED state. Please re-install these components. Service components in INSTALL_FAILED state: [TEZ:TEZ_CLIENT on c3gw], [OOZIE:OOZIE_CLIENT on c3gw], [SLIDER:SLIDER on c3gw], [TEZ:TEZ_CLIENT on cn104], [SQOOP:SQOOP on cn111], [SLIDER:SLIDER on cn104], [SQOOP:SQOOP on c3gw], [PIG:PIG on c3gw], [TEZ:TEZ_CLIENT on cn111], [SQOOP:SQOOP on cn104.].\nFailed on: SLIDER,SQOOP,TEZ,OOZIE,PIG\n</pre><p>How do I trigger a reinstall of the failing components? </p><p><img src=\"/storage/attachments/2597-screen-shot-2016-03-04-at-075758.png\"></p><p><img src=\"/storage/attachments/2598-screen-shot-2016-03-04-at-075852.png\"></p>","tags":["Ambari","hdp-2.3.4","active-directory","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-04 12:24:03.0","id":21201,"title":"What level of hardware monitoring is done by HDP / Node Manager in HDP 2.2.4.2 versus HDP 2.4.0?","body":"<p>One customer wants to know the details about hardware monitoring differences between HDP 2.2.4.2 and  HDP 2.4.0. Any ideas, guys? Really appreciate if you could provide some documents as well.</p>","tags":["monitoring","YARN","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-04 14:59:57.0","id":21248,"title":"Spark action always submit to 0.0.0.0:8032","body":"<p>I'm trying to submit a spark action using Oozie with master=yarn-cluster and mode=cluster but the job hangs trying to submit to RM <strong>0.0.0.0:8032</strong></p><p>In the YARN logs of the application there are the the following 2 lines:</p><pre>org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at hdp24node00/192.168.178.30:8050\norg.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\norg.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n...\n...\norg.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</pre><p>It first try to connect to the right resource manager, but then defaults to 0.0.0.0:8032 until it fails.</p><p>yarn.resourcemanager.address is set to <em>hdp24node00:8050 </em>and job-tracker property is also set to <em>hdp24node00:8050</em>.</p><p>Where is the problem?</p>","tags":["YARN","Spark","resource-manager","Oozie"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-04 15:36:16.0","id":21252,"title":"Zeppelin Tech Preview with LDAP Authentication","body":"<p>Team,</p><p>Congratulations. We just published Zeppelin <a href=\"http://hortonworks.com/hadoop-tutorial/apache-zeppelin-hdp-2-4/\">technical preview</a> #2 on HDP 2.4</p><p>The major features in this TP are:</p><ul><li>Notebook Import/Export </li><li>LDAP Authentication </li><li>Ambari Managed Installation</li></ul><p>Please take it out for a spin.</p><p>Thanks for your input.</p>","tags":["zeppelin","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-04 18:34:37.0","id":21291,"title":"SAS and HDP 2.3.4 integration documentation","body":"<p>Hi,</p><p>I am using HDP 2.3.4 distributed cluster and would like to integrate SAS for the analytics. Is there any standard documentation which is followed for this setup? </p><p>Regards,</p><p>Krishna</p>","tags":["integration","hdp-2.3.4"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-07 02:09:47.0","id":21528,"title":"[HDP-2.3.2] Kafka 0.9 consumer version issue.","body":"<p>I'm on HDP-2.3.2 with kafka-0.9.0</p><p>I can produce messages using v0.8.2 and 0.9.0 okay and consume it using v0.8.2.</p><p>But when consuming messages using V.0.9.x kafka, I get this error:</p><p>Error in fetch kafka.consumer.ConsumerFetcherThread$FetchRequest@9439eee. Possible cause: java.nio.BufferUnderflowException (kafka.consumer.ConsumerFetcherThread)</p><p>Please Help.</p>","tags":["Kafka","hdp-2.3.2"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-17 15:58:29.0","id":23501,"title":"Sqoop import from Netezza to HDFS failing with java.lang.ArrayIndexOutOfBoundsException","body":"<p>I am able to successfully import few tables from Netezza to HDFS. </p><p> Failing tables have primary key constraint on netezza and I see the Sqoop Split-by is using primary key column. I tried changing the split-by to different column and increased the split count as well.</p><p></p><p>However I am getting following error message for few tables.</p><p></p><p>16/03/15 14:00:23 INFO mapreduce.Job: Task Id : attempt_1456951008977_0160_m_000000_0, Status : FAILED\nError: java.lang.ArrayIndexOutOfBoundsException\n  at java.lang.System.arraycopy(Native Method)\n  at org.netezza.sql.NzConnection.receiveDbosTuple(NzConnection.java:739)\n  at org.netezza.internal.QueryExecutor.getNextResult(QueryExecutor.java:177)\n  at org.netezza.internal.QueryExecutor.execute(QueryExecutor.java:73)\n  at org.netezza.sql.NzConnection.execute(NzConnection.java:2688)\n  at org.netezza.sql.NzStatement._execute(NzStatement.java:849)\n  at org.netezza.sql.NzPreparedStatament.executeQuery(NzPreparedStatament.java:169)\n  at org.apache.sqoop.mapreduce.db.DBRecordReader.executeQuery(DBRecordReader.java:111)\n  at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:235)\n  at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)\n  at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n  at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n  at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n  at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n  at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n  at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</p><p>16/03/15 14:00:43 INFO mapreduce.Job: Task Id : attempt_1456951008977_0160_m_000000_1, Status : FAILED\nError: java.lang.ArrayIndexOutOfBoundsException\n  at org.netezza.sql.NzConnection.receiveDbosTuple(NzConnection.java:739)\n  at org.netezza.internal.QueryExecutor.update(QueryExecutor.java:340)\n  at org.netezza.sql.NzConnection.updateResultSet(NzConnection.java:2704)\n  at org.netezza.sql.NzResultSet.next(NzResultSet.java:1924)\n  at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:237)\n  at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)\n  at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\n  at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\n  at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n  at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)\n  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n  at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n  at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</p>","tags":["Sqoop","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-17 18:07:27.0","id":23524,"title":"Storm Service Check failed","body":"<p>After upgrading Ambari to version 2.1.1, encountering service check failed for Storm.</p><pre>2016-03-16 10:55:48 b.s.d.nimbus [INFO] Received topology submission for WordCountid1e5a2fd5_date551616 with conf {\"storm.id\" \"WordCountid1e5a2fd5_date551616-2-1458140148\", \"nimbus.host\" \"somehost.com\", \"topology.users\" (), \"topology.acker.executors\" nil, \"topology.kryo.decorators\" (), \"topology.name\" \"WordCountid1e5a2fd5_date551616\", \"topology.submitter.principal\" \"\", \"topology.submitter.user\" \"\", \"topology.debug\" true, \"topology.kryo.register\" nil, \"topology.workers\" 3, \"storm.zookeeper.superACL\" nil, \"topology.max.task.parallelism\" nil}\n2016-03-16 10:55:48 b.s.d.nimbus [WARN] Topology submission exception. (topology name='WordCountid1e5a2fd5_date551616') #&lt;RuntimeException java.lang.RuntimeException: org.apache.storm.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/WordCountid1e5a2fd5_date551616-2-1458140148&gt;\n2016-03-16 10:55:48 o.a.t.s.TNonblockingServer [ERROR] Unexpected exception while invoking!\njava.lang.RuntimeException: org.apache.storm.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/WordCountid1e5a2fd5_date551616-2-1458140148\n  at backtype.storm.util$wrap_in_runtime.invoke(util.clj:47) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]\n  at backtype.storm.zookeeper$create_node.invoke(zookeeper.clj:92) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]\n  at backtype.storm.cluster$mk_distributed_cluster_state$reify__2327.set_data(cluster.clj:104) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]\n  at backtype.storm.cluster$mk_storm_cluster_state$reify__2822.set_credentials_BANG_(cluster.clj:422) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_45]\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_45]\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_45]\nat java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_45]</pre>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-18 04:04:22.0","id":23567,"title":"Upgrade from HDP 2.3.2 to HDP 2.4 - Oozie service check fails","body":"<p>Hi,</p><p>I tried upgrading non kerberized HDP 2.3.2 cluster to HDP 2.4.</p><p>During the upgrade process the Oozie service check failed.</p><p><img src=\"/storage/attachments/2870-error-01-copy.png\"></p><p>Did anyone encounter this? </p><p>Thanks.</p>","tags":["upgrade","hdp2.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-17 13:56:21.0","id":23485,"title":"Can i deploy jsf spring war application on hortonworks sandbox using winscp ?","body":"","tags":["java","spring","hortonwork"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-03-18 10:14:49.0","id":23598,"title":"KeeperErrorCode = ConnectionLoss for /hbase/master","body":"<p><a href=\"/storage/attachments/2885-errorlog.txt\">errorlog.txt</a>This thread is related to my post related to Metrics collector not starting (https://community.hortonworks.com/questions/23512/metrics-collector-is-not-starting-showing-error-re.html). I thought of creating a new thread to highlight the main issue that I see in the log files. In general, Metric Collector is not starting. The logs have many information related to the error. The error that is mainly shown </p><p>exception=org.apache.zookeeper.KeeperException$ConnectionLossException: <strong>KeeperErrorCode = ConnectionLoss for /hbase/master</strong></p><p>In the same log file, I also see the following line </p><p>zookeeper.ClientCnxn: Opening socket connection to server <strong>localhost/0:0:0:0:0:0:0:1:61181</strong>.</p><p>If 'localhost' is the issue here, then I am not sure why localhost is used here instead of FQDN and I do not know which configuration file will help me to use FQN instead of localhost. </p><p>I have read many threads related to this error but none of them helped me fix this issue.  Can anyone please help me understand this issue. </p><p>I am pasting the value some of the properties:</p><p>hbase.rootdir = hdfs://item-70288:8020/apps/hbase/data</p><p>hbase.cluster.distributed= true</p><p>Metrics service operation mode = embedded</p><p>hbase.zookeeper.property.clientPort= 2181</p><p>hbase.zookeeper.quorum =item-70288</p><p><strong>Update #1</strong></p><p>=========</p><p>The command netstat -anp |grep 61181 is not returning anything. It seems nothing is listening on 61181.</p><p>I am attaching the full error log with this post.</p>","tags":["metrics-collector","zookeeper","Hbase"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-18 06:09:22.0","id":23573,"title":"Help on analysis of unstructured dynamic data","body":"<p>We want to analyze using hive on the below type of data. Below are the challenges.</p><p>Source data are flat files from different sources.Multiple source file on daily basis. There is no fixed columns (each files have different columns). Each file have very large number of rows. No:of columns,order of the column are diffrent. each field will be comma seperated, but field value might have quotes (\"\").</p><p>Please suggest what would be the ideal aproch in this. Load to hbase and create hive table on top of that? or is it possible to create hive table with dynamic schema?</p>","tags":["hadoop","Hbase","Hive","analytics"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-18 14:39:19.0","id":23596,"title":"Ambari-server setup - Ambari env","body":"<p>Hi</p><p>I am working behind a firewall and need to use a proxy. I downloaded repo and installed through yum by modifying respective wgetrc and yum.conf to include proxies.</p><p>Then I followed instruction to modify /var/lib/ambari-server/ambari-env.sh to add </p><p>export AMBARI_JVM_ARGS='-Dhttp.proxyHost=myProxy -Dhttp.proxyPort=80 -Dhttp.proxyUser=user -Dhttp.proxyPassword=passwrd'\n</p><p>However, when I ran ambari-server setup, it still got stuck in Download JDK step. Finally I had to manually wget JDK and JCE files to proceed. I further dug down and found force-sownload function uses urllib2 to open the link, but does not use any proxy override. </p><p>If it is a bug, hapy to post in Jira. </p>","tags":["ambari-server","proxy"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-03-18 23:32:18.0","id":23700,"title":"How Do You Install Python Packages in HDP Spark Cluster?","body":"<p>Hi,</p><p> I can ssh into my HDP cluster servers as user 'cloudbreak', but I cannot sudo or su to root in order to install python modules and packages I need for pyspark. I spent time searching for answers but came up with nothing. Any help appreciated.</p>","tags":["Spark","ssh","python","pyspark"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-18 18:14:05.0","id":23666,"title":"Resynchronize the HBase data betweentwo clusters","body":"<p>I found records which exist in the replicated HBase cluster but not in the source HBase cluster.  </p><p>How to easily resynchronize the two clusters when the replicated hbase cluster is correct?   Any ideas?</p>","tags":["replication","Hbase","clustering"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-18 23:32:47.0","id":23674,"title":"HIVE Query exection fails to create tables... sort of...","body":"<p>Downloaded sandbox; installed in VMWare ESX environment, came up without a hitch.</p><p>I'm now wading through the \"Hadoop Tutorial - Getting Started with HDP\" and have run into an issue in Lab 2, Step 2.</p><p>I pasted in the query text to define the tables, kicked off execution, and it sat there \"running\" a long time. You could even see it in a \"running\" state in the History tab. I let it \"run\" for several minutes and was concerned that a simple table definition should take so long. So I attempted to stop execution, which had no effect at all. After about 20 minutes or so, I rebooted the sandbox. </p><p>Once the sandbox was back up, I went back into Hive and attempted to define the table again. This time the query said it was \"running\" again and sat there for minutes again. I got sick of waiting this second time and attempted to stop execution and boom... error message pops up on the upper right telling me the script failed because the table already exists. Um... ok...</p><p>I look in the database tree and yep, there's the table created by a script that never appeared to finish and should've been killed by a reboot.</p><p>I shrugged it off and went to create the second table. Same deal. Run table creation query and it will just sit there... for hours if I let it (I did). I try to kill it, no apparent change. I reboot the sandbox, no apparent change. I try to run the table create again, no apparent change. I try to kill the table create a second time and boom... error saying the script failed because the table's now suddenly there.</p><p>What gives? I'd like to hand this off to some other personnel to start testing how we can integrate the Hadoop stack into various things around here but it's not a good sign when the canned sandbox is doing odd things like this.</p>","tags":["Sandbox","hdp-2.4.0","Hive","tutorial-100"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-19 00:41:52.0","id":23702,"title":"what's best practice to integrate ambari metrics with datacenter wide ganglia & nagios infrastructure ?","body":"<p>For deployments that use ganglia & nagios for datacenter level monitoring, how can one integrate with ambari metrics for a single overview of hadoop and non-hadoop nodes and services ?</p>","tags":["nagios","Ambari","ambari-metrics","ganglia"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-19 08:17:00.0","id":23717,"title":"Ranger Solr on HDP 2.3.4 unable to refresh policies","body":"<p>\n\tI just installed a new cluster on HDP 2.3.4 with Solr 5.2.1 including Kerberos + Ranger Solr Plugin. Following this article https://community.hortonworks.com/articles/15159/securing-solr-collections-with-ranger-kerberos.html\n</p>\n<p>\n\tHowever when I enable the Ranger Solr Plugin and restart solr I am seeing the following error:\n</p>\n<pre>\n632300 [Thread-15] WARN  org.apache.ranger.plugin.util.PolicyRefresher  [   ] – cache file does not exist or not readble 'null'\n662301 [Thread-15] ERROR org.apache.ranger.plugin.util.PolicyRefresher  [   ] – PolicyRefresher(serviceName=null): failed to refresh policies. Will continue to use last known version of policies (-1)\ncom.sun.jersey.api.client.ClientHandlerException: java.lang.IllegalArgumentException: URI is not absolute\n        at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:151)\n        at com.sun.jersey.api.client.Client.handle(Client.java:648)\n        at com.sun.jersey.api.client.WebResource.handle(WebResource.java:680)\n        at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)\n        at com.sun.jersey.api.client.WebResource$Builder.get(WebResource.java:507)\n        at org.apache.ranger.admin.client.RangerAdminRESTClient.getServicePoliciesIfUpdated(RangerAdminRESTClient.java:73)\n        at org.apache.ranger.plugin.util.PolicyRefresher.loadPolicyfromPolicyAdmin(PolicyRefresher.java:205)\n        at org.apache.ranger.plugin.util.PolicyRefresher.loadPolicy(PolicyRefresher.java:175)\n        at org.apache.ranger.plugin.util.PolicyRefresher.run(PolicyRefresher.java:154)\nCaused by: java.lang.IllegalArgumentException: URI is not absolute\n        at java.net.URI.toURL(URI.java:1088)\n        at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:159)\n        at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149)\n        ... 8 more\n</pre>\n<p>\n\tThis is what it looks like on HDP 2.3.2\n</p>\n<pre>\nINFO  - 2016-03-15 16:54:50.478; [   ] org.apache.ranger.plugin.util.PolicyRefresher; PolicyRefresher(serviceName=mycluster_solr): found updated version. lastKnownVersion=-1; newVersion=61\n</pre>\n<p>\n\tOn 2.3.4 the <strong>serviceName</strong> is not replaced by the actual repository name, which was set in the install.properties file. On 2.3.2 the serviceName is replaced by the repository name &lt;clustername&gt;_solr.\n</p>\n<p>\n\tLooks like a bug :(\n</p>\n<p>\n\t Anyone seen this issue before? Any possible workaround?\n</p>","tags":["SOLR","hdp-2.3.4","solrcloud","ranger-0.5.0"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-18 20:40:36.0","id":23637,"title":"HDF - Flatten a xml file","body":"<p>Is it possible to flat xml file using NiFi? For example:  <a target=\"_blank\" href=\"https://odieweblog.wordpress.com/2011/12/13/how-to-flatten-out-an-xml-hierarchical-structure/\">Link</a></p><p><a target=\"_blank\" href=\"https://odieweblog.wordpress.com/2011/12/13/how-to-flatten-out-an-xml-hierarchical-structure/\"></a> </p>","tags":["hdf","flatten","xml","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-19 19:52:24.0","id":23762,"title":"Help understanding corrupt ORC file in Hive","body":"<p><a rel=\"user\" href=\"/users/3280/rchapin.html\" nodeid=\"3280\">@Ryan Chapin</a></p><p>Looking for some suggestions. We have a query that was hanging indefinitely in Hive on Tez. We are running with HDP 2.4.0. After some debugging, we narrowed it down to a single ORC file in a Hive partition that contained that file plus about 10 others. If we move this one file out of the partition and test the query, it now completes. If we include ONLY that one file in the partition, the query hangs. Even a simple \"select * from table\" hangs. The query never gets beyond the first map task. I then discovered the ORC file dump feature of Hive and ran the following on this file:</p><pre>hive --orcfiledump --skip-dump --recover -d hdfs://vmwhaddev01:8020/tmp/000002_0 &gt; orc.dump</pre><p>This command never returns to the command line and hangs, similar to what Hive does. I Tested this on a known good file and the dump completes successfully and returns control to the command line as expected. So, even this dump test is hanging. If I tail orc.dump during the hang, the last line of the file looks complete. </p><p>So, I am wondering if this line is the end of a stripe and the next stripe is corrupt? The ORC reader seems to get into some infinite loop at this point. Once the dumped output file size stops increasing, at about 382 MB, top command shows the Hive dump process continuously using about 99.9% CPU until I CTRL+C the command. It dumps about 382 MB of data before the dump command hangs. Here's last line which is complete:</p><pre>{\"_col0\":\"DSN001000325021\",\"_col1\":10784199,\"_col2\":1457431200,\"_col3\":1457434800,\"_col4\":20209,\"_col5\":0,\"_col6\":60,\"_col7\":10,\"_col8\":1456222331,\"_col9\":120,\"_col10\":117,\"_col11\":114,\"_col12\":0,\"_col13\":0,\"_col14\":0,\"_col15\":0,\"_col16\":0,\"_col17\":0,\"_col18\":120,\"_col19\":121,\"_col20\":123,\"_col21\":124,\"_col22\":125,\"_col23\":0,\"_col24\":0,\"_col25\":15296815,\"_col26\":2,\"_col27\":0,\"_col28\":163528,\"_col29\":88,\"_col30\":1498,\"_col31\":29082,\"_col32\":874908,\"_col33\":51565,\"_col34\":104138,\"_col35\":149,\"_col36\":6,\"_col37\":0,\"_col38\":0,\"_col39\":1508,\"_col40\":0,\"_col41\":2248,\"_col42\":46961,\"_col43\":624,\"_col44\":1732,\"_col45\":0,\"_col46\":0,\"_col47\":0,\"_col48\":0,\"_col49\":41,\"_col50\":159,\"_col51\":12,\"_col52\":30,\"_col53\":0,\"_col54\":0,\"_col55\":0,\"_col56\":0,\"_col57\":0,\"_col58\":0,\"_col59\":0,\"_col60\":0,\"_col61\":1398668,\"_col62\":5916915,\"_col63\":5916855,\"_col64\":5986115,\"_col65\":249,\"_col66\":66,\"_col67\":547,\"_col68\":76,\"_col69\":132618,\"_col70\":17398,\"_col71\":140325,\"_col72\":19012,\"_col73\":0,\"_col74\":0,\"_col75\":0,\"_col76\":0,\"_col77\":\"TUC04HNSIGW63B002Adv\",\"_col78\":1456805959,\"_col79\":0,\"_col80\":158,\"_col81\":136,\"_col82\":0,\"_col83\":0,\"_col84\":1,\"_col85\":0,\"_col86\":12,\"_col87\":12,\"_col88\":0,\"_col89\":0,\"_col90\":0,\"_col91\":0,\"_col92\":12,\"_col93\":12,\"_col94\":0,\"_col95\":0,\"_col96\":0,\"_col97\":0,\"_col98\":18,\"_col99\":14,\"_col100\":0,\"_col101\":0,\"_col102\":0,\"_col103\":0,\"_col104\":12,\"_col105\":12,\"_col106\":0,\"_col107\":0,\"_col108\":0,\"_col109\":0,\"_col110\":51565,\"_col111\":37383,\"_col112\":0,\"_col113\":402,\"_col114\":0,\"_col115\":449,\"_col116\":3126,\"_col117\":46256,\"_col118\":28682,\"_col119\":4,\"_col120\":0,\"_col121\":0,\"_col122\":0,\"_col123\":0,\"_col124\":0,\"_col125\":0,\"_col126\":0,\"_col127\":0,\"_col128\":0,\"_col129\":0,\"_col130\":0,\"_col131\":0,\"_col132\":0,\"_col133\":0,\"_col134\":0,\"_col135\":0,\"_col136\":0,\"_col137\":0,\"_col138\":0,\"_col139\":0,\"_col140\":0,\"_col141\":0,\"_col142\":0,\"_col143\":\"20.2.1-3569\",\"_col144\":0,\"_col145\":0,\"_col146\":0,\"_col147\":0,\"_col148\":0,\"_col149\":0,\"_col150\":0,\"_col151\":0,\"_col152\":0,\"_col153\":0,\"_col154\":0,\"_col155\":0,\"_col156\":0,\"_col157\":0,\"_col158\":0,\"_col159\":0,\"_col160\":0,\"_col161\":0,\"_col162\":0,\"_col163\":0,\"_col164\":0,\"_col165\":0,\"_col166\":0,\"_col167\":0,\"_col168\":0,\"_col169\":0,\"_col170\":0,\"_col171\":0,\"_col172\":0,\"_col173\":4,\"_col174\":\"12-AUG-15\",\"_col175\":12,\"_col176\":\"HT1100\",\"_col177\":\"1\",\"_col178\":\"B4WB16S2\",\"_col179\":\"CORE_DSN_PROD_HT1100_50K\",\"_col180\":\"SW_DSN_PROD_HT1100_50K\",\"_col181\":\"3.2.0.24\",\"_col182\":\"1000\"}</pre><p>I am trying to determine if I have uncovered a bug or somehow the data that I am inserting into the ORC somehow resulted in this condition. Either way, it seems like a bug if you can insert data that causes an ORC file to become corrupted. The ingest pipeline for this data is as follows.</p><ol><li>I convert raw CSV files into Avro and land them in an HDFS directory. There could be multiple Avro schemas in play here as there are multiple versions of these CSV files in flight. The Avro schemas are designed such that I can include all versions in the same Hive table. Typically, newer versions of these stats files add more columns of stats.</li><li>Once a day, I move all the Avro files that have accumulated to a temp directory and create an external table over the files in that directory</li><li>I run a query that selects * from the external table and inserts all the results into another Hive managed table that is in ORC format, effectively using Hive to perform the Avro to ORC conversion. This query also performs a join with some data from one other table to enrich the data landing in the ORC table. This table is partitioned by year/month/day.</li><li>Because the resulting ORC files are relatively small for HDFS, I perform one final step after the ORC insert query completes. I run a Hive query against the newly created partition to effectively compact the ORC files. Typically, the reduce part generates around 70 ORC files. I run a query like the following for the appropriate year, month, and day of the partition just created which typically compacts all 70 ORC files into about 5 much larger ones that are about 2-3 HDFS blocks (128 MB) in size each.</li></ol><pre>alter table table_name partition (year=2016, month=3, day=16) concatenate;</pre><p>This is the first such issue we've seen in over two months of ingesting such files in this manner. </p><ul><li>Does anyone have any ideas of where to look further to possibly understand the root cause of this problem?</li><li>Maybe the concatenate operation happened to cause the file corruption in this case? Anyone heard of such a thing?</li><li>Should I file a bug report and provide this corrupt ORC file for some forensic analysis? I don't really want to start trying to hex dump and decode ORC to figure out what happened. </li></ul><ol></ol>","tags":["Tez","corruption","Hive","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-19 23:55:26.0","id":23766,"title":"ImportError: No module named bs4  - but it's right there!","body":"<p>Hello,</p><p>I'm using a UDF in Pig to extract HTML tags from some text in Sandbox. </p><p>Python version: 2.6 and I've installed bs4 (beautifulsoup4). </p><p>However when I run my script it says:</p><pre>  File \"/tmp/pig5620474456855825046tmp/soup.py\", line 3, in &lt;module&gt; \n     from bs4 import BeautifulSoup \n ImportError: No module named bs4</pre><p>Locate shows that bs4 in /usr/lib/python2.6/site-packages/bs4</p><p>I installed bs4 using both easy_install and pip, just to be sure. </p><p>Any ideas why it is not detecting bs4? </p><p>Apologies if this is the incorrect forum - please move it if necessary. Also, I'm a total beginner so apologies too if this is a stupid question. </p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-21 00:15:13.0","id":23807,"title":"Running Python Scripts on data in HDFS","body":"<p>Hi,</p><p>I am using the Sandbox on a VM within the Azure Cloud, I am neither a developer nor a scientist so excuse my ignorance. </p><p>I have loaded my csv file to HDFS and I am currently trying to get it into a Hive table so that I can run some queries against it. </p><p>Using MapReduce I need to run a TF-IDF algoritm on a number of entries in the table. I believe I need to write the TF-IDF algorithm in Python but I am unsure how I go about this. Do I need to install a Python compiler on the Azure VM or can I write the code locally on my laptop and query the Hive table?</p>","tags":["Hive","MapReduce","python","Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-20 07:41:14.0","id":23774,"title":"What version of HDP to use to get kafka 0.9.0.1","body":"<p>Does HDP 2.3.2 has kafka latest updated version 0.9.0.1. The patch which has many bug fixes</p>","tags":["Kafka","hdp-2.4","hdp-2.3.2"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-20 16:19:03.0","id":23797,"title":"How the hive metastore works","body":"<p>I have some hive tables with data. And now I will use spark sql to query this data.</p><p>But Im not understanding the role of a Hive component in this process, the Hive Metastore.</p><p>The Hive Metastore stores all info about the tables. And we can execute spark sql queries because spark can interact with Hive Metastore.</p><p>But, how that works, its automatic? I have the hive tables with data, now to execute spark sql queries I need to create the Hive Metastore? Or its automatic? We need to do something?</p><p>Im relatively new in Hive and Im not understanding well this concept in this scenario. </p><p>Thanks!!</p>","tags":["hadoop","HDFS","Hive","metastore"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-21 16:59:01.0","id":23942,"title":"Shutdown HiveMetaStore on client disconnect or timeout","body":"<p>I facing issue when user either trying to disconnect or got timeout, the Hive Meta Store got shutdown.</p>","tags":["hiveserver2","metastore"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-21 21:42:31.0","id":23963,"title":"Sandbox HDFS Replication Set to 3 - Why?","body":"<p>While running the latest Sandbox (HDP 2.4 on Hortonworks Sandbox), I noticed HDFS had 500+ under replicated blocks (via Ambari).  Opening /etc/hadoop/conf/hdfs-site.xml,  dfs.replication=3 (default http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)</p><p>Does anyone know why the Sandbox uses a HDFS replication factor of 3, aside from the fact that its the HDFS default?  I'd assume most Sandbox users are running a virtual machine representing one node.  If this is the case, dfs.replication=1 in the Sandbox to prevent under replicated blocks.  Is my assumption incorrect?  </p>","tags":["replication","Sandbox","HDFS"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-21 09:45:48.0","id":23864,"title":"Hive http transport mode problem","body":"<p>Hi,</p><p>I'm trying to get Hive working with http transport mode with beeline client. We are using HDP 2.3.4.</p><p>hive.server2.transport.mode=http</p><p>hive.server2.thrift.http.port=10001</p><p>hive.server2.thrift.http.path=cliservice</p><p>Hiveserver2 does not use any authentication methods.</p><p>I'm trying to connect to hiveserver2 using beeline client from the same host where hiveserver2 is running.</p><p>I'm getting strange error message and I was not able to find anything relevant from documentation. </p><p>\nConnection error: </p><p>beeline&gt; !connect jdbc:hive2://localhost:10001?transportMode=http;httpPath=cliservice </p><p>Connecting to jdbc:hive2://localhost:10001?transportMode=http;httpPath=cliservice </p><p>Enter username for jdbc:hive2://localhost:10001?transportMode=http;httpPath=cliservice: </p><p>Enter password for jdbc:hive2://localhost:10001?transportMode=http;httpPath=cliservice: </p><p>Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10001?transportMode=http;httpPath=cliservice: Invalid status 72 (state=08S01,code=0)\njava.sql.SQLException: </p><p>Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10001?transportMode=http;httpPath=cliservice: Invalid status 72\nat org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:210)\nat org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:156)\nat org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)\nat java.sql.DriverManager.getConnection(DriverManager.java:664)\nat java.sql.DriverManager.getConnection(DriverManager.java:208)\nat org.apache.hive.beeline.DatabaseConnection.connect(DatabaseConnection.java:142)\nat org.apache.hive.beeline.DatabaseConnection.getConnection(DatabaseConnection.java:207)\nat org.apache.hive.beeline.Commands.connect(Commands.java:1149)\nat org.apache.hive.beeline.Commands.connect(Commands.java:1070)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:497)\nat org.apache.hive.beeline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:52)\nat org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:980)\nat org.apache.hive.beeline.BeeLine.execute(BeeLine.java:823)\nat org.apache.hive.beeline.BeeLine.begin(BeeLine.java:781)\nat org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:485)\nat org.apache.hive.beeline.BeeLine.main(BeeLine.java:468)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:497)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: org.apache.thrift.transport.TTransportException: Invalid status 72\nat org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)\nat org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:184)\nat org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:307)\nat org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\nat org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:185)\n... 24 more\n0: jdbc:hive2://localhost:10001 (closed)&gt;</p><p>NB! I'm able to connect to hiveserver2 if transport mode is configured to use as binary. Also I don't use any username and password on connection using beeline. </p><p>Maybe someone can direct me to right direction getting it to work. </p><p>\nThanks in advance.</p>","tags":["Hive","http","beeline"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-21 23:25:48.0","id":23972,"title":"Location of Maven artifact \"org.apache.spark:spark-yarn_2.10\" version 1.6.0.2.4.0.0-169?","body":"<p>Looking for the latest HDP 2.4 version of org.apache.spark:spark-yarn_2.10 . The most recent version in <a href=\"http://repo.hortonwoks.com/\">http://repo.hortonwoks.com</a> is 1.6.0.2.3.4.1-10:</p><p>http://repo.hortonworks.com/content/repositories/releases/org/apache/spark/spark-yarn_2.10/</p><p>Is a release planned for HDP-2.4?  If so, when can we expect it to be available?</p>","tags":["Spark","maven","development"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-22 01:37:22.0","id":23990,"title":"Dropping Hive Table - Lost Metastore Connection","body":"<p>HDP 2.3</p><p>--------------------</p><p>When I drop a table, Hiveserver2 got disconnected from Metastore: Please see the log:</p><p>2016-03-22 01:29:08,416 INFO \n[main]: parse.ParseDriver (ParseDriver.java:parse(185)) - Parsing\ncommand: <strong><u>DROP TABLE emp</u></strong></p><p>2016-03-22 01:29:08,420 INFO \n[main]: parse.ParseDriver (ParseDriver.java:parse(209)) - Parse\nCompleted</p><p>2016-03-22 01:29:08,420 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &lt;/PERFLOG\nmethod=parse start=1458610148415 end=1458610148420 duration=5\nfrom=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,420 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG\nmethod=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,646 INFO \n[main]: ql.Driver (Driver.java:compile(466)) - Semantic Analysis\nCompleted</p><p>2016-03-22 01:29:08,646 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &lt;/PERFLOG\nmethod=semanticAnalyze start=1458610148420 end=1458610148646 duration=226\nfrom=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,646 INFO \n[main]: ql.Driver (Driver.java:getSchema(246)) - Returning Hive schema:\nSchema(fieldSchemas:null, properties:null)</p><p>2016-03-22 01:29:08,646 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG\nmethod=doAuthorization from=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,647 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &lt;/PERFLOG\nmethod=doAuthorization start=1458610148646 end=1458610148647 duration=1\nfrom=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,647 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &lt;/PERFLOG\nmethod=compile start=1458610148414 end=1458610148647 duration=233\nfrom=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,647 INFO \n[main]: ql.Driver (Driver.java:compile(543)) - We are resetting the\nhadoop caller context to</p><p>2016-03-22 01:29:08,647 INFO \n[main]: ql.Driver (Driver.java:checkConcurrency(166)) - Concurrency mode\nis disabled, not creating a lock manager</p><p>2016-03-22 01:29:08,647 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG\nmethod=Driver.execute from=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,647 INFO \n[main]: ql.Driver (Driver.java:execute(1373)) - Setting caller context\nto query id saxon_20160322012908_b2183147-565b-423e-8683-627532554bd7</p><p>2016-03-22 01:29:08,647 INFO \n[main]: ql.Driver (Driver.java:execute(1376)) - Starting\ncommand(queryId=saxon_20160322012908_b2183147-565b-423e-8683-627532554bd7):\nDROP TABLE emp</p><p>2016-03-22 01:29:08,648 INFO \n[main]: hooks.ATSHook (ATSHook.java:&lt;init&gt;(90)) - Created ATS Hook</p><p>2016-03-22 01:29:08,648 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG\nmethod=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook\nfrom=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,649 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &lt;/PERFLOG\nmethod=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1458610148648\nend=1458610148649 duration=1 from=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,650 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &lt;/PERFLOG\nmethod=TimeToSubmit start=1458610148413 end=1458610148649 duration=236\nfrom=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,650 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG\nmethod=runTasks from=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,650 INFO \n[main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG\nmethod=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver&gt;</p><p>2016-03-22 01:29:08,650 INFO \n[main]: ql.Driver (Driver.java:launchTask(1701)) - Starting task\n[Stage-0:DDL] in serial mode</p><p>2016-03-22 01:29:29,320 WARN \n[main]: metastore.RetryingMetaStoreClient\n(RetryingMetaStoreClient.java:invoke(184)) - <strong><u>MetaStoreClient lost connection.</u></strong>\nAttempting to reconnect.</p><p>MetaException(message:javax.jdo.JDODataStoreException: You\nhave an error in your SQL syntax; check the manual that corresponds to your\nMySQL server version for the right syntax to use near 'OPTION\nSQL_SELECT_LIMIT=DEFAULT' at line 1</p><p>  at\norg.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)</p><p>  at\norg.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)</p><p>  at\norg.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:1005)</p><p>  at\norg.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:937)</p><p>  at\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>  at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</p><p>  at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>  at\njava.lang.reflect.Method.invoke(Method.java:498)</p><p>  at\norg.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)</p><p>  at\ncom.sun.proxy.$Proxy2.getTable(Unknown Source)</p><p>  at\norg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1804)</p>","tags":["hiveserver2","metastore","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-22 02:19:31.0","id":23981,"title":"NiFi - Capture error message in Bulletin","body":"<p>Is there a way to capture error messages that appear in Bulletin for more than 5 minutes ? After an error, I want to log detail error message from Bulletin in a permanent log, so I can look up error later. Currently Bulletin error messages provide most amount of detail on error (including detail logs) but they rollover every 5 minutes. </p><p>Is there another way of capturing detail error messages ?</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-18 16:15:36.0","id":23647,"title":"Ranger admin install fails with \"007-updateBlankPolicyName.sql import failed\"","body":"<p>Installing Ranger via Ambari on AWS, got this:</p><p>2016-03-18 16:10:44,048  [JISQL] /usr/jdk64/jdk1.8.0_60/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/hdp/current/ranger-admin/jisql/lib/* org.apache.util.sql.Jisql -driver mysqlconj -cstring jdbc:mysql://mysqldb/ranger -u 'user' -p '********' -noheader -trim -c \\; -input /usr/hdp/current/ranger-admin/db/mysql/patches/007-updateBlankPolicyName.sql</p><pre>Error executing: CREATE FUNCTION `getTempPolicyCount`(assetId bigint, resId bigint) RETURNS int(11) BEGIN DECLARE tempPolicyCount int default 1; DECLARE dbResourceId bigint; DECLARE exitLoop int DEFAULT FALSE; DECLARE policyList CURSOR FOR  \tSELECT id from x_resource where asset_id = assetId; DECLARE CONTINUE HANDLER FOR NOT FOUND SET exitLoop = true; OPEN policyList; readPolicy : LOOP \tFETCH policyList into dbResourceId; \tIF exitLoop THEN \t\tset tempPolicyCount = tempPolicyCount + 1; \t\tLEAVE readPolicy; \tEND IF; \tIF (resId = dbResourceId) THEN \t\tLEAVE readPolicy; \tEND IF; \tset tempPolicyCount = tempPolicyCount + 1; END LOOP; CLOSE policyList; RETURN tempPolicyCount; END  \njava.sql.SQLException: This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you *might* want to use the less safe log_bin_trust_function_creators variable)\nSQLException : SQL state: HY000 java.sql.SQLException: This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you *might* want to use the less safe log_bin_trust_function_creators variable) ErrorCode: 1418\n2016-03-18 16:10:44,438  [E] 007-updateBlankPolicyName.sql import failed!</pre>","tags":["ranger-admin","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-21 10:55:36.0","id":23857,"title":"Executing Hive queries through oozie java action on kerberized cluster","body":"<p>Hi,</p><p>I am trying to execute hive queries through oozie Java\naction on kerberized cluster, however I am getting below exception:</p><pre>&gt;&gt;&gt; Invoking Main class now &gt;&gt;&gt;\nLaunch time = 1458542514082\nJob launch time = 1458542514082 mapreduce.job.tags = oozie-2e8d7ed9fc7551a353667830e09bef2b\nMain class        : com.citiustech.main.Test\nArguments         :\n\n&lt;&lt;&lt; Invocation of Main class completed &lt;&lt;&lt;\n\nFailing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.JavaMain], main() threw exception, org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://10.60.2.26:10000/default;principal=hive/example-qa1-dn2@EXAMPLE.com;auth=NOSASL: GSS initiate failed\norg.apache.oozie.action.hadoop.JavaMainException: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://10.60.2.26:10000/default;principal=hive/example-qa1-dn2@EXAMPLE.com;auth=NOSASL: GSS initiate failed\n\tat org.apache.oozie.action.hadoop.JavaMain.run(JavaMain.java:58)\n\tat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:39)\n\tat org.apache.oozie.action.hadoop.JavaMain.main(JavaMain.java:36)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:226)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://10.60.2.26:10000/default;principal=hive/example-qa1-dn2@EXAMPLE.com;auth=NOSASL: GSS initiate failed\n\tat org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:80)\n\tat org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:391)\n\tat org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:471)\n\tat org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:481)\n\tat org.springframework.jdbc.core.JdbcTemplate.queryForObject(JdbcTemplate.java:491)\n\tat org.springframework.jdbc.core.JdbcTemplate.queryForObject(JdbcTemplate.java:497)\n\tat com.citiustech.main.Test.main(Test.java:26)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.oozie.action.hadoop.JavaMain.run(JavaMain.java:55)\n\t... 15 more\nCaused by: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://10.60.2.26:10000/default;principal=hive/example-qa1-dn2@EXAMPLE.com;auth=NOSASL: GSS initiate failed\n\tat org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:210)\n\tat org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:156)\n\tat org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:571)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:187)\n\tat org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriverManager(DriverManagerDataSource.java:153)\n\tat org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriver(DriverManagerDataSource.java:144)\n\tat org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnectionFromDriver(AbstractDriverBasedDataSource.java:155)\n\tat org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnection(AbstractDriverBasedDataSource.java:120)\n\tat org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:111)\n\tat org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:77)\n\t... 26 more\nCaused by: org.apache.thrift.transport.TTransportException: GSS initiate failed\n\tat org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)\n\tat org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:316)\n\tat org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\n\tat org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)\n\tat org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)\n\tat org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:185)\n\t... 36 more\n\nOozie Launcher failed, finishing Hadoop job gracefully\n\nOozie Launcher, uploading action data to HDFS sequence file: hdfs://example-qa1-nn:8020/user/admin/oozie-oozi/0000000-160317141545276-oozie-oozi-W/javaAction--java/action-data.seq\n\nOozie Launcher ends\n</pre><p>Please find below workflow xml specifying oozie java action\nand java program for executing hive queries:</p><p>Workflow.xml:</p><pre>&lt;workflow-app name=\"WorkFlowForJavaActionToExecuteHiveQuery\" xmlns=\"uri:oozie:workflow:0.2.5\"&gt;\n\t\t&lt;credentials&gt;\n\t\t\t&lt;credential name='hive_credentials' type='hcat'&gt;\n\t\t\t\t&lt;property&gt;\n\t\t\t\t\t&lt;name&gt;hcat.metastore.uri&lt;/name&gt;\n\t\t\t\t\t&lt;value&gt;thrift://example-qa1-dn2:9083&lt;/value&gt;\n\t\t\t\t&lt;/property&gt;\n\t\t\t\t&lt;property&gt;\n\t\t\t\t\t&lt;name&gt;hcat.metastore.principal&lt;/name&gt;\n\t\t\t\t\t&lt;value&gt;hive/_HOST@EXAMPLE.COM&lt;/value&gt;\n\t\t\t\t&lt;/property&gt;\n\t\t\t&lt;/credential&gt;\n\t\t&lt;/credentials&gt;\n\t\t&lt;start to=\"javaAction\"/&gt;\n\t\t&lt;action name=\"javaAction\" cred=\"hive_credentials\"&gt;\n\t\t\t&lt;java&gt;\n\t\t\t\t&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n\t\t\t\t&lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n\t\t\t\t&lt;main-class&gt;com.demo.main.Test&lt;/main-class&gt;\n\t\t\t&lt;/java&gt;\n\t\t\t&lt;ok to=\"end\"/&gt;\n\t\t\t&lt;error to=\"fail\"/&gt;\n\t\t&lt;/action&gt;\n\t\t&lt;kill name=\"fail\"&gt;\n\t\t\t&lt;message&gt;Job failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n\t\t&lt;/kill&gt;\n\t\t&lt;end name=\"end\"/&gt;\n\t&lt;/workflow-app&gt;\n\n</pre><p>Java Program:</p><pre>package com.demo.main;\nimport java.io.IOException;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.jdbc.datasource.DriverManagerDataSource;\npublic class Test {\n\tpublic static void main(String[] args) throws IOException {\n\t\tUserGroupInformation.loginUserFromKeytab(\"admin@EXAMPLE.COM\", \"/etc/security/keytabs/admin.keytab\");\n\t\torg.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration();\n\t\tconf.set(\"hadoop.security.authentication\", \"Kerberos\");\n\t\tUserGroupInformation.setConfiguration(conf);\n\t\tUserGroupInformation.loginUserFromKeytab(\"admin@EXAMPLE.COM\", \"/etc/security/keytabs/admin.keytab\");\n\t\tDriverManagerDataSource dataSource = new DriverManagerDataSource(\n\t\t\t\t\"jdbc:hive2://10.60.2.26:10001/default;principal=hive/example-qa1-dn2@EXAMPLE.com\", \"hive\", \"\");\n\t\tdataSource.setDriverClassName(\"org.apache.hive.jdbc.HiveDriver\");\n\t\tJdbcTemplate jdbcTemplate = new JdbcTemplate();\n\t\tjdbcTemplate.setDataSource(dataSource);\n\t\tlong count = 0;\n\t\tcount = jdbcTemplate.queryForObject(\"SELECT COUNT(*) FROM provider\", Long.class);\n\t\tSystem.out.println(\"Count is *************************\" + count);\n\t}\n}\n\n</pre><p>I have submitted workflow job using admin user who have\naccess to hive. </p><p>I did kinit programmatically as well using\nUserGroupInformation API for admin user, but still I am getting above\nexception. </p><p>I have kept only spring core, spring jdbc and hive jdbc jars\nin oozie HDFS cache. </p><p>I also tried all possible solutions mentioned on hortonworks(https://community.hortonworks.com/questions/8995/access-to-hive-from-oozie-java-action-with-kerbero.html),\nstack overflow and other forums, however I am still facing same issue.</p><p>Can anyone suggest any solution for this issue?</p>","tags":["Oozie","kerberos","sasl","Hive"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-03-22 07:45:14.0","id":24024,"title":"Is there any way to migrate existing hbase database into phoenix database ?? Or Best way to migrate existing hbase database into phoenix ??","body":"","tags":["Phoenix","Hbase","phoenix4.4"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-22 15:30:18.0","id":24089,"title":"Install : UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 191: ordinal not in range(128)","body":"<p>\n\tEnv: Ubuntu 14.04.1 HDP 2.4</p><p>\tTrying to install nifi with https://github.com/abajwa-hw/ambari-nifi-service but I am ending up with </p><pre>\n\tTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/NIFI/package/scripts/master.py\", line 201, in &lt;module&gt;\n    Master().execute(\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/NIFI/package/scripts/master.py\", line 109, in install\n    self.configure(env, True)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/NIFI/package/scripts/master.py\", line 145, in configure\n    Execute(format(\"cd {params.conf_dir}; mv flow.xml.gz flow_$(date +%d-%m-%Y).xml.gz ;\"), user=params.nifi_user, ignore_failures=True)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 163, in run\n    Logger.info(\"Skipping failure of %s due to ignore_failures. Failure reason: %s\" % (resource, str(ex)))\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 191: ordinal not in range(128)</pre><p>More info <a href=\"https://github.com/abajwa-hw/ambari-nifi-service/issues/5\">here</a> </p>","tags":["Ambari","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-22 13:57:56.0","id":24078,"title":"HDP 2.3 on AWS H060 Unable to open Hive session","body":"<p>I've built an HDP 2.3 cluster on AWS, and using Ambari I created a view for Hive, and granted the Ambari admin login permission to that view. However, when logged in as the Ambari admin account, whenever I attempt to access that view this error message is thrown:</p><p>H060 Unable to open Hive session: org.apache.thrift.protocol.TProtocolException: Required field 'serverProtocolVersion' is unset!</p><p>in the HDFS advance configuration, custom core-site section, these properties have been set:</p><p>hadoop.proxyuser.root.hosts=*</p><p>hadoop.proxyuser.root.groups=*</p><p>hadoop.proxyuser.ec2-user.hosts=*</p><p>hadoop.proxyuser.ec2-user.groups=*</p><p>Any recommendations to remedy this error are welcome.</p>","tags":["Hive","ambari-views"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-22 13:44:46.0","id":24063,"title":"HDFS Snapshot - Space consumption report","body":"<p>Is there a simple way to get space usage by current data and for each of the hdfs snapshots from a specific hdfs directory?</p><p>The only way I can think would be executing hadoop fsck, listing all snapshots, files and blocks and then writing a custom script that would do:</p><p>1- sum block size for all files in current version</p><p>2- create a distinct list of blocks from current version</p><p>3- look at first snapshot block list, find blocks not reported before and sum block size for those diff blocks, then add diff blocks to distinct list of blocks.</p><p>4- repeat step 3 for all remaining snapshot.</p><p>5- generate a report with space used by current data and all snapshots </p><p>Thanks.</p><p>Guilherme</p>","tags":["snapshot","HDFS","space"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-22 15:01:40.0","id":24086,"title":"Trouble with Capacity Scheduler link to local Users/Groups","body":"<p>I am trying to demo out the Capacity Scheduler and am running into issues when I try to use more than one group or user.  The system allows me to enter one no problem.  I would like to have it set up so I can have a couple different groups (from Manage Ambari) linked to a couple different capacity scheduler groups.  </p><p><img src=\"/storage/attachments/2932-capacity-scheduler-1.jpg\"></p><p>When I try to add another group it gives me an error and doesn't allow me to save the configuration.  I looked in the documentation and it says to use a comma to separate values.</p><p><img src=\"/storage/attachments/2933-capacity-scheduler-2.jpg\"></p><p>Below are the groups I created in Manage Ambari -&gt; Groups</p><p><img src=\"/storage/attachments/2934-capacity-scheduler-3.jpg\" style=\"width: 618px;\"></p><p>Here are the Queues I created in the Capacity Scheduler.</p><p><img src=\"/storage/attachments/2936-capacity-scheduler-4.jpg\"></p>","tags":["groups","yarn-scheduler","user-groups","trouble"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-22 15:39:56.0","id":24098,"title":"NameNode format and deleting/emptying physical hdfs folder.","body":"<p>I know how to format namenode and I also know that, in order to free the space, I should also delete all the files from the physical folder that HDFS refers to. My question is about the existing folder that I see in the current '/' root folder of HDFS. In my HDFS root I see the following folders:</p><pre>amshbase \napp-logs \napps \nhdp \nmapred \nmr-history\ntmp \nuser</pre><p>I want to know that if I clean the physical folder where the HDFS related files are kept, then what will happen to the above folders? Will it get created automatically by each service? </p>","tags":["ambari-2.1.2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-22 09:47:46.0","id":24015,"title":"Policy filter doesn't work properly on Ranger UI.","body":"<p>I can not get the enabled policies in the list of policies for Hive.</p><p><img src=\"/storage/attachments/2930-image2016-3-10-11-55-14.png\"></p><p>When I type \"status=true\", I can get the enabled policies.</p><p><img src=\"/storage/attachments/2931-image2016-3-10-11-50-0.png\"></p><p>I looked up source code and found the cause of this problem.</p><p>https://github.com/hortonworks/ranger-release/blob/HDP-2.4.0.0-tag/security-admin/src/main/java/org/apache/ranger/common/ServiceUtil.java#L731-L732</p><p>AppConstants.STATUS_DISABLED and AppConstants.STATUS_ENABLED are used for the statusEnum. However, these values don't exist. I think that RangerCommonEnums.STATUS_DISABLED and RangerCommonEnums.STATUS_ENABLED are proper values.</p><p>Please check the source code of Ranger.</p>","tags":["Ranger","ranger-0.5.0"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-22 20:18:41.0","id":24139,"title":"sudo apt-get autoclean  // clean /var/cache/apt/archives","body":"<p>sudo apt-get autoclean  // clean /var/cache/apt/archives</p>","tags":["YARN","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-23 08:08:49.0","id":24194,"title":"Kerberos KDC not reachable","body":"<p><a href=\"/storage/attachments/2951-kdc-unreachable.jpg\">kdc-unreachable.jpg</a>I am trying to kereeberise my HDP cluster. I have installed a KDC on the ambari host itself and i want to use that. so i selected option 1 (existing KDC) in ambari. But when i try to test the KDC connection it fails and i get the following error.</p><p>23 Mar 2016 13:16:29,457  WARN [qtp-ambari-client-18131] KdcServerConnectionVerification:187 - An unexpected exception occurred while attempting to communicate with the KDC server at hostname:88 over TCP\n23 Mar 2016 13:16:29,459  WARN [qtp-ambari-client-18131] KdcServerConnectionVerification:187 - An unexpected exception occurred while attempting to communicate with the KDC server at hostname:88 over UDP\n23 Mar 2016 13:16:29,460 ERROR [qtp-ambari-client-18131] KdcServerConnectionVerification:113 - Failed to connect to the KDC at hostname:88 using either TCP or UDP</p><p>But when i try to do kinit or invoke any kerberos command from the ambari/KDC host it is working fine. It is pretty strange and i dont see any network related issues. this error is seen only when a wrong kdc information is provided. But in my case even after providing the correct details it fails. From the logs i couldnt trace anything.</p><p>Is there any way to debug or trace it.</p><p>i used ping, telenet to do the basic checks and everything is fine.</p><p>P.s i have just replaced my KDC host names with the string \"hostname\"in the above error message.</p>","tags":["kdc","kerberos","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-23 10:21:00.0","id":24205,"title":"Storm - AutoHBase [ERROR] Could not obtain credentials from credentials map.","body":"<p>Hi,</p><p>Storm topology is created and deployed in a Kerberized evironment.\nWe have a bolt to write data in HBase & HDFS but it is unable to get HBase and HDFS credentials.</p><p>I got the same error earlier but it got resolved by restarting the cluster. I doubt if it was right solution. </p><p>I also faced similar scenario where in the bolts were working fine but suddenly gave the exception after the cluster was unused for couple of days.\n\n<strong></strong></p><p>\n<strong></strong></p><p><strong>Are these credentials cached in a temp directory?</strong></p><p>\n<strong></strong></p><p>Please find below the worker logs for the same :</p><p>2016-03-23 08:38:19 b.s.s.a.AuthUtils [INFO] Got AutoCreds [com.config.setup.storm.security.AutoHBase@71f139b7, com.config.setup.storm.security.AutoHDFS@51f6292b]\n2016-03-23 08:38:19 c.c.h.s.c.s.s.s.AutoHBase [ERROR] Could not obtain credentials from credentials map.\njava.io.StreamCorruptedException: invalid stream header: 64756D6D</p><p style=\"margin-left: 40px;\">at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:804) ~[na:1.7.0_67]\n  at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:299) ~[na:1.7.0_67]\n  at com.config.setup.storm.security.AutoHBase.getCredentials(AutoHBase.java:96) [config.jar:na]\n  at com.config.setup.storm.security.AutoHBase.addCredentialToSubject(AutoHBase.java:129) [config.jar:na]\n  at com.config.setup.storm.security.AutoHBase.populateSubject(AutoHBase.java:122) [config.jar:na]\n  at backtype.storm.security.auth.AuthUtils.populateSubject(AuthUtils.java:189) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n  at backtype.storm.daemon.worker$fn__7701$exec_fn__1271__auto____7702.invoke(worker.clj:424) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n  at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]\n  at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]\n  at clojure.core$apply.invoke(core.clj:617) [clojure-1.5.1.jar:na]\n  at backtype.storm.daemon.worker$fn__7701$mk_worker__7778.doInvoke(worker.clj:408) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n  at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]\n  at backtype.storm.daemon.worker$_main.invoke(worker.clj:540) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n  at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]\n  at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]\n  at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]</p><p>2016-03-23 08:38:19 c.c.h.s.c.s.s.s.AutoHBase [INFO] No credential found in credentials map.\n2016-03-23 08:38:19 c.c.h.s.c.s.s.s.AutoHDFS [ERROR] Could not obtain credentials from credentials map.\njava.io.StreamCorruptedException: invalid stream header: 64756D6D</p><p style=\"margin-left: 40px;\">at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:804) ~[na:1.7.0_67]\n  at java.io.ObjectInputStream.&lt;init&gt;(ObjectInputStream.java:299) ~[na:1.7.0_67]\n  at com.config.setup.storm.security.AutoHDFS.getCredentials(AutoHDFS.java:96) [sm-config.jar:na]\n  at com.config.setup.storm.security.AutoHDFS.addCredentialToSubject(AutoHDFS.java:128) [sm-config.jar:na]\n  at com.config.setup.storm.security.AutoHDFS.populateSubject(AutoHDFS.java:121) [sm-config.jar:na]\n     at backtype.storm.daemon.worker$fn__7701$exec_fn__1271__auto____7702.invoke(worker.clj:424) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n  at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]\n  at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]\n  at clojure.core$apply.invoke(core.clj:617) [clojure-1.5.1.jar:na]\n  at backtype.storm.daemon.worker$fn__7701$mk_worker__7778.doInvoke(worker.clj:408) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n  at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]\n  at backtype.storm.daemon.worker$_main.invoke(worker.clj:540) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]\n  at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]\n  at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]\n  at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.3.2.2.9.0-3393.jar:0.9.3.2.2.9.0-3393]</p><p style=\"margin-left: 20px;\">2016-03-23 08:38:19 c.c.h.s.c.s.s.s.AutoHDFS [INFO] No credential found in credentials</p>\n","tags":["Storm","Kafka","kerberos","Hbase"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-23 14:48:43.0","id":24225,"title":"Hadoop can list s3 contents but spark-shell throws ClassNotFoundException","body":"<p>In short I'm trying to create a teststack for spark - aim being to read a file from an s3 bucket and then write it to another.  Windows env.\nI was repeatedly encountering errors when trying to access S3 or S3n as a ClassNotFoundException was being thrown. </p><p> These classes were added to the core-site.xml as the s3 and s3n.impl\nI added the hadoop/share/tools/lib to the classpath to no avail, I then added the `aws-java-jdk` and `hadoop-aws` jars to the share/hadoop/common folder and I am now able to list the contents of a bucket using haddop on the command line.\n`hadoop fs -ls \"s3n://bucket\"` shows me the contents, this is great news :) </p><p>In my mind the hadoop configuration should be picked up by spark so solving one should solve the other however when I run spark-shell and try to save a file to s3 I get the usual ClassNotFoundException as shown below. </p><p>I'm still quite new to this and unsure if I've missed something obvious, hopefully someone can help me solve the riddle? </p><p> Any help is greatly appreciated, thanks. </p><pre>The exception: \n    java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3native.NativeS3FileSystem not found\n            at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2074)\n            at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2578)\n            at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2591) </pre><p>my core-site.xml(which I believe to be correct now as hadoop can access s3): </p><pre>    &lt;property&gt;   &lt;name&gt;fs.s3.impl&lt;/name&gt;   &lt;value&gt;org.apache.hadoop.fs.s3.S3FileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt;\n&lt;name&gt;fs.s3n.impl&lt;/name&gt; &lt;value&gt;org.apache.hadoop.fs.s3native.NativeS3FileSystem&lt;/value&gt;\n&lt;description&gt;The FileSystem for s3n: (Native S3) uris.&lt;/description&gt;\n&lt;/property&gt; </pre><p>and finally the hadoop-env.cmd showing the classpath(which is seemingly ignored): </p><pre>        set HADOOP_CONF_DIR=C:\\Spark\\hadoop\\etc\\hadoop\n    @rem ##added as s3 filesystem not found.http://stackoverflow.com/questions/28029134/how-can-i-access-s3-s3n-from-a-local-hadoop-2-6-installation\n    set HADOOP_USER_CLASSPATH_FIRST=true\n    set HADOOP_CLASSPATH=%HADOOP_CLASSPATH%:%HADOOP_HOME%\\share\\hadoop\\tools\\lib\\*\n    @rem Extra Java CLASSPATH elements.  Automatically insert capacity-scheduler.\n    if exist %HADOOP_HOME%\\contrib\\capacity-scheduler (\n      if not defined HADOOP_CLASSPATH (\n        set HADOOP_CLASSPATH=%HADOOP_HOME%\\contrib\\capacity-scheduler\\*.jar\n      ) else (\n        set HADOOP_CLASSPATH=%HADOOP_CLASSPATH%;%HADOOP_HOME%\\contrib\\capacity-scheduler\\*.jar\n      )\n    )</pre>","tags":["Spark","hadoop","aws","spark-shell"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-22 07:58:37.0","id":24026,"title":"How to import existing repositories's contents (files) into HDFS ?","body":"<p>Hi guys,</p><p>I want to import/migrate my existing TBs of content (documents) into HDFS, How can I do that as easy as possible ?</p>","tags":["hadoop","data-ingestion","hdfs-ha","HDFS"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-23 12:40:48.0","id":24212,"title":"Install hue in HDP 2.4","body":"<p>Hi, </p><p>How can we install, Hue in HDP 2.4? </p><p>I have installed HDP2.4 but that also not stable, some of port is not working, I'm not sure, how can we make it as reachable. </p><p>However, when i start to start Hue Service, i got the below error, </p><p><img src=\"/storage/attachments/2946-capture-hue.png\" style=\"background-color: initial;\"></p><p>Can anyone, suggest me, how to install hue?</p><p>Thanks,</p>","tags":["hue","hdp2.4"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-23 15:45:02.0","id":24247,"title":"Ranger audit log is empty .... although policy is applied","body":"<p>Hello,</p><p>I have a fresh installation of HDP2.2.4 including Ranger 0.4</p><p>After enabling and configuring HDFS policy, that policy is getting applied, but I have no entries in the Audit=&gt;Access tab of Ranger UI, it is empty, even after waiting for some minutes and triggering several actions. In the Audit=&gt;Agents tab I can see all the HDFS/Hive/HBase agents connected.</p><p>Where can I check for issues what is going wrong here ?</p><p>Thanks...</p>","tags":["audit","ranger-0.4.0","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-23 19:08:50.0","id":24290,"title":"How to Specify the multiple delimiters in hive table?","body":"","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-23 20:45:37.0","id":24304,"title":"Hue Job Browser issue on a kerberized cluster","body":"<p>We recently enabled kerberos ( MIT-KDC ) on a cluster HDP 2.3.2/Ambari 2.1.2.1.</p><p>I also installed Hue and configured using the following link ..</p><p>http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_installing_manually_book/content/ch_installing_hue_chapter.html</p><p>I am able to see all the jobs when I click on the Hue Job Browser but when I try to click on any job ID .. i see the following error</p><p><strong>Could not find job application_1458248146517_0015.</strong></p><p>Job application_1458248146517_0015 could not be found: &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\"/&gt; &lt;title&gt;Error 401 Authentication required&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;h2&gt;HTTP ERROR 401&lt;/h2&gt; &lt;p&gt;Problem accessing /ws/v1/applicationhistory/apps/application_1458248146517_0015. Reason: &lt;pre&gt; Authentication required&lt;/pre&gt;&lt;/p&gt;&lt;hr /&gt;&lt;i&gt;&lt;small&gt;Powered by Jetty://&lt;/small&gt;&lt;/i&gt;&lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;/body&gt; &lt;/html&gt; (error 401)</p><p>Could some one please point me in the right direction on how to troubleshoot this issue?</p>","tags":["configuration","kerberos","hue","hdp-2.3.2"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-23 16:58:20.0","id":24254,"title":"zeppelin - how to remove results cache from notebook","body":"<p>Dear Experts,</p><p>I created a quite complex zeppelin notebook and accidentally returned a rather big result set for one of the queries (that I had to abort).</p><p>Now when I reopen the notebook in the browser, zeppelin takes ages and breaks after a while (I assume my browsers buffers are running over ...)</p><p>Any Idea on how I can clear/reset the zeppelin result sets without accessing/entering the notebook?</p><p>Any other idea on how I can bring back my notebook?</p><p>Thanks and br,</p><p>Rainer</p>","tags":["zeppelin-notebook","zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-23 16:12:10.0","id":24265,"title":"Phoenix UDF not loading","body":"<p>Have been trying to load a custom UDF in Phoenix. Getting </p><p>16/03/23 10:17:14 WARN util.DynamicClassLoader: Failed to load new jar xxxx.jar\njava.io.FileNotFoundException: /grid/01/hadoop/hbase/local/jars/xxxx.jar (Permission denied)</p><p>        at java.io.FileOutputStream.open(Native Method)\n        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:221)\n        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.&lt;init&gt;(RawLocalFileSystem.java:222)\n        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.&lt;init&gt;(RawLocalFileSystem.java:209)\n        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:305)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:293)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:326)\n        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.&lt;init&gt;(ChecksumFileSystem.java:393)\n        at ---</p><p>//-------//</p><p>//-------//</p><p>sqlline.Commands.execute(Commands.java:822)\n        at sqlline.Commands.sql(Commands.java:732)\n        at sqlline.SqlLine.dispatch(SqlLine.java:808)\n        at sqlline.SqlLine.begin(SqlLine.java:681)\n        at sqlline.SqlLine.start(SqlLine.java:398)\n        at sqlline.SqlLine.main(SqlLine.java:292)</p><p>Checked all the data nodes where HBase Region server hosted & it looks like :</p><pre>[dchandra@lnxhdpdt02 ~]$ ls -ltr /grid/01/hadoop/hbase/local/\ntotal 4\ndrwxrwxr-x 2 hbase hadoop 4096 Aug 28  2014 jars\n[dchandra@lnxhdpdt02 ~]$ ls -ltr /grid/01/hadoop/hbase/local/jars\ntotal 0</pre>","tags":["udf","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-23 15:10:22.0","id":24245,"title":"Querying over Phoenix view vs Table","body":"<p>Is Phoenix view return result sooner than direct querying the table? If so, how does views function?</p>","tags":["Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-24 08:09:30.0","id":24342,"title":"Newly added DataNodes won't joining the party","body":"<p>Hello,</p><p>We're running a cluster with 12 DataNode servers, each has 12 physical disks mounted as follows:</p><pre>/dev/sda5 on /grid/0 type ext4 (rw,noatime)\n/dev/sdb1 on /grid/1 type ext4 (rw,noatime)\n/dev/sdc1 on /grid/2 type ext4 (rw,noatime)\n/dev/sdd1 on /grid/3 type ext4 (rw,noatime)\n/dev/sde1 on /grid/4 type ext4 (rw,noatime)\n/dev/sdf1 on /grid/5 type ext4 (rw,noatime)\n/dev/sdg1 on /grid/6 type ext4 (rw,noatime)\n/dev/sdh1 on /grid/7 type ext4 (rw,noatime)\n/dev/sdi1 on /grid/8 type ext4 (rw,noatime)\n/dev/sdj1 on /grid/9 type ext4 (rw,noatime)\n/dev/sdk1 on /grid/10 type ext4 (rw,noatime)\n/dev/sdl1 on /grid/11 type ext4 (rw,noatime)\n</pre><p>We've tried adding 5 newly deployed DataNodes which are stronger and greater by all means (capacity, cpu & ram) to the cluster with the following disk layout:</p><pre>/dev/sda1 on /grid/0 type ext4 (rw,noatime)\n/dev/sdb1 on /grid/1 type ext4 (rw,noatime)\n/dev/sdc1 on /grid/2 type ext4 (rw,noatime)\n/dev/sdd1 on /grid/3 type ext4 (rw,noatime)\n/dev/sde1 on /grid/4 type ext4 (rw,noatime)\n/dev/sdf1 on /grid/5 type ext4 (rw,noatime)\n/dev/sdg1 on /grid/6 type ext4 (rw,noatime)\n/dev/sdh1 on /grid/7 type ext4 (rw,noatime)\n/dev/sdi1 on /grid/8 type ext4 (rw,noatime)\n/dev/sdj1 on /grid/9 type ext4 (rw,noatime)\n/dev/sdk1 on /grid/10 type ext4 (rw,noatime)\n/dev/sdl1 on /grid/11 type ext4 (rw,noatime)\n/dev/sdm1 on /grid/12 type ext4 (rw,noatime)\n/dev/sdn1 on /grid/13 type ext4 (rw,noatime)\n/dev/sdo1 on /grid/14 type ext4 (rw,noatime)\n/dev/sdp1 on /grid/15 type ext4 (rw,noatime)\n</pre><p>New DataNodes have 4 disks extra so we added /grid/12/hadoop/hdfs/data, /grid/13/hadoop/hdfs/data, /grid/14/hadoop/hdfs/data, /grid/15/hadoop/hdfs/data to DataNode directories in HDFS config.</p><p>Everywhere we searched, was written that in case directories does not exist, they will be ignored (because previous DataNodes lacks /grid/12,13,14,15 mount points).</p><p>What actually happened is, on previous DataNodes under /grid folders (12/,13/,14/,15/) were created and are filling up with HDFS data. Since they are mounted on / (and not on a dedicated block device), space is about to run out which is probably, not a good thing.</p><p>How to proceed now? How to remove the data which landed there to free root (/) partition space?</p><p>Thanks,</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-24 15:10:35.0","id":24414,"title":"HBase memstore adjacent families flush","body":"<p>When flushing occur - adjacent families are flushed as well.  Does that mean all regions on region server or all CF for that specific table are flushed?</p>","tags":["Hbase","memstore"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-14 17:54:45.0","id":22902,"title":"Oozie conflicts with existing Tomcat installation","body":"<p>I have lost track of the number of unsuccessful attempts at the deployment through the console. All requisites, such as selinux disabled, THS, ntp sync, password less sync from master node (amari server) to data node, etc are fine. </p><ol><li>I once again got a Java Process warning, after hosts were successfully registered, on the masternode. \n<p><em>Process Issues (1)\nThe following process should not be running\n/usr/lib/jvm/jre/bin/java -classpath </em></p></li><li>Warnings on namenode - end to end. No warning messages were shown\n</li><li>Failures on the datanode, right from DataNode Install\n<strong>\nHelp please! </strong>\n<p><strong></strong>Thanks</p></li></ol><p><strong>\n</strong></p><ol><li><pre>resource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.4 | tail -1`' returned 1. Traceback (most recent call last):\n  File \"/usr/bin/hdp-select\", line 375, in &lt;module&gt;\n    setPackages(pkgs, args[2], options.rpm_mode)\n  File \"/usr/bin/hdp-select\", line 268, in setPackages\n    os.symlink(target + \"/\" + leaves[pkg], linkname)\nOSError: [Errno 17] File exists</pre></li></ol>","tags":["Oozie","Ambari","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-24 14:52:44.0","id":24403,"title":"HDFS File exists error when re-running Metron Installer","body":"<p>I ran into an issue when i ran the Metron Installer on AWS based on these instructions:</p><p>https://github.com/apache/incubator-metron/tree/Metron_0.1BETA_rc5/deployment/amazon-ec2</p><p>I fixed that issue and I re-ran the installer via the command: </p><p>ansible-playbook -i ec2.py playbook.yml --skip-tags=\"wait\"</p><p>However, then I ran into the following error:</p><pre>03-24 06:22:36,900 p=68310 u=gvetticaden |  fatal: [ec2-54-186-178-244.us-west-2.compute.amazonaws.com]: FAILED! =&gt; {\"changed\": true, \"cmd\": [\"hdfs\", \"dfs\", \"-put\", \"/usr/metron/0.1BETA/config/patterns\", \"/apps/metron\"], \"delta\": \"0:00:02.300088\", \"end\": \"2016-03-24 11:22:36.562397\", \"failed\": true, \"rc\": 1, \"start\": \"2016-03-24 11:22:34.262309\", \"stderr\": \"put: `/apps/metron/patterns/asa': File exists\\nput: `/apps/metron/patterns/common': File exists\\nput: `/apps/metron/patterns/fireeye': File exists\\nput: `/apps/metron/patterns/sourcefire': File exists\\nput: `/apps/metron/patterns/yaf': File exists\", \"stdout\": \"\", \"stdout_lines\": [], \"warnings\": []}\n\n2016-03-24 06:22:36,904 p=68310 u=gvetticaden |         to retry, use: --limit @playbook.retry</pre>","tags":["tech-preview","Metron","installer"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-03-24 20:55:47.0","id":24469,"title":"Scoop error when connecting to SAP R3","body":"<p><strong></strong><strong>Spoop error:</strong> </p><p><strong>When connecting to SAPR3  that is an oracle DB and have tried many variations similar to this:</strong></p><p>sqoop import --connect jdbc:oracle:thin:@sapabc01.company.com:1527:RD1 --driver oracle.jdbc.OracleDriver --username ****** --password ******* --table SAPR3.TableName --m 1\n</p><p><strong>We were able to get connected to our SAP Hana BW environment using this code:</strong></p><p>sqoop import --username ****** --password ******** --connect jdbc:sap://sapabc01.company.com:30015/?currentschema=BHANA --driver com.sap.db.jdbc.Driver --table TSTC --m 1</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-25 03:20:39.0","id":24473,"title":"cloudbreak on GCP","body":"<p>Is there a step by step guide available for using cloudbreak on GCP or AWS to create a HDP cluster.</p><p>If there is any please do share.</p>","tags":["cloud","Cloudbreak","hdp-2.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-25 09:08:38.0","id":24499,"title":"Flume Twitter File size problem.","body":"<p>I have tried everything with the configuration for file size but nothing works. I want 64 MB file but I am getting kbs files changes every configuration but nothing works.<a href=\"/storage/attachments/2994-files.jpg\">files.jpg</a></p><p>Here is configuration file <a href=\"/storage/attachments/2993-flume.jpg\">flume.jpg</a></p><p>Please suggest.</p>","tags":["twitter","flume-1.6","Flume","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-25 17:19:31.0","id":24539,"title":"Ranger KMS wont start after re-install","body":"<p>I removed Ranger KMS from the cluster using REST API and then re-installed it using Ambari Add Service option, however when I tried to start Ranger KMS server it is failing to come up ..</p><p>HDP 2.3.2/Ambari 2.1.2.1</p><p>Here is the error I see when I try to restart ..</p><pre> Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/RANGER_KMS/0.5.0.2.3/package/scripts/kms_server.py\", line 82, in &lt;module&gt;\n    KmsServer().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 216, in execute\n    method(env)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 484, in restart\n    self.start(env)\n  File \"/var/lib/ambari-agent/cache/common-services/RANGER_KMS/0.5.0.2.3/package/scripts/kms_server.py\", line 55, in start\n    enable_kms_plugin()\n  File \"/var/lib/ambari-agent/cache/common-services/RANGER_KMS/0.5.0.2.3/package/scripts/kms.py\", line 277, in enable_kms_plugin\n    get_repo_flag = get_repo(params.policymgr_mgr_url, params.repo_name, ambari_username_password_for_ranger)\n  File \"/var/lib/ambari-agent/cache/common-services/RANGER_KMS/0.5.0.2.3/package/scripts/kms.py\", line 389, in get_repo\n    raise Fail('Get repository failed, {0}'.format(str(e))) \nresource_management.core.exceptions.Fail: Get repository failed, HTTP Error 404: Not Found</pre>","tags":["configuration","installation","Ranger","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-25 15:29:36.0","id":24515,"title":"Phoenix on Oozie","body":"<p>Anybody tried running Phoenix queries on Oozie in the secured cluster ?</p>","tags":["Oozie","security","Phoenix","query"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-25 16:58:54.0","id":24538,"title":"Why is Hive running out of memory OOM on an Azure VM with 4 cores and 28Gb of Ram (Sandbox)","body":"<p>Hi, I am trying to run some simple enough queries on my files (250000 records) and I am receiving an out of memory message as follows:</p><pre>ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1458912652919_0002_1_00, diagnostics=[Task failed, taskId=task_1458912652919_0002_1_00_000006, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap spaceat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:159)at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139).........and this continues for many lines. </pre><p>I have tried researching this issue and I came across this post but I am lost as to how I can modify the settings discussed: https://azure.microsoft.com/en-us/blog/hive-memory-settings-resolve-out-of-memory-errors-using-azure-hdinsight/</p><p>Can anyone shed any light on this for me please? By the way, below is the query I was trying to run on my Hive table:</p><pre>SELECT * FROM pigoutputhive SORT BY Score DESC LIMIT 10;</pre><p>Many thanks in advance.</p>","tags":["HDFS","Hive","Sandbox","azure"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-25 17:32:30.0","id":24541,"title":"Resource Manager HA and Yarn service-check is failing","body":"<p>We enabled Yarn Resource Manager HA on our cluster ( HDP 2.3.2 and Ambari 2.1.2.1) and it was working fine until we re-installedRanger KMS server from the cluster. When the ResrouceManager HA was working, I saw one of them as active Resource Manager and the other one as Stand-by but they both are now showing as ResourceManager and also when I run the service check on yarn .. it is failing with the following error message</p><p><img src=\"/storage/attachments/2998-rm.png\"></p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/service_check.py\", line 142, in &lt;module&gt;\n    ServiceCheck().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 216, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/service_check.py\", line 138, in service_check\n    raise Exception(\"Could not get json response from YARN API\")\nException: Could not get json response from YARN API</pre>","tags":["ha","YARN","configuration","resource-manager","hdp-2.3.2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-25 16:36:06.0","id":24527,"title":"Ranger Web UI is loading very slow and Policies take long time to load","body":"<p>Hi Team,</p><p>When i try login to ranger UI it takes time to load. Also I when i click on adding/deleting/modifying new policies, the policy page takes lot lot lot of time to open.</p><p>I check ranger database and found that the table size of \"x_group_users\" is almost 160MB.</p><p>Do you think 160Mb is something very large which can create problem for Ranger policy page to be very slow ?</p><p>If \"Yes\" - can you suggest how to tweak mysql server to handle big tables and huge row/column count by ranger. We are using the default mysql setup.</p><p>This is on priority for me.</p>","tags":["database","Ranger","policy","ranger-webui"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-25 19:40:55.0","id":24550,"title":"No heartbeats on ambari server while serve is up and running.","body":"<p>Hi </p><p>I am facing some issue in ambari host machines:</p><p>I have also tried to stop amari-server and restart it. I am getting this \nproblem from yesterday, when i was running spark program and then i got \n some error: <strong>kernel:BUG: soft lockup - CPU#7 stuck for 62s! [java:3474]</strong></p><p>I reboot my servers and restarted ambari but Ambari was showing no heartbeat for that server.</p><p>I am getting message: <strong>No heartbeats on ambari server for more than 3 minutes</strong>, even though my server is up and running.</p><p>Further details: </p><p><strong>vintim-mbp:~ vintim$ ssh </strong><strong>root@ttsv-lab-vmdb-01</strong></p><pre>[root@ttsv-lab-vmdb-01 ~]# curl -u admin:admin http://localhost:8080/api/v1/hosts\n{\n  \"href\" : \"http://localhost:8080/api/v1/hosts\",\n  \"items\" : [\n  {\n  \"href\" : \"http://localhost:8080/api/v1/hosts/ttsv-lab-vmdb-01.englab.juniper.net\",\n  \"Hosts\" : {\n  \"cluster_name\" : \"codecoverage\",\n  \"host_name\" : \"ttsv-lab-vmdb-01.englab.juniper.net\"\n  }\n  },\n  {\n  \"href\" : \"http://localhost:8080/api/v1/hosts/ttsv-lab-vmdb-02.englab.juniper.net\",\n  \"Hosts\" : {\n  \"cluster_name\" : \"codecoverage\",\n  \"host_name\" : \"ttsv-lab-vmdb-02.englab.juniper.net\"\n  }\n  }\n  ]\n}</pre><pre>&lt;strong&gt;[root@ttsv-lab-vmdb-01 ~]# curl -u admin:admin http://localhost:8080/api/v1/clusters/codecoverage/hosts&lt;/strong&gt;\n{\n  \"href\" : \"http://localhost:8080/api/v1/clusters/codecoverage/hosts\",\n  \"items\" : [\n  {\n  \"href\" : \"http://localhost:8080/api/v1/clusters/codecoverage/hosts/ttsv-lab-vmdb-01.englab.juniper.net\",\n  \"Hosts\" : {\n  \"cluster_name\" : \"codecoverage\",\n  \"host_name\" : \"ttsv-lab-vmdb-01.englab.juniper.net\"\n  }\n  },\n  {\n  \"href\" : \"http://localhost:8080/api/v1/clusters/codecoverage/hosts/ttsv-lab-vmdb-02.englab.juniper.net\",\n  \"Hosts\" : {\n  \"cluster_name\" : \"codecoverage\",\n  \"host_name\" : \"ttsv-lab-vmdb-02.englab.juniper.net\"\n  }\n  }\n  ]</pre>","tags":["Spark"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-26 07:52:30.0","id":24557,"title":"Pyspark missing hadoop_currentpath","body":"<p>I followed the instructions on http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/ and was able to get the iPython working in the web browser. I can import pydoop, however pydoop.hdfs does not work. The pydoop.hadoop_currentpath() command in the jupyter notebook returns a list of .jar files that is shorter than when I run the same command from a python line in an ssh shell. Where does pyspark get the path from and how do I fix it so that I can run pydoop.hdfs commands from pyspark?</p>","tags":["tutorial-380","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-26 09:43:20.0","id":24590,"title":"Hive - Word split and count","body":"<p>Hi all,</p><p>I am trying to perform a version of the word count function in Hive. Ii have the following fields: Owner_key and Post. I want to split the post into its individual words and then group by each UserId along with giving a count of each word. For example, say if this was my data:</p><table><tbody><tr><td>Owner_key</td><td>Post</td></tr><tr><td>1</td><td>apple orange apple</td></tr><tr><td>2</td><td>melon kiwi\n\n</td></tr></tbody></table><p>I would like the following output:</p><table><tbody><tr><td>Owner_key</td><td>word</td><td>count</td></tr><tr><td>1</td><td>apple</td><td>2</td></tr><tr><td>1</td><td>orange</td><td>1</td></tr><tr><td>2</td><td>melon</td><td>1</td></tr><tr><td>2</td><td>kiwi</td><td>1</td></tr></tbody></table><p>The code I have attempted is below. Hive is not necessarily giving me an error message; however it never shows me any results even when the status is at 100%.</p><p>Can anyone help?</p><p>Thanks in advance.</p><p></p><pre>SELECT owner_key, word, \n\ncount(*)FROM stackdata_updtd\n\nLATERAL VIEW explode(split(lower(post), '\\\\W+')) t1 AS word\n\nGROUP BY owner_key, word;</pre>","tags":["Hive","split"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-27 02:34:02.0","id":24628,"title":"the riskfactor pig script completed scucessfully but the target table is not populated with data. any idea, why?","body":"<p>Results:\n        \n        \n          \n          </p><pre>Apache Pig version 0.15.0.2.4.0.0-169 (rexported) \ncompiled Feb 10 2016, 07:50:04\n\nUSAGE: Pig [options] [-] : Run interactively in grunt shell.\n       Pig [options] -e[xecute] cmd [cmd ...] : Run cmd(s).\n       Pig [options] [-f[ile]] file : Run cmds found in file.\n  options include:\n    -4, -log4jconf - Log4j configuration file, overrides log conf\n    -b, -brief - Brief logging (no timestamps)\n    -c, -check - Syntax check\n    -d, -debug - Debug level, INFO is default\n    -e, -execute - Commands to execute (within quotes)\n    -f, -file - Path to the script to execute\n    -g, -embedded - ScriptEngine classname or keyword for the ScriptEngine\n    -h, -help - Display this message. You can specify topic to get help for that topic.\n        properties is the only topic currently supported: -h properties.\n    -i, -version - Display version information\n    -l, -logfile - Path to client side log file; default is current working directory.\n    -m, -param_file - Path to the parameter file\n    -p, -param - Key value pair of the form param=val\n    -r, -dryrun - Produces script with substituted parameters. Script is not executed.\n    -t, -optimizer_off - Turn optimizations off. The following values are supported:\n            ConstantCalculator - Calculate constants at compile time\n            SplitFilter - Split filter conditions\n            PushUpFilter - Filter as early as possible\n            MergeFilter - Merge filter conditions\n            PushDownForeachFlatten - Join or explode as late as possible\n            LimitOptimizer - Limit as early as possible\n            ColumnMapKeyPrune - Remove unused data\n            AddForEach - Add ForEach to remove unneeded columns\n            MergeForEach - Merge adjacent ForEach\n            GroupByConstParallelSetter - Force parallel 1 for \"group all\" statement\n            PartitionFilterOptimizer - Pushdown partition filter conditions to loader implementing LoadMetaData\n            PredicatePushdownOptimizer - Pushdown filter predicates to loader implementing LoadPredicatePushDown\n            All - Disable all optimizations\n        All optimizations listed here are enabled by default. Optimization values are case insensitive.\n    -v, -verbose - Print all error messages to screen\n    -w, -warning - Turn warning logging on; also turns warning aggregation off\n    -x, -exectype - Set execution mode: local|mapreduce|tez, default is mapreduce.\n    -F, -stop_on_failure - Aborts execution on the first failed job; default is off\n    -M, -no_multiquery - Turn multiquery optimization off; default is on\n    -N, -no_fetch - Turn fetch optimization off; default is on\n    -P, -propertyFile - Path to property file\n    -printCmdDebug - Overrides anything else and prints the actual command used to run Pig, including\n                     any environment variables that are set by the pig command.\n</pre><h4>\n          <a href=\"http://192.168.80.128:8080/views/PIG/1.0.0/Pig/#\">\n            \n            Logs \n          </a></h4>","tags":["tutorial-100","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-26 15:04:12.0","id":24610,"title":"Query not Showing in the Nifi Twitter Demo","body":"<p>I've successfully setup the Nifi Twitter demo before but recently started from scratch using the HDP 2.4 sandbox. Twitter data is coming in and persisting to HDFS. I've setup the Banana dashboard but I never see any data. What I'm noticing is when I navigate to http://sandbox.hortonworks.com:8983/solr/#/tweets_shard1_replica1/query there is no query displayed. </p><p><img src=\"/storage/attachments/3011-tweetcollection.png\"></p><p>Any suggestions on how to get the query to install? I've ran all the steps without any errors and have restarted HDP Search. Navigating to http://sandbox.hortonworks.com:8983/solr/tweets_shard1_replica1/select?q=*:* gives me this:</p><p><img src=\"/storage/attachments/3012-2016-03-26-10-05-34.png\"></p>","tags":["twitter","SOLR","Nifi","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-26 01:12:54.0","id":24566,"title":"Compare hive metadata tables","body":"<p>I have control table and a working table.  The working table schema changes (add,remove,alter) columns need to be applied to control table. I am using orc.  Any quick and easy way to do this nightly?</p>","tags":["metadata","Hive","schema"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-26 16:17:16.0","id":24595,"title":"How to shut down/kill all queries in Hive","body":"<p>Hi all,</p><p>I have been using Hive on Sandbox for the past few days. It was working fine up until yesterday when I noticed that my queries were taking an unusually long time to run or, more annoyingly, not running at all.</p><p>On further investigation, I checked the 'History' tab and noticed that there are a large number of queries which are still running.</p><p></p><p><img src=\"/storage/attachments/3008-bmczj.png\"></p><p>I have been trying to terminate/kill the sessions without success (It will say \"stopping\" but never turns to killed). I have also tried rebooting and redeploying my VM.</p><p>Does anyone know how I can stop all running processes in Hive?</p><p>Thanks in advance.</p>","tags":["process","Hive","stop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-27 04:43:19.0","id":24632,"title":"Schema for Array of Array in Avro","body":"<p>Greetings!</p><p>I\n was trying to write some sample Avro schemas, and was unable to create \none for an array of array. </p><div><div>\n\t\tSample json -<div><pre>{\n  \"testarray\":[[1,2,3][4,5,6] ... ]\n}\t\t</pre>\t\t\t\t\t\t<div>\n\t\t\t\tMy best try to create the schema - \n<pre>{\n  \"name\": \"testarray\",\n  \"type\": {\n    \"type\":\"array\",\n    \"items\": {\n      \"type\": {\n        \"type\":\"array\",\n        \"items\": {\n          \"name\": \"temp\",\n          \"type\":\"long\"\n        }\n      }\n    }\n  }\n}</pre><p>\t\t\t\tBut the schema parser throws this error -</p><p>\t\t\t\t---</p><p>\t\t\t\tException in thread \"main\" org.apache.avro.SchemaParseException: No type: {\"name\":\"testarray\",\"type\":{\"type\":\"array\",\"items\":{\"type\":{\"type\":\"array\",\"items\":{\"name\":\"temp\",\"type\":\"long\"}}}}}</p><p>\t\t\t\t  at org.apache.avro.Schema.getRequiredText(Schema.java:1372)</p><p>\t\t\t\t  at org.apache.avro.Schema.parse(Schema.java:1231)</p><p>\t\t\t\t  at org.apache.avro.Schema$Parser.parse(Schema.java:1031)</p><p>\t\t\t\t  at org.apache.avro.Schema$Parser.parse(Schema.java:996)</p><p>\t\t\t\t  at com.flipkart.de.App.main(App.java:54)</p><p>\t\t\t\t  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>\t\t\t\t  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</p><p>\t\t\t\t  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>\t\t\t\t  at java.lang.reflect.Method.invoke(Method.java:606)</p><p>\t\t\t\t  at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)</p>\t\t\t\tProcess finished with exit code 1\n\t\t\t\t<div>---</div>\n<div>Is there something wrong with the schema I've written, or is this not supported in Avro? </div>\n<div>Any help would be much appreciated. \n\t\t\t\t</div>\n</div></div></div></div>","tags":["avro","array","schema"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-27 13:46:13.0","id":24648,"title":"Cluster Installation","body":"<p>Hi</p><p>I  am beginner in hadoop , I  installed two   HortonWorks sandbox in 2 differents servers  in default mode  ( as-is standalone mode). Every thing seems working.\nMy next  step is to join  these  two servers under ( a cluster)  onde NameNode. with two data Node. </p><p>Can I do that just with ambari interface  (  the admin accompt is  working in both servers) ? </p><p>which service Can I keep running in two servers ? </p><p>Please  any tutorial ou documentation  to help me ?</p><p>Thanks in advance.</p>","tags":["cluster","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-27 18:38:51.0","id":24642,"title":"hadoop run coin - linear programming solver","body":"<p>hi, is it possible to run coin in Hadoop, has anyone tried it ? any suggestions ?</p><p>Thanks</p>","tags":["hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-29 06:02:42.0","id":24853,"title":"Client not found in kerberos database error","body":"<p>Hello, </p><p>All services are failing post enabling kerberos with error - \"client not found in kerberos database\" </p><p>Kinit yields the same error while using svchdfs account through keytab. kinit to svchdfs works fine if logged in through password. Same error post regenerating keytabs. </p><p>Appreciate any pointers. </p><p>1) HDP 2.3.4.0, Ambari 2.2.0.</p><p>2) Pre-created service account are used.</p><p>3) AD as Kerberos.</p><p>4) AD Structure </p><p>OU ---level1---&gt; HADOOP</p><p>      ---level1---&gt; cluster1 - serviceprincipals</p><p>      ---level1---&gt; PROD</p><p>     --------level2--------&gt; cluster2 serviceprincipals</p><p>cluster1 is working fine, cluster2 fails. </p><p>Regards</p><p>PranayVyas</p>","tags":["active-directory","security","Ambari"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-29 10:47:03.0","id":24914,"title":"Are there any products offering differential privacy in the Hortonworks stack?","body":"<p>I'm researching privacy products for a Big Data install.  I've found many academic references to differential privacy (ε-differential, etc.) but I can't find a product in the Hortonworks stack that offers this.  Is anyone aware of such a product, or if one is in the pipeline?</p><p>I'm aware of Privitar which deals with k-anonymity which would solve one of my issues but not the other.</p><p>Thanks in advance.</p>","tags":["development","analytics","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-29 12:52:25.0","id":24931,"title":"Exception while while writing to S3n in Hadoop Job","body":"<p>Hello,</p><p>Reducers are failing in Hadoop job while writing o/p to S3n.</p><p>Each reducer is producing ~1 GB of data to write to S3.</p><p>Error: java.io.IOException: Exception occured in one of the callables at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3MultiPartFsOutputStream.handleMultipartExceptions(NativeS3FileSystem.java:709) at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3MultiPartFsOutputStream.multipartWrite(NativeS3FileSystem.java:677) at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3MultiPartFsOutputStream.write(NativeS3FileSystem.java:623) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58) at java.io.DataOutputStream.write(DataOutputStream.java:107) at org.apache.avro.file.DataFileWriter$BufferedFileOutputStream$PositionFilter.write(DataFileWriter.java:446) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.write(BufferedOutputStream.java:121) at org.apache.avro.io.BufferedBinaryEncoder$OutputStreamSink.innerWrite(BufferedBinaryEncoder.java:216) at org.apache.avro.io.BufferedBinaryEncoder.writeFixed(BufferedBinaryEncoder.java:150) at org.apache.avro.file.DataFileStream$DataBlock.writeBlockTo(DataFileStream.java:366) at org.apache.avro.file.DataFileWriter.writeBlock(DataFileWriter.java:383) at org.apache.avro.file.DataFileWriter.sync(DataFileWriter.java:401) at org.apache.avro.file.DataFileWriter.flush(DataFileWriter.java:410) at org.apache.avro.file.DataFileWriter.close(DataFileWriter.java:433) at org.apache.avro.mapreduce.AvroKeyRecordWriter.close(AvroKeyRecordWriter.java:83) at org.apache.avro.mapreduce.AvroMultipleOutputs.close(AvroMultipleOutputs.java:595) at com.audiencescience.adprofilebuilder.mapreduce.AdProfileBuilderReducer.cleanup(AdProfileBuilderReducer.java:156) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:165) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:160)</p><p>Thanks</p><p>Shubham</p>","tags":["hadoop","s3"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-29 13:13:16.0","id":24953,"title":"Solution for \"Hive Runtime Error while processing row\" (only on MR)","body":"<p>We have several queries that fail on MR but succeed on Tez.</p><p>When they fail, the logs are full of errors like the ones below. They usually point to specific rows. However, if I reduce the scope of the query, but include the \"bad\" rows, the queries usually succeed without errors. So it clearly isn't specific to those rows. </p><p>I'm guessing there is some kind of overflow happening internally.</p><p>I have submitted several instances of this in support tickets, and the feedback is always \"please upgrade or just use Tez\", but that really isn't a solution, and we just upgraded recently.</p><p>I'm looking for guidance on ways that we might tune our Hive or MR settings to work around this.</p><p>Thanks.</p><pre>2016-03-29 08:30:03,751 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {&lt;row data&gt;}\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:159)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)\n\t... 9 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1450)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1346)\n\tat java.io.DataOutputStream.writeInt(DataOutputStream.java:197)\n\tat org.apache.hadoop.io.BytesWritable.write(BytesWritable.java:186)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1146)\n\tat org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:607)\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:531)\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:380)\n\t... 15 more\n</pre>","tags":["MapReduce","Tez","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-29 13:20:04.0","id":24956,"title":"user and groups in hue","body":"<p>Hi:</p><p>i can access with any user to the window explorer in hue to see the files why??</p><pre>No se puede acceder: /user/hdfs.\nSecurityException: Failed to obtain user group information: org.apache.hadoop.security.authorize.AuthorizationException: User: hue is not allowed to impersonate hdfs (error 403)\n</pre><p>hdfs is in hadoop group on hue </p><pre>[root@a01hop01 hue]# id hue\nuid=1015(hue) gid=1015(hue) groups=1015(hue)\n</pre><p>Thanks</p>","tags":["hue","HDFS"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-29 12:44:44.0","id":24935,"title":"Spark SQL as a Federated DB in Production?","body":"<p>There are some great articles and threads here on HCC about using Spark to query data from other JDBC sources and mash them up with anything else you can get into an RDD. Has anyone seen this pattern (Spark as a Federated DB including JDBC sources) actually used in Production (with JDBC thrift server)? What is the right configuration within a secure, multi-tenant Hadoop cluster?</p>","tags":["Spark","spark-sql","spark-thriftserver"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-29 13:50:46.0","id":24964,"title":"Falcon for data synch with offsite Hadoop cluster","body":"<p>What are the steps for setting up Falcon to act as a data synchronization pipeline to an offsite Hadoop cluster?</p>","tags":["architecture","Atlas","Oozie","operations"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-29 16:30:45.0","id":25022,"title":"Does Ranger provide TDE for hbase or solr?","body":"<p>Does Ranger provide TDE for hbase or solr as it does for HDFS?</p>","tags":["Hbase","Ranger","SOLR"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-29 14:04:35.0","id":24979,"title":"Virtualized Hadoop with Isilon","body":"<p>What are advantages and disadvantages/pitfalls of using Isilon with a virtualized Hadoop cluster?</p>","tags":["architecture","hadoop","virtualization"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-29 14:28:49.0","id":24995,"title":"HDFS over object storage for Hadoop on demand?","body":"<p>Azure HDInsight service provides that capability to create a Hadoop cluster that can be torn down and brought back up without losing any data (including meta store). Can this setup be achieved with Open Stack Swift and Cloud Break? If so, what are the steps and considerations to implement this architecture?</p>","tags":["Cloudbreak","openstack","architecture","cloud"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-29 16:39:53.0","id":25027,"title":"How to Migrate all the HIVE data from one secure cluster to another secure cluster running on same REALM","body":"<p>One of our clients is looking  a easy way to transfer all the HIVE data from one cluster to another running on same REALM.</p><p>Both the clusters are secured and running their own KDC however have same REALM setup.</p><p>Thanks in advance</p><p>Mayank</p>","tags":["distcp","kerberos","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-29 17:20:40.0","id":25037,"title":"Namenode move via Ambar \"Move master Wizard\" trashes HA on  cluster","body":"<p>Versions1: </p><p>   Amabri: 2.2.1.1 </p><p>   HDP: 2.4.0.0-169</p><p>Versions2</p><p>   Ambari: 2.1.2</p><p>   HDP: 2.3.4.0-3485</p><p>Ambari has mismatch in terms of Namenode (nn1) and SecondaryNamenode (nn2), and when I try to move  nn1 or nn2</p><p>to a different host ambari asks me to run scripts on the wrong host, resulting in a cluster with no namenode.</p><p>Existing state :</p><p>        nn1=host1 </p><p>        nn2=host2</p><p>New state desired:</p><p>          nn1=host1</p><p>          nn2=host3</p><p>Ambari Wizard thinks nn1 is actually host2, and asks me to run the hdfs commands there.</p><p>its claimed <a href=\"https://issues.apache.org/jira/browse/AMBARI-12688\">https://issues.apache.org/jira/browse/AMBARI-12688</a> here the bug is fixed</p><p>but its still there per <a href=\"https://issues.apache.org/jira/browse/AMBARI-15126\">https://issues.apache.org/jira/browse/AMBARI-15126</a></p><p>Both the versions have the same problem, what version should I use to do the namenode move by Ambari GUI ?</p><p>or should i use the REST Api's to do the move ?</p><p>I'm using all the standard ports and out of the box Ambari and Hdp with no tweaks.</p>","tags":["Ambari","centos"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-30 03:40:00.0","id":25101,"title":"Is there a way to connect to  HBase using C#?","body":"","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-29 18:53:15.0","id":25065,"title":"HCatalog for HDP Sandbox","body":"<p>Hello,</p><p>I am looking Hcatalog software for HDP 2.4 Sandbox\ninstallation (on Windows NT OS). Can anyone please chime the software details? When\nI look for the details, It's giving for Linux/Unix related not with Windows. </p><p>Please advise. </p><p>Thank you,</p>","tags":["hcatalog"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-30 03:31:15.0","id":25097,"title":"Falcon/Storm/Flume not Installed Properly as i am not able to Start/Stop Services .","body":"<p><img src=\"/storage/attachments/3079-install-error.jpg\"></p><p>How can i delete these services and install again? </p>","tags":["Kafka","Storm","Falcon","Knox"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-30 04:55:38.0","id":25113,"title":"Node Labeling benefits","body":"<p>What applications/services benefits from node labeling?  Node Labeling is not allowed (unless node label default set on queue) when mapreduce job is submitted.  This impacts applications which run mapreduce (ie pig/hive).</p>","tags":["yarn-node-labels","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-30 05:05:56.0","id":25114,"title":"Cannot install Maven","body":"<p>Hi All,</p><p>I am unable to install the maven at all. The first command</p><p><strong>curl -o /etc/yum.repos.d/epel-apache-maven.repo https://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo</strong> fails. </p><p>It says \"Failed to connect to 2610:28:3090:3001:5054:ff:fea7:9474: Network is unreachable\"</p><p>Am I missing something here?</p>","tags":["tutorial-170","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-30 06:35:56.0","id":25128,"title":"balancer is slow...","body":"<p>Hello</p><p>We added 5 new DataNodes to our HDP cluster.\nWe run the balancer manually so the newly added DNs will be balanced with the rest (manually because of the Bug in Ambari which is described here <a target=\"_blank\" href=\"https://community.hortonworks.com/articles/4595/balancer-not-working-in-hdfs-ha.html\">https://community.hortonworks.com/articles/4595/balancer-not-working-in-hdfs-ha.html</a>) with the default threshold of 10, and changed the default dfs.datanode.balance.bandwidthPerSec to 100000000.\nHowever, it seems that the progress of balancing data from the old DNs to the new ones is roughly 20-30gb a day.\nIt's been running 3 days straight and the newly added disks have only 63gb~ of data.\n</p><p>Is there a way to increase the balancer's speed ??\nBy the way - All of the jobs were stopped so the cluster is completely idle.</p><p>Thanks in advance.</p><p>Adi</p>","tags":["hadoop","rebalance","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-30 06:30:30.0","id":25126,"title":"Do you now or will you support Oracle Solaris 11.3 in the future?","body":"<p>We are especially interested in using Ambari on Oracle Solaris 11.3 x86/x64.</p>","tags":["installation","oracle","os"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-30 20:45:31.0","id":25240,"title":"HDP support of the Chinese language","body":"<p>I want to land files with Chinese characters in HDFS.  I assume this should not problem on hdfs. Is there any issues I should be aware of during processing these files or reporting on files?</p>","tags":["language","HDFS"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-30 16:23:04.0","id":25207,"title":"I have avro format hive table with twitter data from flume..  I tried count(*)   or any where condition on this table , it throws error. pls help.","body":"","tags":["Hive","avrostorage"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-30 19:28:49.0","id":25232,"title":"Adding User defined attributes to NIFI flowfiles.","body":"<p>After adding user defined attributes with UpdateAttribute processor, is it possible to store these attributes with the files in HDFS or local file system?</p><p>TIA</p>","tags":["HDFS","Nifi","filesystem","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-31 10:13:22.0","id":25336,"title":"Cannot delete an entity in Falcon","body":"<p>I am getting following error on trying to delete the <strong><em>last </em></strong>entity in Falcon data pipeline</p><p>\"Failed to cleanup entity path for (process) cleanseEmailProcess on cluster primaryCluster\"</p><p>PS- Followed instructions verbatim on:\nhttp://hortonworks.com/hadoop-tutorial/defining-processing-data-end-end-data-pipeline-apache-falcon</p><p>Understand cannot edit an entity either?\nhttps://community.hortonworks.com/questions/14631/how-to-update-falcon-cluster-entities-that-have-fe.html</p>","tags":["delete","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-03-31 07:11:27.0","id":25319,"title":"All new: HDP 2.3 with Centos 6 upgrade to HDP2.4 with Centos7","body":"<p>Hello everyone,</p><p>we'd like to upgrade a lot on our Cluster. First we'd like to upgrade our Centos Version because of some security issues. As far as we know, HDP 2.3 does not work with Centos7.</p><p>We also want to upgrade HDP 2.3 to 2.4. The guide seems to be very detailed and helpful.</p><p>My question is: has anyone experience with this or faced remarkable problems on trying? </p><p>Any ideas what configurations need to change to enable the HDP working on Centos7?</p><p>Thank you,</p><p>tuffelchen</p>","tags":["hdp-2.3.0","centos","hdp-2.4","upgrade"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-07 12:50:37.0","id":26384,"title":"install packet service","body":"<p>i cant install any packet from ambari:</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py\", line 124, in &lt;module&gt;\n    FlumeHandler().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py\", line 47, in install\n    import params\n  File \"/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/params.py\", line 61, in &lt;module&gt;\n    ambari_state_file = format(\"{flume_run_dir}/ambari-state.txt\")\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/format.py\", line 90, in format\n    return ConfigurationFormatter().format(format_string, args, **result)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/format.py\", line 54, in format\n    result_protected = self.vformat(format_string, args, all_params)\n  File \"/usr/lib64/python2.6/string.py\", line 549, in vformat\n    result = self._vformat(format_string, args, kwargs, used_args, 2)\n  File \"/usr/lib64/python2.6/string.py\", line 582, in _vformat\n    result.append(self.format_field(obj, format_spec))\n  File \"/usr/lib64/python2.6/string.py\", line 599, in format_field\n    return format(value, format_spec)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py\", line 81, in __getattr__\n    raise Fail(\"Configuration parameter '\" + self.name + \"' was not found in configurations dictionary!\")\nresource_management.core.exceptions.Fail: Configuration parameter 'flume-env' was not found in configurations dictionary!</pre><p>what is happening???</p>","tags":["version","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-11 17:33:51.0","id":26994,"title":"Reducers failling due to Error: java.io.IOException: File already exists:s3n://basket1/part-r-00016.deflate","body":"<p>Hello,</p><p>I have set job output path to \"s3://basket1\". I am writing to already existing directory/basket.</p><p>Few reducers are failing due to following error even though deflate files do not exist before starting job.</p><p>Error:</p><p>App &gt; 16/04/11 17:25:47 INFO mapreduce.Job: Task Id : attempt_1460385692451_0006_r_000016_2, Status : FAILED\nApp &gt; Error: java.io.IOException: File already exists:s3n://basket1/part-r-00016.deflate\nApp &gt; at org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(NativeS3FileSystem.java:813)\nApp &gt; at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:914)\nApp &gt; at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:895)\nApp &gt; at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:792)\nApp &gt; at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)\nApp &gt; at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.&lt;init&gt;(ReduceTask.java:540)\nApp &gt; at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:614)\nApp &gt; at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\nApp &gt; at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:165)\nApp &gt; at java.security.AccessController.doPrivileged(Native Method)\nApp &gt; at javax.security.auth.Subject.doAs(Subject.java:415)\nApp &gt; at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)\nApp &gt; at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:160)</p><p>-Shubham</p>","tags":["MapReduce","s3"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-12 07:44:52.0","id":27098,"title":"How to install KAFKA and Spark in CDH 4.7","body":"<p>Want to install kafka in cloudera 4.7 version </p>","tags":["cloudera","installation","Kafka"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-12 09:23:55.0","id":27115,"title":"how to start Nifi and disable any existing templates on canvas?","body":"<p>I'd like to start Nifi and ignore any previously-running templates that were on canvas prior to nifi shutdown.</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-12 13:56:37.0","id":27127,"title":"Does NiFi have a processor to expand a tar or zip file?","body":"<p>Does NiFi have a processor to expand a tar or zip file and allow access/process to files within?  </p>","tags":["nifi-processor","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-12 08:10:23.0","id":27104,"title":"Does Cloudbreak support launching instances on vSphere","body":"<p>Does Cloudbreak support launching instances on vSphere?</p><p>I get the feeling it might be through <a href=\"http://sequenceiq.com/cloudbreak-docs/latest/spi/\">SPI</a> but I don't see any working examples or references where anyone got this up and running.</p>","tags":["vmware","Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-11 18:33:18.0","id":27015,"title":"upgrading from HDP 2.2 to 2.4","body":"<p>Upgrading from HDP 2.2 to 2.4</p><p>I am trying to upgrade a cluster from 2.2. to 2.4 and was just checking the documentation and see  a lot of manual intervention for 2.2 to 2.3, do those also apply for 2.2 to 2.4 or its all taken care via ambari</p><p><strong>#1 FYI 2.2 to 2.3 </strong></p><p>http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.0.0/bk_upgrading_Ambari/content/_Upgrade_HDFS_mamiu.html</p><p><strong>Other  doc</strong></p><p>http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.1.1/bk_upgrading_Ambari/content/_upgrading_HDP_install_target_version.html</p><p>\nDo i still have to follow the steps in #1 (currently i am on HDP 2.2 and Ambari 2.0)</p>","tags":["HDFS","upgrade","hdfs-maintenance"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-11 16:17:49.0","id":26998,"title":"error install Hbase","body":"<p>Hi:</p><p>I am trying install Hbase and i am receiving this error:</p><pre>[root@lnxbig06 current]# tail -200f /var/lib/ambari-agent/data/errors-6126.txt\nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/hook.py\", line 37, in &lt;module&gt;\n    AfterInstallHook().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/hook.py\", line 31, in hook\n    setup_hdp_symlinks()\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/after-INSTALL/scripts/shared_initialization.py\", line 44, in setup_hdp_symlinks\n    hdp_select.select_all(version)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/hdp_select.py\", line 122, in select_all\n    Execute(command, only_if = only_if_command)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 158, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 121, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 238, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.4.0.0-169 | tail -1`' returned 1. Traceback (most recent call last):\n  File \"/usr/bin/hdp-select\", line 379, in &lt;module&gt;\n    printVersions()\n  File \"/usr/bin/hdp-select\", line 236, in printVersions\n    result[tuple(map(int, versionRegex.split(f)))] = f\nValueError: invalid literal for int() with base 10: 'lib'\nERROR: set command takes 2 parameters, instead of 1\n\n\nusage: hdp-select [-h] [&lt;command&gt;] [&lt;package&gt;] [&lt;version&gt;]\n\n\nSet the selected version of HDP.\n\n\npositional arguments:\n  &lt;command&gt;   One of set, status, versions, or packages\n  &lt;package&gt;   the package name to set\n  &lt;version&gt;   the HDP version to set\n\n\noptional arguments:\n  -h, --help  show this help message and exit\n  -r, --rpm-mode  if true checks if there is symlink exists and creates the symlink if it doesn't\n\n\nCommands:\n  set      : set the package to a specified version\n  status   : show the version of the package\n  versions : show the currently installed versions\n  packages : show the individual package names\n\n\n</pre>","tags":["Hbase"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-12 19:11:28.0","id":27221,"title":"What is the best amount of memory for YARN AppMaster?","body":"<p>I've seen 12GB of RAM and sometimes 9GB for the allocated memory of YARN AppMaster. AppMaster is just managing the lifecycle of the containers and don't think it needs more than 1GB of RAM. </p>","tags":["performance","yarn_containe","appmaster","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-12 23:30:55.0","id":27248,"title":"Hive View on Amabri","body":"<p>Ambari 2.2.0.0 vs Ambari 2.1.2.1</p><p>Hive View in Ambari 2.1.2.1 does not have 'Table Upload' but Ambari 2.2.0.0 does have 'Table Upload'.</p><p>Am I doing something wrong in installation? Can anybody verify this by referring HDP document/link?</p>","tags":["ambari-views","Ambari","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-13 04:28:40.0","id":27269,"title":"i am installing hortonworks single node cluster on google cloud, how can i  generate ssh-keygen?","body":"","tags":["google","cloud","permission-denied","ssh"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-13 11:51:27.0","id":27329,"title":"Factors determining how a table is stored in HDFS","body":"<p>I wrote a query to copy a very large table (ORC compressed -&gt; ORC compressed).</p><p>The query took a very long time to run, and I noticed that only 238 reducers were spun up, which was the bottleneck: the average reduce time was almost 20 minutes, where the average map time was only 8 minutes (874 mappers).</p><p>In the end, the new copy of the table is stored across 238 files in HDFS, which explains the 238 reducers. </p><p>But I'm trying to understand why 238.</p><p>I see that the HFDS files backing the file follow an interesting pattern. Every other one is about 530MB, and every other one is about 1.6GB.</p><p>I have my \"max reducers\" setting in Hive set to 416, which is 13 data nodes x 16 disks per node x 2.</p><p>So, I would expect to see 416 reducers.</p><p>However, I also see a \"data per reducer\" setting in Hive. It looks like someone recently increased this to 1GB.</p><p>The table I copied is about 238GB, so it makes sense to me that, based on this setting, I got 238 reducers.</p><p>So my question is: what is the relationship between: </p><p>hdfs file chunk size (ours is set to 64 mb)</p><p>max reducers (ours is 416)</p><p>and \"data per reducer\" ?</p><p>It seems like the \"data per reducer\" trumps everything and the hdfs file chunk size is not taken into account at all.</p><p>Should \"data per reducer\" be the same as hdfs file chunk size or a small multiple of it?</p><p>Thanks,</p><p>Zack</p>","tags":["HDFS","optimization","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-13 16:48:16.0","id":27397,"title":"hbase  client java not able to connect to   hbase","body":"<p>Hi I am trying to connect to hbase in a remote machine from my java program.  i get unknown host node3.bigdatau error  (the hostname where zookeeper is installed ), so i add  addresse ip of all  node of the cluster to file /etc/hosts (client machine windows) ,after that i'am getting  connection refused error.  </p><p> the line that i added to /etc/host </p><pre>195.154.43.111 node2.bigdatau node2  \n62.210.13.113  node3.bigdatau node3 \n195.154.55.93  node4.bigdatau node4 \n</pre><p>hbase-site.xml </p><pre> &lt;property&gt;\n      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;\n      &lt;value&gt;node4.bigdatau,node3.bigdatau&lt;/value&gt;\n    &lt;/property&gt;\n &lt;property&gt;\n      &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;\n      &lt;value&gt;/hbase-unsecure&lt;/value&gt;\n    &lt;/property&gt;\n\n\n\n</pre><pre> try {\n        \t Configuration conf = HBaseConfiguration.create();\n        \t \n        \t\n             conf.set(\"hbase.zookeeper.quorum\", \"62.210.13.113\");\n             \n             conf.set(\"hbase.client.retries.number\", Integer.toString(1));\n             conf.set(\"zookeeper.session.timeout\", Integer.toString(60000));\n             conf.set(\"zookeeper.recovery.retry\", Integer.toString(1));\n             conf.set(\"zookeeper.znode.parent\", \"/hbase-unsecure\");\n              conf.set(\"hbase.zookeeper.property.clientport\", \"2181\");\n             org.apache.hadoop.hbase.client.Connection conn=ConnectionFactory.createConnection(conf);\n             \n             HBaseAdmin admin = new HBaseAdmin(conf);\n             admin.disableTable(\"test\");\n             admin.deleteTable(\"test\");\n             System.out.println(\"delete table  ok.\");\n             \n             \n             \n             \n             \n          \n             System.out.println( \"work\");\n             \n          } catch (Exception exp) {\n        \t  \n                  System.out.println( \"\"+exp.getMessage());\n          }\n</pre>","tags":["zookeeper","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-13 19:13:12.0","id":27437,"title":"unable to upgrade via ambari","body":"<p><img src=\"/storage/attachments/3430-screen-shot-2016-04-14-at-124232-am.png\"></p><p>I upgraded ambari from 2.0 to 2.2.1 and i am facing issues while adding  a new repo to upgrade the hdp cluster</p><p>It throws this error on the UI</p><p><a target=\"_blank\" href=\"https://community.hortonworks.com/storage/attachments/3430-screen-shot-2016-04-14-at-124232-am.png\"><img src=\"https://community.hortonworks.com/storage/attachments/3430-screen-shot-2016-04-14-at-124232-am.png\">screen-shot-2016-04-14-at-124232-am.png(125.3 kB)</a></p><p>the logs on the server show this error</p><p>14 Apr 2016 00:18:56,271 ERROR [qtp-ambari-client-466] AbstractResourceProvider:159 - Couldn't retrieve Current stack entry from Map.\n14 Apr 2016 00:18:56,280 ERROR [qtp-ambari-client-466] ReadHandler:91 - Caught a runtime exception executing a query\njava.lang.NullPointerException\n   at org.apache.ambari.server.controller.internal.CompatibleRepositoryVersionResourceProvider.getResources(CompatibleRepositoryVersionResourceProvider.java:154)\n   at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:945)\n   at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:132)\n   at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:508)\n   at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:463)\n   at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:482)\n   at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:436)\n   at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:216)\n   at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:68)\n   at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)\n   at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:106)\n   at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:75)\n   at org.apache.ambari.server.api.services.StacksService.getStacks(StacksService.java:47)\n   at sun.reflect.GeneratedMethodAccessor139.invoke(Unknown Source)\n   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   at java.lang.reflect.Method.invoke(Method.java:497)</p><p>Any pointers ?</p><p>Jabir</p>","tags":["ambari-2.2.0","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-15 15:15:05.0","id":27789,"title":"Phoenix function in the Where clause","body":"<p>We have a phoenix table with \"key\" as time in millisec. I am trying to get count of number of records for the last 10 minutes by using Phoenix functions in the where clause, but not going anywhere. It just returns the zero count.</p><p> Anyone tried functions on the where clause something like below ? </p><p>select count(*) from CDC where CDC.\"key\" &gt; TO_CHAR((TO_NUMBER(NOW())-600000);</p><p>-Datta</p>","tags":["phoenix4.4","Phoenix","query"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-15 19:01:39.0","id":27830,"title":"HBase as HL7 database","body":"<p>I am looking into capturing HL7 data and feeding it into Spark for machine learning and HBase for persistent storage. The record oriented property of HBase makes it an ideal solution for storing and query across large HL7 data. HBase uses the concept of Regions for scalability purposes. Regions are a subset of the table’s data and they are essentially a contiguous, sorted range of rows that are stored together.</p><p>I'd like to look at some samples on how to design HBase shema for HL7 messages. </p>","tags":["HDFS","Hbase","hl7"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-18 03:40:44.0","id":27966,"title":"KafkaSpout fails with zookeeper socket issues in kerberised HDP 2.4","body":"<p>I have a kerberised HDP 2.4 sandbox VM.  I am using JAVA API's to write and read from Kafka.</p><p>I can write to kerberised Kafka using java API.</p><p>I have a topology running which reads the kafka topic and does some processing.</p><p>I have followed the 2.4 documentation in creating the spout. I have set the security protocol in spout config. Also added ACL for storm user on the topic. The zookeeper.connect has also been set in kafka configuration.</p><p>The topology is non-local and is deployed ok. The topology Activates the KafkaSpout OK. If fails in the next step of getting offsets from the partitions in zookeeper.</p><p>It throws socket EOFException when kafka tries to read from zookeeper.</p><p><u>Code Snippet:</u></p><p>...</p><p>BrokerHosts zkHosts = new ZkHosts(\"sandbox.hortonworks.com:2181\");\nSpoutConfig spoutCfg = new SpoutConfig(zkHosts, \"qa\", \"/qa\", \"qa\");\n spoutCfg.scheme = new SchemeAsMultiScheme(new StringScheme());\n \n if (isSecured) {\n spoutCfg.securityProtocol = \"PLAINTEXTSASL\";\n}</p><p>KafkaSpout kspout = new KafkaSpout(spoutCfg);</p><p>TopologyBuilder builder = new TopologyBuilder();</p><p>builder.setSpout(\"ingest\", kspout, 1);</p><p>tormSubmitter.submitTopology(topologyName, cfg, builder.createTopology());</p><p>...</p><p>Error Stack:</p><p>2016-04-17 12:56:04.406 o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED</p><p>2016-04-17 12:56:04.412 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=sandbox.hortonworks.com:6667}}</p><p>2016-04-17 12:56:04.413 s.k.KafkaUtils [INFO] Task [1/1] assigned [Partition{host=sandbox.hortonworks.com:6667, partition=0}]</p><p>2016-04-17 12:56:04.413 s.k.ZkCoordinator [INFO] Task [1/1] Deleted partition managers: []</p><p>2016-04-17 12:56:04.413 s.k.ZkCoordinator [INFO] Task [1/1] New partition managers: [Partition{host=sandbox.hortonworks.com:6667, partition=0}]</p><p>2016-04-17 12:56:04.576 s.k.PartitionManager [INFO] Read partition information from: /qa/qa/partition_0  --&gt; null</p><p>2016-04-17 12:56:04.644 k.c.SimpleConsumer [INFO] Reconnect due to socket error: java.io.EOFException</p><p>2016-04-17 12:56:04.655 b.s.util [ERROR] Async loop died!</p><p>java.lang.RuntimeException: java.io.EOFException\n  at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103)  ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>      at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:138) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]\n  </p><p>    at backtype.storm.daemon.executor$fn__7177$fn__7192$fn__7221.invoke(executor.clj:596) ~[storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at backtype.storm.util$async_loop$fn__544.invoke(util.clj:475) [storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]</p><p>    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_95]</p><p>Caused by: java.io.EOFException</p><p>    at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:83) ~[kafka-clients-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:140) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.network.BlockingChannel.receive(BlockingChannel.scala:131) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:115) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:99) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:165) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:86) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:74) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:64) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.PartitionManager.&lt;init&gt;(PartitionManager.java:89) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>  ... 6 more</p><p>2016-04-17 12:56:04.664 b.s.d.executor [ERROR]\njava.lang.RuntimeException: java.io.EOFException</p><p>    at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:138) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at backtype.storm.daemon.executor$fn__7177$fn__7192$fn__7221.invoke(executor.clj:596) ~[storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at backtype.storm.util$async_loop$fn__544.invoke(util.clj:475) [storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>   at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]\n  at java.lang.Thread.run(Thread.java:745) [?:1.7.0_95]</p><p>Caused by: java.io.EOFException</p><p>    at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:83) ~[kafka-clients-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:140) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.network.BlockingChannel.receive(BlockingChannel.scala:131) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:115) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:99) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:165) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:86) ~[kafka_2.10-0.9.0.2.4.0.0-169.jar:?]</p><p>    at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:74) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:64) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at storm.kafka.PartitionManager.&lt;init&gt;(PartitionManager.java:89) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]\n  </p><p>    at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[storm-kafka-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>... 6 more</p><p>2016-04-17 12:56:04.695 b.s.util [ERROR] Halting process: (\"Worker died\")</p><p>java.lang.RuntimeException: (\"Worker died\")</p><p>    at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:332) [storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:?]\n  at backtype.storm.daemon.worker$fn__7818$fn__7819.invoke(worker.clj:636) [storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at backtype.storm.daemon.executor$mk_executor_data$fn__7078$fn__7079.invoke(executor.clj:256) [storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at backtype.storm.util$async_loop$fn__544.invoke(util.clj:485) [storm-core-0.10.0.2.4.0.0-169.jar:0.10.0.2.4.0.0-169]</p><p>    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]\n  at java.lang.Thread.run(Thread.java:745) [?:1.7.0_95]</p>","tags":["Storm","hdp-2.4","sasl","Kafka","kerberos"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-18 07:11:51.0","id":27977,"title":"Error while running insert statement on encryption + Kerberos Enabled cluster","body":"<p>Hi,</p><p>I have kerberos+Encryption(TDE) enabled cluster. I am getting following error while executing following insert statement.</p><p>I am using beeline to connect to server.</p><p>insert into test_s(100);</p><p>Error:</p><p>java.io.IOException: java.lang.reflect.UndeclaredThrowableException\nat org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:888)\nat org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension.addDelegationTokens(KeyProviderDelegationTokenExtension.java:86)\nat org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2243)\nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)\nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)\nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)\nat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:166)\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\nat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:575)\nat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:570)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:570)\nat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:561)\nat org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:431)\nat org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)\nat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\nat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\nat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1703)\nat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1460)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1237)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1096)\nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\nat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\nat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.reflect.UndeclaredThrowableException\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1672)\nat org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:870)\n... 40 more\nCaused by: org.apache.hadoop.security.authentication.client.AuthenticationException: Authentication failed, status: 403, message: Forbidden\nat org.apache.hadoop.security.authentication.client.AuthenticatedURL.extractToken(AuthenticatedURL.java:274)\nat org.apache.hadoop.security.authentication.client.PseudoAuthenticator.authenticate(PseudoAuthenticator.java:77)\nat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:128)\nat org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:214)\nat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:128)\nat org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:215)\nat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:285)\nat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.getDelegationToken(DelegationTokenAuthenticator.java:166)\nat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.getDelegationToken(DelegationTokenAuthenticatedURL.java:371)\nat org.apache.hadoop.crypto.key.kms.KMSClientProvider$2.run(KMSClientProvider.java:875)\nat org.apache.hadoop.crypto.key.kms.KMSClientProvider$2.run(KMSClientProvider.java:870)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n... 41 more</p><p>From Hive CLI mode same query works fine. Can someone explain what could be wrong/missing here?</p>","tags":["encryption"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-18 11:26:27.0","id":28023,"title":"The configuration to enable PERFLOG in hiveserver2 or hive?","body":"<p>Hi, what is the right way to enable PERFLOG on hiveserver2 or hive?</p><pre>2016-03-29 15:28:22,372 INFO  [HiveServer2-Background-Pool: Thread-3718518]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - &lt;/PERFLOG method=Driver.execute start=1459232878361 end=1459232902372 duration=24011 from=org.apache.hadoop.hive.ql.Driver&gt;\n2016-03-29 15:28:22,373 INFO  [HiveServer2-Background-Pool: Thread-3718518]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - &lt;/PERFLOG method=releaseLocks start=1459232902373 end=1459232902373 duration=0 from=org.apache.hadoop.hive.ql.Driver&gt;\n2016-03-29 15:28:22,373 INFO  [HiveServer2-Background-Pool: Thread-3718518]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - &lt;/PERFLOG method=Driver.run start=1459232878361 end=1459232902373 duration=24012 from=org.apache.hadoop.hive.ql.Driver&gt;</pre><p>I'm a bit confusing because it seems there are several ways..\nhttps://issues.apache.org/jira/browse/HIVE-10119?focusedCommentId=14570071&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14570071</p>","tags":["performance","hiveserver2","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-18 17:25:51.0","id":28088,"title":"How can I calculate the elapsed time within a Zeppelin Spark notebook?","body":"<p>Frequently, when running a notebook in Zeppelin, I want to know how long it took to run the whole notebook. This is important when tuning Spark, Yarn, etc, to understand whether the tuning is successful. Zeppelin prints the \"per cell\" timings, however, it does not provide full elapsed time for the notebook. How can I add code to calculate this?</p>","tags":["Spark","zeppelin-notebook","zeppelin","scala"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-19 09:45:30.0","id":28250,"title":"Driver hive-cassandra 2.1.11 over Cassandra 2.0.10","body":"<p>Hi</p><p>It is possible to use the driver huve-cassandra2.1.11 with hdp over cassandra 2.0.10?</p>","tags":["cassandra","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-19 17:53:54.0","id":28286,"title":"Apache slider and Hbase timeout setting","body":"<p>I use apache slider for launching hbase containers. Is there a setting which controls how long it takes for slider to consider region server as dead? It takes region server some time to shutdown even when HMaster marks a region server as dead. This could be due to a GC pause it is dealing with. However, slider will not launch a new container/ region server unless this container is not given up by existing region server which is hung/ already marked dead by master. In such a case, the wait time to launch a new region server instance can be arbitrarily long. How does slider monitor health of region server? Is there a way to make it sync with HMaster in deciding if region server is dead?</p>","tags":["Hbase","slider","yarn-container","YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-19 19:41:50.0","id":28350,"title":"HAWQ to HIVE data type mapping","body":"<p>I'm putting together a dataType mapping table and I'm looking for suggestions specifically around \"interval\", \"bit\", \"dates\".</p><p>1.  HAWQ -&gt; Hive, Interval</p><p>2. HAWQ -&gt; Hive, bit.  I see that boolean doesn't convert, is the only option TinyInt?</p><p>3. HAWQ -&gt; Hive, date.  Best practice String or Date.  I know Hive can read a String in Date format dynamically. </p><p>4. HAWQ -&gt; Hive, Timestamp as String or Timestamp?</p><p>5. Does sqoop cover HAWQ?  </p><table>\n <tbody><tr>\n  <td><strong>HAWQ</strong></td>\n  <td><strong>HIVE</strong></td>\n  <td><strong>Suggestion</strong></td>\n </tr>\n <tr>\n  <td>bigint</td>\n  <td>bigint</td>\n  <td>Bigint</td>\n </tr>\n <tr>\n  <td>integer</td>\n  <td>int</td>\n  <td>Int</td>\n </tr>\n <tr>\n  <td>character varying(20)</td>\n  <td>varchar(20)</td>\n  <td>Varchar(20)</td>\n </tr>\n <tr>\n  <td>timestamp without time\n  zone</td>\n  <td>timestamp</td>\n  <td>Timestamp</td>\n </tr>\n <tr>\n  <td>numeric</td>\n  <td>int</td>\n  <td>Int</td>\n </tr>\n <tr>\n  <td>timestamp without time\n  zone DEFAULT now()</td>\n  <td>timestamp</td>\n  <td>Timestamp</td>\n </tr>\n <tr>\n  <td>character varying</td>\n  <td>int</td>\n  <td>Varchar, string</td>\n </tr>\n <tr>\n  <td>text</td>\n  <td>string</td>\n  <td>String</td>\n </tr>\n <tr>\n  <td>double precision</td>\n  <td>double</td>\n  <td>Double if &lt; 9-15 precision, else Decimal(0,0)</td>\n </tr>\n <tr>\n  <td>text[]</td>\n  <td>array&lt;string&gt;</td>\n  <td>Array&lt;string&gt;</td>\n </tr>\n <tr>\n  <td>boolean</td>\n  <td>boolean</td>\n  <td>Boolean</td>\n </tr>\n <tr>\n  <td>date</td>\n  <td>date</td>\n  <td>Date</td>\n </tr>\n <tr>\n  <td>character varying[]</td>\n  <td>array&lt;string&gt;</td>\n  <td> array&lt;string&gt;\n  </td>\n </tr>\n <tr>\n  <td>timestamp with time zone</td>\n  <td>timestamp</td>\n  <td> Timestamp </td>\n </tr>\n <tr>\n  <td>interval</td>\n  <td>String</td>\n  <td>Int</td>\n </tr>\n <tr>\n  <td>bit(1)</td>\n  <td> </td>\n  <td>TinyInt</td>\n </tr>\n <tr>\n  <td>Char</td>\n  <td>Char</td>\n  <td>Char\n  : will hold whitespace but DB engine will ignore in case of “USA”== “USA  ”</td>\n </tr></tbody></table>","tags":["schema","Hive","data-model","hawq"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-20 22:56:30.0","id":28600,"title":"Issues with storm-hbase connectors! Issue with either of \"HBaseLookupBolt\" or \"HBaseBolt\" or HBase Table update!!","body":"<p>I have storm topology, which reads messages from kafka, does a lookup from hbase table, apply some business login on message and update hbase table!c</p>\nBelow are steps, how a message flow from kafka to hbase happens in my storm topolgy!!\n\n<ol><li>Read message from kafka spout (Kafka Spout)</li><li>Extract id column from the message read from\nkafka in step1 (General Bolt)</li><li>Do a lookup from hbase table for the id (\nstorm-hbase connector – HbaseLookup Bolt)</li><li>Based on the retrieved value from the Hbase in\nstep3, apply business logic, then update the same HBase table used for lookup, for\nthe id.</li></ol><p><strong>Issue</strong>\nStorm hbase lookup bolt is reading wrong value from the HBase table! </p><p><strong>\nTo explain in more detail:</strong></p><p>when incoming messages arrive at a time gap of few milli secs to secs, \"HBaseLookupBolt\" is reading old value from HBase table, even though the table is updated already using \"HBaseBolt\" !!!</p><p>Can someone please share your thoughts, on how can i ensure lookup bolt takes the right value ?</p><p>Or is it possible thru any setting, to slow down the speed at which storm processes the records or any other of your valuable thoughts.... ?</p>","tags":["hadoop-ecosystem","Storm","Hbase"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-20 18:17:39.0","id":28557,"title":"How to create Home directory for Root in user Hortonwork sandbox 2.4","body":"<p><img src=\"/storage/attachments/3568-root.jpg\"></p>","tags":["hortonwork","hdfs-permissions"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-21 05:27:37.0","id":28628,"title":"Status of Camus","body":"<p>What's the status of <a href=\"http://docs.confluent.io/1.0/camus/docs/index.html\">Camus</a>? It has been considered a new, great way to transfer and sync data from Kafka to HDFS. Do we have any working examples, and what's the prevailing opinion. Is it fully compatible with Kafka packaged in HDP? Because it's distributed together with Confluent version of Kafka. And, finally there is now Goblin, is it just a new name for Camus or a new project?</p>","tags":["HDFS","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-20 09:17:41.0","id":28431,"title":"What version of Kylin should I use with hdp 2.4 (installed with ambari 2.2.1)","body":"<p>So I have installed a cluster using ambari 2.2.1 and Kylin won't work. I have tried different versions and the best case scenario is Kylin starting with no errors, no logs are generated but the web page is just blank.</p><p>The version of HBase is 1.1.2 but on the Kylin web site they only propose binaries for HBase 0.98/0.99 or 1.1.3+.</p><p>Hence the question : what version of Kylin should I use ? Or is there a way to make Kylin 1.5.1 work with HDP 2.4 ?</p><p>Thanks for your help !</p>","tags":["hdp-2.4","kylin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-21 10:20:42.0","id":28667,"title":"Could the upcoming patch releases be known prior to the scheduled date?","body":"<p>Hello,\nWe at Teradata India Pvt Ltd are partners with Hortonworks and we have been delivering projects successfully \nthrough hortonworks services. Certain clients do request us to let them know about upcoming HDP upgrades and patches. The main reason of contacting you is to know if we could get an update about upcoming releases; not the major version releases but the minor releases, upgrades and patches. It would certainly help to serve our customers better. Hope to get a positive response from soon. </p><p>Thanks.</p>","tags":["hortonwork","hdp-2.3.0","upgrade"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-04-21 07:34:36.0","id":28642,"title":"Issue with the beeline command","body":"<p>I was testing a beeline query with a command '--outputormat=csv2' and the result of the query was not in CSV2 format. Later I have realized that I have missed an 'f' in the command 'outputformat'. When I have misspelled a command it shouldn't have allowed me to run the query. Should it?</p><p>My question is, is this a bug? If so how to report it to the Hive developers through Jira?</p>","tags":["beeline"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-21 19:06:22.0","id":28788,"title":"Node manager restart fails during upgrade / downgrade between 2.3.4 and 2.4.0","body":"<p>We are upgrading HDP from 2.3.4 to 2.4.0. by following the\ninstructions in the below link: </p><p><a href=\"https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.1.1/bk_upgrading_Ambari/content/_upgrade_ambari.html\">https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.1.1/bk_upgrading_Ambari/content/_upgrade_ambari.html</a></p><p>All the steps in the upgrade document till “4.2 Perform\nexpress upgrade” have been completed successfully. </p><p>During the express upgrade, step “Restarting NodeManager on\n2 hosts”  fails in 1 host and succeeds in the other. I tried to downgrade\nbut downgrade too failed at the same step: </p><p>&gt;&gt; </p><p><strong>On host 1: </strong></p><p>[yarn@node1 ~]$ yarn node -list -states=RUNNING</p><p>16/04/21 13:49:25 INFO impl.TimelineClientImpl: Timeline\nservice address: <a href=\"http://usa0300lx259.na.xerox.net:8188/ws/v1/timeline/\">http://node2.domain.net:8188/ws/v1/timeline/</a></p><p>16/04/21 13:49:25 INFO client.RMProxy: Connecting to\nResourceManager at node2.domain.net/13.111.111.11:8050</p><p>Total Nodes:2</p><p> \nNode-Id \nNode-State Node-Http-Address \nNumber-of-Running-Containers</p><p>node1:45454 \nRUNNING node1:8042 \n 0</p><p>node2:45454 \nRUNNING node2:8042 \n0</p><p>Below is the error message I see in the error log: </p><p>resource_management.core.exceptions.Fail: NodeManager with ID node1.domain.net:45454\nwas not found in the list of running NodeManagers</p><p><strong>On host 2: </strong></p><p>[yarn@node2 sbin]$ yarn node -list -states=RUNNING</p><p>16/04/21 13:49:35 INFO impl.TimelineClientImpl: Timeline\nservice address: <a href=\"http://usa0300lx259.na.xerox.net:8188/ws/v1/timeline/\">http://node2.domain.net:8188/ws/v1/timeline/</a></p><p>16/04/21 13:49:35 INFO client.RMProxy: Connecting to\nResourceManager at node2.domain.net/13.111.111.11:8050</p><p>Total Nodes:2</p><p> \nNode-Id \nNode-State Node-Http-Address \nNumber-of-Running-Containers</p><p>node1:45454 \nRUNNING node1:8042 \n0</p><p>node2:45454 \nRUNNING node2:8042 \n0</p><p>NO errors reported while restarting node manager in this\nserver. </p><p>&lt;&lt; </p><p>Nodemanager status looks exactly same in both nodes but I am\nnot sure why the restart status check fails in one node and not on the other. </p><p>How to fix this issue? </p><p><a href=\"/storage/attachments/3590-node1-downgrade-log.txt\">node1-downgrade-log.txt</a><a href=\"/storage/attachments/3601-node2-downgrade-log.txt\">node2-downgrade-log.txt</a></p>","tags":["nodemanager","ambari-server","downgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-11 09:27:25.0","id":3213,"title":"beeline returns \"Failed to find any Kerberos tgt\" after enabling kerberos","body":"<p>Basically followed the instruction in http://hortonworks.com/blog/enabling-kerberos-hdp-active-directory-integration/</p><p>From ambari, everything looks OK but beeline command fails with \"GSS initiate failed (state=08S01,code=0)\"</p><pre>Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)\n        ... 28 more\n</pre><p>Checked time is synchronized and resolving IP/hostname is correct.</p><p>Built another HDP 2.3.2 and Ambari 2.1.2 without AD this time (simple MIT KDC)</p><p>And Ambari looks OK but, again, beeline fails with same error.</p><p>How I'm starting beeline is like below:</p><p>su - hive</p><p>beeline -u \"jdbc:hive2://hiveserver2_fqdn:10000/default;principal=hive/hiveserver2_fqdn@MY_REALM\"</p><p>I think i'm forgetting some setting...</p><p>I appreciate any advice from you.</p><p>Thank you</p>","tags":["beeline","kerberos","hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-12 18:12:16.0","id":3347,"title":"How to turn on Yarn capacity queue ACL?","body":"<p>HDP 2.2.8, Ambari 2.1.2, non-kerberoized cluster.</p><p>Client is trying to turn on capacity queue ACL--- only certain users are allowed to submit jobs to a queue; but unlisted users can still submit queues after \"save and refresh queues\".</p><p>Also, got this error wheile refreshServiceAcl</p><p>[nveyarn@hg003rdv ~]$ yarn rmadmin -refreshServiceAcl</p><p>refreshServiceAcl: java.io.IOException: Service Authorization (hadoop.security.authorization) not enabled.</p>","tags":["yarn-scheduler","help","security","YARN"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-16 20:32:39.0","id":4067,"title":"Snappy vs. Zlib - Pros and Cons for each compression in Hive/ Orc files","body":"<p>I had couple of questions on the file compression. We plan on using ORC format for a data zone that will be heavily accessed by the end-users via Hive/JDBC.</p><p>What is the recommendation when it comes to <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-HiveQLSyntax\">compressing</a> ORC files?</p><p>Do you think Snappy is a better option (over ZLIB) given Snappy’s better read-performance?  (Snappy is more performant in a read-often scenario, which is usually the case for Hive data.)  When would you choose zlib?</p><p>\nAs a side note: Compression is a double-edged sword, as you can go also have performance issue going from larger file sizes spread among multiple nodes to the smaller size & HDFS block size interactions.  You can blunt this by using <a href=\"https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.compression.strategy\">compression strategy</a>.</p>","tags":["Hive","orc","hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-13 02:21:52.0","id":3370,"title":"If Falcon is used to move a file from a folder with Ranger folder-level security policy to a new folder with less restrictions, which policy will apply to the file?","body":"<p>If Falcon is used to move a file from a folder with Ranger folder-level security policy to a new folder with less restrictions, which policy will apply to the file?  If the less restrictive one, is this not a vulnerability?  How can we prevent that?</p>","tags":["Ranger","Falcon","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-17 18:19:02.0","id":4137,"title":"HDFS replication factor for a directory.","body":"<p>Is there a way to configure a non-default replication factor for a HDFS directory such that all future files and sub-directories in that directory use that specific replication factor? Currently, we are using a work around of running a daemon process to set the replication factor for all files in the required directory. </p><p>Is there a better way to do this? </p><pre>while true; do\n    hdfs dfs -setrep -w 2 /tmp/\n    sleep 30\ndone</pre><p>I see at one point there was this JIRA <a href=\"https://issues.apache.org/jira/browse/HDFS-199\">https://issues.apache.org/jira/browse/HDFS-199</a> opened but is blocked by this JIRA [<a href=\"https://issues.apache.org/jira/browse/HADOOP-4771\">https://issues.apache.org/jira/browse/HADOOP-4771</a>] </p>","tags":["HDFS","replication"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-19 03:34:23.0","id":4284,"title":"Ranger Knox plugin auto-complete behavior","body":"<p>For a Knox repo that appears to be properly configured, auto-complete of the topology names is working as expected:</p><p><img src=\"/storage/attachments/523-topology-auto.png\"></p><p>The service names do not auto-complete. For example, WEBHDFS:</p><p><img src=\"/storage/attachments/524-service-noauto.png\"></p><p>Is there a way to make the service names auto-complete too, or is this a new feature request?</p>","tags":["help","Knox","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-18 21:59:44.0","id":4272,"title":"Process for moving HDP Services manually","body":"<p>We are looking for a documented process for moving services that are currently not supported by Ambari.   The Services I am currently looking for information for:</p><p>1. HDFS Jornal Node</p><p>2. Map Reduce Job History Server</p><p>3. Spark History Server</p>","tags":["ambari-service","spark-history-server","jhs","journalnode","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-18 13:36:20.0","id":4212,"title":"Spotfire with and without Krb","body":"<p>Hey team,</p><p>Do we have some feedback regarding Spotfire / HDP without & with Kerberos / Knox and Best practices ?</p><p>Thanks and kind regards.</p>","tags":["spotfire"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-19 22:55:14.0","id":4373,"title":"File Watcher scenario in HDF","body":"<p>  Can we have a file watcher kind of mechanism in Nifi, where the data flow gets triggered when ever a file shows up at source? Is it same as scheduling a getfile processor or run always? </p>","tags":["nifi-processor","Nifi","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-25 11:58:38.0","id":4792,"title":"Ambari Audit log","body":"<p>Where can I find an audit trail of every changes done via Ambari ? I would like something similar to the configuration diff that we can do using the UI with the addition of the username. </p><p>Eg Olivier has changed umask to 077 in hdfs-site on Monday 5th of December 2014 at 2:20:21.123  </p><p>I've found /var/log/ambari-server/ambari-config-changes.log but it doesn't show the specific change which has happened. I understand that I've got the version and i can diff w/ the previous version but i was wondering if we were recording it somewhere else.</p>","tags":["Ambari","audit","security"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-02 20:28:39.0","id":5074,"title":"Kafka Performance Testing - JMX Parameters Input","body":"<p>Hi,</p><p>I am trying to do some Kafka performance testing and want to input JMX paramters, what is the best way to do to that? In the sh file or via Ambari?</p>","tags":["performance","Kafka"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-30 16:08:04.0","id":4933,"title":"Cisco ToR Switch Recommendations","body":"<p>Looking for some up to date recommendations for Cisco top of rack switches. Recommendations for small (&lt;24 nodes), medium (24-100), and larger clusters would be greatly appreciated.</p>","tags":["network","switches","cisco"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-12-02 15:42:31.0","id":5062,"title":"Cloudbreak-- Failed to create cluster: Unsupported operation: create, on old azure clusters the only supported operation is termination","body":"<p>Trying multiple cluster configurations always results in the following error when creating a new cluster:</p><p>&lt;&lt; Failed to create cluster: Unsupported operation: create, on old azure clusters the only supported operation is termination &gt;&gt;.</p><p>Any idea what is causing the error?</p>","tags":["azure","Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-02 11:49:39.0","id":5030,"title":"What is the URL to open non-default banana dashboard?","body":"<p>In previous solr we could access non-default banana dashboards using a URL like this:</p><p>http://sandbox.hortonworks.com:8983/banana/#/dashboard/file/your_custom_dashboard.json</p><p>With new version: lucidworks-hdpsearch-2.3-2 it's not working, getting 404 error.</p><p>Does anyone now what is the URL to access it? Dashboard json file is already copied to: /opt/lucidworks-hdpsearch/solr/server/solr-webapp/webapp/banana/app/dashboards/</p>","tags":["solrcloud","SOLR","lucidworks"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-30 20:32:59.0","id":4940,"title":"HDP OS Upgrade/Patching Best practices","body":"<p>Are there any best practices/ documentation around patching or upgrading an OS (e.g. upgrading CentOS 6 --&gt; 7  or security patching ) while the cluster is running? </p><p>Thanks,</p>","tags":["hdp-2.3.4","best-practices","os"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-02 20:52:03.0","id":5084,"title":"What are the blueprint properties for specifying the Oozie/Hive database user and password when using an external db?","body":"","tags":["ambari-blueprint","Hive","Oozie","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-03 20:58:29.0","id":5143,"title":"SQOOP HANA to HIVE ORC","body":"<p>I am attempting to use SQOOP on a HANA tables of size 180 TB (compressed, 800TB on disk) into a HIVE table. When I pass LIMIT in query argument, the number of rows I get is 4 times the amount passed as LIMIT. So 250 LIMIT fetched 1000 rows. And they are not duplicated. </p><p>Another issue I am facing is with fetch-size. When I pass the fetch size, the process errors out with the message, \"Search Limit exceeded\"</p>","tags":["sap-hana","Sqoop"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-04 09:15:34.0","id":5157,"title":"Enabling HDFS compression","body":"<p>The documentation (http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_hdfs_admin_tools/content/ch04.html) to enable HDFS is recommending to use deprecated properties, where do I find the correct documentation that guides on how to enable compression on HDFS.</p>","tags":["compression","notify-docs","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-08 05:26:48.0","id":5413,"title":"What are common use cases for Spark and Data science?","body":"<p>What are common use cases for Spark and Data science across different verticals?</p>","tags":["use-cases","data-science","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-07 15:18:16.0","id":5310,"title":"Are there any issues with HDP on large RAM configurations (512G - 1T)?","body":"<p>What are the issues an hadoop operator might encounter when running HDP on a physical server with 1TB or RAM?</p>","tags":["hdp-2.3.4","HDFS","configuration","YARN","operations","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-07 19:27:45.0","id":5340,"title":"MySQL 5.7 Support?","body":"<p>I was reviewing the HWX documentation around supported databases and was trying to find out if MySQL 5.7 is supported.  The database matrix shows '5.x' and the minimum requirements documentation for HDP 2.3 calls out 5.6.  I am assuming that means 5.6 is the latest, but wanted to know if others are using 5.7 or have heard anything on when that may be added?</p>","tags":["operations","mysql"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-08 14:09:23.0","id":5444,"title":"Best practice for Avro schema/field naming regarding aliases","body":"<p>Reference: <a href=\"https://avro.apache.org/docs/1.7.7/spec.html#Aliases\">https://avro.apache.org/docs/1.7.7/spec.html#Alia...</a></p><p>I am setting up a new cluster and work flow to ingest CSV into Hive for a data warehouse style application. I have chosen to convert everything to Avro on the edge of ingest to standardize all processing on Avro to better handle schema evolution and detection and unify downstream processing. My question is in regard to best practice recommendations for using aliases in the Avro schemas when evolving one stat 'class' of CSV over time where a 'class' is one or more CSV files that all contain the same logical set of stats but, over time, more stats are added as the feature that generates that class of stats is enhanced. So, for example, I might see a files like these over time which all belong in the same Hive table and same stats 'class' of data:</p><pre>StatClassX_Version_1.csv\ndevice_id,timestamp,bytes_sent,bytes_received\n\nStatClassX_Version_2.csv (adds one new field)\ndevice_id,timestamp,bytes_sent,bytes_received,packet_discarded\n\nStatClassX_Version_3.csv (adds one new field, renames one existing field)\ndevice_id,timestamp,bytes_sent,bytes_received,packets_discarded,retries\n</pre><p>So, I want to drop all three of these in one 'StatClassX' Hive table. This is totally supported in Hive when using Avro storage format and by following proper schema evolution policies. This I have proven and have no questions about. My question is more about the best practices in evolving the Avro schema.</p><p>To make this example work would require three schema like the following:</p><pre>{\n    \"type\": \"record\",\n    \"name\": \"StatClassX\",\n    \"version\": \"1\",\n    \"namespace\": \"com.example.stats\",\n    \"fields\": [{\n        \"name\": \"device_id\",\n        \"type\": \"string\"\n    }, {\n        \"name\": \"timestamp\",\n        \"type\": \"string\"\n    }, {\n        \"name\": \"bytes_sent\",\n        \"type\": \"long\"\n    }, {\n        \"name\": \"bytes_received\",\n        \"type\": \"long\"\n    }]\n}\n\n{\n    \"type\": \"record\",\n    \"name\": \"StatClassX\",\n    \"version\": \"2\",\n    \"namespace\": \"com.example.stats\",\n    \"fields\": [{\n        \"name\": \"device_id\",\n        \"type\": \"string\"\n    }, {\n        \"name\": \"timestamp\",\n        \"type\": \"string\"\n    }, {\n        \"name\": \"bytes_sent\",\n        \"type\": \"long\"\n    }, {\n        \"name\": \"bytes_received\",\n        \"type\": \"long\"\n    }, {\n        \"name\": \"packet_discarded\",\n        \"type\": \"long\"\n        \"default\": \"0\"\n    }]\n}\n\n{\n    \"type\": \"record\",\n    \"name\": \"StatClassX\",\n    \"version\": \"3\",\n    \"namespace\": \"com.example.stats\",\n    \"fields\": [{\n        \"name\": \"device_id\",\n        \"type\": \"string\"\n    }, {\n        \"name\": \"timestamp\",\n        \"type\": \"string\"\n    }, {\n        \"name\": \"bytes_sent\",\n        \"type\": \"long\"\n    }, {\n        \"name\": \"bytes_received\",\n        \"type\": \"long\"\n    }, {\n        \"name\": \"packets_discarded\",\n        \"type\": \"long\",\n        \"default\": \"0\",\n        \"aliases\": [\"packet_discarded\"]\n    }, {\n        \"name\": \"retries\",\n        \"type\": \"int\",\n        \"default\": \"0\"\n    }]\n}\n</pre><p>Here, I have used a user-defined property field \"version\" to indicate the schema version which my custom conversion logic uses for reasons outside the scope of this question. But, it also happens to be the only 'human-readable' easy way to tell one schema from another and some how tie it back, to say, some Word document that defines the source CSV schema. Getting to the point, about aliases, I have introduced one in version 2 to handle the name change of one field. However, what I discovered recently is that you can also use aliases on the schema name itself - I thought it only applied to field names. </p><p><strong><u>Question</u></strong>: I know I MUST use the alias on the renamed field. However, I could also name each schema with a name that captures the version, For example, versions 1-3 could be named as follows using header level aliases:</p><pre>{\n    \"type\": \"record\",\n    \"name\": \"StatClassX.1\",\n    \"namespace\": \"com.example.stats\",\n    \"fields\": [{...}]\n}\n\n{\n    \"type\": \"record\",\n    \"name\": \"StatClassX.2\",\n    \"aliases\": [\"StatClassX.1\"],\n    \"namespace\": \"com.example.stats\",\n    \"fields\": [{...}]\n}\n\n{\n    \"type\": \"record\",\n    \"name\": \"StatClassX.3\",\n    \"aliases\": [\"StatClassX.1\", \"StatClassX.2\"],\n    \"namespace\": \"com.example.stats\",\n    \"fields\": [{...}]\n}</pre><p>This intuitively feels weird. I prefer that the schema name be more reflective of the \"class\" of stats and not be overloaded with some notion of version. But, I just wonder what use case might exist for using different schema names that actually all refer to a similar class of data? One thought would be that you just decided to adopt a different naming convention and this would allow you to roll that out. For versioning, it does not seem to fit but I want to solicit thoughts from some experts nonetheless.</p><p>Thanks!</p>","tags":["avro","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-08 05:13:41.0","id":5402,"title":"Error sending message to Solr : o collection param specified on request and no default collection has been set.","body":"<p>I built Hadoop cluster with HDP 2.3.0. This cluster is kerberized.</p><p>I'm trying Apache Ranger 0.5.0.2.3 for managing the access control and running into the following error.</p><pre>2015-12-08 12:49:10,427 ERROR [hiveServer2.async.summary.multi_dest.batch_destWriter]: provider.BaseAuditHandler (BaseAuditHandler.java:logError(318)) - Error sending message to Solr\norg.apache.solr.client.solrj.SolrServerException: No collection param specified on request and no default collection has been set.\n        at org.apache.solr.client.solrj.impl.CloudSolrClient.directUpdate(CloudSolrClient.java:519)\n        at org.apache.solr.client.solrj.impl.CloudSolrClient.sendRequest(CloudSolrClient.java:918)\n        at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:809)\n        at org.apache.solr.client.solrj.impl.CloudSolrClient.request(CloudSolrClient.java:752)\n        at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:131)\n        at org.apache.solr.client.solrj.SolrClient.add(SolrClient.java:76)\n        at org.apache.solr.client.solrj.SolrClient.add(SolrClient.java:62)\n        at org.apache.ranger.audit.destination.SolrAuditDestination.log(SolrAuditDestination.java:132)\n        at org.apache.ranger.audit.provider.BaseAuditHandler.logJSON(BaseAuditHandler.java:161)\n        at org.apache.ranger.audit.queue.AuditFileSpool.sendEvent(AuditFileSpool.java:882)\n        at org.apache.ranger.audit.queue.AuditFileSpool.runDoAs(AuditFileSpool.java:830)\n        at org.apache.ranger.audit.queue.AuditFileSpool$2.run(AuditFileSpool.java:759)\n        at org.apache.ranger.audit.queue.AuditFileSpool$2.run(AuditFileSpool.java:757)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:360)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)\n        at org.apache.ranger.audit.queue.AuditFileSpool.run(AuditFileSpool.java:765)\n        at java.lang.Thread.run(Thread.java:745)\n2015-12-08 12:49:10,427 ERROR [hiveServer2.async.summary.multi_dest.batch_destWriter]: queue.AuditFileSpool (AuditFileSpool.java:logError(710)) - Error sending logs to consumer. provider=hiveServer2.async.summary.multi_dest.batch, consumer=hiveServer2.async.summary.multi_dest.batch.solr\n</pre><p>I find this error in the hiveserver log file, /var/log/hive/hiveserver2.log, and audit logs for hive access don't appear on Audit tab page in Ranger UI.</p><p>I'm trying to solve the error, but I can't solve yet.</p><p>Please let me know what I should check.</p><p>Version</p><p>HDP 2.3.0.0</p><p>Ranger 0.5.0.2.3</p><p>Hive 1.2.1.2.3</p><p>Solr 5.2.1</p><p>I have bad English, so I apologize if I say something strange.</p><p>Thanks.</p>","tags":["hiveserver2","ranger-0.5.0","solrcloud"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-14 14:47:23.0","id":1444,"title":"Steps to upgrade to Spark 1.5.1 on Sandbox","body":"<p>Has anyone tried manually upgrading to Spark 1.5.1 on Hortonworks Sandbox and faced any issues?</p>","tags":["Sandbox","Spark"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-08 17:52:45.0","id":5463,"title":"Is there any Oozie integration with AutoSys?","body":"","tags":["Oozie","autosys"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-09 21:54:45.0","id":5676,"title":"NodeManager memory setting best practice?","body":"<p>I asked a question regarding OOM in NodeManager log and it turned out I need to increase the NM's heap size.</p><p>So that I was wondering if there is any document which advises me how much memory I should assign?</p><p>I'm hoping something similar to HDFS NameNode heap size ( <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_installing_manually_book/content/ref-80953924-1cbf-4655-9953-1e744290a6c3.1.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-...</a> )</p><p>Or as it would be hard to estimate, I should just try twice higher and see how it goes?</p>","tags":["best-practices","nodemanager","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-10 00:40:47.0","id":5699,"title":"does anyone know why nifi not included in hdp 2.3.2 ?","body":"<p>I just install hdp 2.3.2, I wanted to include nifi for testing but its not included among the services how can I install nifi?</p>","tags":["hdp-2.3.2","Nifi","ambari-2.1.2"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-10 03:19:45.0","id":5733,"title":"After installing Knox and Ranger, I do not see any way to configure Security policies","body":"<p>Hi,</p><p>I have installed HDP(2.3) latest release, and configured knox, ranger service.</p><p>Logged into the ranger UI.</p><p>I could see screens changed very much and unable create security stuff.</p><p>Is there any screens/user interface docs to use latest ranger?</p>","tags":["hdp-2.3.0","Knox","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-09 16:39:10.0","id":5609,"title":"Spark Thrift Server goes down/runs into OOM after some time when running large jobs from Tableau or beeline","body":"<p>Thrift server started as: \n/usr/hdp/current/spark-thriftserver/sbin/<a href=\"http://start-thriftserver.sh/\">start-thriftserver.sh</a> --master yarn-client --executor-memory 20G --num-executors 20 --executor-cores 12 --hiveconf hive.server2.thrift.port=10001 </p><p>1. Cached data in spark memory by using \n  Cache table bo_5years </p><p>\n2. ran select * from bo_5years from beeline  </p><pre>Error in logs: \n15/12/04 16:03:54 WARN DefaultChannelPipeline: An exception was thrown by a user handler while handling an exception event ([id: 0x5e66418a, /10.105.167.206:53903 =&gt; /10.105.164.205:60270] EXCEPTION: java.lang.OutOfMemoryError: Java heap \nspace) \njava.lang.OutOfMemoryError: Java heap space \n        at java.lang.Object.clone(Native Method) \n        at akka.util.CompactByteString$.apply(ByteString.scala:410) \n        at akka.util.ByteString$.apply(ByteString.scala:22) \n        at akka.remote.transport.netty.TcpHandlers$class.onMessage(TcpSupport.scala:45) \n        at akka.remote.transport.netty.TcpServerHandler.onMessage(TcpSupport.scala:57) \n        at akka.remote.transport.netty.NettyServerHelpers$class.messageReceived(NettyHelpers.scala:43) \n        at akka.remote.transport.netty.ServerHandler.messageReceived(NettyTransport.scala:180) \n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) \n        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) \n        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) \n        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310) \n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) \n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) \n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) \n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) \n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) \n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) \n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) \n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) \n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) \n        at java.lang.Thread.run(Thread.java:745) \n15/12/04 16:03:56 ERROR ErrorMonitor: Uncaught fatal error from thread [sparkDriver-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriver] \njava.lang.OutOfMemoryError: Java heap space \n        at org.spark_project.protobuf.ByteString.copyFrom(ByteString.java:192) \n        at org.spark_project.protobuf.CodedInputStream.readBytes(CodedInputStream.java:324) \n        at akka.remote.WireFormats$SerializedMessage.&lt;init&gt;(WireFormats.java:3030) \n        at akka.remote.WireFormats$SerializedMessage.&lt;init&gt;(WireFormats.java:2980) \n        at akka.remote.WireFormats$SerializedMessage$1.parsePartialFrom(WireFormats.java:3073) \n        at akka.remote.WireFormats$SerializedMessage$1.parsePartialFrom(WireFormats.java:3068) \n        at org.spark_project.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309) \n        at akka.remote.WireFormats$RemoteEnvelope.&lt;init&gt;(WireFormats.java:993) \n        at akka.remote.WireFormats$RemoteEnvelope.&lt;init&gt;(WireFormats.java:927) \n        at akka.remote.WireFormats$RemoteEnvelope$1.parsePartialFrom(WireFormats.java:1049) \n        at akka.remote.WireFormats$RemoteEnvelope$1.parsePartialFrom(WireFormats.java:1044) \n        at org.spark_project.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309) \n        at akka.remote.WireFormats$AckAndEnvelopeContainer.&lt;init&gt;(WireFormats.java:241) \n        at akka.remote.WireFormats$AckAndEnvelopeContainer.&lt;init&gt;(WireFormats.java:175) \n        at akka.remote.WireFormats$AckAndEnvelopeContainer$1.parsePartialFrom(WireFormats.java:279) \n        at akka.remote.WireFormats$AckAndEnvelopeContainer$1.parsePartialFrom(WireFormats.java:274) \n        at org.spark_project.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:141) \n        at org.spark_project.protobuf.AbstractParser.parseFrom(AbstractParser.java:176) \n        at org.spark_project.protobuf.AbstractParser.parseFrom(AbstractParser.java:188) \n        at org.spark_project.protobuf.AbstractParser.parseFrom(AbstractParser.java:193) \n        at org.spark_project.protobuf.AbstractParser.parseFrom(AbstractParser.java:49) \n        at akka.remote.WireFormats$AckAndEnvelopeContainer.parseFrom(WireFormats.java:409) \n        at akka.remote.transport.AkkaPduProtobufCodec$.decodeMessage(AkkaPduCodec.scala:181) \n        at akka.remote.EndpointReader.akka$remote$EndpointReader$$tryDecodeMessageAndAck(Endpoint.scala:995) \n        at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:928) \n        at akka.actor.Actor$class.aroundReceive(Actor.scala:465) \n        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415) \n        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) \n        at akka.actor.ActorCell.invoke(ActorCell.scala:487)</pre>","tags":["oom","sparksql","spark-thriftserver"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-10 01:45:55.0","id":5705,"title":"Zepplin error in Ambari Sandbox + HDP 2.3.2","body":"<p>Hi,</p><p>I have installed HDP 2.3.2 sandbox for virtualbox and when I try to access Zepplin through Ambari .</p><p>I am getting below screen .</p><p><img src=\"/storage/attachments/725-ambari-sandbox-zepplin-error.png\"></p><p>Am I missing any configuration ?</p><p>Would really appreciate your help.</p><p>Thanks,</p><p>Divya </p>","tags":["hdp-2.3.2","Spark","Sandbox","zeppelin","vmware"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-09 15:41:56.0","id":5564,"title":"Exception in thread \"main\" java.lang.ClassNotFoundException","body":"<p>I am trying to run a MapReduce job using(using new API) on Hadoop 2.7.1 using command line. </p><p>I have followed the below steps.</p><pre>javac -cp `hadoop classpath`MaxTemperatureWithCompression.java -d /Users/gangadharkadam/hadoopdata/build\njar -cvf MaxTemperatureWithCompression.jar /Users/gangadharkadam/hadoopdata/build\nhadoop jar MaxTemperatureWithCompression.jar org.myorg.MaxTemperatureWithCompression user/ncdc/input /user/ncdc/output</pre><p>No error in compiling and creating a jar file. But on execution I am gettign the folowing error</p><pre>Error Messages- Exception in thread \"main\" java.lang.ClassNotFoundException: org.myorg.MaxTemperatureWithCompression at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:274) at org.apache.hadoop.util.RunJar.run(RunJar.java:214) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n</pre><pre>Java Code-\npackage org.myorg;\n\n//Standard Java Classes\nimport java.io.IOException;\nimport java.util.regex.Pattern;\n\n//extends the class Configured, and implements the Tool utility class\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\n//send debugging messages from inside the mapper and reducer classes\nimport org.apache.log4j.Logger;\n\n//Job class in order to create, configure, and run an instance of your MapReduce\nimport org.apache.hadoop.mapreduce.Job;\n\n//extend the Mapper class with your own Map class and add your own processing instructions\nimport org.apache.hadoop.mapreduce.Mapper;\n\n//extend it to create and customize your own Reduce class\nimport org.apache.hadoop.mapreduce.Reducer;\n\n//Path class to access files in HDFS\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.fs.FileSystem;\n\n//pass required paths using the FileInputFormat and FileOutputFormat classes\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\n//Writable objects for writing, reading,and comparing values during map and reduce processing\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.GzipCodec;\n\npublic class MaxTemperatureWithCompression extends Configured implements Tool {\n    private static final Logger LOG = Logger.getLogger(MaxTemperatureWithCompression.class);\n\n    //main menhod to invoke the toolrunner to create instance of MaxTemperatureWithCompression\n    public static void main(String[] args) throws Exception {\n\n        int res = ToolRunner.run(new MaxTemperatureWithCompression(), args);\n        System.exit(res);\n\n    }\n\n    //call the run method to configure the job\n    public int run(String[] args) throws Exception {\n        if (args.length != 2) {\n            System.err.println(\"Usage: MaxTemperatureWithCompression &lt;input path&gt; \" + \"&lt;output path&gt;\");\n            System.exit(-1);\n          }\n\n        Job job = Job.getInstance(getConf(), \"MaxTemperatureWithCompression\");\n\n        //set the jar to use based on the class\n        job.setJarByClass(MaxTemperatureWithCompression.class);\n\n        //set the input and output path\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //set the output key and value\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        //set the compressionformat     \n        /*[*/FileOutputFormat.setCompressOutput(job, true);\n            FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);/*]*/\n\n        //set the mapper and reducer class\n        job.setMapperClass(Map.class);\n        job.setCombinerClass(Reduce.class);\n        job.setReducerClass(Reduce.class);\n\n        return job.waitForCompletion(true) ? 0 : 1;\n\n    }\n\n    //mapper\n    public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n        private static final int MISSING = 9999;\n\n        @Override\n        public void map(LongWritable key, Text value, Context context)\n        throws IOException,InterruptedException {\n\n            String line = value.toString();\n            String year = line.substring(15,19);\n\n            int airTemperature;\n\n            if (line.charAt(87) == '+') {\n                airTemperature = Integer.parseInt(line.substring(88, 92));\n            }\n            else {\n                airTemperature = Integer.parseInt(line.substring(87, 92));\n            }\n            String quality = line.substring(92,93);\n\n            if (airTemperature != MISSING && quality.matches(\"[01459]\")) {\n                context.write(new Text(year), new IntWritable(airTemperature));\n            }\n        }\n\n    }\n\n    //reducer\n    public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\n        @Override\n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) \n        throws IOException, InterruptedException {\n            int maxValue = Integer.MIN_VALUE;\n            for (IntWritable value : values) {\n                maxValue = Math.max(maxValue, value.get());\n            }\n            context.write(key, new IntWritable(maxValue));\n        }\n\n    }\n}\n</pre><p>I checked jar file and the folder structure org/myorg/MaxTemperatureWithCompression.class   is present. What could be the reason for this error. Any help in resolving this is highly apprciated. Thanks.\n</p>","tags":["hadoop","YARN","HDFS","MapReduce"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-10 04:09:16.0","id":5750,"title":"Is Secondary Name node is mandatory for any distributed hadoop cluster?","body":"","tags":["hadoop","hadoop-ecosystem","namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-10 13:02:11.0","id":5833,"title":"Create Hive table to read parquet files from parquet/avro schema","body":"<p>Hello Experts ! </p><p>We are looking for a solution in order to create an external hive table to read data from parquet files according to a parquet/avro schema. </p><p>\nin other way, how to generate a hive table from a parquet/avro schema ?</p><p>\nthanks :)</p>","tags":["Hive","parquet","avro"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-10 08:16:43.0","id":5789,"title":"Does anybody have tried sandbox in vmware vSphere ? Is it compatible ?. Thank you.","body":"","tags":["vmware","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-10 04:46:22.0","id":5758,"title":"Can I use WebHDFS (via Knox) for remote files (around 7lakh small per day) transfers ?","body":"","tags":["Knox","webhdfs"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-10 18:55:46.0","id":5881,"title":"I have Hortonworks sandbox on azure . HBase crashes all the time and I have to restart the machine all the time is there a solution for this?","body":"","tags":["Hbase"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-10 15:42:24.0","id":5859,"title":"Spark Join is so slow and taking long time","body":"","tags":["Spark","spark-sql"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-10 21:55:53.0","id":5943,"title":"Database Sizing and recommendation for Ambari and HDP components","body":"<p>What’s the approximate load on DB if we collocate ambari, oozie, hive, ranger admin & audit etc on the same DB cluster? </p><p>- Cluster Size ~ &lt;100 nodes, 100-500 nodes, 500-1000 nodes and 1000+</p><p>- Number of Users for Hive and Oozie ~100+</p>","tags":["hdp-2.3.4","best-practices","sizing","Ambari","operations","Oozie","Hive"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-12-10 08:27:19.0","id":5798,"title":"Spark - Hive tables not found when running in YARN-Cluster mode","body":"<p>\n\tI have a Spark (version 1.4.1) application on HDP 2.3.2. It works fine when running it in YARN-Client mode. However, when running it on YARN-Cluster mode none of my Hive tables can be found by the application.</p><p>\n\tI submit the application like so:</p>\n<pre>  ./bin/spark-submit \n  --class com.myCompany.Main \n  --master yarn-cluster \n  --num-executors 3 \n  --driver-memory 4g \n  --executor-memory 10g \n  --executor-cores 1 \n  --jars lib/datanucleus-api-jdo-3.2.6.jar,lib/datanucleus-rdbms-3.2.9.jar,lib/datanucleus-core-3.2.10.jar /home/spark/apps/YarnClusterTest.jar  \n  --files /etc/hive/conf/hive-site.xml\n</pre><p>\n\tHere's an excerpt from the logs:</p>\n<pre>5/12/02 11:05:13 INFO hive.HiveContext: Initializing execution hive, version 0.13.1\n15/12/02 11:05:14 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore\n15/12/02 11:05:14 INFO metastore.ObjectStore: ObjectStore, initialize called\n15/12/02 11:05:14 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n15/12/02 11:05:14 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n15/12/02 11:05:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager worker2.xxx.com:34697 with 5.2 GB RAM, BlockManagerId(1, worker2.xxx.com, 34697)\n15/12/02 11:05:16 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n15/12/02 11:05:16 INFO metastore.MetaStoreDirectSql: MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: \"@\" (64), after : \"\".\n15/12/02 11:05:17 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n15/12/02 11:05:17 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n15/12/02 11:05:18 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n15/12/02 11:05:18 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n15/12/02 11:05:18 INFO metastore.ObjectStore: Initialized ObjectStore\n15/12/02 11:05:19 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa\n15/12/02 11:05:19 INFO metastore.HiveMetaStore: Added admin role in metastore\n15/12/02 11:05:19 INFO metastore.HiveMetaStore: Added public role in metastore\n15/12/02 11:05:19 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty\n15/12/02 11:05:19 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.\n15/12/02 11:05:19 INFO parse.ParseDriver: Parsing command: SELECT * FROM streamsummary\n15/12/02 11:05:20 INFO parse.ParseDriver: Parse Completed\n15/12/02 11:05:20 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.\n15/12/02 11:05:20 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=streamsummary\n15/12/02 11:05:20 INFO HiveMetaStore.audit: ugi=spark  ip=unknown-ip-addr  cmd=get_table : db=default tbl=streamsummary   \n15/12/02 11:05:20 DEBUG myCompany.Main$: no such table streamsummary; line 1 pos 14\n</pre><p>\n\tI basically run into the same 'no such table' problem for any time my application needs to read from or write to the Hive tables.</p><p>\n\tThanks in advance!</p><p>\n\t<strong>UPDATE:</strong></p><p>\n\tI tried submitting the spark application with the --files parameter provided before --jars as per <a rel=\"user\" href=\"/users/238/gbraccialli.html\" nodeid=\"238\">@Guilherme Braccialli</a>'s suggestion, but doing so now gives me an exception saying that the HiveMetastoreClient could not be instantiated.</p><p>\n\t<em>spark-submit:</em></p>\n<pre>  ./bin/spark-submit \\\n  --class com.myCompany.Main \\\n  --master yarn-cluster \\\n  --num-executors 3 \\\n  --driver-memory 1g \\\n  --executor-memory 11g \\\n  --executor-cores 1 \\\n  --files /etc/hive/conf/hive-site.xml \\\n  --jars lib/datanucleus-api-jdo-3.2.6.jar,lib/datanucleus-rdbms-3.2.9.jar,lib/datanucleus-core-3.2.10.jar \\&lt;br&gt;  /home/spark/apps/YarnClusterTest.jar\n</pre><p>\n\t<em>code:</em></p><pre>// core.scala\ntrait Core extends java.io.Serializable{\n/**\n *  This trait should be mixed in by every other class or trait that is dependent on `sc`\n * \n */\n  val sc: SparkContext\n  lazy val sqlContext = new HiveContext(sc)\n}\n\n// yarncore.scala\ntrait YarnCore extends Core {\n/** \n * This trait initializes the SparkContext with YARN as the master\n */\n  val conf = new SparkConf().setAppName(\"my app\").setMaster(\"yarn-cluster\")\n  val sc = new SparkContext(conf)\n}\n\nmain.scala\n\tobject Test {\n\t  def main(args:Array[String]){\n\t\n\t  /**initialize the spark application**/\n\t  val app = new YarnCore  // initializes the SparkContext in YARN mode\n\t  with sqlHelper  // provides SQL functionality\n\t  with Transformer  // provides UDF's for transforming the dataframes into the marts\n\t\n\t  /**initialize the logger**/\n\t  val log = Logger.getLogger(getClass.getName)\n\t\n\t  val count = app.sqlContext.sql(\"SELECT COUNT(*) FROM streamsummary\")\n\t\n\t  log.info(\"streamsummary has ${count} records\")\n\t\n\t  /**Shut down the spark app**/\n\t  app.sc.stop\n\t  }\n\t}</pre><p>\n\t<em>exception:</em></p><pre>15/12/11 09:34:55 INFO hive.HiveContext: Initializing execution hive, version 0.13.1\n15/12/11 09:34:56 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\njava.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.&lt;init&gt;(ClientWrapper.scala:117)\n\tat org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:165)\n\tat org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:163)\n\tat org.apache.spark.sql.hive.HiveContext.&lt;init&gt;(HiveContext.scala:170)\n\tat com.epldt.core.Core$class.sqlContext(core.scala:13)\n\tat com.epldt.Test$anon$1.sqlContext$lzycompute(main.scala:17)\n\tat com.epldt.Test$anon$1.sqlContext(main.scala:17)\n\tat com.epldt.Test$.main(main.scala:26)\n\tat com.epldt.Test.main(main.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$anon$2.run(ApplicationMaster.scala:486)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:62)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)\n\t... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)\n\t... 19 more\nCaused by: java.lang.NumberFormatException: For input string: \"1800s\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1258)\n\tat org.apache.hadoop.hive.conf.HiveConf.getIntVar(HiveConf.java:1211)\n\tat org.apache.hadoop.hive.conf.HiveConf.getIntVar(HiveConf.java:1220)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:293)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:214)\n\t... 24 more</pre><p></p>","tags":["spark-sql","Spark","YARN","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-11 04:22:11.0","id":6020,"title":"Type Error when attempting Linear Regression","body":"<pre>import org.apache.spark.mllib.regression.LinearRegressionWithSGD\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport sqlContext.implicits._\n\n\nval df = sqlContext.sql(\"select mnemonic, average, median, stddev from wellbook.curve_statistics\")\n\n\nval indexer = new StringIndexer()\n  .setInputCol(\"mnemonic\")\n  .setOutputCol(\"mnemonicIndex\")\n  .fit(df)\nval indexed = indexer.transform(df)\n\n\nval encoder = new OneHotEncoder().setInputCol(\"mnemonicIndex\").\n  setOutputCol(\"mnemonicVec\")\nval encoded = encoder.transform(indexed)\nval data = encoded.select(\"mnemonicVec\", \"average\", \"median\", \"stddev\")\n\n\nval parsedData = data.map(row =&gt; LabeledPoint(row.getDouble(0), row.getAs[Vector](1)))\n</pre><h3></h3><h3></h3><h3>&lt;console&gt;:297: error: kinds of the type arguments (Vector) do not conform to the expected kinds of the type parameters (type T).\nVector's type parameters do not match type T's expected parameters:\ntype Vector has one type parameter, but type T has none\n       val parsedData = data.map(row =&gt; LabeledPoint(row.getDouble(0), row.getAs[Vector](1))</h3>","tags":["scala","Spark","dataframe"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-11 02:11:02.0","id":5993,"title":"Yarn timeline server periodically fails","body":"<p>Hi,</p><p> I'm using HDP 2.3.0 and Yarn app timeline server is failing periodically. Checking app timeline server log, the cause is due to GC overhead limit exceeded.</p><pre>2015-12-02 12:48:56,548 ERROR mortbay.log (Slf4jLog.java:warn(87)) - /ws/v1/timeline/spark_event_v01\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n        at org.codehaus.jackson.util.TextBuffer.contentsAsString(TextBuffer.java:350)\n        at org.codehaus.jackson.impl.Utf8StreamParser.getText(Utf8StreamParser.java:278)\n        at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:59)\n        at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:204)\n        at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:47)\n        at org.codehaus.jackson.map.ObjectReader._bindAndClose(ObjectReader.java:768)\n        at org.codehaus.jackson.map.ObjectReader.readValue(ObjectReader.java:486)\n        at org.apache.hadoop.yarn.server.timeline.GenericObjectMapper.read(GenericObjectMapper.java:93)\n        at org.apache.hadoop.yarn.server.timeline.GenericObjectMapper.read(GenericObjectMapper.java:77)\n        at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.getEntityEvent(LeveldbTimelineStore.java:1188)\n        at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.getEntity(LeveldbTimelineStore.java:437)\n        at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.getEntityByTime(LeveldbTimelineStore.java:685)\n        at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.getEntities(LeveldbTimelineStore.java:557)\n        at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getEntities(TimelineDataManager.java:134)\n        at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntities(TimelineWebServices.java:119)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)</pre><p>It seems that the timeline server fails to delete old Leveldb data so every time it must load a large volume of old entries which cause GC overhead. Checking the log there is a lot of lines like the following:</p><pre>2015-12-02 12:48:14,471 WARN  timeline.LeveldbTimelineStore (LeveldbTimelineStore.java:deleteNextEntity(1459)) - Found no start time for reverse related entity tez_appattempt_1447379225800_23982_000001 of type TEZ_APPLICATION_ATTEMPT while deleting dag_1447379225800_23982_1 of type TEZ_DAG_ID\n2015-12-02 12:48:14,471 WARN  timeline.LeveldbTimelineStore (LeveldbTimelineStore.java:deleteNextEntity(1459)) - Found no start time for reverse related entity tez_appattempt_1447379225800_23982_000001 of type TEZ_APPLICATION_ATTEMPT while deleting dag_1447379225800_23982_1 of type TEZ_DAG_ID\n</pre><p>And checking the volume of the timeline data folder gives the following info:</p><pre>40K     timeline/timeline-state-store.ldb\n7.0G    timeline/leveldb-timeline-store.ldb\n7.0G    timeline\n3.4G    timeline-data/leveldb-timeline-store.ldb\n3.4G    timeline-data\n</pre><p>With app timeline server failing, currently I cannot see the history of my Spark Jobs. Any help is appreciated.</p>","tags":["YARN","hdp-2.3.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-10 22:44:43.0","id":5954,"title":"Solr Banana Dashboard charts and data download","body":"<p> I have a Solr Banana dashboard that has shows some panels with charts and tables. Is there a way to export a dashboard with data so that a user can play with it offline without being connected to the solr server? </p>","tags":["banana","SOLR","dashboard"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-11 11:12:07.0","id":6034,"title":"Getting Error while executing this command","body":"<pre>rdd = sc.parallelize(r1)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  c = list(c)  # Make it a list so we can compute its length\nTypeError: 'PipelinedRDD' object is not iterable</pre><p>~~~~~~~~~~~~~~~~~My commands are ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p><pre>&gt;&gt;&gt; R = sc.textFile(filename);\n&gt;&gt;&gt; R.collect()\n&gt;&gt;&gt; r1 = R.map(lambda s: s.split(\",\"))\n&gt;&gt;&gt; r1.collect()\n&gt;&gt;&gt; rdd = sc.parallelize(r1)</pre>","tags":["Spark","rdd","python"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-11 19:03:34.0","id":6093,"title":"Ambari server 2.1.0: How to enable TLSv1.2 for the ports 8440 and 8441?","body":"<p>Our stringent security policies require using TLSv1.2 for connections supporting SSL/TLS traffic. Since ports 8440 and 8441 use HTTPS, I need to enable TLSv1.2 for both. I couldn't find anything in the documentation suggesting that it's possible to configure the underlying SSL protocol used by secure connections. Is this kind of setup supported by Ambari v2.1.0?</p>","tags":["ambari-2.1.0","security","ssl"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-11 03:38:35.0","id":6018,"title":"Atlas as an enterprise metadata tool?","body":"<p>We know that Atlas works well within Hadoop ecosystem components. Is there any existing or future possibility as per the Atlas roadmap to use it as an Enterprise Metadata repository with end-to-end lineage including RDBMS and conventional ETL tools?</p><p>If we could have just one metadata data tool, what are some of the existing Tools which are more universal and work with all data sources within an enterprise and capture end-to-end data lineage?</p><p>Thanks in advance for your response...</p>","tags":["data-lineage","Atlas","metadata"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-12 14:33:59.0","id":6168,"title":"When Reducer Operation actually starts..I mean with respect to Copy Phase ?After completion of copy phase or  while copy phase is going on???","body":"<a rel=\"user\" href=\"/users/140/nsabharwal.html\" nodeid=\"140\">@Neeraj Sabharwal</a>","tags":["MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-13 04:04:14.0","id":6193,"title":"Unable to Start Sandbox 2.3.2 on virtual box","body":"<p>Getting following error while starting sandbox on windows 7 64 bit os and virtual box. Please help in resolving.</p><p>The virtual machine 'Hortonworks Sandbox with HDP 2.3.2' has terminated unexpectedly during startup with exit code 1 (0x1). More details may be available in 'C:\\Users\\Admin\\VirtualBox VMs\\Hortonworks Sandbox with HDP 2.3.2\\Logs\\VBoxHardening.log'.</p><p>Result Code: \nE_FAIL (0x80004005)</p><p>Component: \nMachineWrap</p><p>Interface: \nIMachine {f30138d4-e5ea-4b3a-8858-a059de4c93fd}</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-11 21:57:31.0","id":6111,"title":"Hue + UDF access issue","body":"<p>I\n am facing a peculiar issue with an UDF in Hue. My \nteam has a UDF which has 2 functions : encrypt and decrypt which they \nregister in hive as p.aes_encrypt and p.aes_decrypt. All users can\naccess both the functions using Hive CLI but the non-root users (except hadoop group) cannot use the p.aes_decrypt \nfunction via Hue (gives NoClassDefFoundError) but can use the \np.aes_encrypt function (from the same jar).\n</p>I have already tried with \nre-registering the function and the decrypt function still doesn't work \nvia Hue.(while the same works on CLI). If I register the function with a different function name, it works. I have also checked the classpath\n and the corresponding jar is present. The permissions\non the jar and the jar path are also correct.\n\n\nPlease provide any insights \nso as to why can't a user use the p.aes_decrypt function while he can\n use the p.aes_encrypt function and both are from same jar.\n\nThanks,Vaibha\n","tags":["hue","Hive","hive-udf"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 12:49:54.0","id":6349,"title":"Query Multiple HBase table through custom Knox service","body":"<p>I've created a custom Knox service with the intention of using custom .class files to be able to query multiple HBase tables and/or return multiple HBase table rows The work is a PoC to extent the capabilities of Knox through a custom service. Is this possible? I am still unable to get the new service to even begin calling the custom .class files I've uploaded to replace HBaseDispatch. I am doing this within the HDP 2.3.2 Sandbox on Windows.</p>","tags":["Knox","Hbase","service","security","Sandbox"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 15:08:10.0","id":6381,"title":"Limitations on # of Hive Columns","body":"<p>What is the Hive Column count Limit? When should you consider moving the table into Hbase/Phoenix due to performance issues? Is 100,000 columns too many for Hive to handle? </p><p>Thanks,</p>","tags":["Phoenix","Hbase","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 17:48:02.0","id":6397,"title":"Open Kafka to requests from other hosts","body":"<p>Hi all,</p><p>I'm currently evaluating Hortonworks and wondering if it's possible to open access to the Kafka-Service, for more than just the Ambari-Host itself. Actually it would be a requirement for the Ops-Team.</p><p>I tried to manipulate the configfiles, but without any success. Even SSH-Port-Tunneling to the Server doesn't help me accessing the Kafka port 6667.</p><p>Thanks in advance for any help,</p><p>Matthias</p>","tags":["configuration","Kafka"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-15 10:07:11.0","id":6491,"title":"PIG script stays in running status","body":"<p>I am trying the hello world tutorial in sandbox and step no 3.4. The PIG scripts is not getting completed, always stays in running status. An empty table riskfactor is getting created in Hive. How do I know what's going wrong? Where to check and fix the issue?</p>","tags":["Pig","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-15 21:36:05.0","id":6597,"title":"Sqooping data from LOCAL MSSQL Server to HDInsight over Azure?","body":"<p>I am trying to Sqoop data from a local on-site Microsoft SQL server to Hive in HDinsights in Azure.</p><p>Inside Secure Shell in HDInsight, I CAN sqoop data in from Azure SQL Server but UNABLE to sqoop data from SQL Server locally.</p><p>I can also setup Azure SQL Server, setup firewall setting and synchronize with local SQL Server. I cannot connect/ping the external SQL Server's ip.</p><p>- why can't my sqoop connect to the local SQL Server?</p><p>- are there any firewalls setup in HDInsight that prevents connection?</p>","tags":["Sqoop","azure","hdinsight"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-15 21:45:21.0","id":6609,"title":"ODBC connection unsuccessful in Mac when connecting to Hive in HDInsight over Azure!","body":"<p>Folks,</p><p>I need to connect a mac client to Hive in HDInsights over Azure. Installed Mac ODBC Drivers from [1], and use [2] to configure the ODBC driver. However, the test is failing.</p><p>[Hortonworks][HiveODBC](22) Error from the server:connect() failed: Operation Timed Out.</p><p>[1] <a href=\"http://hortonworks.com/hdp/addons/\">http://hortonworks.com/hdp/addons/</a> </p><p>[2] <a href=\"https://azure.microsoft.com/en-us/documentation/articles/hdinsight-connect-excel-hive-odbc-driver/\">https://azure.microsoft.com/en-us/documentation/ar...</a></p>","tags":["osx","odbc","azure","hdinsight"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-07 14:21:02.0","id":38202,"title":"How to install and use SANDBOX on a remote Virtual Dedicated Server VDS (cent os 64)","body":"<p>I try to install and use sandbox on remote VDS (cent os 64).</p><p>How to proceed?</p>","tags":["Sandbox","installation"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-07 16:36:31.0","id":38245,"title":"secondary name node stopped","body":"<p>Secondary name node  stopped and i have an error while restarting which says </p><p>ERROR namenode.SecondaryNameNode (LogAdapter.java:error(69)) - RECEIVED SIGNAL 15: SIGTERM </p>","tags":["namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-07 18:00:24.0","id":38278,"title":"Prevent host failure  or Ambari inaccessibility during docker upgrade","body":"<p>When you do a 'yum update', after you install some new software on one or more nodes on the cluster, it does upgrade the docker container was well. So will this not cause any issues like node failure from cloudbreak shell or CLI?</p>","tags":["docker","Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-07 20:32:57.0","id":38318,"title":"Pipeline not working","body":"<p>Hello guys,</p><p>I am with two feeds and one process.\nThe feeds are running okay, but the process is stuck....it never ends and is taking too much Yarn memory.\nIs it a configuration problem?\nHere are the xmls:</p><p>&lt;feed xmlns='uri:falcon:feed:0.1' name='input' description='input'&gt;\n  &lt;tags&gt;input=input&lt;/tags&gt;\n  &lt;groups&gt;input&lt;/groups&gt;\n  &lt;frequency&gt;hours(1)&lt;/frequency&gt;\n  &lt;timezone&gt;UTC&lt;/timezone&gt;\n  &lt;clusters&gt;\n    &lt;cluster name='primaryCluster' type='source'&gt;\n      &lt;validity start='2016-06-07T17:00Z' end='2016-06-08T16:35Z'/&gt;\n      &lt;retention limit='months(1)' action='delete'/&gt;\n      &lt;locations&gt;\n        &lt;location type='data'&gt;\n        &lt;/location&gt;\n        &lt;location type='stats'&gt;\n        &lt;/location&gt;\n        &lt;location type='meta'&gt;\n        &lt;/location&gt;\n      &lt;/locations&gt;\n    &lt;/cluster&gt;\n  &lt;/clusters&gt;\n  &lt;locations&gt;\n    &lt;location type='data' path='/user/ambari-qa/falcon/input/${YEAR}-${MONTH}-${DAY}-${HOUR}'&gt;\n    &lt;/location&gt;\n    &lt;location type='stats' path='/'&gt;\n    &lt;/location&gt;\n    &lt;location type='meta' path='/'&gt;\n    &lt;/location&gt;\n  &lt;/locations&gt;\n  &lt;ACL owner='ambari-qa' group='users' permission='0755'/&gt;\n  &lt;schema location='/none' provider='/none'/&gt;\n  &lt;properties&gt;\n    &lt;property name='jobPriority' value='NORMAL'&gt;\n    &lt;/property&gt;\n    &lt;property name='timeout' value='minutes(3)'&gt;\n    &lt;/property&gt;\n  &lt;/properties&gt;\n&lt;/feed&gt;</p><p>&lt;feed xmlns='uri:falcon:feed:0.1' name='output' description='output'&gt;\n  &lt;tags&gt;output=output&lt;/tags&gt;\n  &lt;groups&gt;output&lt;/groups&gt;\n  &lt;frequency&gt;hours(1)&lt;/frequency&gt;\n  &lt;timezone&gt;UTC&lt;/timezone&gt;\n  &lt;clusters&gt;\n    &lt;cluster name='primaryCluster' type='source'&gt;\n      &lt;validity start='2016-06-07T17:03Z' end='2016-06-08T16:39Z'/&gt;\n      &lt;retention limit='months(1)' action='delete'/&gt;\n      &lt;locations&gt;\n        &lt;location type='data'&gt;\n        &lt;/location&gt;\n        &lt;location type='stats'&gt;\n        &lt;/location&gt;\n        &lt;location type='meta'&gt;\n        &lt;/location&gt;\n      &lt;/locations&gt;\n    &lt;/cluster&gt;\n  &lt;/clusters&gt;\n  &lt;locations&gt;\n    &lt;location type='data' path='/user/ambari-qa/falcon/filtered/${YEAR}-${MONTH}-${DAY}-${HOUR}'&gt;\n    &lt;/location&gt;\n    &lt;location type='stats' path='/'&gt;\n    &lt;/location&gt;\n    &lt;location type='meta' path='/'&gt;\n    &lt;/location&gt;\n  &lt;/locations&gt;\n  &lt;ACL owner='ambari-qa' group='users' permission='0755'/&gt;\n  &lt;schema location='/none' provider='/none'/&gt;\n  &lt;properties&gt;\n    &lt;property name='jobPriority' value='NORMAL'&gt;\n    &lt;/property&gt;\n    &lt;property name='timeout' value='minutes(3)'&gt;\n    &lt;/property&gt;\n  &lt;/properties&gt;\n&lt;/feed&gt;</p><p>&lt;process xmlns='uri:falcon:process:0.1' name='process'&gt;\n  &lt;tags&gt;process=process&lt;/tags&gt;\n  &lt;clusters&gt;\n    &lt;cluster name='primaryCluster'&gt;\n      &lt;validity start='2016-06-07T17:05Z' end='2016-06-08T17:16Z'/&gt;\n    &lt;/cluster&gt;\n  &lt;/clusters&gt;\n  &lt;parallel&gt;1&lt;/parallel&gt;\n  &lt;order&gt;FIFO&lt;/order&gt;\n  &lt;frequency&gt;hours(1)&lt;/frequency&gt;\n  &lt;timezone&gt;UTC&lt;/timezone&gt;\n  &lt;inputs&gt;\n    &lt;input name='input' feed='input' start='now(0,0)' end='now(0,0)'&gt;\n    &lt;/input&gt;\n  &lt;/inputs&gt;\n  &lt;outputs&gt;\n    &lt;output name='output' feed='output' instance='now(0,0)'&gt;\n    &lt;/output&gt;\n  &lt;/outputs&gt;\n  &lt;workflow engine='pig' path='/user/ambari-qa/falcon/pig/ufForn.pig'/&gt;\n  &lt;retry policy='periodic' delay='minutes(30)' attempts='3'/&gt;\n  &lt;ACL owner='ambari-qa' group='users' permission='0755'/&gt;\n&lt;/process&gt;</p><p>And here the pig script:</p><p>\nA = load '$input' using PigStorage(';');\nB = filter A by (chararray) $2 != '-9';\nstore B into '$output' USING PigStorage(';');</p><p>Best</p>","tags":["Falcon","Oozie","YARN"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-07 19:19:53.0","id":38297,"title":"How to extract Ambari metrics using Hortonworks DataFlow ?","body":"<p>Hello,</p><p>In order to build a centralized monitoring platform using the ELK stack I'm trying to to get the metrics from Ambari using HDF.</p><p>For instance, using HDF (Nifi) I would like to get the \"cluster disk\" metrics, csv formatted as when you export them manually from Ambari dashboard  <a href=\"https://community.hortonworks.com/storage/attachments/2349-2200-1.png\">as seen here</a></p><p>- Has anyone already did that ?</p><p>- what is the REST-API call ?</p><p>- How do I have to configure my processor if the ambari server is configured to use HTTPS ?</p><p>Thxx!!</p>","tags":["Nifi","ambari-metrics","metrics-collector"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-08 06:36:14.0","id":38386,"title":"Is there an way to get the number of Apps completed by YARN in last 24 hours using ambari rest API","body":"<p>We are trying to build an dashboard with basic info and need info on Apps complete in last 24 hours or previous day. We are pulling the data from Ambari rest API. what fields and timing parameters would fetch me the above results. We are on Ambari 1.7</p>","tags":["ambari-metrics","ambari-service","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-08 08:27:44.0","id":38409,"title":"Not able to monitor consumer group lag with new consumer and kerberos","body":"<p>Hi,</p><div>I'm not able to use kafka-consumer-groups.sh to monitor the lag of my consumers when my cluster is kerberized.\n<div>I'm using kafka version 0.9.0 installed on an hortonworks hdp 2.4.0 cluster.\n<div>\nI've replicated my setup on two sandboxes, one without kerberos and one with kerberos. \n\n\nOn the one without kerberos, I'm able to get an answer from the consumer-groups client with the following command:<div>./kafka-consumer-groups.sh --list --new-consumer --bootstrap-server <a href=\"http://sandbox.hortonworks.com:6667\">sandbox.hortonworks.com:6667</a>\n\n\n<div>\nOn\n the one with kerberos activated, I've tried the same command without \nspecifying the security-protocol and also with specifying it (PLAINTEXTSASL or SASL_PLAINTEXT). I then see a \njava.io.EOFException when connecting to the broker.\n<div>\n\n<div>In both the cases \nwith kerberos, without activating the debug logs in the client, it \nappears to hang since it is stuck in an infinite loop of retrying to \nconnect with the broker (and failing)\n\n\n<div>Am\n I missing something in order to use this tool in a kerberized \nenvironment ? As of now, I'm not seeing any other way to monitor \nconsumer offsets, since they are not stored in zookeeper anymore.\n\n<div>\nThanks in advance,Pierre\n\n<p>\n****** Full stack of execution with kerberos and security-protocol specified ******</p><pre>./kafka-consumer-groups.sh --new-consumer --security-protocol \nSASL_PLAINTEXT --bootstrap-server ntybd-dn1.i-tracing.lan:6667 --list\n[2016-06-08 08:25:55,746] INFO Successfully logged in. (org.apache.kafka.common.security.kerberos.Login)\n[2016-06-08 08:25:55,747] DEBUG It is a Kerberos ticket (org.apache.kafka.common.security.kerberos.Login)\n[2016-06-08 08:25:55,751] INFO TGT refresh thread started. (org.apache.kafka.common.security.kerberos.Login)\n[2016-06-08 08:25:55,769] DEBUG Found TGT Ticket (hex) = \n0000: 61 82 04 9F 30 82 04 9B  A0 03 02 01 05 A1 10 1B  a...0...........\n0010: 0E 49 2D 54 52 41 43 49  4E 47 2E 44 41 54 41 A2  .I-TRACING.DATA.\n0020: 23 30 21 A0 03 02 01 02  A1 1A 30 18 1B 06 6B 72  #0!.......0...kr\n0030: 62 74 67 74 1B 0E 49 2D  54 52 41 43 49 4E 47 2E  btgt..I-TRACING.\n0040: 44 41 54 41 A3 82 04 5B  30 82 04 57 A0 03 02 01  DATA...[0..W....\n0050: 12 A1 03 02 01 02 A2 82  04 49 04 82 04 45 FF D9  .........I...E..\n0060: B6 DC 59 08 DD 44 99 36  78 DC 51 23 11 EA FC F2  ..Y..D.6x.Q#....\n0070: F3 00 5B 21 4C 0C 78 8C  CE 31 AD A9 A1 1A 83 9C  ..[!L.x..1......\n0080: 34 0B 4E 97 10 6B 5E 52  C3 CE 55 34 3A 00 BB 5A  4.N..k^R..U4:..Z\n0090: F3 2F 85 68 D9 10 8B 21  1C 3A 3B 9D 51 AF 15 FF  ./.h...!.:;.Q...\n00A0: D3 6B 80 B4 0C 26 B3 80  29 21 49 1C 83 0D 11 B3  .k...&..)!I.....\n00B0: BA E4 F7 C9 87 24 10 29  63 26 DF 76 60 F2 DA E5  .....$.)c&.v`...\n00C0: F2 3F E6 1D 6D 5B 40 54  80 79 47 AE F1 6F 41 1A  .?..m[@T.yG..oA.\n00D0: C5 D0 8B 4A E6 6A FE DC  2B BB DD 7D D2 49 E8 8A  ...J.j..+....I..\n00E0: C8 2E 5E A6 33 DD 83 D2  1D 99 D9 66 61 F2 B1 C6  ..^.3......fa...\n00F0: BE 32 CD D0 91 88 D5 8B  E0 C0 47 1B 1B 22 32 B6  .2........G..\"2.\n0100: 18 45 F1 AB CC 86 BC 88  83 AE 61 CE 53 B1 97 B5  .E........a.S...\n0110: C7 14 30 16 1A 6D A2 F8  27 6B A2 6C DC EF E0 49  ..0..m..'k.l...I\n0120: 85 96 3C 71 96 10 C6 56  3C 94 A6 20 C8 DC 77 9D  ..&lt;q...V&lt;.. ..w.\n0130: 13 0B 3F 2F 01 82 7F 5C  BD 73 84 2E 6A 48 1D 56  ..?/...\\.s..jH.V\n0140: F7 84 93 D7 5C F1 C9 60  3D 18 D5 A5 7B 44 60 DE  ....\\..`=....D`.\n0150: DC F6 53 1F EA E7 EA 10  44 47 1D 9D CD DE D6 25  ..S.....DG.....%\n0160: C9 01 28 33 35 5B 19 96  10 DE 87 79 28 DB 8C 18  ..(35[.....y(...\n0170: F0 20 08 12 58 50 D7 7E  DC 4A CA BE 66 E0 B3 F9  . ..XP...J..f...\n0180: AA FE 3D F6 E5 DF EE B6  30 35 B4 98 81 17 A8 2F  ..=.....05...../\n0190: 3A 40 BF 4E 58 09 78 70  0A A5 91 95 9A AA 56 A2  :@.NX.xp......V.\n01A0: 7B DC 1B 3F BC 9E 37 0C  0E 0B 94 81 39 EA E2 21  ...?..7.....9..!\n01B0: 69 59 9B 30 F6 B4 94 A5  01 CA A4 2A FF FE 27 8B  iY.0.......*..'.\n01C0: B0 F7 40 A3 BD F6 A5 9D  5D 17 64 4F 99 CC 1E 17  ..@.....].dO....\n01D0: 2D 8C 95 0A 11 2D 47 3D  5A D0 60 32 D1 AB 86 61  -....-G=Z.`2...a\n01E0: 58 59 F2 7C 76 E7 AF 5C  46 C9 94 E4 4E E1 82 6F  XY..v..\\F...N..o\n01F0: BD EC 9A 12 06 1D 84 BE  A2 DB F4 5D 5A 38 8B D7  ...........]Z8..\n0200: F5 EE 8E F5 4F 9D FC BD  A5 27 A9 B5 62 69 43 A7  ....O....'..biC.\n0210: F4 1C 66 6A 97 5C D7 D5  5A 4E 97 21 38 5D D6 B7  ..fj.\\..ZN.!8]..\n0220: FD AE 26 16 C1 DE DA 13  AB 8F 0E 48 34 F9 36 AE  ..&........H4.6.\n0230: 1A A9 EA 38 AA 6E 6E D6  7C AF 94 A1 92 ED 75 66  ...8.nn.......uf\n0240: E6 36 26 3A 05 D1 A9 BF  DA 2F 82 8B E7 C5 3A 2F  .6&:...../....:/\n0250: 93 6D 7E 7B CF B0 CE AE  8D 58 EC 47 C0 75 CC 39  .m.......X.G.u.9\n0260: F3 EF B7 F9 72 F4 F6 8D  F9 0C 1A B7 BF D7 FD 9A  ....r...........\n0270: 3D BC BB E2 B6 56 61 35  69 22 52 D1 FD 3A D0 A8  =....Va5i\"R..:..\n0280: EB B3 04 A7 39 75 C4 C6  12 78 BD AA AF 5A F5 D5  ....9u...x...Z..\n0290: 93 0F 8C 55 39 19 38 FC  52 E8 10 24 8D D7 36 EA  ...U9.8.R..$..6.\n02A0: 21 5D A4 92 DA 8C B9 A7  32 B6 44 81 BA 2C 52 0D  !]......2.D..,R.\n02B0: EF 80 7B B7 4F 04 58 75  57 5C F8 C4 E5 F9 B2 29  ....O.XuW\\.....)\n02C0: 8E C5 51 40 B1 AB DD A5  5F 57 8B C4 07 1A 98 5C  ..Q@...._W.....\\\n02D0: FC 41 23 35 7B 61 7D 53  89 6C 37 0D D9 63 1A D6  .A#5.a.S.l7..c..\n02E0: 27 92 E4 59 1F 81 66 0F  7F 9F BB 92 70 C2 F3 29  '..Y..f.....p..)\n02F0: 3D BF 3E 86 37 C9 C6 6C  9C 9B D2 A7 EB 25 79 00  =.&gt;.7..l.....%y.\n0300: 50 92 D9 23 F1 A4 18 B1  94 DF 7E 42 DF 6C F5 A9  P..#.......B.l..\n0310: 7D 32 02 FE AD AC 2B D8  55 3A D5 36 B3 98 37 35  .2....+.U:.6..75\n0320: 2B 7B DB 2F E2 1E 28 F7  65 40 68 66 2D 38 AB DA  +../..(.e@hf-8..\n0330: F9 CF 7C 9F F1 82 2B 60  C9 4F BC 1C 63 ED C4 D0  ......+`.O..c...\n0340: CE 07 71 CE BB 37 9E AC  6E 4A 72 DD A4 37 94 F6  ..q..7..nJr..7..\n0350: E0 62 B5 71 03 26 89 3B  F1 94 E3 6E C0 9A FE 8D  .b.q.&.;...n....\n0360: 3C 17 EB E1 6D C7 04 96  E6 50 79 A2 62 D4 20 3D  &lt;...m....Py.b. =\n0370: 21 CC 3F BC 98 57 14 4C  EE 00 1D AA A2 9B 44 05  !.?..W.L......D.\n0380: 05 AC 45 43 EA 4E A7 93  E5 BB DF 84 78 F6 4B F9  ..EC.N......x.K.\n0390: 86 CB E2 5E AA EB 95 42  97 B9 F1 0F A9 47 0B 11  ...^...B.....G..\n03A0: 9A E4 AC 12 29 99 1C D6  1A C9 A8 07 AB D6 27 2C  ....).........',\n03B0: 8B 1B 01 33 C7 D5 34 F2  70 BC 78 01 74 CC F4 F6  ...3..4.p.x.t...\n03C0: 36 5A 1A E7 C8 1C E2 AD  4F 72 77 F0 0A 30 C5 F7  6Z......Orw..0..\n03D0: 0D DB 34 4B 2F A0 CF 72  3D 31 39 EB 2A 40 FB 35  ..4K/..r=19.*@.5\n03E0: 7E 22 4A 8D 2F 83 D8 93  72 EE FC F5 A9 EF C2 12  .\"J./...r.......\n03F0: B6 9B 40 88 36 F3 29 3A  01 63 8B 60 F1 7A 01 54  ..@.6.):.c.`.z.T\n0400: 92 0D 7D 4B 99 FD 9D 4E  80 B8 2A C9 C5 E8 E9 A5  ...K...N..*.....\n0410: E6 F0 0A E4 AC A9 4B F7  21 20 FA 60 C3 29 33 5A  ......K.! .`.)3Z\n0420: 1E AC 39 3F 7D 82 AA 89  69 6C FA F3 4D 25 57 43  ..9?....il..M%WC\n0430: 38 52 6F D7 11 41 2D 33  8E 77 6A 6B D7 8C 7F E0  8Ro..A-3.wjk....\n0440: 8A 4E 28 A9 D7 5E 5E 46  6D 8C D3 15 FE 34 EF B2  .N(..^^Fm....4..\n0450: 8A 1F 23 71 71 27 ED 4A  25 B9 5B C0 31 24 2A E6  ..#qq'.J%.[.1$*.\n0460: 90 41 8D 30 28 58 9C 73  9A DA 28 48 A1 5F 7E 7C  .A.0(X.s..(H._..\n0470: F1 1A 89 6D BD D3 6B E2  31 03 E6 7D E6 24 23 98  ...m..k.1....$#.\n0480: B6 46 D5 F7 94 7F 17 AD  92 64 CE 4A CC AF ED 61  .F.......d.J...a\n0490: 10 77 AB 9F 7C 04 4D B1  21 3C 93 DF 98 2A 68 23  .w....M.!&lt;...*h#\n04A0: 84 0E D6  ...\n\nClient Principal = plabiausse@I-TRACING.DATA\nServer Principal = krbtgt/I-TRACING.DATA@I-TRACING.DATA\nSession Key = EncryptionKey: keyType=18 keyBytes (hex dump)=\n0000: 53 B2 21 06 91 AE 21 16  A7 BD 72 E0 3C 79 D2 FA  S.!...!...r.&lt;y..\n0010: 71 81 3E 5C E4 6E 14 67  6E F4 C2 C8 97 49 E5 11  q.&gt;\\.n.gn....I..\n\n\nForwardable Ticket true\nForwarded Ticket false\nProxiable Ticket false\nProxy Ticket false\nPostdated Ticket false\nRenewable Ticket true\nInitial Ticket true\nAuth Time = Wed Jun 08 07:23:44 UTC 2016\nStart Time = Wed Jun 08 07:23:44 UTC 2016\nEnd Time = Wed Jun 08 17:23:44 UTC 2016\nRenew Till = Wed Jun 15 07:23:42 UTC 2016\nClient Addresses  Null . (org.apache.kafka.common.security.kerberos.Login)\n[2016-06-08\n 08:25:55,770] INFO TGT valid starting at: Wed Jun 08 07:23:44 UTC 2016 \n(org.apache.kafka.common.security.kerberos.Login)\n[2016-06-08 08:25:55,770] INFO TGT expires: Wed Jun 08 17:23:44 UTC 2016 (org.apache.kafka.common.security.kerberos.Login)\n[2016-06-08\n 08:25:55,771] ERROR The TGT cannot be renewed beyond the next expiry \ndate: Wed Jun 08 17:23:44 UTC 2016.This process will not be able to \nauthenticate new SASL connections after that time (for example, it will \nnot be able to authenticate a new connection with a Kafka Broker).  Ask \nyour system administrator to either increase the 'renew until' time by \ndoing : 'modprinc -maxrenewlife null ' within kadmin, or instead, to \ngenerate a keytab for null. Because the TGT's expiry cannot be further \nextended by refreshing, exiting refresh thread now. \n(org.apache.kafka.common.security.kerberos.Login)\n[2016-06-08 \n08:25:55,790] DEBUG Updated cluster metadata version 1 to Cluster(nodes =\n [Node(-1, ntybd-dn1.i-tracing.lan, 6667)], partitions = []) \n(org.apache.kafka.clients.Metadata)\n[2016-06-08 08:25:55,794] DEBUG Added sensor with name connections-closed: (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,796] DEBUG Added sensor with name connections-created: (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,796] DEBUG Added sensor with name bytes-sent-received: (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,796] DEBUG Added sensor with name bytes-sent: (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,796] DEBUG Added sensor with name bytes-received: (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,796] DEBUG Added sensor with name select-time: (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,796] DEBUG Added sensor with name io-time: (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08\n 08:25:55,817] DEBUG Initiating connection to node -1 at \nntybd-dn1.i-tracing.lan:6667. (org.apache.kafka.clients.NetworkClient)\n[2016-06-08 08:25:55,867] DEBUG Added sensor with name node--1.bytes-sent (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,867] DEBUG Added sensor with name node--1.bytes-received (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,867] DEBUG Added sensor with name node--1.latency (org.apache.kafka.common.metrics.Metrics)\n[2016-06-08 08:25:55,867] DEBUG Completed connection to node -1 (org.apache.kafka.clients.NetworkClient)\n[2016-06-08\n 08:25:55,917] DEBUG Sending metadata request \nClientRequest(expectResponse=true, callback=null, \nrequest=RequestSend(header={api_key=3,api_version=0,correlation_id=3,client_id=admin-1},\n body={topics=[]}), isInitiatedByNetworkClient, \ncreatedTimeMs=1465374355917, sendTimeMs=0) to node -1 \n(org.apache.kafka.clients.NetworkClient)\n[2016-06-08 08:25:55,920] \nDEBUG Connection with ntybd-dn1.i-tracing.lan/172.29.129.69 disconnected\n (org.apache.kafka.common.network.Selector)\njava.io.EOFException\n   at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:83)\n   at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:71)\n   at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:160)\n   at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:141)\n   at org.apache.kafka.common.network.Selector.poll(Selector.java:286)\n   at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:270)\n   at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)\n   at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)\n   at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)\n   at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)\n   at kafka.admin.AdminClient.kafka$admin$AdminClient$$send(AdminClient.scala:52)\n   at kafka.admin.AdminClient$$anonfun$sendAnyNode$1.apply(AdminClient.scala:67)\n   at kafka.admin.AdminClient$$anonfun$sendAnyNode$1.apply(AdminClient.scala:64)\n   at scala.collection.immutable.List.foreach(List.scala:318)\n   at kafka.admin.AdminClient.sendAnyNode(AdminClient.scala:64)\n   at kafka.admin.AdminClient.findAllBrokers(AdminClient.scala:93)\n   at kafka.admin.AdminClient.listAllGroups(AdminClient.scala:101)\n   at kafka.admin.AdminClient.listAllGroupsFlattened(AdminClient.scala:122)\n   at kafka.admin.AdminClient.listAllConsumerGroupsFlattened(AdminClient.scala:126)\n   at kafka.admin.ConsumerGroupCommand$KafkaConsumerGroupService.list(ConsumerGroupCommand.scala:322)\n   at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:73)\n   at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)\n[2016-06-08 08:25:55,924] DEBUG Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2016-06-08\n 08:25:55,924] TRACE Cancelled request \nClientRequest(expectResponse=true, \ncallback=org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler@6e0dec4a,\n \nrequest=RequestSend(header={api_key=3,api_version=0,correlation_id=2,client_id=admin-1},\n body={topics=[]}), createdTimeMs=1465374355917, \nsendTimeMs=1465374355919) due to node -1 being disconnected \n(org.apache.kafka.clients.NetworkClient)\n[2016-06-08 08:25:55,924] \nTRACE Cancelled request ClientRequest(expectResponse=true, \ncallback=null, \nrequest=RequestSend(header={api_key=3,api_version=0,correlation_id=3,client_id=admin-1},\n body={topics=[]}), isInitiatedByNetworkClient, \ncreatedTimeMs=1465374355917, sendTimeMs=1465374355917) due to node -1 \nbeing disconnected (org.apache.kafka.clients.NetworkClient)\n[2016-06-08\n 08:25:55,925] DEBUG Give up sending metadata request since no node is \navailable (org.apache.kafka.clients.NetworkClient)</pre>\n</div></div></div></div></div></div></div></div></div>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-08 11:52:16.0","id":38449,"title":"Use solr with kerberos","body":"<p>Hi <a rel=\"user\" href=\"/users/113/jstraub.html\" nodeid=\"113\">@Jonas Straub</a> and <a rel=\"user\" href=\"/users/132/abajwa.html\" nodeid=\"132\">@Ali Bajwa</a>,</p><p>I read <a target=\"_blank\" href=\"https://community.hortonworks.com/articles/15159/securing-solr-collections-with-ranger-kerberos.html\">this</a> article, but if I run any curl query solr returned:</p><pre>WARN  org.apache.hadoop.security.authentication.server.AuthenticationFilter  [   ] – Authentication exception: GSSException: No valid credentials provided (Mechanism level: Attempt to obtain new ACCEPT credentials failed!)\norg.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Attempt to obtain new ACCEPT credentials failed!)</pre><p>I have set correctly jaas.conf file and I have upload on zookeeper this:</p><pre>/zkcli.sh -zkhost tst-master1:2181,tst-master3:2181,tst-master2:2181 -cmd put /solr/security.json '{\"authentication\":{\"class\": \"org.apache.solr.security.KerberosPlugin\"}}'</pre><p>I have omitted the part related to ranger because it is not used in this cluster.</p><p>thanks in advance.</p><p>P.S.: This is my klist output:</p><pre>[solr@tst-master1 ~]$ klist\nTicket cache: FILE:/tmp/krb5cc_6002\nDefault principal: solr/tst-master1@MYREALM.COM\nValid starting     Expires            Service principal\n06/08/16 13:27:53  06/09/16 13:27:53  krbtgt/MYREALM.COM@MYREALM.COM\n06/08/16 13:28:03  06/09/16 13:27:53  HTTP/tst-master1@MYREALM.COM</pre>","tags":["SOLR","solrcloud","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-12 03:45:10.0","id":39208,"title":"Choice for a Web Application Cache","body":"<p>Would you use HBase?</p><p>Or an in-memory grid like Apache Ignite or Apache Geode.</p>","tags":["java","ignite","geode","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-15 06:12:21.0","id":39829,"title":"Illegal Client token while running mapreduce job in kerberos enabled cluster","body":"<p>\n\tI have enabled kerberos using Ambari in HDP 2.4 . I have also enabled debugging while running the job using the yar job submission command \"yarn --loglevel DEBUG jar &lt;jarname&gt; \" . I get the following WARNING, though the job succeeds. </p><p>I was wondering , why <strong>auth:SIMPLE </strong>is set in a kerberos enabled cluster. Also I observed that the auth:KERBEROS is set once the job reached the COMPLETION state and is then able to connect successfully to the MR App server.</p><pre>\t16/06/15 05:37:40 DEBUG security.UserGroupInformation: PrivilegedActionException as:shad@hortonworks.com (auth:SIMPLE) cause:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Illegal client-token!\n16/06/15 05:37:40 DEBUG security.UserGroupInformation: PrivilegedAction as:shad@hortonworks.com (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:648)\n16/06/15 05:37:40 WARN ipc.Client: Exception encountered while connecting to the server : \norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Illegal client-token!\nat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)\nat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:558)\nat org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:373)\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:727)\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:723)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)</pre><p>After this I get the following log...</p><pre>16/06/15 05:37:50 DEBUG security.UserGroupInformation: PrivilegedAction as:shad@hortonworks.com (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:722)\n16/06/15 05:37:50 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE\n16/06/15 05:37:50 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE\nauths {\n  method: \"TOKEN\"\n  mechanism: \"DIGEST-MD5\"\n  protocol: \"\"\n  serverId: \"default\"\n  challenge: \"realm=\\\"default\\\",nonce=\\\"OtFuCLYWc7p4ugsdMub70Nr/Nq8gtmWB6l8I4UsG\\\",qop=\\\"auth\\\",charset=utf-8,algorithm=md5-sess\"\n}\nauths {\n  method: \"KERBEROS\"\n  mechanism: \"GSSAPI\"\n  protocol: \"jhs\"\n  serverId: \"sandbox.hortonworks.com\"\n}\n16/06/15 05:37:50 DEBUG security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB info:org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo$2@2009f5c4</pre>","tags":["MapReduce","YARN","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-15 03:32:47.0","id":39793,"title":"Zeppelin Spark Maxmind jackson.databind NoSuchMethodError","body":"<p>Hi, I'm experimenting with using Zeppelin / Spark to perform geo-location on IP addresses using the Maxmind GeoIP library. I am encountering a NoSuchMethodError which from reading the forums appears to be a dependency issue with the method not being in certain versions of the jackson lib. How can I go about identifying and resolving this dependency issue in Zeppelin? I load geoip2 via %dep and have removed the older versions of the jackson lib from zeppelin/lib/lib to no avail. Thanks!</p><pre>%dep\nz.addRepo(\"geoip2\").url(\"http://mvnrepository.com/artifact/com.maxmind.geoip2/geoip2/2.7.0\")\nz.load(\"com.maxmind.geoip2:geoip2:2.7.0\")</pre><pre>java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.node.ArrayNode.&lt;init&gt;(Lcom/fasterxml/jackson/databind/node/JsonNodeFactory;Ljava/util/List;)V</pre>","tags":["zeppelin","Spark","dependency"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-15 07:00:33.0","id":39847,"title":"Name node heap size inceases and decreases gradually. may i know the possible reason behind it?","body":"<p><a rel=\"user\" href=\"/users/140/nsabharwal.html\" nodeid=\"140\">@Neeraj Sabharwal</a></p>","tags":["heap"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-15 08:48:36.0","id":39875,"title":"Datanode heap exhaustion","body":"<p>I recently upgraded our cluster from HDP2.2 to HDP2.3.4.7, and I'm now getting datanode \"heap alerts\" in Ambari after the processes have been running for a few days.  I've increased the heap memory from 1G -&gt; 2G -&gt; 3G, but it seems like it's not making much (if any) difference.</p><p>The datanodes have just over 0.5M blocks on them.  (Is this too high? I just noticed CDH generates an alarm if a datanode goes above 0.5M blocks...)</p><p>Any tips on how to troubleshoot this?  I'm not entirely clear if this is expected behavior for the datanode in HDP2.3 (does it really need a lot more memory?) or if there's something else going on.</p><p>(At this point I've been doing a rolling restart of my datanodes to get the heap back)</p><p>Also, in case it's useful:</p><p>jmap -heap output for the datanode process:</p><p>--</p><p>Heap Usage:\nNew Generation (Eden + 1 Survivor Space):\n   capacity = 188743680 (180.0MB)\n   used     = 57840904 (55.16138458251953MB)\n   free     = 130902776 (124.83861541748047MB)\n   30.645213656955296% used</p><p>Eden Space:\n   capacity = 167772160 (160.0MB)\n   used     = 36869384 (35.16138458251953MB)\n   free     = 130902776 (124.83861541748047MB)\n   21.975865364074707% used</p><p>From Space:\n   capacity = 20971520 (20.0MB)\n   used     = 20971520 (20.0MB)\n   free     = 0 (0.0MB)\n   100.0% used</p><p>To Space:\n   capacity = 20971520 (20.0MB)\n   used     = 0 (0.0MB)\n   free     = 20971520 (20.0MB)\n   0.0% used</p><p>concurrent mark-sweep generation:\n   capacity = 2877292544 (2744.0MB)\n   used     = 2615359104 (2494.2008056640625MB)\n   free     = 261933440 (249.7991943359375MB)\n   90.89653081866118% used</p><p>-- ps:</p><p>hdfs     20635  4.7  4.7 5005732 3144808 ?     Sl   Jun12 152:17 jsvc.exec -Dproc_datanode -outfile /var/log/hadoop/hdfs/jsvc.out -errfile /var/log/hadoop/hdfs/jsvc.err -pidfile /var/run/hadoop/hdfs/hadoop_secure_dn.pid -nodetach -user hdfs -cp /usr/hdp/current/hadoop-client/conf:/usr/hdp/2.3.4.7-4/hadoop/lib/*:/usr/hdp/2.3.4.7-4/hadoop/.//*:/usr/hdp/2.3.4.7-4/hadoop-hdfs/./:/usr/hdp/2.3.4.7-4/hadoop-hdfs/lib/*:/usr/hdp/2.3.4.7-4/hadoop-hdfs/.//*:/usr/hdp/2.3.4.7-4/hadoop-yarn/lib/*:/usr/hdp/2.3.4.7-4/hadoop-yarn/.//*:/usr/hdp/2.3.4.7-4/hadoop-mapreduce/lib/*:/usr/hdp/2.3.4.7-4/hadoop-mapreduce/.//* -Xmx3072m -Dhdp.version=2.3.4.7-4 -Djava.net.preferIPv4Stack=true -Dhdp.version= -Djava.net.preferIPv4Stack=true -Dhdp.version= -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop/ -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.3.4.7-4/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/2.3.4.7-4/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.3.4.7-4/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhdp.version=2.3.4.7-4 -Dhadoop.log.dir=/var/log/hadoop/ -Dhadoop.log.file=hadoop-hdfs-datanode-kibo-b2.int.mathereconomics.com.log -Dhadoop.home.dir=/usr/hdp/2.3.4.7-4/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,RFA -Djava.library.path=:/usr/hdp/2.3.4.7-4/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.3.4.7-4/hadoop/lib/native:/usr/hdp/2.3.4.7-4/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.3.4.7-4/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop/hdfs -Dhadoop.id.str=hdfs -jvm server -server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop//hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop//gc.log-201606122237 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms2944m -Xmx2944m -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop//hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop//gc.log-201606122237 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms2944m -Xmx2944m -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop//hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop//gc.log-201606122237 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms2944m -Xmx2944m -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter</p>","tags":["java","datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-15 10:10:26.0","id":39898,"title":"All the control node services showing stopped but actually all the services are up and running","body":"<p>Hi Team ,</p><p>I have a 10 node cluster ( control , 3 edge nodes , 6 worker nodes) where I am finding all the services stopped on Control Node </p><p>Even If I manually start the NN, SNN , again its going back to stopped state after some time and could not see that from logs  .</p><p>I have checked the logs but nothing exception found  . Attaching the Ambari UI .Does anything issue with ports or with Ambari</p><p>Can someone help  on the below concern and appreciate your help!! </p><p>Thanks</p><p>Hari</p><p><img src=\"/storage/attachments/5029-dev08.png\"></p>","tags":["hadoop-ecosystem"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-15 15:21:52.0","id":39937,"title":"Cannot connect 'Streaming Data Ingest' to secured Hive instance","body":"<p>I've downloaded the Hortonworks Sandbox 2.4 to develop some tools locally on my machine. One of the first things I want to do is load data into Hive. I've first tried to the regular JDBC connector, which worked but was way to slow. </p><p>When doing this I ran across the first interesting issue: the sandbox has authentication enabled and controlled by Ranger. So when I connect using beeline and the URL jdbc:hive2://localhost:10000 I was asked for username and password. However, when connecting from Java, this was not required and could read and insert data. Can someone explain this?</p><pre>public DataSource dataSource() {\n\treturn new SimpleDriverDataSource(new HiveDriver(), \"jdbc:hive2://localhost:10000/variantdatabase\");\n}</pre><p>Then I learned about the streaming API which seemed a better alternative for loading lot's of data into Hive ( regular load file doesn't work for me ). So I started following this article: https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest#StreamingDataIngest-StreamingMutationAPI .</p><p>Relevant code:</p><pre>HiveEndPoint hiveEP = new HiveEndPoint(\"hive2://localhost:10000\", \"variantdatabase\", \"variant\", null);this.connection = hiveEP.newConnection(true);</pre><p>However, connecting takes ages, and after a while I get the following message in the client:</p><pre>17:03:47.742 [main] INFO  org.apache.hive.jdbc.HiveConnection - Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10000/variantdatabase\n17:03:48.518 [main] DEBUG o.a.h.h.streaming.HiveEndPoint - Overriding HiveConf setting : hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\n17:03:48.519 [main] DEBUG o.a.h.h.streaming.HiveEndPoint - Overriding HiveConf setting : hive.support.concurrency = true\n17:03:48.519 [main] DEBUG o.a.h.h.streaming.HiveEndPoint - Overriding HiveConf setting : hive.metastore.execute.setugi = true\n17:03:48.519 [main] DEBUG o.a.h.h.streaming.HiveEndPoint - Overriding HiveConf setting : hive.execution.engine = mr\n17:03:48.706 [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17:03:48.735 [main] INFO  hive.metastore - Trying to connect to metastore with URI hive2://localhost:10000\n17:13:48.814 [main] WARN  hive.metastore - set_ugi() not successful, Likely cause: new client talking to old server. Continuing without it.\norg.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\nat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129) ~[hive-exec-1.2.1.jar:1.2.1]</pre><p>When I look in the server log it says something about SASL, but I don't understand why, because JDBC didn't need it? And where I can define any username/password?</p><pre>Caused by: org.apache.thrift.transport.TTransportException: Invalid status -128\n        at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)\n        at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:184)\n        at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125)</pre>","tags":["streaming"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-08 14:42:41.0","id":1115,"title":"Ambari 2.1.1 decommission a node","body":"<p>in Ambari 2.1.1, I have to decommission each individual components, is there a way to decommission the whole node all in one click?</p>","tags":["ambari-2.1.1","operations","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-06 09:54:28.0","id":953,"title":"Can NiFi be used to pipe the data from Oracle Database to Kafka ?","body":"<p>I want to  ingest data from Oracle database to Kafka using Logstash. Wondering if NiFi can  perform the ingestion from relational database to Kafka ?</p>","tags":["data-ingestion","Kafka","hdf","Nifi","oracle"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-08 13:31:40.0","id":1109,"title":"Per node label capacity queues HDP 2.3","body":"<p>Does HDP 2.3 support per node label capacity scheduler queues? We'd like to be able to label nodes as \"high memory\", \"dense storage\", etc and split the workload within those nodes to maintain SLAs.</p>","tags":["yarn-scheduler","hdp-2.3.0","YARN","yarn-node-labels"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-05 19:45:42.0","id":918,"title":"What kind of speed/throughput should I be expecting to AWS S3?","body":"","tags":["performance","aws","s3"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-07 15:00:09.0","id":1060,"title":"How to improve the performance of NFS Server?","body":"<p>A customer is experiencing slowness with NFS when copying large files through NFS. </p><p>Question: </p><p>1) Is NFS recommended for large files? </p><p>2) What performance tuning options are available for NFS Server?</p>","tags":["nfs","performance","HDFS"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-12 18:50:05.0","id":1302,"title":"Running NiFi in Azure","body":"<p>Are there any gotchas, known issues or tips for deploying NiFi in Azure? I expect that to be a very straightforward one (especially if everything is further wrapped in a docker container), but please share if there are any bits of wisdom around.</p>","tags":["hdf","azure","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-08 18:29:56.0","id":1142,"title":"Does NiFi have the ability to connect to IBM MQ?","body":"<p>Does NiFi have the ability to connect to IBM MQ? According to the JMS Processor, only Active MQ is available as an option.</p><p>Should we write our own custom Processor for this?</p><p><img src=\"/storage/attachments/220-jmsprovider2.png\"></p>","tags":["mq","Nifi","jms","ibm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-12 20:46:57.0","id":1315,"title":"Is there a connector to HDP from Office 365 PowerBI?","body":"<p><a href=\"/storage/attachments/236-powerbidatabases.png\">powerbidatabases.png</a>Within the Power BI workspace in Office 365 I'm only seeing connectors for Azure SQL Database, Azure SQL Data Warehouse, SSAS, and Spark on Azure HDInsight. I'm not seeing a connector to Hadoop or HDI.  Is this feature not available?</p>","tags":["microsof","office365","hdi","hdinsight"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-09-30 15:09:31.0","id":626,"title":"How do I resolve maven dependencies for building application on HDP stack?","body":"<p>Two ways to deal with this\n</p><ol>\n<li>If you have a internal maven repository manger like Sonatype Nexus, one can setup a proxy pointing to http://repo.hortonworks.com/content/repositories/releases/ and internally add it to the nexus maven repository groups. </li><li>If one doesn't have any type of maven repository manager they can directly use the public groups url \"http://repo.hortonworks.com/content/groups/public/\" in there maven settings.xml as a mirror </li></ol>","tags":["development","maven","repository"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-10-12 23:20:57.0","id":1322,"title":"LDAP enabled hiveserver2 for HDP2.3 doesn't error out for incorrect users or passwords.  Any additional configs needed?","body":"<p>User set up hiveserver2 authentication on HDP-2.3 to use LDAP.  However, when typing in a bogus username or password, beeline does not error out like it did before in HDP-2.1.  The same LDAP server is in use in both the older and newer installations. </p><p>On HDP-2.3, this is what we see when typing in a bad username or password:</p><pre>[root@deepal-h2-namenode ~]# beeline \nWARNING: Use \"yarn jar\" to launch YARN applications. Beeline version 1.2.1.2.3.0.0-2557 by Apache Hive \nbeeline&gt; !connect jdbc:hive2://localhost:10000 Connecting to jdbc:hive2://localhost:10000 \nEnter username for jdbc:hive2://localhost:10000: hive \nEnter password for jdbc:hive2://localhost:10000: ******************* \nConnected to: Apache Hive (version 1.2.1.2.3.0.0-2557) Driver: Hive JDBC (version 1.2.1.2.3.0.0-2557) Transaction isolation: TRANSACTION_REPEATABLE_READ \n0: jdbc:hive2://localhost:10000&gt; \n</pre><p>On HDP-2.1, we at least get an error message doing the same thing:</p><pre>[root@deepal-h1-namenode ~]# beeline \nBeeline version 0.13.0.2.1.7.0-784 by Apache Hive \nbeeline&gt; !connect jdbc:hive2://localhost:10000 scan complete in 19ms Connecting to jdbc:hive2://localhost:10000 \nEnter username for jdbc:hive2://localhost:10000: hive \nEnter password for jdbc:hive2://localhost:10000: ***************** \nError: Could not open connection to jdbc:hive2://localhost:10000: Peer indicated failure: Error validating the login (state=08S01,code=0) \n0: jdbc:hive2://localhost:10000&gt; </pre><p>The hive-site.xml is also identical for both setups:</p><pre>    &lt;property&gt;\n      &lt;name&gt;hive.server2.authentication&lt;/name&gt;\n      &lt;value&gt;LDAP&lt;/value&gt;\n    &lt;/property&gt;\n    \n    &lt;property&gt;\n      &lt;name&gt;hive.server2.authentication.ldap.baseDN&lt;/name&gt;\n      &lt;value&gt;ou=People,dc=maxcrc,dc=com&lt;/value&gt;\n    &lt;/property&gt;\n    \n    &lt;property&gt;\n      &lt;name&gt;hive.server2.authentication.ldap.url&lt;/name&gt;\n      &lt;value&gt;ldap://WUSDS186049-2XJ.td.teradata.com:389/&lt;/value&gt;\n    &lt;/property&gt;\n</pre><p>Is there anything else that needs to be in place on HDP-2.3.0 so that we get the same behavior? </p>","tags":["ldap","Hive","hiveserver2","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-09 17:26:16.0","id":1216,"title":"Techniques for dealing with malformed data (Hive )","body":"<p>What are some best practices/techniques for ingesting malformed data into a Hive table? </p><p>For example, the CSV should have 15 columns (and thus 15 commas) but some rows only have 3 columns without having the additional 12 blank commas. </p>","tags":["data-model","etl","data-migration","Hive","best-practices"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-12 17:35:27.0","id":1298,"title":"Any best practice/methodology to migrate data from Riak database to Hbase?","body":"<p>How can we migrate data from Riak database to hbase? I have seen there are some riak hadoop connectors available. What is the impact of using it on production system. Any pointers?</p>","tags":["best-practices","Hbase","riak","data-migration"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-13 16:14:11.0","id":1345,"title":"switch to TEZ result in \"getDelegationToken() can be called only in thrift (non local) mode\" error","body":"<p>switch hive execution engine from MR to TEZ, but when executing the same query, got \"getDelegationToken() can be called only in thrift (non local) mode\", any idea?\n\nIt is HDP 2.2.8.</p>","tags":["Tez","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-03 00:22:30.0","id":830,"title":"How to Do I get the Hive JDBC Driver for My Client","body":"<p>I would like to use SquirrelSQL or Oracle SQL Developer or another SQL tool to connect to Hive via JDBC.  Where do I find the Hive JDBC Client?</p>","tags":["jdbc","sql","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-15 19:03:34.0","id":1538,"title":"How to Deregister Hiverserver2 from Zookeeper after a crash","body":"<p><a href=\"/storage/attachments/248-screen-shot-2015-10-15-at-22222-pm.png\"></a>HiveServer2 crashed and wasn't deregistered properly from  Zookeeper.</p><p>Now anytime I try restarting HiveServer2, it starts in Ambari, shows green but immediately shutdowns.</p>","tags":["hdp-2.3.0","zookeeper","Ambari","Hive","hiveserver2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-16 04:30:02.0","id":1565,"title":"Phoenix Query Server - Documentation","body":"<p>On our Dal Enablement session for Hbase, we saw a demo of Phoenix Query Server, with curl examples, as slide below.</p><p>Two questions, please:</p><p>1- Do we have any documentation for this? Page: https://phoenix.apache.org/server.html does not help a lot and clearly says that it might change in future.</p><p>2- PPT below says it is managed by ambari, but I couldnt find it. Is it available from Ambari?</p><p>I have a prospect really interested in using Phoenix + .NET</p><p>slide:</p><p><img src=\"/storage/attachments/251-phoenix-query-server.png\"></p><p>Thanks.</p>","tags":["documentation","Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-06 15:47:54.0","id":995,"title":"How to Configure Ranger Repository with Multiple HiveServer2 Instances","body":"<p>Hi,</p><p>How do you configure a Ranger Hive repository if you have two instances of HiveServer2 running in a cluster? </p><p>I'm assuming you don't create a separate repository for each one, so which jdbc URL do you provide to the repository configs? Do you perhaps have to have <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_hadoop-ha/content/ha-hs2-service-discovery.html\">Zookeeper discovery</a> enabled and then use that URL? What happens if you don't want to use the multiple HS2 instances in a failover kind of structure, but rather have them as two separate processes running at the same time? Is such a thing supported?</p><p>Thanks in advance,</p><p>Ana</p><p>EDIT: This post here has a great explanation of different JDBC URLs for ZK discovery: http://community.hortonworks.com/questions/1342/how-to-enable-zookeeper-discovery-for-hiveserver2.html</p>","tags":["high-availability","Ranger","hiveserver2","namenode-ha"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-15 01:56:02.0","id":1478,"title":"NiFi - How to use the GetSFTP processor without repeating files","body":"<p>Is there a way I can configure nifi, not to pull same files over? It appears to pull the same files more than once if some files in the directory were modified.</p><p>I.e. could you explain the full algorithm behavior on how GetSFTP tracks which files it has downloaded?</p><p>And how does this behave in case NiFi process or the server restarts?</p>","tags":["hdf","data-ingestion","Nifi","sftp"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-16 13:23:47.0","id":1581,"title":"Streaming Ingest - Transactions and Performance","body":"<p>When ingesting Streaming data from Kafka to Hbase (Using Storm bolt) , what is the recommended level of granularity for writes? Will writing individual records in Storm affect the latency of applications consuming data from the Hbase layer ? Should we design this layer with added micro batching ? If there are concrete numbers please share. </p>","tags":["Storm","data-ingestion","Kafka","performance","Hbase"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-09-26 23:53:08.0","id":347,"title":"A user using Spark on Hortonworks 2.3 gets a Spark memory error. What does it mean?","body":"<p>A user posted this question on <a target=\"_blank\" href=\"http://stackoverflow.com/questions/32802579/hortonworks-2-3-spark-memory-error?utm_source=twitterfeed&utm_medium=twitter\">StackOverFlow</a>.  When reading file into an RDD, the user gets the following stack trace. You can read more in the link.</p><pre>scala&gt; val inFile=sc.textFile(\"/user/root/MyDirectory/spam.data\") INFO memoryStore: ensureFreeSpace(215012) called with curMem=3395084, maxMem=278302556 INFO memoryStore: Bloack broadcast_13 stored as values in memory (estimated size 210.0 KB, free 262.0MB) Info memoryStore: ensureFreeSpace(46152) called with curMem=3610096, maxMem=278302556\nInfo MemoryStore: BloackMaanagerInfo broadcast_13_piece0 stored as bytes in memory (estimated size 45.1 KB, free:264.8\nMB) Info MemoryStore: BloackMaanagerInfo broadcast_13_piece0 stored in memory on (estimated size 45.1 KB, free:264.8 MB)\nsandbox.hortonworks.com:54460 (size 45.1 KB, free:264.8 MB) Info BlockManagerMaster: updated info of bloack broadcast_13_piece0\ninfile:org.apache.spark.rdd.RDD[String] = /user/root/MyDirectory/spam.data MapPartitionsRDD[27] at textFile at\n&lt; console &gt;:21\n</pre>","tags":["hdp-2.3.0","Sandbox","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-10-16 10:52:53.0","id":1573,"title":"How to handle kafka.common.OffsetOutOfRangeException in Storm ?","body":"<p>I am getting following error with Storm topology failing to read the kafka as the data from kafka topic as  data gets deleted based on retention(Size). Ideally we want to have storm to move (fast forward) to the first available message. Is there anyway to do this in Storm ?</p><p>[2015-10-13 14:12:28,204] ERROR [KafkaApi-2] Error when processing fetch request for partition [&lt;topic_name&gt;,0] offset 4231749539 from consumer with correlation id 0 (kafka.server.KafkaApis) \nkafka.common.OffsetOutOfRangeException: Request for offset 4231749539 but we only have log segments in the range 4255773954 to 4376049622. \nat kafka.log.Log.read(Log.scala:380) </p>","tags":["Storm","error","offset","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-20 15:06:44.0","id":1720,"title":"How to Map HBase Table to Phoenix (\"Table undefined\" error)","body":"<p>What is the proper way to map an existing HBase table to a new Phoenix table? Phoenix documentation gives a <a href=\"https://phoenix.apache.org/faq.html#How_I_map_Phoenix_table_to_an_existing_HBase_table\">light example</a> on how to do this. When we try this on existing HBase table the Phoenix \"create table\" command is accepted, however it fails with \"table undefined\" error when we try to query the new Phoenix table.</p><p>How could the create table syntax succeeed, yet the \"table undefined\" error occur when we query it?</p><p><strong>Hbase table definition</strong></p><p>tablename = weblog</p><p> Columnfamily = clicks </p><p>  column=clicks:Compression_ratio</p><p>  column=clicks:Cookie1</p><p>  column=clicks:Cookie2</p><p>  column=clicks:Data_center</p><p>  column=clicks:Host</p><p>  column=clicks:Incoming_client_protocol</p><p>  column=clicks:Rqst_status</p><p>  column=clicks:Tran_dt</p><p>  column=clicks:Xforwarder</p><p>  column=clicks:fdx_cbid</p><p>  column=clicks:header_size</p><p>  column=clicks:input_bytes</p><p>  column=clicks:millisecs_to_serv_rqst</p><p>  column=clicks:output_bytes</p><p>  column=clicks:referring_ip_addr</p><p>  column=clicks:rqst_first_ln</p><p>  column=clicks:unknown</p><p>  column=clicks:user_agent</p><p>  column=clicks:web_user</p><p>  column=clicks:web_user2</p><p><strong>View DDL used in phoenix</strong></p><p>CREATE VIEW \"weblog\" ( pk VARCHAR PRIMARY KEY,</p><p>\"clicks\".Compression_ratio VARCHAR,</p><p>\"clicks\".Cookie1 VARCHAR,</p><p>\"clicks\".Cookie2 VARCHAR,</p><p>\"clicks\".Data_center VARCHAR,</p><p>\"clicks\".Host VARCHAR,</p><p>\"clicks\".Incoming_client_protocol VARCHAR,</p><p>\"clicks\".Rqst_status VARCHAR,</p><p>\"clicks\".Tran_dt VARCHAR,</p><p>\"clicks\".Xforwarder VARCHAR,</p><p>\"clicks\".fdx_cbid VARCHAR,</p><p>\"clicks\".header_size VARCHAR,</p><p>\"clicks\".input_bytes VARCHAR,</p><p>\"clicks\".millisecs_to_serv_rqst VARCHAR,</p><p>\"clicks\".output_bytes VARCHAR,</p><p>\"clicks\".referring_ip_addr VARCHAR,</p><p>\"clicks\".rqst_first_ln VARCHAR,</p><p>\"clicks\".unknown VARCHAR,</p><p>\"clicks\".user_agent VARCHAR,</p><p>\"clicks\".web_user VARCHAR,</p><p>\"clicks\".web_user2 VARCHAR)</p><p><strong>List of tables from Phoenix</strong></p><p>0: jdbc:phoenix:drh70018.bigdata.fedex.com:21&gt; !tables</p><p>+------------------------------------------+------------------------------------------+-----------------------------------------+</p><p>|  TABLE_CAT  |  TABLE_SCHEM  |  TABLE_NAME   |</p><p>+------------------------------------------+------------------------------------------+-----------------------------------------+</p><p>| null  | SYSTEM  | CATALOG   |</p><p>| null  | SYSTEM  | SEQUENCE  |</p><p>| null  | SYSTEM  | STATS   |</p><p>| null  | null  | CUSTOMERS  |</p><p>| null  | null  | EXAMPLE   |</p><p>| null  | null  | WEB_STAT  |</p><p>| null  | null  | weblog</p><p><strong>Error from Phoenix:</strong></p><p>0: jdbc:phoenix:drh70018.bigdata.fedex.com:21&gt; select * from weblog;</p><p>Error: ERROR 1012 (42M03): Table undefined. tableName=WEBLOG (state=42M03,code=1012)</p><p>0: jdbc:phoenix:drh70018.bigdata.fedex.com:21&gt;</p>","tags":["Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-20 19:40:18.0","id":1745,"title":"Ambari server hanging after upgrading Ambari from 2.0.2 to 2.1.2","body":"<p>Everything works fine until we setup external DB(mysql) for hive by running:</p><p>ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar</p><p>which is the step 17of the instruction: http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.0/bk_upgrading_Ambari/content/_upgrade_ambari.html</p>","tags":["upgrade","ambari-2.1.2","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-19 23:25:27.0","id":1692,"title":"Any recommendation on how to partition disk space for a Namenode?","body":"","tags":["installation","operations","disk","namenode","partitioning"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-21 13:17:57.0","id":1777,"title":"With Ambari AD management is it possible to have user names prefixed automatically?","body":"","tags":["active-directory","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-20 11:49:39.0","id":1711,"title":"Can we get better performance for hive queries by using SSD?","body":"<p>One of my client is using Azure based IaaS for their HDP cluster. They are open to using more expensive storage to get better performance. </p><p>Is it recommended to use SSD for some of the data in hive tables, to get that boost in performance? Also what are the steps to make your temporary storage to point to SSD, that is used by Tez/MR jobs?</p>","tags":["performance","Hive","ssd"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-08 13:31:46.0","id":1110,"title":"How to Completely Clean, Remove or Uninstall Ambari for Fresh Install","body":"<p>Sometimes when installing Ambari run repos are downloaded or there are maybe issues with python.</p>","tags":["hdp-2.3.4","installation","Ambari","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-21 18:57:44.0","id":1839,"title":"Transparent Data Encryption (TDE) and Local Disks encryption for intermediate data","body":"<p>There is the recommendation that local disks should be encrypted for intermediate Data; need more info on this.  Why is this so?  How do we proposed encrypting the disk?\n\nIs this because Tez stores intermediate data on local disks?    \n\nAlso Map Reduce stores data in local disk with the \"<a target=\"_blank\" href=\"https://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml\">mapreduce.cluster.local.dir</a>\" parameter.  So this has to be encrypted right?</p><p>So what are the best practices to encrypt the local disks for intermediate data? What is the manual effort involved?\n\nIs <a href=\"https://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html\">Hadoop Encrypted Shuffle</a> enough?\n</p>","tags":["encryption","kms","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-23 02:08:22.0","id":1958,"title":"Integration with IBM Security Access Manager (ISAM) for user authentication?","body":"<p>Does HDP 2.3.x support integration with IBM Security Access Manager (ISAM) for user authentication?</p>","tags":["ibm","isam"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-24 14:06:02.0","id":2067,"title":"ORC vs Parquet - When to use one over the other","body":"<p>Hi All,</p><p>While ORC and Parquet are both columnar data stores that are supported in HDP, I was wondering if there was additional guidance on when to use one over the other? Or things to consider before choosing which format to use?</p><p>Thanks,</p><p>Andrew</p>","tags":["Hive","parquet","orc","HDFS"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-23 22:12:24.0","id":128,"title":"Apache Zeppelin Tech Preview Live","body":"<p><a href=\"http://hortonworks.com/hadoop-tutorial/apache-zeppelin/\">http://hortonworks.com/hadoop-tutorial/apache-zeppelin/</a></p><p>The Zeppelin TP is built against Spark 1.4.1 in HDP. We are also about to publish Spark 1.5.1 TP very soon and once that is out Zeppelin TP will also be revised to carry instructions for Spark 1.5.1.</p><p>Please play with it and post here if you run into any issues.</p>","tags":["spark-sql","data-science","Spark","zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-10-25 15:15:50.0","id":2081,"title":"Atlas Tutorials and Examples","body":"<p>Hi,</p><p>Are there any Atlas tutorials or examples? I don't see any on the Hortonworks website.</p><p>Thanks,</p>","tags":["Atlas","examples"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-26 13:35:01.0","id":2118,"title":"HBase using WASB as rootDir","body":"<p>Has this been tested? I cannot see region info, pre-splitting a table occurs but I do not see whether it actually took effect. Deleting/disabling/pre-splitting a table takes a long time.</p>","tags":["help","azure","Hbase","wasb"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-25 17:07:51.0","id":2084,"title":"Which compression is used in Site-to-Site (Remote Process Group)","body":"<ol><li>Which compression algorithm is used when a remote port communication is set up?</li><li> Can it be customized?</li><li> Does it work on a FlowFile level or the batch that s2s protocol negotiated for transmission?</li></ol><p><img src=\"/storage/attachments/320-remote-port-connection-status.png\"></p>","tags":["Nifi","site2site","dataflow","hdf","compression"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-28 16:55:32.0","id":2304,"title":"Which SSO providers are supported with Knox?","body":"<p>I keep seeing different answers in the docs, blogs, etc.  I'm sure most can be integrated but wondering more from what is supported today; SiteMinder, OAM, etc.  Thanks </p>","tags":["sso","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-29 14:50:58.0","id":2349,"title":"TIP: when you get a message in job log user [Dr. Who] is not authorized to view the log","body":"<p>here's the message</p><p><img src=\"/storage/attachments/355-error-message.png\"></p><p>here's a quick fix, replace root with the user executing the job</p><p><img src=\"/storage/attachments/356-correction.png\"></p>","tags":["operations","authorization","jobs","tip","logs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-29 18:59:20.0","id":2408,"title":"Ranger implementation - Hive impersonation false","body":"<p>Customer wants Hive column level ACLs to be set up in Ranger, so we suggested to set Hive doAs property to 'false' to impersonate as hive user and set Hive Column level ACLs in ranger. In this case all the jobs will be shown as to run as 'hive' user in Resource manager. At the same time, customer wants to know the resource utilization at the user level. Which is not possible because all the jobs will be run as hive user. Is there a way out to satisfy customer's requirement  ? Thanks</p>","tags":["resource-manager","Hive","hiveserver2","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-30 02:41:15.0","id":2481,"title":"Nifi Now() Format","body":"<p>According to the docs ${now()} should yeild </p><p>\"current date and time as the number of milliseconds since Midnight GMT on\n\tJanuary 1, 1970.\"</p><p>Which sounds like a Unix epoch style timestamp in miliseconds </p><p>but what I actually receive is Fri Oct 30 02:36:08 UTC 2015</p><p>Does now() have options I am missing?</p>","tags":["date","hdf","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-29 20:22:58.0","id":2434,"title":"Why is WebHCat returning \"returned 7. stty: standard input: Inappropriate ioctl for device\" in a Kerberized cluster??","body":"<p>WebHCat returning \"returned 7. stty: standard input: Inappropriate ioctl for device\" in a Kerberized cluster.</p><p>Full command output from Ambari test:</p><pre>Execution of \n'ambari-sudo.sh su ambari-qa -l -s /bin/bash -c \n'curl -k --negotiate -u : \n-b /var/lib/ambari-agent/data/tmp/cookies/ab6b1598-9aca-4813-bb71-1611ce2e6b03 \n-c /var/lib/ambari-agent/data/tmp/cookies/ab6b1598-9aca-4813-bb71-1611ce2e6b03 \n-w '\"'\"'%{http_code}'\"'\"' \n'\"'\"'http://xxxx.prod.xxxx.com:50111/templeton/v1/status?user.name=ambari-qa'\"'\"' \n--connect-timeout 5 --max-time 7 \n-o /dev/null 1&gt;/tmp/tmpduww6R 2&gt;/tmp/tmpyzCphz'' returned 7. stty: standard input: Inappropriate ioctl for device</pre>","tags":["webhcat","kerberos","Ambari","active-directory"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-01 21:59:40.0","id":2577,"title":"HBase Coprocessor and security","body":"<p>Is there a way of restricting access to an HBase coprocessor in a multi tenant environment ? What should i be taking into consideration when using coprocessor ?</p>","tags":["Storm","Hbase","security"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-30 12:47:35.0","id":2512,"title":"What is a recommended value to be set for \"overcommit_memory\" on a hadoop cluster ?","body":"<p>overcommit_memory </p><p>Defines the conditions that determine whether a large memory request is accepted or denied. There are three possible values for this parameter: </p><ul><li>0 — The default setting. The kernel performs heuristic memory overcommit handling by estimating the amount of memory available and failing requests that are blatantly invalid. Unfortunately, since memory is allocated using a heuristic rather than a precise algorithm, this setting can sometimes allow available memory on the system to be overloaded. </li><li>1 — The kernel performs no memory overcommit handling. Under this setting, the potential for memory overload is increased, but so is performance for memory-intensive tasks. </li><li>2 — The kernel denies requests for memory equal to or larger than the sum of total available swap and the percentage of physical RAM specified in overcommit_ratio. This setting is best if you want a lesser risk of memory overcommitment.</li></ul>","tags":["os","hadoop","cluster","clustering"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-30 22:05:07.0","id":2551,"title":"Is there a better yarn.client.failover-proxy-provider class implementation example over default round robin org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider","body":"<p>Client, ApplicationMaster and NodeManager on RM failover\n</p><p>When there are multiple RMs, the configuration (yarn-site.xml) used by clients and nodes is expected to list all the RMs. Clients, ApplicationMasters (AMs) and NodeManagers (NMs) try connecting to the RMs in a round-robin fashion until they hit the Active RM. If the Active goes down, they resume the round-robin polling until they hit the \"new\" Active. This default retry logic is implemented as org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider. You can override the logic by implementing org.apache.hadoop.yarn.client.RMFailoverProxyProvider and setting the value of yarn.client.failover-proxy-provider to the class name.</p>","tags":["resource-manager","fail-over","YARN","high-availability"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-30 17:16:53.0","id":2526,"title":"Content Repository and Archival","body":"<p>Hi, I'm trying to understand how content repository, sizing/capping and archiving are related, reading through the Admin Guide: <a target=\"_blank\" href=\"https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html\">https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html</a></p><p>By default the archive feature is disabled.</p>","tags":["hdf","Nifi","archive"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-02 19:53:49.0","id":2630,"title":"Zeppelin - Sparksql and Hive , Access existing hive tables","body":"<p>Test environment:</p><p>I can read testns using beeline</p><p>Zeppelin</p><p></p><pre>java.lang.RuntimeException: Table Not Found: testns\n\tat scala.sys.package$.error(package.scala:27)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:115)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:229)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala</pre><pre>0: jdbc:hive2://nsfed02.cloud.hortonworks.com&gt; select * from testns;+------------------------------------------------------------------------------------------------------+--+|                                             testns.name                                              |+------------------------------------------------------------------------------------------------------+--+| drwxr-xr-x. 18 cloud-user cloud-user  4096 Oct 11 09:43 Python-2.7.9                                 || drwxrwxr-x.  2 hbase      hadoop      4096 Oct 18 05:11 hbase-hbase                                  || drwxr-xr-x.  2 hdfs       hadoop      4096 Oct 18 05:15 hadoop-hdfs                                  || -rw-------.  1 hbase      hadoop       782 Oct 18 05:15 krb5cc_1050                                  || -rw-------.  1 ambari-qa  hadoop       798 Oct 18 05:17 krb5cc_1043                                  || drwxr-xr-x.  3 hdfs       hadoop      4096 Oct 18 05:21 Jetty_localhost_48216_datanode____t0h8vu     |</pre>","tags":["zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-03 19:52:25.0","id":2713,"title":"NFS Share for HDFS Storage support multiple mount points ?","body":"<p>Customer is asking us to build a cluster with VMs based on NFS Share for HDFS Storage. Can NFS Share support multiple mount points ?</p>","tags":["installation","HDFS","nfs","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-05 04:11:43.0","id":2862,"title":"Ambari 2.1.2 ranger installation failed, api delete and reinstallation failing @ step 6 with \"server error\" message.","body":"<p>Initial attempt to Add Ranger service to running HDP 2.3.2 cluster managed with Ambari 2.1.2 on Redhat 7.1, Oracle JDK 8 and Oracle 12C backend database for Ranger.</p><p>\nambari ranger DB test connection successful but failed to connect database @step 6 with following error.</p><pre>File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\nraise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'python /usr/hdp/current/ranger-admin/dba_script.py -q' returned 1. 2015-11-04 11:46:19,927 [I] Running DBA setup script. QuiteMode:True\n2015-11-04 11:46:19,927 [I] Using Java:/usr/java/default/bin/java\n2015-11-04 11:46:19,927 [I] DB FLAVOR:ORACLE\n2015-11-04 11:46:19,927 [I] DB Host:roadrunner.example.com\n2015-11-04 11:46:19,927 [I] ---------- Creat\ning Ranger Admin db user ---------- \n2015-11-04 11:46:19,927  [I] Checking connection\nSQLException : SQL state: 08006 java.sql.SQLRecoverableException: IO Error: The Network Adapter could not establish the connection ErrorCode: 17002\n2015-11-04 11:46:20,196  [E] Can't establish connection,Change configuration or Contact Administrator!!Following procedure and followed to re-add ranger service.1) curl -i -uadmin:admin -H \"X-Requested-By: ambari\" -d '{\"HostRoles\": { \"state\": \"UNKNOWN\"}}' -X DELETE \"http://hdpqa001.example.com:8080/api/v1/clusters/hdpclu/services/RANGER\";\nHTTP/1.1 200 OK\nUser: admin\nSet-Cookie: AMBARISESSIONID=1aul0secz52361k6uhu9akhjbd;Path=/;HttpOnly\nExpires: Thu, 01 Jan 1970 00:00:00 GMT\nContent-Type: text/plain\nContent-Length: 0\nServer: Jetty(8.1.17.v20150415)2) Ambari add Ranger service wizard.------  Multiple attempts failed at stage 6 with a error popup \"server error\"What is best approach to cleanup and add Ranger service.\nOr Any debug options to collect Add service process logs to understand error.</pre>","tags":["ambari-2.1.2","security","Ranger"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-10 20:18:31.0","id":3171,"title":"Common Questions for HDP Search","body":"<p><strong>1.Does HDP Search include full support now for SolrCloud? Or if it’s a future, then when?</strong></p><p>Supported.</p><p>Within the documentation it describes setting up and using SolrCloud as part of HDP Search.  For HDP 2.3 and HDP Search, that documentation can be <a target=\"_blank\" href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_search/index.html\">found here</a>.</p><p>&lt;snippet&gt;</p><p>When using Solr with HDPSearch, you should run Solr in SolrCloud mode, which provides central configuration for a cluster of Solr servers, automatic load balancing and fail-over for queries, and distributed index replication.. This mode is set when starting Solr.</p>SolrCloud relies on Apache ZooKeeper to coordinate requests between the nodes of the cluster. It’s recommended to use the ZooKeeper ensemble running with HDP 2.3 for this purpose.<p>More information about the concepts included in SolrCloud are contained in the Apache Solr Reference Guide section on <a href=\"https://cwiki.apache.org/confluence/display/solr/SolrCloud\">SolrCloud</a>. This guide will review some basics to get you started.</p><p>&lt;/snippet&gt;</p><p><strong>2.What components (including any kind of adapters, connectors, etc.) are included in HDP Search?</strong></p><p>HDP Search (as of July 2015) consists of: </p><ul><li>Apache Solr 5.2.1 </li><li>Banana 1.5 for analytical dashboards on Solr indexes </li><li>Connectors for indexing content from HDFS, MapReduce, Hive, Pig, HBase, Storm, and Spark</li></ul><p><strong>3.What counts as a “Node” for HDP Search support?  Do we have it defined anywhere?</strong></p><p>This is defined within the Hortonworks Price Book.  Node: a server or virtual machine running the software.</p>","tags":["search","SOLR"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-09 17:15:11.0","id":3069,"title":"How do you remove SmartSense when in a pending install state?","body":"<p>SmartSense install failed and is stuck in a \"pending\" state. We want to remove it from Ambari and retry the install.</p>","tags":["smartsense","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-06 19:04:25.0","id":2982,"title":"Kerberos, AD/LDAP and Ranger","body":"<p>Bringing up couple of FAQ</p><p>1) Do we have to use Kerberos? We are ok with AD/LDAP authentication</p><p>2) Will Ranger work without Kerberos? Do we need Kerberos for Ranger to secure Ranger?</p>","tags":["active-directory","kerberos","ldap","Ranger","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-16 01:40:38.0","id":3420,"title":"Best way to override nifi.properties values","body":"<p>Is there a way to override a configuration value on startup? E.g. I'd expect to have a cascade of settings from cli to properties file to defaults, though didn't see anything like it in the docs.</p>","tags":["configuration","Nifi","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-17 13:09:21.0","id":4111,"title":"oozie : DR replication","body":"<p>As part of my DR, I would like to have a way of replicating my oozie job into a DR cluster. Is there a good way of doing it ? </p><p>I was thinking of having the workflow in version control systems ( git, subversion, ... )  and to push them into the DR oozie server when moving to DR. Unfortunately, it's not possible as it is consider a change and will require a change request ( lengthy internal process ).  </p><p>Is there a plan to do it with Falcon in the future ?</p>","tags":["Oozie","disaster-recovery"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-18 22:08:55.0","id":4276,"title":"NiFi Clustering: One NCM to manage multiple separate \"logical\" Nifi clusters","body":"<p>Is it possible to configure a NCM to manage multiple distinct sets of slave (nodes)? For example could one NCM coordinate the following scenario:</p><ul><li>Flow1 deployed to Nodes 1-3 by NodeControlManager 1</li><li>Flow2 deployed to Nodes 4-6 by NodeControlManager 2</li></ul><p>Does NCM have the ability to selectively deploy flows to separate nodes, effectively creating separate logical sets of NiFi cluster workers (nodes)?</p><p>In summary can we have ... \"One NCM to rule them all and in the darkness bind them\"</p>","tags":["Nifi","ncm","hdf","cluster"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-19 20:19:21.0","id":4345,"title":"Querying JSON data using Hive","body":"<p>I am using https://github.com/rcongiu/Hive-JSON-Serde to query the JSON data via hive.</p><p>As part of testing, I am using an external table to query the JSON plain text file in HDFS.</p><p>i am able to query the data from hive using select, However when i do select * from JSON_EXTERNAL_TABLE  limit 1, the output is an Invalid JSON though the message in HDFS is a valid JSON. Is this an expected one ?</p>","tags":["hive-serde","Hive","json"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-19 13:16:49.0","id":4303,"title":"Patterns for batch processing time-series data?","body":"<p>What patterns or practices exist for dealing with time-series data specifically in batch mode, i.e, Tez or MR as opposed to Spark. Sorting orders the data within a block or ORC split, but how are boundaries between blocks usually handled?  For instance, finding derivatives, inflection points, etc. breaks down at file boundaries---are there standard patterns or libraries to deal with this?</p>","tags":["MapReduce","Tez","data-model","data","scientific-computing"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-20 02:27:07.0","id":4384,"title":"hbase.bucketcache.percentage.in.combinedcache is being deprecated in HBase 2.0","body":"<p>according to http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Sys_Admin_Guides/content/ref-db219cd6-c586-49c1-bc56-c9c1c5475276.1.html this property is still being advised to be configured. The JIRA for deprecation is https://issues.apache.org/jira/browse/HBASE-11520, is there a better instruction set to follow to enable bucketcache than the doc we provide?</p>","tags":["notify-docs","bug","documentation","Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-20 21:54:46.0","id":4500,"title":"Do we have a Ranger 0.5 DB Schema defined somewhere?","body":"<p>Audit Logs stored in Ranger Audit DB needs to be piped to SIEM system. Need to know what table(s) I can query on to pull failed policies (ie. \"Denied\" access). This information will eventually be pushed to SIEM. </p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-21 02:33:40.0","id":4521,"title":"Ambari is showing wrong hostnames for Journalnodes","body":"<p>Ambari is showing wrong hostnames for Journalnodes when drilled down from UI and sometimes also shows wrong Services components on adhoc basis. </p><p>Ambari2.1.2</p>","tags":["operations","journalnode","installation","upgrade","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-20 22:04:13.0","id":4496,"title":"how to migrate Hive data over to new cluster?","body":"<p>We have a Old Cluster with HDP 2.0.6 (Hive 0.12.0).. New Cluster with HDP 2.3.2 (Hive 1.2.1). We need to hive query the same tables in new environment and old environment after data forklift in same manner (implying database names, partitioning, etc. work). We migrated all the hdfs data over using distcp.</p>","tags":["Hive","hiveserver2","mysql","data-migration"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-26 09:12:19.0","id":4858,"title":"SolrCloud Performance - HDFS index/data","body":"<p>According to the docs, Solr relies heavily on fast bulk reads and writes during index updates. Lets say I want to index thousands of documents (word, pdf, html, ...) or I want to store my Ranger audit logs in my SolrCloud. Is it a good idea to use HDFS as index and data store or should I go with a local non-hdfs data directory? </p><p>Ranger Audit Logs documentation mentions \"<em>1 TB free space in the volume where Solr will store the index data.</em>\", which sounds like non-hdfs?!</p>","tags":["solrcloud","Ranger","search","SOLR","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-24 16:27:11.0","id":4706,"title":"Why is one container from my Hive query (with Tez) transitioning from RUNNING to EXITED_WITH_SUCCESS, while the other containers transition to KILLED?  This single container is extending the job by 200%.","body":"<p>I am running a Hive on Tez query over 110 GB of data stored as ORC.  There are 100 total mappers generated and 53 will run in parallel.  99 of the mappers completed in under a minute and transitioned from RUNNING to KILLED. One container stays alive, while doing very little, and then transitions to EXITED_WITH_SUCCESS two minutes after the other containers are completed.</p>","tags":["yarn-container","Hive","Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-24 18:51:32.0","id":4726,"title":"Ambari Metric Collector: Error sending metric to server. timed out","body":"<p>Had a disk full issue. After making some space in /var then trying to restart Metric Collector from Ambari, got error:</p><p>----------error ---------------</p><pre>2015-11-24 11:35:54,281 [INFO] controller.py:110 - Adding event to cache,  : {u'metrics': [], u'collect_every': u'15'}\n2015-11-24 11:35:54,281 [INFO] main.py:65 - Starting Server RPC Thread: /usr/lib/python2.6/site-packages/resource_monitoring/main.py start\n2015-11-24 11:35:54,281 [INFO] controller.py:57 - Running Controller thread: Thread-1\n2015-11-24 11:35:54,282 [INFO] emitter.py:45 - Running Emitter thread: Thread-2\n2015-11-24 11:35:54,282 [INFO] emitter.py:65 - Nothing to emit, resume waiting.\n2015-11-24 11:36:54,283 [INFO] emitter.py:91 - server: <a href=\"http://xxxxxxx.com:6188/ws/v1/timeline/metrics\">http://xxxxxxx.com:6188/ws/v1/timeline/metrics</a>\n2015-11-24 11:37:44,334 [WARNING] emitter.py:74 - Error sending metrics to server. timed out\n2015-11-24 11:37:44,334 [WARNING] emitter.py:80 - Retrying after 5 ...</pre>","tags":["ambari-metrics","ambari-server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-24 03:33:51.0","id":4641,"title":"Would it be a bad idea to add UseGCOverheadLimit for YARN NodeManager?","body":"<p>Seeing the following error on HDP 2.3.0:</p><pre>2015-10-19 07:33:03,353 ERROR mapred.ShuffleHandler (ShuffleHandler.java:exceptionCaught(1053)) - Shuffle error:\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n   at java.util.Arrays.copyOf(Arrays.java:2219)\n   at java.util.ArrayList.grow(ArrayList.java:242)\n   at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:216)\n   at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:208)\n   at java.util.ArrayList.add(ArrayList.java:440)\n--\n   at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)\n   at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)\n   at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)\n   at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n   at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)</pre><pre>2015-10-21 07:05:13,532 FATAL yarn.YarnUncaughtExceptionHandler (YarnUncaughtExceptionHandler.java:uncaughtException(51)) - Thread Thread[Container Monitor,5,main] threw an Error.  Shutting down now...\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n   at java.io.BufferedReader.&lt;init&gt;(BufferedReader.java:98)\n   at java.io.BufferedReader.&lt;init&gt;(BufferedReader.java:109)\n   at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:545)\n   at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.updateProcessTree(ProcfsBasedProcessTree.java:225)\n   at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:439)</pre><p>Not only try increasing \"<strong>mapreduce.reduce.memory.mb</strong>\" but also can I add \"-XX:-<strong>UseGCOverheadLimit</strong>\" in \"<strong>mapreduce.admin.reduce.child.java.opts</strong>\"?</p><p>Also would it be a good idea to reduce \"<strong>mapreduce.reduce.shuffle.input.buffer.percent</strong>\" ?</p>","tags":["garbage-collector","MapReduce","YARN","oom"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-25 11:47:32.0","id":4789,"title":"Is there any way to find out how much time is taken for Major Compaction of a specific table?","body":"<p>We can find it from regionserver logs but I want to know if there is any other better way.</p>","tags":["regionserver","how-to-tutorial","compaction","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-01 14:33:55.0","id":4980,"title":"Ambari stuck with \"Install Pending\" when creating cluster thru api","body":"<p>version: Ambari 1.7.0</p><p>Have anyone seen error below when trying to deploy a new cluster using ambari api.</p><pre>30 Nov 2015 17:24:06,826  WARN [ambari-action-scheduler] ActionScheduler:200 - Exception received\njava.lang.RuntimeException: org.apache.ambari.server.ClusterNotFoundException: Cluster not found, clusterName=clusterID=6\n        at org.apache.ambari.server.actionmanager.ExecutionCommandWrapper.getExecutionCommand(ExecutionCommandWrapper.java:116)\n        at org.apache.ambari.server.actionmanager.ActionScheduler.isStageHasBackgroundCommandsOnly(ActionScheduler.java:428)\n        at org.apache.ambari.server.actionmanager.ActionScheduler.filterParallelPerHostStages(ActionScheduler.java:416)\n        at org.apache.ambari.server.actionmanager.ActionScheduler.doWork(ActionScheduler.java:246)\n        at org.apache.ambari.server.actionmanager.ActionScheduler.run(ActionScheduler.java:195)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.ambari.server.ClusterNotFoundException: Cluster not found, clusterName=clusterID=6\n        at org.apache.ambari.server.state.cluster.ClustersImpl.getClusterById(ClustersImpl.java:250)\n        at org.apache.ambari.server.actionmanager.ExecutionCommandWrapper.getExecutionCommand(ExecutionCommandWrapper.java:74)\n        ... 5 more\n30 Nov 2015 17:24:09,931  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:146 - Heartbeat lost from host brtlvlts0233pl.redecorp.br\n30 Nov 2015 17:24:09,931  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:146 - Heartbeat lost from host brtlvlts0234pl.redecorp.br\n30 Nov 2015 17:24:09,932  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:146 - Heartbeat lost from host brtlvlts0232pl.redecorp.br\n30 Nov 2015 17:24:09,932  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:146 - Heartbeat lost from host brtlvlts0237pl.redecorp.br\n30 Nov 2015 17:24:09,932  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:146 - Heartbeat lost from host brtlvlts0235pl.redecorp.br\n30 Nov 2015 17:24:09,932  WARN [ambari-hearbeat-monitor] HeartbeatMonitor:146 - Heartbeat lost from host brtlvlts0236pl.redecorp.br\n30 Nov 2015 17:24:13,274 ERROR [alert-event-bus-1] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:\n</pre><p>If we log in ambari webui, cluster stuck with install pending:</p><p><img src=\"/storage/attachments/565-screenshot1.png\"></p><p><img src=\"/storage/attachments/566-screenshot2.png\"></p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-03 22:28:03.0","id":5150,"title":"HBase shell error: java.lang.UnsatisfiedLinkError","body":"<p>On HDP 2.3.1, when trying to start hbase shell, got error message:</p><p>$ hbase shell\njava.lang.RuntimeException: java.lang.UnsatisfiedLinkError: /tmp/jffi7007828132073784430.tmp: /tmp/jffi7007828132073784430.t  mp: failed to map segment from shared object: Operation not permitted\n  at com.kenai.jffi.Foreign$InValidInstanceHolder.getForeign(Foreign.java:90)\n  at com.kenai.jffi.Foreign.getInstance(Foreign.java:95)\n  at com.kenai.jffi.Library.openLibrary(Library.java:151)\n  at com.kenai.jffi.Library.getCachedInstance(Library.java:125)\n  at com.kenai.jaffl.provider.jffi.Library.loadNativeLibraries(Library.java:66)\n  at com.kenai.jaffl.provider.jffi.Library.getNativeLibraries(Library.java:56)\n  at com.kenai.jaffl.provider.jffi.Library.getSymbolAddress(Library.java:35)\n  at com.kenai.jaffl.provider.jffi.Library.findSymbolAddress(Library.java:45)\n  at com.kenai.jaffl.provider.jffi.AsmLibraryLoader.generateInterfaceImpl(AsmLibraryLoader.java:188)\n  at com.kenai.jaffl.provider.jffi.AsmLibraryLoader.loadLibrary(AsmLibraryLoader.java:110)\n  at com.kenai.jaffl.provider.jffi.Provider.loadLibrary(Provider.java:31)\n  at com.kenai.jaffl.provider.jffi.Provider.loadLibrary(Provider.java:25)\n  at com.kenai.jaffl.Library.loadLibrary(Library.java:76)\n  at org.jruby.ext.posix.POSIXFactory$LinuxLibCProvider$SingletonHolder.&lt;clinit&gt;(POSIXFactory.java:108)\n  at org.jruby.ext.posix.POSIXFactory$LinuxLibCProvider.getLibC(POSIXFactory.java:112)\n  at org.jruby.ext.posix.BaseNativePOSIX.&lt;init&gt;(BaseNativePOSIX.java:30)\n  at org.jruby.ext.posix.LinuxPOSIX.&lt;init&gt;(LinuxPOSIX.java:17)\n  at org.jruby.ext.posix.POSIXFactory.loadLinuxPOSIX(POSIXFactory.java:70)\n  at org.jruby.ext.posix.POSIXFactory.loadPOSIX(POSIXFactory.java:31)\n  at org.jruby.ext.posix.LazyPOSIX.loadPOSIX(LazyPOSIX.java:29)\n  at org.jruby.ext.posix.LazyPOSIX.posix(LazyPOSIX.java:25)\n  at org.jruby.ext.posix.LazyPOSIX.isatty(LazyPOSIX.java:159)\n  at org.jruby.RubyIO.tty_p(RubyIO.java:1897)\n  at org.jruby.RubyIO$i$0$0$tty_p.call(RubyIO$i$0$0$tty_p.gen:65535)\n  at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:292)\n  at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:135)\n  at org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:63)\n  at org.jruby.ast.IfNode.interpret(IfNode.java:111)\n  at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n  at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)\n  at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:147)\n  at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:183)\n  at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:292)\n  at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:135)\n  at org.jruby.ast.VCallNode.interpret(VCallNode.java:86)\n  at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n  at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n  at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)\n  at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)\n  at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:191)\n  at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)\n  at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:144)\n  at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:148)\n  at org.jruby.RubyClass.newInstance(RubyClass.java:822)\n  at org.jruby.RubyClass$i$newInstance.call(RubyClass$i$newInstance.gen:65535)\n  at org.jruby.internal.runtime.methods.JavaMethod$JavaMethodZeroOrNBlock.call(JavaMethod.java:249)\n  at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:292)\n  at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:135)\n  at usr.hdp.$2_dot_3_dot_2_dot_0_minus_2950.hbase.bin.hirb.__file__(/usr/hdp/2.3.2.0-2950/hbase/bin/hirb.rb:128)\n  at usr.hdp.$2_dot_3_dot_2_dot_0_minus_2950.hbase.bin.hirb.load(/usr/hdp/2.3.2.0-2950/hbase/bin/hirb.rb)\n  at org.jruby.Ruby.runScript(Ruby.java:697)\n  at org.jruby.Ruby.runScript(Ruby.java:690)\n  at org.jruby.Ruby.runNormally(Ruby.java:597)\n  at org.jruby.Ruby.runFromMain(Ruby.java:446)\n  at org.jruby.Main.doRunFromMain(Main.java:369)\n  at org.jruby.Main.internalRun(Main.java:258)\n  at org.jruby.Main.run(Main.java:224)\n  at org.jruby.Main.run(Main.java:208)\n  at org.jruby.Main.main(Main.java:188)\nCaused by: java.lang.UnsatisfiedLinkError: /tmp/jffi7007828132073784430.tmp: /tmp/jffi7007828132073784430.tmp: failed to map  segment from shared object: Operation not permitted\n  at java.lang.ClassLoader$NativeLibrary.load(Native Method)\n  at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937)\n  at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822)\n  at java.lang.Runtime.load0(Runtime.java:809)\n  at java.lang.System.load(System.java:1086)\n  at com.kenai.jffi.Init.loadFromJar(Init.java:164)\n  at com.kenai.jffi.Init.load(Init.java:78)\n  at com.kenai.jffi.Foreign$InstanceHolder.getInstanceHolder(Foreign.java:49)\n  at com.kenai.jffi.Foreign$InstanceHolder.&lt;clinit&gt;(Foreign.java:45)\n  at com.kenai.jffi.Foreign.getInstance(Foreign.java:95)\n  at com.kenai.jffi.Internals.getErrnoSaveFunction(Internals.java:44)\n  at com.kenai.jaffl.provider.jffi.StubCompiler.getErrnoSaveFunction(StubCompiler.java:68)\n  at com.kenai.jaffl.provider.jffi.StubCompiler.&lt;clinit&gt;(StubCompiler.java:18)\n  at com.kenai.jaffl.provider.jffi.AsmLibraryLoader.generateInterfaceImpl(AsmLibraryLoader.java:146)\n  ... 50 more\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBind  er.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBi  nder.class]\nSLF4J: See <a href=\"http://www.slf4j.org/codes.html#multiple_bindings\">http://www.slf4j.org/codes.html#multiple_bindings</a> for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nForeign.java:90:in `getForeign': java.lang.RuntimeException: java.lang.UnsatisfiedLinkError: /tmp/jffi7007828132073784430.tm  p: /tmp/jffi7007828132073784430.tmp: failed to map segment from shared object: Operation not permitted\n  from Foreign.java:95:in `getInstance'\n  from Library.java:151:in `openLibrary'\n  from Library.java:125:in `getCachedInstance'\n  from Library.java:66:in `loadNativeLibraries'\n  from Library.java:56:in `getNativeLibraries'\n  from Library.java:35:in `getSymbolAddress'\n  from Library.java:45:in `findSymbolAddress'\n  from DefaultInvokerFactory.java:51:in `createInvoker'\n  from Library.java:27:in `getInvoker'\n  from NativeInvocationHandler.java:90:in `createInvoker'\n  from NativeInvocationHandler.java:74:in `getInvoker'\n  from NativeInvocationHandler.java:110:in `invoke'\n  from null:-1:in `isatty'\n  from BaseNativePOSIX.java:300:in `isatty'\n  from LazyPOSIX.java:159:in `isatty'\n  from RubyIO.java:1897:in `tty_p'\n  from RubyIO$i$0$0$tty_p.gen:65535:in `call'\n  from CachingCallSite.java:292:in `cacheAndCall'\n  from CachingCallSite.java:135:in `call'\n  from CallNoArgNode.java:63:in `interpret'\n  from IfNode.java:111:in `interpret'\n  from NewlineNode.java:104:in `interpret'\n  from ASTInterpreter.java:74:in `INTERPRET_METHOD'\n  from InterpretedMethod.java:147:in `call'\n  from DefaultMethod.java:183:in `call'\n  from CachingCallSite.java:292:in `cacheAndCall'\n  from CachingCallSite.java:135:in `call'\n  from VCallNode.java:86:in `interpret'\n  from NewlineNode.java:104:in `interpret'\n  from BlockNode.java:71:in `interpret'\n  from ASTInterpreter.java:74:in `INTERPRET_METHOD'\n  from InterpretedMethod.java:169:in `call'\n  from DefaultMethod.java:191:in `call'\n  from CachingCallSite.java:302:in `cacheAndCall'\n  from CachingCallSite.java:144:in `callBlock'\n  from CachingCallSite.java:148:in `call'\n  from RubyClass.java:822:in `newInstance'\n  from RubyClass$i$newInstance.gen:65535:in `call'\n  from JavaMethod.java:249:in `call'\n  from CachingCallSite.java:292:in `cacheAndCall'\n  from CachingCallSite.java:135:in `call'\n  from /usr/hdp/2.3.2.0-2950/hbase/bin/hirb.rb:128:in `__file__'\n  from /usr/hdp/2.3.2.0-2950/hbase/bin/hirb.rb:-1:in `load'\n  from Ruby.java:697:in `runScript'\n  from Ruby.java:690:in `runScript'\n  from Ruby.java:597:in `runNormally'\n  from Ruby.java:446:in `runFromMain'\n  from Main.java:369:in `doRunFromMain'\n  from Main.java:258:in `internalRun'\n  from Main.java:224:in `run'\n  from Main.java:208:in `run'\n  from Main.java:188:in `main'\nCaused by:\nClassLoader.java:-2:in `load': java.lang.UnsatisfiedLinkError: /tmp/jffi7007828132073784430.tmp: /tmp/jffi700782813207378443  0.tmp: failed to map segment from shared object: Operation not permitted\n  from ClassLoader.java:1937:in `loadLibrary0'\n  from ClassLoader.java:1822:in `loadLibrary'\n  from Runtime.java:809:in `load0'\n  from System.java:1086:in `load'\n  from Init.java:164:in `loadFromJar'\n  from Init.java:78:in `load'\n  from Foreign.java:49:in `getInstanceHolder'\n  from Foreign.java:45:in `&lt;clinit&gt;'\n  from Foreign.java:95:in `getInstance'\n  from Internals.java:44:in `getErrnoSaveFunction'\n  from StubCompiler.java:68:in `getErrnoSaveFunction'\n  from StubCompiler.java:18:in `&lt;clinit&gt;'\n  from AsmLibraryLoader.java:146:in `generateInterfaceImpl'\n  from AsmLibraryLoader.java:110:in `loadLibrary'\n  from Provider.java:31:in `loadLibrary'\n  from Provider.java:25:in `loadLibrary'\n  from Library.java:76:in `loadLibrary'\n  from POSIXFactory.java:108:in `&lt;clinit&gt;'\n  from POSIXFactory.java:112:in `getLibC'\n  from BaseNativePOSIX.java:30:in `&lt;init&gt;'\n  from LinuxPOSIX.java:17:in `&lt;init&gt;'\n  from POSIXFactory.java:70:in `loadLinuxPOSIX'\n  from POSIXFactory.java:31:in `loadPOSIX'\n  from LazyPOSIX.java:29:in `loadPOSIX'\n  from LazyPOSIX.java:25:in `posix'\n  from LazyPOSIX.java:159:in `isatty'\n  from RubyIO.java:1897:in `tty_p'\n  from RubyIO$i$0$0$tty_p.gen:65535:in `call'\n  from CachingCallSite.java:292:in `cacheAndCall'\n  from CachingCallSite.java:135:in `call'\n  from CallNoArgNode.java:63:in `interpret'\n  from IfNode.java:111:in `interpret'\n  from NewlineNode.java:104:in `interpret'\n  from ASTInterpreter.java:74:in `INTERPRET_METHOD'\n  from InterpretedMethod.java:147:in `call'\n  from DefaultMethod.java:183:in `call'\n  from CachingCallSite.java:292:in `cacheAndCall'\n  from CachingCallSite.java:135:in `call'\n  from VCallNode.java:86:in `interpret'\n  from NewlineNode.java:104:in `interpret'\n  from BlockNode.java:71:in `interpret'\n  from ASTInterpreter.java:74:in `INTERPRET_METHOD'\n  from InterpretedMethod.java:169:in `call'\n  from DefaultMethod.java:191:in `call'\n  from CachingCallSite.java:302:in `cacheAndCall'\n  from CachingCallSite.java:144:in `callBlock'\n  from CachingCallSite.java:148:in `call'\n  from RubyClass.java:822:in `newInstance'\n  from RubyClass$i$newInstance.gen:65535:in `call'\n  from JavaMethod.java:249:in `call'\n  from CachingCallSite.java:292:in `cacheAndCall'\n  from CachingCallSite.java:135:in `call'\n  from /usr/hdp/2.3.2.0-2950/hbase/bin/hirb.rb:128:in `__file__'\n  from /usr/hdp/2.3.2.0-2950/hbase/bin/hirb.rb:-1:in `load'\n  from Ruby.java:697:in `runScript'\n  from Ruby.java:690:in `runScript'\n  from Ruby.java:597:in `runNormally'\n  from Ruby.java:446:in `runFromMain'\n  from Main.java:369:in `doRunFromMain'\n  from Main.java:258:in `internalRun'\n  from Main.java:224:in `run'\n  from Main.java:208:in `run'\n  from Main.java:188:in `main'</p>","tags":["Hbase","shell"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-03 14:42:56.0","id":5116,"title":"Is anyone else having errors with Netty using Storm?","body":"<p>I'm getting errors from my spout that look like:</p><p>2015-12-03 08:30:57.425 b.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-storm03/XXX.XXX.XXX.XXX:6700 is being closed</p><p>Is this an error with Netty or something to do with my stack? We're on HDP 2.3.2</p><p>Adam</p>","tags":["Storm","netty"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-03 04:31:38.0","id":5098,"title":"Single Hive table pointing to multiple storage- S3 and HDFS","body":"<p>Lets assume we have data in hive table for past 60 days. How to automatically move the data beyond a time period (30 days) to S3 and have only the latest 30 days data in hdfs. How to write a hive query to read the entire 60 days data ? How to point single hive table to multiple data storage - S3 and hdfs ?</p><p>Also is it possible to configure S3 as archival storage ?</p><p>http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_hdfs_admin_tools/content/configuring_archival_storage.html</p>","tags":["Hive","s3","HDFS","aws"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-04 16:23:13.0","id":5225,"title":"Is there a way to prevent the start of the Tez AM from the Hive CLI?","body":"<p>The desired end result is that I want no YARN containers automatically started when I enter the Hive CLI and the engine default is Tez. </p><p>hive.prewarm prevents default work containers but I need to stop the Tez AM from launching.</p>","tags":["Hive","Tez","cli"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-04 18:56:24.0","id":5249,"title":"How to configure Knox to protect Phoenix JDBC access?","body":"<p>I am trying to use Phoenix to access HBase from Tableau with Knox being used to provide autentication and access control. Is it possible to use Phoenix/JDBC with Knox and how would I configure it to work?</p>","tags":["security","Phoenix","Knox","jdbc"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-07 15:15:39.0","id":5309,"title":"How to set: Tez Job Name?","body":"<p>MapReduce allowed us to set a unique job name with mapred.job.name. This allowed us to easily track jobs after execution via searching for this name in JobTracker UI and logs.</p><p>With Tez there doesn't appear to be a tez.job.name which can be set. Is there another similar config setting we can use to set a unique Tez job name/tag?</p><p>Related Jira: <a href=\"https://issues.apache.org/jira/browse/HIVE-12357\">https://issues.apache.org/jira/browse/HIVE-12357</a></p><p>Thanks</p>","tags":["monitoring","Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-06 15:40:12.0","id":5289,"title":"Will Erasure Coding in HDP 3.0 use ISA-L, and to what extent does ISA-L performance depend on using Intel chips?","body":"<p>Erasure Coding is apparently greatly speeded up by the ISA-L libraries.  Will HDP be using these libraries? They are open-source, but I assume they have some optimizations for Intel processors, as they were written by Intel. </p>","tags":["HDFS","hdp-3.0.0","erasure-coding"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-07 20:58:47.0","id":5361,"title":"Cannot drop Hive partition with control characters in partition name","body":"<p>Cleaning up some bad partitions, created in error.</p><p>We currently have an external table with a Hive Partition that I am unable to drop via Alter statement.  The Partition has control characters (%0D - what was a Carriage Return) in the partition name field.</p><p>The table is External and is called  &lt;tableName&gt;.  It's partitioned by fiscal_year and erp.  </p><pre>show partitions &lt;tableName&gt;;    \nfiscal_year=2014%0D/erp=ae_na%0D%0D    \nfiscal_year=2014/erp=ae_na    \nfiscal_year=2014/erp=be_na \n...</pre><p>The underlying files in HDFS were deleted long ago and no longer exist.  </p><p>I have tried the following commands without success: </p><pre>ALTER TABLE &lt;tableName&gt; DROP IF EXISTS PARTITION(fiscal_year='2014%0D', erp='ae_na%0D%0D');\nALTER TABLE &lt;tableName&gt; DROP PARTITION(fiscal_year='2014%0D', erp='ae_na%0D%0D');\nALTER TABLE &lt;tableName&gt; DROP PARTITION(fiscal_year&gt;'2014');</pre><p>Is there a way to drop this partition, or do I have to copy the data out, rebuild the table and move the data back in?  </p><p>Thanks in advance.  </p>","tags":["help","corruption","Hive","partitioning"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-08 05:14:16.0","id":5387,"title":"What security is available for Spark?","body":"<p>What security is available for Spark? How would the below security aspects work with Spark?</p><ol><li>authentication</li><li>authorization</li><li>audit</li><li>encryption</li></ol><p> For deployments where security is a concern, what mode of Spark should be used?</p>","tags":["authentication","security","audit","Spark","authorization"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-08 23:53:53.0","id":5488,"title":"What are the required steps we need to follow in setting up a crontab job/run a shell script against a kerberised cluster?","body":"<p>When using crontab to schedule the execution of shell scripts in a kerberized HDP environment, we have to go through a ticket generation process to execute\nany shell scripts directly or to schedule a crontab job.</p><p>I'm guessing a suitable SPN will need to repeatedly refresh the service account's Kerberos ticket? (Probably using crontab to perform a kinit regularly using a keytab?).  Can anyone provide a demonstration script?  Are there any downsides to this approach?</p><p>Or perhaps Knox could be used to access the service using a simple username/password? (assuming the service was available through Knox, of course)</p>","tags":["shell","crontab","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-09 10:16:21.0","id":38744,"title":"while connecting to hiveserver2 from my java client I am getting this exception","body":"<p>org.apache.thrift.protocol.TProtocolException: Required field &apos;serverProtocolVersion&apos; is unset! Struct:TOpenSessionResp(status:TStatus(statusCode:SUCCESS_STATUS), serverProtocolVersion:null, sessionHandle:TSessionHandle(), configuration:{})\nat org.apache.hive.service.cli.thrift.TOpenSessionResp.validate(TOpenSessionResp.java:578)\nat org.apache.hive.service.cli.thrift.TOpenSessionResp$TOpenSessionRespStandardScheme.read(TOpenSessionResp.java:676)\nat org.apache.hive.service.cli.thrift.TOpenSessionResp$TOpenSessionRespStandardScheme.read(TOpenSessionResp.java:612)\nat org.apache.hive.service.cli.thrift.TOpenSessionResp.read(TOpenSessionResp.java:520)\nat org.apache.hive.service.cli.thrift.TCLIService$OpenSession_result$OpenSession_resultStandardScheme.read(TCLIService.java:2361)\nat org.apache.hive.service.cli.thrift.TCLIService$OpenSession_result$OpenSession_resultStandardScheme.read(TCLIService.java:2346)\nat org.apache.hive.service.cli.thrift.TCLIService$OpenSession_result.read(TCLIService.java:2293)\nat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)\nat org.apache.hive.service.cli.thrift.TCLIService$Client.recv_OpenSession(TCLIService.java:160)\nat org.apache.hive.service.cli.thrift.TCLIService$Client.OpenSession(TCLIService.java:147)</p>","tags":["hiveserver2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-09 14:54:17.0","id":38819,"title":"How to change SparkR working directory? How to find default SparkR working directory on HDFS?","body":"<p>Hello,</p><p>I know these questions seem very basic, but there seems to be a discrepancy between the HDFS structure in my sparkR and what I see in Ambari.  In SparkR, the default working directory is \"/usr/hdp/2.4.0.0-169/spark\".  But in Ambari, I don't see /usr, but /user, which does contain a /spark directory but this just contains a /.sparkStaging direcotry, which is empty.</p><p>I have tried to change the workign directory with setwd() but if I just pass directory path as string, e.g. \"/user/\" it throws error cannot change working directory.  I can only seem to change to /tmp.</p><p>I could include more details, but I think I am missing something basic here, which will probably solve lots of other questions.  Help please?</p><p>Thanks</p><p>Aidan</p>","tags":["HDFS","Sandbox","sparkr","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-09 14:59:09.0","id":38822,"title":"hdfs exception","body":"<p>we are usin HDP2.2 GA in production, and found below exception on hdfs logs.</p><p>Anyone can help on this? Thanks in advance.</p><p>#####Error Log Part###########</p><p>2016-06-09 11:35:03,587 ERROR datanode.DataNode (DataXceiver.java:writeBlock(711)) - DataNode{data=FSDataset{dirpath='[/data01/hadoop/hdfs/data/current, /data02/hadoop/hdfs/data/current, /data03/hadoop/hdfs/data/current, /data04/hadoop/hdfs/data/current, /data05/hadoop/hdfs/data/current, /data06/hadoop/hdfs/data/current, /data07/hadoop/hdfs/data/current, /data08/hadoop/hdfs/data/current, /data09/hadoop/hdfs/data/current, /data10/hadoop/hdfs/data/current, /data11/hadoop/hdfs/data/current, /data12/hadoop/hdfs/data/current, /data13/hadoop/hdfs/data/current, /data14/hadoop/hdfs/data/current, /data15/hadoop/hdfs/data/current, /data16/hadoop/hdfs/data/current, /data17/hadoop/hdfs/data/current, /data18/hadoop/hdfs/data/current, /data19/hadoop/hdfs/data/current, /data20/hadoop/hdfs/data/current, /data21/hadoop/hdfs/data/current, /data22/hadoop/hdfs/data/current, /data23/hadoop/hdfs/data/current, /data24/hadoop/hdfs/data/current]'}, localName='dn04.prod.com:50010', datanodeUuid='ceb4714b-03c7-401d-9052-0564576d4563', xmitsInProgress=0}:Exception transfering block BP-1618578856-10.50.1.99-1448364772611:blk_1084624180_10917511 to mirror 10.50.1.175:50010: java.net.SocketTimeoutException: 65000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.50.1.174:34397 remote=/10.50.1.175:50010]\n2016-06-09 11:35:03,587 INFO  datanode.DataNode (DataXceiver.java:writeBlock(771)) - opWriteBlock BP-1618578856-10.50.1.99-1448364772611:blk_1084624180_10917511 received exception java.net.SocketTimeoutException: 65000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.50.1.174:34397 remote=/10.50.1.175:50010]\n2016-06-09 11:35:03,587 ERROR datanode.DataNode (DataXceiver.java:run(253)) - dn04.prod.com:50010:DataXceiver error processing WRITE_BLOCK operation  src: /10.50.1.197:48201 dst: /10.50.1.174:50010\njava.net.SocketTimeoutException: 65000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.50.1.174:34397 remote=/10.50.1.175:50010]\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2201)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:682)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-09 17:10:15.0","id":38854,"title":"Hive - Big Table or Multiple Tables?","body":"<p>Hi experts,</p>In HDFS I've 80 text files... In the end I want analyze some relationships between each file. But my point here is:\n\nBefore I use Spark to do some data transformation, I want to load this files into Hive to do some pre-analysis and data understanding. In yuour opinion (I only experience with BI projects, not Big Data):\n\n<ul><li><strong>Should I insert all this 80 text files in 80 different tables?\n</strong></li><li><strong>Should I aggregate this 80 text files into only one Big Table?\n\n</strong></li></ul><p>Like I said after this step in Hive I will use Spark to do some data cleansing.\n\nThanks!</p>","tags":["Hive","HDFS","Spark"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-10 02:52:42.0","id":38944,"title":"Sqoop UI / Web Site","body":"<p>Is\nthere a UI for Sqoop or an Ambari View?</p><p>Something like the Pig UI in Ambari</p>","tags":["Sqoop","ambari-views","Ambari"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-10 10:25:10.0","id":39032,"title":"my hive query is failing with NPE when I am querying column with timestamp on hive partitioned orc table.","body":"<p></p><p>Caused by: java.lang.NullPointerException </p><p>at\norg.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl$TimestampStatisticsImpl.getMinimum(ColumnStatisticsImpl.java:795)\n</p><p>at\norg.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getMin(RecordReaderImpl.java:2343)\n</p><p>at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicate(RecordReaderImpl.java:2366)\n</p><p>at\norg.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:2564)\n</p><p>at\norg.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:2627)\n</p><p>at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:3060)\n</p><p>at\norg.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:3102)\n</p><p>at\norg.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.&lt;init&gt;(RecordReaderImpl.java:288)\n</p><p>at\norg.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:534) </p><p>at\norg.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.&lt;init&gt;(OrcRawRecordMerger.java:183)</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-09 15:38:45.0","id":38833,"title":"import with sqoop smalldatetime from sql server to timestamp in Hive","body":"<p>Hello, </p><p>I have a smalldatetime data in my database server sql data and when I matter with Sqoop , this data is stored in the Hive String format because smalldatetime does not exist in the hive tool. This becomes probmématique for my work.\nAnyone know if there a way to import through Sqoop , smallldatetime a data type of sql server and store the timestamp format recognized by Hive . </p><p>thanks</p>","tags":["Sqoop","Hive","Spark","timestamp"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-13 10:07:40.0","id":39333,"title":"jobtracker port","body":"<p>how to get the jobTracker port no ?</p><p>Please help me</p>","tags":["port"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-14 10:14:11.0","id":39613,"title":"Need to restart brokers every few hours because of performance decrease","body":"<p>Hello,</p><p>Thank you very much for reading my question. We have a hadoop cluster with 4 brokers installed and 7 topics currently in production state. Since some days ago, there is a need to restart kafka brokers every few hours because we see that producers performance decrease a lot, and the time needed to insert a message in kafka grow from some miliseconds to seconds...</p><p>Consumers seem to be alright.</p><p>Our problem is that we don't know what is happening and what parameters to follow to resolve this problem. Where should we start for diagnosing ? Today we know that restarting a broker every 7 or 8 minutes resolve the problem for some time, but we don´t know why.</p><p>Can anybody help us?</p><p>Thank you very much,</p><p>Silvio</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-14 18:27:12.0","id":39733,"title":"Tez View issue on a Kerberized Cluster","body":"<div><p>Below error occurs while accessing TEZ view. </p><p>Any one see this error before ? </p><p><a href=\"http://localhost:38080/views/TEZ/0.7.0.2.3.4.0-161/TEZ_CLUSTER_INSTANCE/\">error code: 403, message: Forbidden{\"message\":\"Failed\nto fetch results by the proxy from url: http:hostname: //8188/ws/v1/timeline/TEZ_DAG_ID?limit=11&_=1465928433925&\",\"status\":403,\"trace\":\"{\\n\n\\\"RemoteException\\\" : {\\n \\\"message\\\" : \\\"User:\nambari-server@Domainname  is not allowed\nto impersonate \\\",\\n \\\"exception\\\" :\n\\\"AuthorizationException\\\",\\n \\\"javaClassName\\\" :\n\\\"org.apache.hadoop.security.authorize.AuthorizationException\\\"\\n\n}\\n}\"}</a></p><p>￬￬</p></div>","tags":["Tez"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-15 06:13:56.0","id":39839,"title":"How to import metadata from Hive into Atlas? And the atlas offcial guide maybe have wrong.","body":"<p>After built Atlas, I just only set the ATLAS_HOME_DIR in atlas-env.sh, and other settings, in atlas-env.sh and atlas-application.properties, are default.</p><p>I try to import metadata according http://atlas.apache.org/Bridge-Hive.html</p><p>After set $HIVE_CONF_DIR, I found that I can't set following configuration in atlas-application.properties.</p><pre>&lt;property&gt;\n\n\t&lt;name&gt;atlas.cluster.name&lt;/name&gt;\n\n\t&lt;value&gt;primary&lt;/value&gt;\n\n&lt;/property&gt;</pre><p>This is a XML style code, but the atlas-application.properties is not XML style, so I can't add this. </p><p>I am wondering if the official guide of atlas is not accurate?</p><p>Then I skip this setting, run import-hive.sh. It showed following:</p><pre>Exception in thread \"main\" org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: \nUnable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\nCaused by: java.lang.reflect.InvocationTargetException\nCaused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\nCaused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the \"BONECP\" plugin to create a ConnectionPool gave an error : The specified datastore driver (\"com.mysql.jdbc.Driver\") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.\nCaused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (\"com.mysql.jdbc.Driver\") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.\n\n</pre><p>in order to import metadata into Atlas, what could I do next ?</p>","tags":["Atlas","mysql","hdp-2.4","metadata","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-15 07:12:22.0","id":39835,"title":"oozie rerun all actions in the path (including the succeded ones) from the aborted node","body":"<p>Hi, I have a workflow which has four actions like this</p><pre>A -&gt; B -&gt; C (if success ok-to option)  \n\n B -&gt; D (if B fails error-to option)</pre><p style=\"margin-left: 20px;\">When I run this workflow, A succeeds and B fails, so C is run and this succeeds. </p><p style=\"margin-left: 20px;\">When I rerun the workflow again, A is skipped (I have selected oozie.wf.rerun.failnodes = true in properties file) </p><p style=\"margin-left: 20px;\">Now B runs again and fails, this time during rerun, C does not run again since it succeeded in previous run. </p><p style=\"margin-left: 20px;\">I want C to run for every run B is failing.  I tried oozie.wf.rerun.skip.nodes=,. This one causes all the actions to rerun again after failure. action A also executes again. I dont want this behaviour. </p><p style=\"margin-left: 20px;\">I want all actions in the work action path subsequent to failed actions to be rerun again when the failed action is rerun by resubmitting the workflow. (in this case since B is rerun, if this fails again I want C to run again irrespective of its status previous time.</p><p style=\"margin-left: 20px;\">Is there any way to achieve this functionality without splitting the workflow? ( B is a hive action and C is a email action, so every time B fails, I want email action to be triggered saying it failed and its error message) </p><p style=\"margin-left: 20px;\">Please suggest.</p><p style=\"margin-left: 20px;\">\n</p>","tags":["Oozie","governance","action","workflow"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-15 20:58:07.0","id":40015,"title":"Anyone else experience memory management issues with 2.3.4.7?","body":"<p>We're starting to run into some 'too coincidental' out of memory and heap space issues with Tez and M/R jobs on the cluster. Anyone aware of any related known issues with 2.3.4.7?</p>","tags":["MapReduce","memory","Tez","heap"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-15 15:52:21.0","id":39962,"title":"query never response in apache drill","body":"<p>Hi:</p><p>In apache dril there is one query but never responde and is very easy:</p><pre>select * from desercion_clientes where id_interno_pe = '1'</pre><p>the thread are like this</p><pre>BitClient-2 id=23 state=RUNNABLE\n    at io.netty.channel.epoll.Native.epollWait0(Native Method)\n    at io.netty.channel.epoll.Native.epollWait(Native.java:148)\n    at io.netty.channel.epoll.EpollEventLoop.epollWait(EpollEventLoop.java:180)\n    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:205)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n    at java.lang.Thread.run(Thread.java:745)\n\nBitClient-3 id=24 state=RUNNABLE\n    at io.netty.channel.epoll.Native.epollWait0(Native Method)\n    at io.netty.channel.epoll.Native.epollWait(Native.java:148)\n    at io.netty.channel.epoll.EpollEventLoop.epollWait(EpollEventLoop.java:180)\n    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:205)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #1 id=121 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nBitClient-4 id=25 state=RUNNABLE\n    at io.netty.channel.epoll.Native.epollWait0(Native Method)\n    at io.netty.channel.epoll.Native.epollWait(Native.java:148)\n    at io.netty.channel.epoll.EpollEventLoop.epollWait(EpollEventLoop.java:180)\n    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:205)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n    at java.lang.Thread.run(Thread.java:745)\n\nBitClient-5 id=26 state=RUNNABLE\n    at io.netty.channel.epoll.Native.epollWait0(Native Method)\n    at io.netty.channel.epoll.Native.epollWait(Native.java:148)\n    at io.netty.channel.epoll.EpollEventLoop.epollWait(EpollEventLoop.java:180)\n    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:205)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #2 id=135 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #3 id=141 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #4 id=167 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #5 id=329 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #6 id=408 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #7 id=1911 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #8 id=1929 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nORC_GET_SPLITS #9 id=1943 state=WAITING\n    - waiting on &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x705dd484&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\nqtp2015874556-11724 id=11724 state=RUNNABLE\n    at sun.management.ThreadImpl.dumpThreads0(Native Method)\n    at sun.management.ThreadImpl.dumpAllThreads(ThreadImpl.java:446)\n    at com.codahale.metrics.jvm.ThreadDump.dump(ThreadDump.java:30)\n    at com.codahale.metrics.servlets.ThreadDumpServlet.doGet(ThreadDumpServlet.java:37)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:738)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:551)\n    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:219)\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1111)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:478)\n    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:183)\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1045)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n    at org.eclipse.jetty.server.Server.handle(Server.java:462)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:279)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:232)\n    at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:534)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:607)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:536)\n    at java.lang.Thread.run(Thread.java:745)\n\nqtp2015874556-11746 id=11746 state=TIMED_WAITING\n    - waiting on &lt;0x0c80a490&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    - locked &lt;0x0c80a490&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at sun.misc.Unsafe.park(Native Method)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:389)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:513)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.access$700(QueuedThreadPool.java:48)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:569)\n    at java.lang.Thread.run(Thread.java:745)\n\n</pre><p>and the log</p><pre>2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:39] INFO  o.a.d.e.w.fragment.FragmentExecutor - 289e877d-1dcd-d389-6d45-50335e4d9106:1:39: State change requested RUNNING --&gt; FINISHED\n\n2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:34] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:34: State to report: FINISHED\n\n2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:9] INFO  o.a.d.e.w.fragment.FragmentExecutor - 289e877d-1dcd-d389-6d45-50335e4d9106:1:9: State change requested RUNNING --&gt; FINISHED\n\n2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:29] INFO  o.a.d.e.w.fragment.FragmentExecutor - 289e877d-1dcd-d389-6d45-50335e4d9106:1:29: State change requested RUNNING --&gt; FINISHED\n\n2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:14] INFO  o.a.d.e.w.fragment.FragmentExecutor - 289e877d-1dcd-d389-6d45-50335e4d9106:1:14: State change requested RUNNING --&gt; FINISHED\n\n2016-06-15 17:47:19,904 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:29] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:29: State to report: FINISHED\n\n2016-06-15 17:47:19,904 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:19] INFO  o.a.d.e.w.fragment.FragmentExecutor - 289e877d-1dcd-d389-6d45-50335e4d9106:1:19: State change requested RUNNING --&gt; FINISHED\n\n2016-06-15 17:47:19,904 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:9] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:9: State to report: FINISHED\n\n2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:24] INFO  o.a.d.e.w.fragment.FragmentExecutor - 289e877d-1dcd-d389-6d45-50335e4d9106:1:24: State change requested RUNNING --&gt; FINISHED\n\n2016-06-15 17:47:19,907 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:24] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:24: State to report: FINISHED\n\n2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:4] INFO  o.a.d.e.w.fragment.FragmentExecutor - 289e877d-1dcd-d389-6d45-50335e4d9106:1:4: State change requested RUNNING --&gt; FINISHED\n\n2016-06-15 17:47:19,903 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:39] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:39: State to report: FINISHED\n\n2016-06-15 17:47:19,907 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:4] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:4: State to report: FINISHED\n\n2016-06-15 17:47:19,907 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:19] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:19: State to report: FINISHED\n\n2016-06-15 17:47:19,907 [289e877d-1dcd-d389-6d45-50335e4d9106:frag:1:14] INFO  o.a.d.e.w.f.FragmentStatusReporter - 289e877d-1dcd-d389-6d45-50335e4d9106:1:14: State to report: FINISHED\n</pre><p>any suggestions?? with other hive table everything work fine it just with this table.</p>","tags":["drill"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-15 19:16:24.0","id":40011,"title":"Presto select query could not find block exception.","body":"<p>I have hortonworks hadoop cluster setup on azure with (3 Master and 3 Datanodes). I also have presto cluster setup  (1 coordinator , 2 workernode).\nI want to access hive table data through presto queries but somehow i am not able to do so. My current configuration allow me to\nrun meta queries like show tables, desc etc. but not a actual content with simple select query. Please find exception as follows.</p><p>\ncom.facebook.presto.spi.PrestoException: Could not obtain block: BP-1050134713-10.0.0.6-1465436591384:blk_1073742418_1594 file=/tmp/data/geolocation.csv\nat com.facebook.presto.hive.GenericHiveRecordCursor.advanceNextPosition(GenericHiveRecordCursor.java:277)\nat com.facebook.presto.spi.RecordPageSource.getNextPage(RecordPageSource.java:99)\nat com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:246)\nat com.facebook.presto.operator.Driver.processInternal(Driver.java:378)\nat com.facebook.presto.operator.Driver.processFor(Driver.java:301)\nat com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:618)\nat com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:529)\nat com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:665)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1050134713-10.0.0.6-1465436591384:blk_1073742418_1594 file=/tmp/data/geolocation.csv\nat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:946)\nat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:604)\nat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:844)\nat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:896)\nat java.io.DataInputStream.read(DataInputStream.java:100)\nat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180)\nat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\nat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\nat org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:206)\nat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:244)\nat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:47)\nat com.facebook.presto.hive.GenericHiveRecordCursor.advanceNextPosition(GenericHiveRecordCursor.java:261)</p><p>\n\nIt looks to me presto nodes doesn't have access to datanodes as my hadoop cluster is different than presto cluster. Any kind of help would be greatly appreciated.</p>","tags":["Hive","metastore","hadoop","presto"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-15 21:52:43.0","id":40042,"title":"Ranger Audit Retention","body":"<p>Hi All,</p><p>I've searched around HCC and was unable to come up with an answer to: There doesn't seem to be a way to automatically define a retention policy for the Ranger Audit Data (audit data is kept indefinitely unless we manually remove it).  Is there a plan to add an automatic retention policy for these audit logs in HDFS and/or Solr? </p><p>* Falcon can be used for retention in HDFS - but will there be an easy-to-configure option under Ambari&gt;Ranger under Audit?</p>","tags":["Ranger","audit","retention"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-09-25 18:14:43.0","id":301,"title":"Using NFS with Ambari 2.1 and above","body":"<p>Before Ambari 2.1, we had to manager the NFS Gateway separately.  Now it's \"kind of\" part of the Ambari process.  At least it shows up in Ambari (HDFS Summary page) as installed and running.</p><p>But I don't see a away to control the bind, etc...  And there aren't any processes running like that.  So what is the process for using NFS with Ambari 2.1+?</p>","tags":["nfs","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 00:58:18.0","id":395,"title":"How to setup High Availability for Oozie?","body":"<p>To have Oozie server in HA, it is mentioned in the Hortonworks documentation that it needs a Loadbalancer, Virtual IP, or Round-Robin DNS. As this is not part of Hadoop ecosystem, what tool is suggest to use here? HAProxy/nginx/or any other commercial one?</p>","tags":["hadoop","high-availability","Oozie"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-28 21:51:35.0","id":369,"title":"Installed Ranger in a cluster and running into the following issues -","body":"","tags":["namenode","HDFS","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 01:05:47.0","id":400,"title":"Should I install Phoenix Query Server on all HBase region server nodes? What is the best practice and consideration?","body":"","tags":["deployment best practice","best-practices","Hbase","hadoop","Phoenix"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-29 00:45:01.0","id":388,"title":"How to connect to HBase using Tableau? Does Hortonworks has anything officially for ODBC support to HBase?","body":"","tags":["odbc","Hbase","tableau"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-30 14:50:40.0","id":622,"title":"How to Increase the Ambari Heap Size?","body":"<p>Where can I increase and set the Ambari heap size?</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 16:44:07.0","id":516,"title":"Help converting Teradata SQL to Hive","body":"<p>Can anyone help converting this Teradata SQL to something that will run in Hive? Currently it fails with the error \"SemanticException Line 0:-1 Both left and right aliases encountered in JOIN 'Week_End'\" and my understanding of SQL is too limited to understand how to refactor the query.</p><pre>(SELECT\nS.Calendar_Date Calendar_Date\n,S.Foll_Monday Foll_Monday\n,W.Week_Id Snapshot_Id\nFROM\n(\nSELECT calendar_date\n,CASE WHEN day_of_week = 1 THEN calendar_date + 1\nWHEN day_of_week = 3 THEN calendar_date + 6\nWHEN day_of_week = 4 THEN calendar_date + 5\nWHEN day_of_week = 5 THEN calendar_date + 4\nWHEN day_of_week = 6 THEN calendar_date + 3\nWHEN day_of_week = 7 THEN calendar_date + 2\nELSE calendar_date END AS Foll_Monday\nFROM td_etl.calendar )\nS LEFT JOIN td_etl.dim_cal W\nON S.Foll_Monday BETWEEN W.Week_Start AND W.Week_End) FM </pre>","tags":["Hive","sql","teradata"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-30 19:10:22.0","id":641,"title":"How to Increase Ranger Admin Heap Size?","body":"<p>How can the heap size of the Ranger Admin service be increased? What is a good recommended value?</p>","tags":["sizing","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-30 04:01:44.0","id":587,"title":"How to automate configuration changes to non default Ambari config group?","body":"<p>/var/lib/ambari-server/resources/scripts/configs.sh is a great way to automate config changes, but seems it only supports the 'Default' config group. Is there a clean way to make changes to other config groups via API?</p>","tags":["configuration","Ambari","api"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-01 23:48:55.0","id":721,"title":"acessing the STORM UI with HDP 2.3 Kerberized cluster.","body":"<p>All users are defined locally without any AD integration. The cluster is Kerberized on HDP 2.3 and Ambari 2.1.0. The knit fails as follow:</p><p> <strong>kinit: krb5_init_creds_set_keytab: Failed to find kafka/<a href=\"mailto:hdpblv10.t-mobile.lab@T-MOBILE.LAB\">hdpblv10.t-mobile.lab@T-MOBILE.LAB</a> in keytab FILE:/etc/security/keytabs/kafka.service.keytab (unknown enctype) .</strong></p><p>The Storm UI uses SPNEGO AUTH when in Kerberos mode. Before accessing the UI, you have to we configured our browser for SPNEGO authorization before accessing the UI as follow:</p><p><strong>Safari: </strong>\nno changes needed. \n<strong>Firefox: </strong>\n1) Go to about:config and search for network.negotiate-auth.trusted-uris. \n2) Double-click and add the following value: \"<a href=\"http://storm-ui-hostname:ui-port/\">http://storm-ui-hostname:ui-port</a>\" \n3) Replace the storm-ui-hostname with the hostname where your UI is running. \n4) Replace the ui-port with the Storm UI port. \n<strong>Google-chrome: </strong>\nfrom the command line, issue: \ngoogle-chrome --auth-server-whitelist=\"storm-ui-hostname\" --auth-negotiate-delegate-whitelist=\"storm-ui-hostname\" \n<strong>Internet Explorer: </strong>\n1) Configure trusted websites to include \"storm-ui-hostname\". \n2) Allow negotiation for the UI website. \n </p>","tags":["kerberos","hdp-2.3.0","Kafka","kinit","Storm"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-01 18:09:37.0","id":704,"title":"Ranger Hive Policies Pass Through If Non-Existing to HDFS Storage","body":"<p>As per title subject. </p><p>Is there a way I can have Ranger setup against Hive, but if a policy does not exist to pass through to the HDFS permissions rather then immediately deny? We had been considering setting up two different HiveServers, one which is using the Storage auth, and another which is using Ranger but we are not sure if that's immediately possible due to the way Ambari now makes this a toggle switch. </p><p>We only have a handful of tables that really require Ranger to be used for Column authorization (and ofc this data on HDFS will be owned by Hive) all the other tables don't require column authorization and have extensive HDFS Extended ACL use so there are many many users added already to these storage policies. </p><p>The ops team does not really want to migrate these unless absolutely required as they had started writing a script to do this in dev, and we end up with 1000's of policies that just make a total mess of everything. </p><p>Thoughts are welcome. </p><p>@Hive </p><p>@Security </p><p>@Ranger</p>","tags":["Hive","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-02 02:22:17.0","id":726,"title":"Which are the HBase performance benchmark tools available like Terasort/Teragen for MapReduce?","body":"","tags":["Hbase","benchmark","performance"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-01 21:18:26.0","id":714,"title":"Support for nested group / cascading group in access control list (ACLs).","body":"<p>Every time a new group in a organization needs access control, hdfs service needs to go through configuration change and service restart. This could be avoided by supporting cascading groups and onus will be on SysAdmins managing AD / LDAP.</p>","tags":["HDFS","hdfs-permissions"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-30 23:30:28.0","id":660,"title":"How to create a collection for SolrCloud on Solr 5.2.0?","body":"","tags":["solrcloud","SOLR"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-04 22:35:22.0","id":865,"title":"HDFS File Permissions","body":"<p>Hello,\nI want to test the file permissions of HDFS. By these Tests I get a strange behavior of Hadoop.\nI created a new directory with the user “root”. The used command was “hadoop fs -mkdir /user/test”.\nAfter this I changed the permissions of this directory to r, w, x only for the owner (“hadoop fs -chmod 700 /user/test”).\nAnd I copied a new file into this directory (“hadoop fs -put test.txt /user/test”) and I changed the permissions of this file (“hadoop fs -chmod 600 /user/test/test.txt”), too.</p><p>I created an new user and a new usergroup and added the new user to this group.\nWith this new user is accessed the folder (“hadoop fs -ls /user/test”) and deleted the file (“hadoop fs -rm ./user/test/test.txt”).\nWith the right permissions i havn’t do this.</p><p>I do this Test with the same file in the UNIX-Filesystem and there the Deletion failed. This is the right behavior I expected in HDFS.</p><p>I used the HDP 2.3 Sandbox with default configuration.</p><p>Had anyone the same behavior or did I a mistake?</p>","tags":["Sandbox","Ranger","HDFS"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 00:51:18.0","id":394,"title":"What are best practices for setting up Backup and Disaster Recovery for Hive in production?","body":"<p>Need Best practices for Backup and DR for</p><p>- Hive Metastore DB i.e. MySQL, Postgres etc</p><p>- Hive Data </p>","tags":["best-practices","Hive","backup","architecture"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-30 20:27:19.0","id":644,"title":"After upgraded Ambari from 1.7 to 2.1.1 then started receiving timeline metrics cannot connect collector socket errors","body":"<p>From ambari-server 2 log:\n</p><pre>28 Sep 2015 19:04:23,498 ERROR [qtp-client-6858] MetricsPropertyProvider:183 - Error getting timeline metrics. Can not connect to collector, socket error. \n28 Sep 2015 19:04:28,989 INFO [AlertNoticeDispatchService] AlertNoticeDispatchService:279 - There are 1 pending alert notices about to be dispatched... \n28 Sep 2015 19:04:28,993 INFO [alert-dispatch-96] EmailDispatcher:88 - Sending email: org.apache.ambari.server.state.alert.AlertNotification@35b0f886 \n28 Sep 2015 19:04:29,245 WARN [ambari-hearbeat-monitor] HeartbeatMonitor:154 - Heartbeat lost from host blpd813-priv.bhdc.att.com \n28 Sep 2015 19:04:33,519 ERROR [qtp-client-6858] MetricsReportPropertyProvider:223 - Error getting timeline metrics. Can not connect to collector, socket error. </pre><p>From the ambari-alerts log:</p><pre>2015-09-28 18:34:01,981 [CRITICAL] [YARN] [yarn_nodemanager_webui] (NodeManager Web UI) Connection failed to http://blpd852-priv.bhdc.att.com:8042 (&lt;urlopen error timed out&gt;) \n2015-09-28 18:34:03,088 [CRITICAL] [YARN] [yarn_nodemanager_webui] (NodeManager Web UI) Connection failed to http://blpd820-priv.bhdc.att.com:8042 (&lt;urlopen error timed out&gt;) \n2015-09-28 18:34:52,711 [OK] [YARN] [yarn_nodemanager_webui] (NodeManager Web UI) HTTP 200 response in 0.000s \n2015-09-28 18:34:53,709 [OK] [YARN] [yarn_nodemanager_webui] (NodeManager Web UI) HTTP 200 response in 0.000s \n2015-09-28 19:04:05,354 [CRITICAL] [YARN] [yarn_nodemanager_webui] (NodeManager Web UI) Connection failed to http://blpd810-priv.bhdc.att.com:8042 (&lt;urlopen error timed out&gt;) \n2015-09-28 19:04:56,024 [OK] [YARN] [yarn_nodemanager_webui] (NodeManager Web UI) HTTP 200 response in 0.000s</pre>","tags":["upgrade","Ambari","ambari-metrics","ambari-2.1.1"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-05 19:52:15.0","id":921,"title":"Phoenix Error Description","body":"<p>I am using phoenix “psql” utility to ingest some file data and it fails with many generic “ERROR 201 (22000): Illegal data<strong>” </strong>without more details. It is difficult to figure out which column data or datatype cause the failure.</p><p><strong><em>Is there any way to get more details?</em></strong></p><p>[root@dev HbaseData]# psql.py -d \"|\" localhost tgb_counter.csv</p><p>……</p><p>15/09/17 17:57:55 ERROR util.CSVCommonsLoader: Error upserting record [263, 1442437680, 1442437730, , 1442437703, 9, Moundville, UNKNOWN, Standard Outdoor TGB w/ Argus/Alpha Supply, 20150916, 20150916, 0, 0, 3, 0, 0, 18, 2, 3, 8, 0, 0, 18, 2, 3, 8, 0, 0, 0, 0, 0, 0, 61, 61, 61, 1442437716, 1442437806, ALOHA, 901162486, ALOHA/Double density reduced, ALOHA, 901112486, ALOHA/Normal, Poll Response/Priority, 901137486, Poll Response/Priority, ALOHA, 901187486, ALOHA/Double density reduced, , , , , , , , , , , , ]: java.sql.SQLException: <strong>ERROR 201 (22000): Illegal data.</strong></p><p>15/09/17 17:57:55 ERROR util.CSVCommonsLoader: Error upserting record [263, 1442437860, 1442437913, , 1442437888, 10, Moundville, UNKNOWN, Standard Outdoor TGB w/ Argus/Alpha Supply, 20150916, 20150916, 5, 0, 1, 0, 0, 20, 6, 21, 11, 4, 0, 20, 6, 21, 11, 0, 0, 0, 0, 0, 0, 61, 61, 61, 1442437995, 1442438125, ALOHA, 901162486, ALOHA/Double density reduced, ALOHA, 901112486, ALOHA/Normal, Poll Response/Priority, 901137486, Poll Response/Priority, ALOHA, 901187486, ALOHA/Double density reduced, , , , , , , , , , , , ]: java.sql.SQLException: <strong>ERROR 201 (22000): Illegal data.</strong></p>","tags":["Phoenix","Hbase","error"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-07 00:38:18.0","id":1030,"title":"Can't drop hive tables after moving hive into encryption zone.","body":"<p>We are validating Hadoop TDE and have moved /apps and /data into encryption zones. The hive.metastore.warehouse.dir is set to /apps/hive/warehouse </p><p>\nThere are 2 issues. </p><p>\n1. We cant delete hive tables because Trash is not in the encryption zone. </p><pre>hive&gt; CREATE DATABASE alan_test; \nOK \nTime taken: 3.027 seconds \nhive&gt; use alan_test; \nOK \nTime taken: 0.315 seconds \nhive&gt; CREATE TABLE a(id int); \nOK \nhive&gt; DROP DATABASE alan_test; \nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database alan_test is not empty. One or more tables exist.) \n</pre><p>2. We can only delete files via hdfs dfs -rm .... if we use the -skipTrash option.  </p><pre>[root@devsl1 ~]# sudo -u ssn hdfs dfs -copyFromLocal TEST /apps/hive/ \n[root@devsl1 ~]# sudo -u ssn hdfs dfs -rm /apps/hive/TEST \n15/10/06 15:51:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes. \nrm: Failed to move to trash: hdfs://devsl1.eng.ssnsgs.net:8020/apps/hive/TEST: /apps/hive/TEST can't be moved from an encryption zone. \n\n\n[root@devsl1 ~]# sudo -u ssn hdfs dfs -rm -skipTrash /apps/hive/TEST \nDeleted /apps/hive/TEST \n</pre><p>I think this is because Hive does not support HDFS TDE yet (HIVE-8065).</p><p>But just want to make sure it's the case and see if there is any workaround.</p>","tags":["encryption","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-05 15:08:52.0","id":893,"title":"Recommendations of Java version and Garbage Collector for NiFi?","body":"<p>We were recently informed that Nifi bottlenecks tend to be, in order of occurrence:</p><ol><li>CPU</li><li>Memory</li><li>Disk</li></ol><p>Is there a recommended Java version  (Java 7 vs 8) and  garbage collector (Concurrent Mark Sweep vs G1) ?</p>","tags":["java","garbage-collector","hdf","Nifi","benchmark"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-07 17:06:51.0","id":1076,"title":"Where can I find the latest Hive ODBC Driver for HDP 2.3?","body":"<p>Where can I find the latest Hive ODBC Driver for HDP 2.3?</p><p>We used to have a link under AddOns on our website, however we only have SmartSense under AddOns at the moment.</p><p>I can find the previous version (Hive ODBC Driver for HDP 2.2, v1.4.14) for HDP 2.2 in or Downloads-Archive =&gt; <a target=\"_blank\" href=\"http://hortonworks.com/products/releases/hdp-2-2/#add_ons\">http://hortonworks.com/products/releases/hdp-2-2/#add_ons</a>, is this version compatible with the Hive version included in HDP 2.3 package?</p><p>Thanks!</p>","tags":["hdp-2.3.0","odbc","Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-08 15:54:01.0","id":1130,"title":"When using the InvokeHTTP processor it uses basic authentication but why is the password still shown in clear text?","body":"","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-09 18:39:43.0","id":1230,"title":"using Oracle as the database for Ambari, Hive and Oozie, how do we go about sizing the each DB? also, which versions of oracle do we support 11g, 12g?","body":"","tags":["installation","Ambari","configuration"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-08 18:58:54.0","id":1148,"title":"Should I use RDMA in Microsoft Azure?","body":"<p>According to https://azure.microsoft.com/en-gb/documentation/articles/virtual-machines-a8-a9-a10-a11-specs/ The A8-9 instances support an RDMA 32MBs backplane for node to node communication on SLES. </p><p>Is the SLES image the preferred / only image which support this networking layer, are there RedHat flavour alternatives.</p><p>Would access to the 32MBs backplane through a multi-home topology make a significant difference to intra-cluster communication vs relatively small CPU scale in A8-9? </p><p>Simon</p>","tags":["network","os","azure","sles","microsoft"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-08 22:50:11.0","id":1176,"title":"Are there any additional metrics to consider that aren't shown under the stock Ambari Metrics?","body":"<p>A customer asked if the stats and information provided out of the box in Ambari Metrics provides all the things needed to represent cluster health and performance on a process level ( example - hive/hbase/hdfs, etc).  </p><p>I figured the stock metrics we display out of the box are 'good enough' and I know that Ambari Metrics will allow us to add even more custom metrics for a given service, if required.   </p><p>Does anyone have recommendations / suggestions on any additional metrics that they've found useful or helpful for customers?   If there are, I figured Ambari Metrics would be the place to add those metrics rather than build out some sort of external dashboard.  Thanks. </p>","tags":["Ambari","ambari-metrics","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-13 15:44:09.0","id":1342,"title":"How to Enable Zookeeper Discovery for HiveServer2 HA","body":"<p>Hi,</p><p>I am trying to set up zookeeper discovery for HS2 and I am getting various connection errors when I try to connect via beeline.</p><p>The following is the connection string.</p><pre>!connect jdbc:hive2://host1.com:2181,host2.com:2181,host3.com:2181;serviceDiscoveryMode=zooKeeper; zooKeeperNamespace=hiveserver2 </pre><p>I am following <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_hadoop-ha/content/ha-hs2-rolling-upgrade.html\">this documentation</a>. Can anyone confirm if there are any other steps/configs required? Is the connection string wrong perhaps? Does HS2 need to run in http mode or is binary ok?</p><p>\n</p><p>Configuration Requirements </p><p>1. Set hive.zookeeper.quorum to the ZooKeeper ensemble (a comma separated list of ZooKeeper server host:ports running at the cluster) </p><p>2. Customize hive.zookeeper.session.timeout so that it closes the connection between the HiveServer2’s client and ZooKeeper if a heartbeat is not received within the timeout period. </p><p>3. Set hive.server2.support.dynamic.service.discovery to true </p><p>4. Set hive.server2.zookeeper.namespace to the value that you want to use as the root namespace on ZooKeeper. The default value is hiveserver2. </p><p>5. The adminstrator should ensure that the ZooKeeper service is running on the cluster, and that each HiveServer2 instance gets a unique host:port combination to bind to upon startup.</p>","tags":["hdp-2.3.0","help","Hive","hiveserver2","zookeeper"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-15 14:07:08.0","id":1502,"title":"Pattern: convert CSV to JSON?","body":"<p>Hi, what's the recommended processor sequence to parse single-line csv entries into JSON? I'm all set on ingest and egress, but a little fuzzy on the conversion part still.</p>","tags":["Nifi","csv","hdf","json"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-20 20:13:04.0","id":1748,"title":"Why does LDAP think the user already exists?  This is the first time we were able to connect to AD to configure Kerberos.","body":"<pre>javax.naming.NameAlreadyBoundException: [LDAP: error code 68 - 00002071: UpdErr: DSID-0305038D, problem 6005 (ENTRY_EXISTS), data 0\n^@]; remaining name 'cn=prodcluster-102015,ou=Hadoop,dc=corp,dc=ds,dc=client,dc=com'\n        at com.sun.jndi.ldap.LdapCtx.mapErrorCode(LdapCtx.java:3082)\n        at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:3033)\n        at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:2840)\n        at com.sun.jndi.ldap.LdapCtx.c_createSubcontext(LdapCtx.java:811)\n        at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_createSubcontext(ComponentDirContext.java:337)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.createSubcontext(PartialCompositeDirContext.java:266)\n        at javax.naming.directory.InitialDirContext.createSubcontext(InitialDirContext.java:202)\n        at org.apache.ambari.server.serveraction.kerberos.ADKerberosOperationHandler.createPrincipal(ADKerberosOperationHandler.java:319)</pre>","tags":["active-directory","ldap","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-06 20:33:08.0","id":1018,"title":"How to Configure Ranger and Usersync for LDAP SSL and Certificate","body":"<p>In the Usync Log we are getting \"Unable to find the valid certification path\"</p><p>Documentation does not have anything on LDAP SSL and ranger.</p><p>Ambari and LDAP ssl is working...<a href=\"/storage/attachments/208-screen-shot-2015-10-06-at-42559-pm.png\">screen-shot-2015-10-06-at-42559-pm.png</a></p><p>Tried to put the java cacerts as the usersync.trustore file as the LDP cert is in java cacerts.</p><p>This does not work.</p><p>What is the exact detail process to get LDAP SSL working for Ranger Usync</p>","tags":["ldap","Ranger","security","ssl"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-27 16:31:25.0","id":2204,"title":"Does Knox allow LDAP Password to be stored outside the the topology xml file?","body":"<p>Is there a way the LDAP password can be stored somewhere other than \"main.ldapRealm.contextFactory.systemPassword\" in the topology XML config file? Customer would like to store this password elsewhere for added security.</p><p>Thanks!</p>","tags":["security","Knox","ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-03 08:13:19.0","id":837,"title":"Falcon mirror recipe: What happens if an instance is still running when the next is scheduled to start?","body":"<p>When using the Falcon \"Mirror Recipe\", what happens if an instance is still running when the next is scheduled to start?</p>","tags":["disaster-recovery","mirroring","Falcon","replication","backup"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-21 09:05:50.0","id":7433,"title":"how to import job on sqoop from sql server connection with only on  windows authentication","body":"","tags":["Sqoop"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-22 11:45:32.0","id":7660,"title":"How to find nth highest salary in hive.","body":"<p>Hi guys,</p><p>I know hive is Data Warehousing tool which is not suited as OLTP. In regular RDBMS Structure i have way like \"Correlated queries\" helps to find nth highest salary from a table as following.</p><p>Is there any way in hive also.The query is related to true/false conditions.</p><pre>SELECT * /*This is the outer query part */\nFROM Employee Emp1\nWHERE (N-1) = ( /* Subquery starts here */\nSELECT COUNT(DISTINCT(Emp2.Salary))\nFROM Employee Emp2\nWHERE Emp2.Salary &gt; Emp1.Salary)\n\n</pre>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-22 23:54:16.0","id":7744,"title":"What dependencies I need to use phoenix jdbc installed on hbase server.","body":"<p>Hi,</p><p>I have hbase server that have Phoenix installed . It is using kerberos . </p><p>phoenix-4.2.0.2.2.4.2-2-client.jar  phoenix-4.2.0.2.2.4.2-2-server.jar . </p><p>I am trying to write java code where I can use phoenix jdbc driver and write a simple sql \" select * from emp \" .</p><p>What are the POM dependencies I need for Phoenix ?</p><p>How should My connection string look like ? I am using : </p><p>conn =  DriverManager.getConnection(\"jdbc:\" + zookeeper + \":2181/hbase-secure:principal@default_domain.com:\" + keytabPath);</p><p>where principal@default_domain.com : krb.principal </p>","tags":["Phoenix","Hbase"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-23 11:56:32.0","id":7839,"title":"Will HDF (Apache NiFi) support Azure Blob Storage?","body":"<p>Hi,</p><p>As far as I'm aware Hortonworks Data Flow (HDF), which is powered by Apache NiFi, does not support data ingress/data egress to Azure Blob Storage. Are there any plans for supporting Azure Blob Storage as a data source/sink? Thanks.</p><p>Cheers</p><p>Ryan</p>","tags":["azure","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-05 19:30:12.0","id":8843,"title":"What is 100% free method to install HDP sandbox and start learning HDP?","body":"<p>I have a laptop that has 4GB memory and can't try ambari owing to resource limitations. As per a tutorial from hortonworks I tried using Azure. In free trial I can't choose A4 sized virtual machine. This is only on paid basis. I would like to know is there a way to start learning HDP using hortonworks sandbox 100% free? Also, I think amazon EC2 is again on paid basis (.30 cents/hour I think)</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-05 06:15:51.0","id":25931,"title":"Sqoop import command generate Java files automatically","body":"<p>Hi Team,</p><p>When I run sqoop command to import tables from oracle, it will automatically generate a java file of same table name in the current path of local system. Following are my questions,</p><p>1. Why they are generated?</p><p>2. How can i change path for the same?</p><p>3. If i delete that java files does it impact on future operations?</p><p>4. How to restrict to generate it?</p><p>Please find attached screenshot for reference.</p><p><img src=\"/storage/attachments/3206-capture.jpg\"></p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-05 19:45:50.0","id":26027,"title":"Can the ranger audit records that are stored in HDFS be viewed through Ranger Audit UI?","body":"<p>Hi, </p><p>I have Ranger 0.5.0.2.3 and HDP 2.3.4. In the Ranger configuration, there is option to store the Ranger audit records to HDFS. Is it possible to view the audit records that are stored in HDFS through the Ranger UI? </p><p>Is it possible to store the audit records to just in HDFS (No Solr, or DB), if so how to view them and what should I be setting the <strong>ranger.</strong><strong>audit.</strong><strong>source.</strong><strong>type </strong>property be set to? </p><p>Thanks, </p><p>Madhavi. </p>","tags":["ranger-0.5.0","audit"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-07 09:22:08.0","id":26327,"title":"Integrate a Oracle DW with Hadoop or transit all operational data to Hadoop?","body":"<p>Hello,\n\nI have a Oracle DW which is connect to my core to record the data. I'm planning introduce the ecosystem Hadoop in my next release. What's the best option/architecture (If anyone knows some article that talk about this, It would be great to share):\n\n</p><ul><li>\nPut Hadoop connect directly to Core and record all the operational data. Basically have 2 steps (Operational Data -&gt; Hadoop)\n</li><li>\nPut Hadoop connect to my Data Warehouse and have 3 stepS (Operational Data -&gt; DW -&gt; Hadoop).\nI repeat, It will be great if anyone could share some articles related to this :) \n\n\nThanks! :)</li></ul>","tags":["oracle","hadoop-ecosystem"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-08 08:53:14.0","id":26581,"title":"REST API to get individual HDP component version","body":"<p>Hello folks,</p><p>could you please help me getting the REST API to get individual HDP component versions. The most most close detail I got was through following but it does not give me the version details :- </p><pre>\"href\" : \"http://192.168.100.130:8080/api/v1/clusters/MyCluster/hosts/ambari.dn2.local/host_components/PIG\",\n  \"HostRoles\" : {\n    \"cluster_name\" : \"MyCluster\",\n    \"component_name\" : \"PIG\",\n    \"desired_stack_id\" : \"HDP-2.3\",\n    \"desired_state\" : \"INSTALLED\",\n    \"hdp_version\" : \"HDP-2.3.4.0-3485\",\n    \"host_name\" : \"ambari.dn2.local\",\n    \"maintenance_state\" : \"OFF\",\n    \"service_name\" : \"PIG\",\n    \"stack_id\" : \"HDP-2.3\",\n    \"stale_configs\" : false,\n    \"state\" : \"INSTALLED\",\n    \"upgrade_state\" : \"NONE\",\t\n</pre>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-08 10:31:33.0","id":26600,"title":"ERROR: Execute oozie insert data from hive table to hbase table","body":"<p>HI:</p><p>   Today, I created a hive table point to hbase table and  a hive native table.</p><p>  I wanted to use oozie execute a job that read hive table and write to hbase table, but the program is error.</p><p><strong>job.properties:</strong></p><p style=\"margin-left: 20px;\">nameNode=hdfs://cluster1.new:8020</p><p style=\"margin-left: 20px;\">\njobTracker=cluster2.new:8050</p><p style=\"margin-left: 20px;\">\nqueueName=default</p><p style=\"margin-left: 20px;\">\nexamplesRoot=examples</p><p style=\"margin-left: 20px;\">\noozie.use.system.libpath=true</p><p style=\"margin-left: 20px;\">\noozie.wf.application.path=${nameNode}/user/${user.name}/hive2</p><p><strong>workflow.xml</strong></p><p>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;workflow-app xmlns=\"uri:oozie:workflow:0.2\" name=\"hive-wf\"&gt; </p><p>    &lt;start to=\"hive-node\"/&gt;</p><p>\n    &lt;action name=\"hive-node\"&gt; </p><p>        &lt;hive xmlns=\"uri:oozie:hive-action:0.2\"&gt;</p><p>\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;</p><p>\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt; </p><p>           \n            &lt;configuration&gt;</p><p>\n                &lt;property&gt;</p><p>\n                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</p><p>\n                    &lt;value&gt;${queueName}&lt;/value&gt; </p><p>                &lt;/property&gt;</p><p>\n            &lt;/configuration&gt; </p><p>            &lt;script&gt;script.q&lt;/script&gt; </p><p>\n        &lt;/hive&gt;\n        &lt;ok to=\"end\"/&gt;</p><p>\n        &lt;error to=\"fail\"/&gt;</p><p>\n    &lt;/action&gt; </p><p>    &lt;kill name=\"fail\"&gt; </p><p>        &lt;message&gt;Hive failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;</p><p>\n    &lt;/kill&gt;</p><p>\n    &lt;end name=\"end\"/&gt; </p><p>&lt;/workflow-app&gt;</p><p><strong>script.q</strong></p><p>insert into default.t2 select ceil(rand() * 100000),c1,count(c1) from default.t1 group by c1;</p><p>The error log said:</p><p>---------------------------------------------------------------------------------------------</p><pre>Map 1: -/-\tReducer 2: 0/1\t\nMap 1: 0/2\tReducer 2: 0/1\t\nMap 1: 0(+1)/2\tReducer 2: 0/1\t\nMap 1: 1(+0)/2\tReducer 2: 0/1\t\nMap 1: 1(+0)/2\tReducer 2: 0(+1)/1\t\nMap 1: 1(+1)/2\tReducer 2: 0(+1)/1\t\nMap 1: 2/2\tReducer 2: 0(+1)/1\t\nMap 1: 2/2\tReducer 2: 0(+1,-1)/1\t\nMap 1: 2/2\tReducer 2: 0(+1,-1)/1\t\nMap 1: 2/2\tReducer 2: 0(+1,-1)/1\t\nMap 1: 2/2\tReducer 2: 0(+1,-1)/1\t\nMap 1: 2/2\tReducer 2: 0(+1,-2)/1\t\nMap 1: 2/2\tReducer 2: 0(+1,-2)/1\t\nMap 1: 2/2\tReducer 2: 0(+1,-3)/1\t\nStatus: Failed\nVertex failed, vertexName=Reducer 2, vertexId=vertex_1460102561391_0019_1_01, diagnostics=[Task failed, taskId=task_1460102561391_0019_1_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:172)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:168)\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:310)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:164)\n\t... 14 more\nCaused by: java.lang.RuntimeException: java.lang.NullPointerException\n\tat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:208)\n\tat org.apache.hadoop.hbase.client.ClientSmallReversedScanner.loadCache(ClientSmallReversedScanner.java:211)\n\tat org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next(ClientSmallReversedScanner.java:185)\n\tat org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1255)\n\tat org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1161)\n\tat org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:370)\n\tat org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:321)\n\tat org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:206)\n\tat org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:183)\n\tat org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1449)\n\tat org.apache.hadoop.hbase.client.HTable.close(HTable.java:1485)\n\tat org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat$MyRecordWriter.close(HiveHBaseTableOutputFormat.java:128)\n\tat org.apache.hadoop.hive.ql.io.HivePassThroughRecordWriter.close(HivePassThroughRecordWriter.java:46)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.closeWriters(FileSinkOperator.java:197)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:1016)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:617)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:631)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:631)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:287)\n\t... 15 more</pre><p>---------------------------------------------------------------------------------------------</p><p>But, I used shell which could make it well.</p><p>So, </p><p>  If somebody know how to solute this problem, please to tell me.</p>","tags":["oozie-hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-11 14:40:43.0","id":26953,"title":"Can there be 2 Authentication Provider in 1 Knox Topology?","body":"<p>Can I configure two authentication provider for Knox and if how would that work?</p>","tags":["knox-gateway","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-25 21:19:39.0","id":327,"title":"Can one Zookeeper quorum support multiple HDFS instances for automatic HA failover?","body":"<p>Is it possible use a single Zookeeper quorum for managing the HA state of more than one HDFS-HA?</p><p>If so:</p><ol><li>Would there be any scaling limits with this approach?  A practical upper limit on number of Namenodes that can work across a single Zookeeper quorum?</li><li>What steps are required to configure the multiple HDFS Namenodes to use the single Zookeeper quorum?</li><li>What impact would this have on upgrades and maintenance?</li><li>Does Hortonworks support provide support for Namenodes or Zookeeper implemented in this configuration?</li></ol><p>In general, are there reasons why this would be not recommended?</p>","tags":["zookeeper","configuration","fail-over","HDFS"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-29 01:14:12.0","id":403,"title":"How to register existing Kafka service to Ambari?","body":"<p>I have upgraded Ambari to 2.1.0 and HDP to 2.2.6, now I want to manage Kafka using Ambari as it was not managed by Ambari in Ambari1.5.1. How to add Kafka service to Ambari?</p>","tags":["hdp-2.3.4","upgrade","ambari-2.1.0","Kafka","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 15:55:41.0","id":503,"title":"Are there any recommendations or best practices for using Anti-virus with Hadoop servers?","body":"","tags":["help","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-29 17:04:16.0","id":521,"title":"Falcon Passing parameters to existing Oozie Workflow with Hive Action","body":"<p>Got an existing oozie workflow that has Hive Action. When called through Falcon, its not dynamically substituting the job.properties values. \nthe oozie job fails with the Variable name not found. \nanyone encountered the scenario ?</p>","tags":["Oozie","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-09-30 14:53:37.0","id":623,"title":"Are Failed Messages a Storm Spout Reports Lost?","body":"<p>If a Storm Spourt reports X number of messages as failed, are these messages lost and were not replayed? So these messages were never processed by the toplogoy? How to find out which messages were effected?</p>","tags":["fail-over","Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-01 14:31:58.0","id":691,"title":"Ambari Views Connection Errors in Multi-Homed Network","body":"<p>Hi everyone,</p><p>Is there any advice on configuring Ambari views when you have a multi-homed network?</p><p>We've got an issue where a newly configured Tez View is throwing this error:</p><pre>error code: 500, message: Server Error java.net.ConnectException: Connection refused Could not retrieve expected data from Timeline Server @ https://HOSTNAME:8190/ws/v1/timeline/TEZ_DAG_ID </pre><p>Looking through yarn-site, I can see that various properties like yarn.resourcemanager.webapp.address, yarn.timeline-service.bind-host, yarn.timeline-service.webapp.address and yarn.timeline-service.webapp.https.address are set to 0.0.0.0:PORT.</p><p><strong>\n</strong></p><p><strong>Question</strong> - is this valid configuration? Is the multi-homing affecting the Tez View and if so what is the recommended configuration?</p><p>-- Ana</p>","tags":["ambari-views","yarn-ats","YARN","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-02 02:24:14.0","id":727,"title":"Ranger - hive doAs as false -  external tables not owned by hive user.","body":"<p>For doAs=false,</p><p>every Hive table has to be owned by “hive” user and if anyone creates an external table, that won’t be owned by Hive user. Are we supposed to restrict usage of external tables?</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-25 17:34:01.0","id":283,"title":"Error while installing component on a host via Ambari REST API","body":"<p>I am trying to install a component on a host via API:</p><pre>$ curl -u admin -i -H \"X-Requested-By:ambari\" -X PUT \\\n  -d '{\"HostRoles\": {\"state\": \"INSTALLED\"}}' \\\n  http://&lt;ambari&gt;:8080/api/v1/clusters/clustername/hosts/node1.hortonworks.com/host_components/ZOOKEEPER</pre><p>...\ngetting error:</p><pre>Caused by: java.lang.NullPointerException\nat org.apache.ambari.server.state.ConfigHelper.getPropertyValuesWithPropertyType(ConfigHelper.java:489)\nat org.apache.ambari.server.controller.AmbariManagementControllerImpl.createHostAction(AmbariManagementControllerImpl.java:1934)\nat org.apache.ambari.server.controller.AmbariManagementControllerImpl.doStageCreation(AmbariManagementControllerImpl.java:2336)\nat org.apache.ambari.server.controller.AmbariManagementControllerImpl.addStages(AmbariManagementControllerImpl.java:2593)\nat org.apache.ambari.server.controller.internal.HostComponentResourceProvider.updateHostComponents(HostComponentResourceProvider.java:612)\nat org.apache.ambari.server.controller.internal.HostComponentResourceProvider$4.invoke(HostComponentResourceProvider.java:753)</pre><p>The operation fails with \"Server Error\"</p><p>Any ideas?</p>","tags":["api","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-05 22:27:20.0","id":927,"title":"Sanity Check / Cluster Validation documents?","body":"<p>Do we have any public-consumable documents for \"Sanity Checking\" a cluster?  Aside from running the service checks and ensuring all services start and stop properly, are there any other tests that are run in the field to help validate and ensure the cluster is running acceptably?</p><p>Thanks!</p>","tags":["hdp-2.3.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-23 14:25:16.0","id":1985,"title":"Phoenix Query Server - upsert fail","body":"<p>Hi,</p><p>I'm testing phoenix query server, working with a prospect that wants to use phoenix + .NET. </p><p>I'm not being able to succeed with upsert thru phoenix query server. curl response seems to be ok, but new data WAS NOT COMMITTED. It might be related to those two jiras:</p><p><a href=\"https://issues.apache.org/jira/browse/PHOENIX-2320\">https://issues.apache.org/jira/browse/PHOENIX-2320</a></p><p><a href=\"https://issues.apache.org/jira/browse/PHOENIX-234\">https://issues.apache.org/jira/browse/PHOENIX-234</a></p><p>my create table is:</p><pre>create table teste(\n  id bigint not null,\n  text varchar \n  constraint pk primary key (id)\n) ;\n</pre><p>Here is a select statement that works thru phoenix query server:</p><pre>[root@hdp23 ~]# curl -XPOST -H 'request: {\"request\":\"prepareAndExecute\",\"connectionId\":\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\",\"sql\":\"select * from teste\",\"maxRowCount\":-1}' http://localhost:8765/\n{\"response\":\"Service$ExecuteResponse\",\"results\":[{\"response\":\"resultSet\",\"connectionId\":\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\",\"statementId\":1955331455,\"ownStatement\":true,\"signature\":{\"columns\":[{\"ordinal\":0,\"autoIncrement\":false,\"caseSensitive\":false,\"searchable\":true,\"currency\":false,\"nullable\":0,\"signed\":true,\"displaySize\":40,\"label\":\"ID\",\"columnName\":\"ID\",\"schemaName\":\"\",\"precision\":0,\"scale\":0,\"tableName\":\"TESTE\",\"catalogName\":\"\",\"type\":{\"type\":\"scalar\",\"id\":-5,\"name\":\"BIGINT\",\"rep\":\"PRIMITIVE_LONG\"},\"readOnly\":true,\"writable\":false,\"definitelyWritable\":false,\"columnClassName\":\"java.lang.Long\"},{\"ordinal\":1,\"autoIncrement\":false,\"caseSensitive\":false,\"searchable\":true,\"currency\":false,\"nullable\":1,\"signed\":false,\"displaySize\":40,\"label\":\"TEXT\",\"columnName\":\"TEXT\",\"schemaName\":\"\",\"precision\":0,\"scale\":0,\"tableName\":\"TESTE\",\"catalogName\":\"\",\"type\":{\"type\":\"scalar\",\"id\":12,\"name\":\"VARCHAR\",\"rep\":\"STRING\"},\"readOnly\":true,\"writable\":false,\"definitelyWritable\":false,\"columnClassName\":\"java.lang.String\"}],\"sql\":null,\"parameters\":[],\"cursorFactory\":{\"style\":\"LIST\",\"clazz\":null,\"fieldNames\":null}},\"firstFrame\":{\"offset\":0,\"done\":true,\"rows\":[[1,\"guilherme\"],[2,\"isabela\"],[3,\"rogerio\"],[4,null]]},\"updateCount\":-1}]}\n</pre><p>And here insert statement, it says it worked, \"updateCount=1\"</p><pre>[root@hdp23 log]# curl -XPOST -H 'request: {\"request\":\"prepareAndExecute\",\"connectionId\":\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\",\"sql\":\"upsert into teste (id) values (10)\",\"maxRowCount\":-1}' http://localhost:8765/\n{\"response\":\"Service$ExecuteResponse\",\"results\":[{\"response\":\"resultSet\",\"connectionId\":\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\",\"statementId\":1069768164,\"ownStatement\":false,\"signature\":null,\"firstFrame\":null,\"updateCount\":1}]}\n</pre><p>If I select table \"teste\" the new line is not there. I was not committed to phoenix table.</p><p>Additionally, /var/log/hbase/phoenix-hbase-server.log does show any message after command above.</p><p>Anyone has any idea what is going on and/or how to debug?</p><p>Thanks.</p><p>Guilherme</p>","tags":["Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-23 18:00:47.0","id":2029,"title":"Best practices for ulimits? (number of open file decriptors and processes)","body":"<p>What are the recommended starting values for ulimits for each component?</p><p>The Ambari 2.1 doc says 10000, but should some services be started higher, say 32k?</p><p>Is there a good way to estimate these values based on cluster size, memory/cpu, number of blocks, etc? How can we proactively adjust ulimit, to avoid waiting until a service fails because the limit is hit?</p>","tags":["performance","best-practices","os"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-26 21:11:17.0","id":2152,"title":"How to increase the frequency of the Feed Retention job within Falcon?","body":"<p>Is there a hack to increase the frequency of Falcon's Feed Retention job from 360 minutes (i.e. six hours) to something more demonstrable in a POC? (say, 10 mins?)</p>","tags":["Falcon","retention"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-28 06:23:55.0","id":2259,"title":"What is the Oozie Heap size recommendation for production?","body":"<p>If a cluster needs to run 100 Oozie workflow concurrently, is there any formula to estimate oozie_heapsize?</p><p>Or is there any internal/external best practice document mentioning about heap size?</p>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-28 21:03:12.0","id":2312,"title":"Will older docs suffice for enabling SSL on Hiveserver2 on HDP-2.3.x?","body":"<p>I know the HDP-2.3 security guide is in progress, but, had two questions on enabling SSL for HiveServer2.  In this use case, there are no plans to deploy Kerberos or Knox at this time, but, the desire is to have some sort of encrypted traffic for Hiveserver2:</p><p>1) Will these docs suffice?:</p><p>http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.5/bk_Security_Guide/content/ch_wire-hiveserver2.html</p><p>2) Does Hiveserver2 SSL work only in http mode? Or it can be turned on for binary mode too? </p>","tags":["Hive","hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-03 01:05:32.0","id":2652,"title":"Ambari – AD LDS (LDAP) integration","body":"<pre>Hi,</pre><p>\nI have setup Ambari integration with LDAP (AD LDS) and user & group sync working fine but I am not able to logging into ambari web using my AD LDS user and password. After further debugging, I have found that “Bind is success but User does not have read access on their own user object attributes from AD LDS”. After discussing with AD LDS admin team, they confirmed modifying the security on the AD LDS directory is not an option. Does ambari works if the logged in user not having read access on their own user object attributes? Can we customize ambari user  web login (spring security) behaviour? </p><pre>\nAmbari ldap configuration in ambari.properties\n============================================== api.authenticate=true\nauthentication.ldap.baseDn=DC=jcs,DC=com\nauthentication.ldap.bindAnonymously=false authentication.ldap.dnAttribute=distinguishedName authentication.ldap.groupMembershipAttr=member; authentication.ldap.groupNamingAttr=cn authentication.ldap.groupObjectClass=group\nauthentication.ldap.managerDn=CN=&lt;manager DN&gt;,OU=ApplAccounts,OU=Applications,DC=jcs,DC=com\nauthentication.ldap.managerPassword=/etc/ambari-server/conf/ldap-password.dat\nauthentication.ldap.primaryUrl=adlds.jcs.com:636 authentication.ldap.referral=ignore\nauthentication.ldap.useSSL=true\nauthentication.ldap.userObjectClass=user authentication.ldap.usernameAttribute=cn bootstrap.dir=/var/run/ambari-server/bootstrap bootstrap.script=/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py\nbootstrap.setup_agent.script=/usr/lib/python2.6/site-packages/ambari_server/setupAgent.py\nclient.security=ldap </pre><p>Error Log: </p><p>============ </p><p>28 Oct 2015 12:16:51,900 DEBUG [qtp-client-22 - /api/v1/users/cmmuriat?fields=*,privileges/PrivilegeInfo/cluster_name,privileges/PrivilegeInfo/permission_name&_=1446052611633] AbstractContextSource:259 - Got Ldap context on server 'ldaps://adlds.jcs.com:636/dc=jcs,dc=com'\n28 Oct 2015 12:16:51,904 DEBUG [qtp-client-22 - /api/v1/users/cmmuriat?fields=*,privileges/PrivilegeInfo/cluster_name,privileges/PrivilegeInfo/permission_name&_=1446052611633] SpringSecurityLdapTemplate:213 - <strong>Searching for entry under DN 'dc=jcs,dc=com', base = '', filter = '(cn={0})' </strong></p><p>28 Oct 2015 12:16:51,905 DEBUG [qtp-client-22 - /api/v1/users/cmmuriat?fields=*,privileges/PrivilegeInfo/cluster_name,privileges/PrivilegeInfo/permission_name&_=1446052611633] SpringSecurityLdapTemplate:229 - <strong>Found DN: cn=cmmuriat,ou=JIPeople</strong> </p><p>28 Oct 2015 12:16:51,909 DEBUG [qtp-client-22 - /api/v1/users/cmmuriat?fields=*,privileges/PrivilegeInfo/cluster_name,privileges/PrivilegeInfo/permission_name&_=1446052611633] BindAuthenticator:108 - <strong>Attempting to bind as cn=cmmuriat,ou=JIPeople,dc=jcs,dc=com </strong></p><p>28 Oct 2015 12:16:52,172 DEBUG [qtp-client-22 - /api/v1/users/cmmuriat?fields=*,privileges/PrivilegeInfo/cluster_name,privileges/PrivilegeInfo/permission_name&_=1446052611633] AbstractContextSource:259 - <strong>Got Ldap context on server 'ldaps://adlds.jcs.com:636/dc=jcs,dc=com' </strong></p><p><strong></strong>28 Oct 2015 12:16:52,172 DEBUG [qtp-client-22 - /api/v1/users/cmmuriat?fields=*,privileges/PrivilegeInfo/cluster_name,privileges/PrivilegeInfo/permission_name&_=1446052611633] BindAuthenticator:116 - <strong>Retrieving attributes... </strong></p><p>28 Oct 2015 12:16:52,180 DEBUG [qtp-client-22 - /api/v1/users/cmmuriat?fields=*,privileges/PrivilegeInfo/cluster_name,privileges/PrivilegeInfo/permission_name&_=1446052611633] AmbariLdapAuthenticationProvider:62 - Got exception during LDAP authentification attempt\norg.springframework.security.authentication.InternalAuthenticationServiceException: [LDAP: error code 32 - 0000208D: NameErr: DSID-031522C9, problem 2001 (NO_OBJECT), data 0, best match of:\n        'DC=jcs,DC=com'\n^@]; nested exception is javax.naming.NameNotFoundException: [LDAP: error code 32 - 0000208D: NameErr: DSID-031522C9, problem 2001 (NO_OBJECT), data 0, best match of:\n        'DC=jcs,DC=com'\n^@]; remaining name 'cn=cmmuriat,ou=JIPeople'</p>","tags":["Ambari","ambari-ldap-sync"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-06 17:46:09.0","id":2965,"title":"How to pass user with Phoenix url?","body":"<p>Currently Have Hbase protected by Ranger. I am not able to connect to Hbase via Phoenix from an external java client since its picking up my machine userid and that is not an authorized user.  Is there a way to pass in the user in phoenix connection url?</p>","tags":["Ranger","Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-11 00:12:01.0","id":3195,"title":"Use cURL to make an API call to change base URL repository for Ambari Blueprints deployment?","body":"<p>I have successfully uploaded a blueprint to an Ambari server. I am trying now to configure default repositories for HDP installation in our classroom lab environment, as per instructions here:\n<a href=\"https://cwiki.apache.org/confluence/display/AMBARI/Blueprints#Blueprints-Step4:SetupStackRepositories%28Optional%29\"></a></p><p><a href=\"https://cwiki.apache.org/confluence/display/AMBARI/Blueprints#Blueprints-Step4:SetupStackRepositories%28Optional%29\">https://cwiki.apache.org/confluence/display/AMBARI/Blueprints#Blueprints-Step4:SetupStackRepositories%28Optional%29</a></p>I have created the a file, named HDPRepo.test, per the instructions.\n\n<p><img src=\"/storage/attachments/446-hdprepo.png\"></p>\nThe command I am using is based on what worked for the blueprint upload, but pointed at the repo URI per the instructions:\n\n<pre>curl -u admin:admin -i -H \n\"X-Requested-By: jedentest1\" -X POST -d @HDPRepo.test \nhttp://node1:8080/api/v1/stacks/HDP/versions/2.3/operating_systems/redhat6/repositories/HDP-2.3/\n</pre>\nI can't find this demo'd anywhere on the web, so I'm \nflying a bit blind. The error being returned is:\n\n{  \"status\" : 500,  \"message\" : \"Cannot create repositories.\"}\n\nMy suspicion is that the \ndocumentation is missing some minor piece (possibly a cURL add-on, or perhaps the API URI isn't quite right) that makes \nthis update of the base_url value work. Has anyone actually done this that can point out what I need to modify in my command? Thanks!\n","tags":["ambari-blueprint","repository","curl","api"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-19 17:42:00.0","id":4329,"title":"Log file location - Is there a way to change /var/log location to /var/log/hdp during the install ?","body":"<p>Log file location - Is there a way to change /var/log location to /var/log/hdp during the install ?</p>","tags":["logs","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-23 16:22:19.0","id":4614,"title":"Importtsv tool - Specify a YARN Queue?","body":"<p>When using the importtsv tool is there a way to specify a YARN queue? Despite attempting to set a YARN queue it always goes to the 'default' queue.</p>","tags":["Hbase","YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-24 15:50:25.0","id":4708,"title":"Federated Queries coming to Hive?","body":"<p>I have data in several clusters distributed worldwide. For legal reasons the raw data can not leave the countries it's in but it's ok to query it and build aggregations across clusters. I think these kind of federated queries where planed for Stinger.next? When are they coming? </p><p>Thanks a lot!</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-02 03:00:41.0","id":5010,"title":"Steps to replace disk on master nodes","body":"<p><a rel=\"user\" href=\"/users/324/ajaysingh.html\" nodeid=\"324\">@ajaysingh</a></p><p>The answer to replace disks in Name Node is covered here - https://community.hortonworks.com/questions/3012/what-are-the-steps-an-operator-should-take-to-repl.html</p><p>Just wondering if the same steps apply if we were to replace disks for other master services listed below :</p><p>- Secondary NameNode</p><p>- Journal Node</p><p>- YARN</p><p>-HBase</p><p>- HDFS</p><p>- Zookeeper</p><p>What is field recommendation around replacing disks?</p><p>Partner has implemented HA, but no RAID.</p>","tags":["disk","master","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-10 11:24:52.0","id":5796,"title":"online recommendation using mahout","body":"<p><strong><em>Dears,</em></strong></p><p><strong><em>Greetings!</em></strong></p><p>How do I implement online recommendation using Mahout. i want to get recommendation from the mahout recommendation engine on real time using some mechanism like REST API.</p><p>please share me any implementation idea</p><p><em><strong>Regards.</strong></em></p>","tags":["help","spring","mahout"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-14 14:31:33.0","id":6365,"title":"Ambari agent not coming up","body":"<p>ambari agent hung and gives error '/usr/sbin ambari-agent : fork : cannot allocate memory'</p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-15 16:05:38.0","id":6535,"title":"Troubleshoot Falcon Web UI Cluster Creation - Invalid Workflow Server or Port Error","body":"<p>I am attempting to create a Falcon cluster entity using the Web UI. </p><p><strong>**My Environment**</strong></p><p>I have a functioning Oozie Server (have successfully run workflows from the command line, and I can access the Oozie Web UI without issue). I have a three-node cluster running in Docker. Both Falcon and Oozie are installed on node2. </p><p><strong>**The Error**</strong></p><p><img src=\"/storage/attachments/845-error.png\"></p><p>Note: The way name resolution is set up, FQDNs are not required in my environment, or rather, \"node2\" is the FQDN equivalent. </p><p><strong>**Troubleshooting So Far**</strong></p><p>I have tried replacing node2 with the IP address. I get the same error:</p><p><img src=\"/storage/attachments/846-error2.png\"></p><p>I have confirmed that I can access this URL via the Oozie Web UI. Just to be pedantic, I also performed a port scan to confirm that port 11000 was truly open on the server. </p><p>What could be causing this error? Are there additional troubleshooting steps I can take?</p>","tags":["Falcon","cluster"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-15 21:35:41.0","id":6602,"title":"How do I limit the number of simultaneous tasks for a single Tez job?","body":"<p>I'm using the Elasticsearch Hadoop connector to push data via Pig to Elasticsearch.  This process has worked very well for me on my 6 node cluster.  I now have a 12 node cluster and I'm running HDP 2.3.  Now it seems that Pig is pushing too much data to Elasticsearch and it can't keep up.</p><p>My cluster is running 134 tasks at once for this Pig job.  Is there any easy way to change the number of simultaneous tasks running for a Pig/Tez job?  I changed my queue configuration to limit the resources to 50% for one of the queues and I'm still overloading Elasticsearch.</p>","tags":["elasticsearch","Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-15 22:51:40.0","id":6642,"title":"Unable to start NodeManager: no leveldbjni64-1.8 in java.library.path","body":"<p>Cannot bring up 2 NodeMangers, the other 19 works fine.\n\nI got this:\n\nFATAL nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(465)) - Error starting NodeManager </p><p>java.lang.UnsatisfiedLinkError: Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, Permission denied]\n</p><p>It is not caused by \"chmod 777 /tmp\", any clue?</p>","tags":["YARN","nodemanager"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-16 11:44:59.0","id":6727,"title":"Benchmark Hortonworks, cloudera and mapR","body":"<p>Hi,</p><p>I have to choose between cloudera, hortonworks and mapR. </p><p>And i don't know how can i test the performance between those distributions.</p><p>After choosing a distribution i have to work with spark and extract data from social networks .\nSo should i just test algorithms with spark in each distribution?</p><p>Any help?</p><p>Thanks in advance</p>","tags":["benchmark","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-16 14:48:24.0","id":6797,"title":"Hive support for Lines Terminator other than '\\n' when creating table","body":"<p>Create Table command only supports '\\n' as terminator. I have '\\n' in one of the DB fields which happens to be an XML data which can not be used as Line terminator. I tried using control character like '\\001' and some others but I get following error message.</p><p>FAILED: SemanticException 13:20 LINES TERMINATED BY only supports newline '\\n' right now. Error encountered near token '';''</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-16 17:00:20.0","id":6850,"title":"HDFS user group mapping with AD","body":"<p>users connect to Hive through Knox uses AD credentials...integrated HDFS with AD groups... now HDFS is not able to recognize local user groups.</p>","tags":["active-directory","HDFS","groups"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-16 20:39:46.0","id":6887,"title":"how to find the worker nodes that were used for a MR/Tez job","body":"<p>If i submit a job, i see only one worker node on the resource manager. I doubt all the data it is looking for is not on one host.</p><p>Is there a way to find what are the nodes a specific job used?</p><p></p>","tags":["YARN","jobs"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 12:08:45.0","id":7059,"title":"Handling multiple records in hive","body":"<p>Hi All,</p><p>I have a scenario where I have to do sum(column x) for month range and sum (column x) for a range of year  and insert both the records into the next table, listing with example</p><p>example: sale_id,saleValue,date</p><p>1,1000,2015/12/14</p><p>2,2000,2015/11/01</p><p>3,3000,2015/12/01</p><p>4,4000,2015/01/01</p><p>Here when we try for this month the output is sum(salevalue) for id's 1 and 3, and for last year it's sum(salevalue) for id's 1,2,3 and 4</p><p>And I have to insert both the values in to next table </p><p>Appreciate your help</p><p></p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 13:41:44.0","id":7074,"title":"Why did the hdfs disk utilization skyrocketed from a few hundred GB to over 7TB in a matter of hours while ingesting data into  HBASE  by openTSDB?","body":"<p><strong>Background :</strong></p><p>Customer has an 8 Node cluster on AWS with ephemeral\nstorage, 5 of which are Hbase. </p><p>OpenTSDB and Grafana were installed on the cluster as\nwell.</p><p>Customer was ingesting time series data with OpenTSDB,\nat a rate of ~50k records/second. </p><p><strong></strong></p><p><strong></strong></p><p><strong>Symptom:  </strong></p><p>In a span of a couple of\nhours, the disk utilization of hdfs skyrocketed from a few hundred GB to over\n6TB all of it in HBASE / openTSDB.</p><p>In attempting to troubleshoot\n– turning off all data ingest and stopping openTSDBand just running HBase\ncaused the disk utilization to continue to grow unabated and out of control\ndozens of GB per minute, even when openTSDB was completely shut down.</p>","tags":["opentsdb","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-18 03:02:05.0","id":7165,"title":"How to copy HDFS file to AWS S3 Bucket?  hadoop distcp is not working.","body":"<p>I used hadoop distcp as given below: </p><p>hadoop distcp hdfs://hdfs_host:hdfs_port/hdfs_path/hdfs_file.txt s3n://s3_aws_access_key_id:s3_aws_access_key_secret@my_bucketname/</p><p>My Hadoop cluster is behind the company http proxy server,  I can't figure out how to specify this when connecting to s3. The error I get is: ERROR tools.DistCp: Invalid arguments: org.apache.http.conn.ConnectTimeoutException: Connect to my_bucketname.s3.amazonaws.com:443 timed out.</p>","tags":["aws","HDFS","s3"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-23 09:40:04.0","id":7801,"title":"Services in unknown state","body":"<p>Setup an HDP 2.3.4 cluster using ambari 2.2 and getting all services in unknown state (yellow question mark). </p><p><img src=\"/storage/attachments/1036-screen-shot-2015-12-23-at-11459-am.png\"></p><p>Usually, restarting ambari-agent resolves this but not here. Log file shows</p><pre># tail  /var/log/ambari-agent/ambari-agent.log\nINFO 2015-12-23 01:12:32,165 DataCleaner.py:120 - Data cleanup started\nINFO 2015-12-23 01:12:32,172 DataCleaner.py:122 - Data cleanup finished\nINFO 2015-12-23 01:12:32,223 PingPortListener.py:50 - Ping port listener started on port: 8670\nINFO 2015-12-23 01:12:32,225 main.py:289 - Connecting to Ambari server at https://sandbox.hortonworks.com:8440 (192.168.191.139)\nINFO 2015-12-23 01:12:32,225 NetUtil.py:60 - Connecting to https://sandbox.hortonworks.com:8440\nERROR 2015-12-23 01:15:52,344 NetUtil.py:84 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol\nERROR 2015-12-23 01:15:52,344 NetUtil.py:85 - SSLError: Failed to connect. Please check openssl library versions.\nRefer to: https://sandbox.hortonworks.com:8440 for more details.\nWARNING 2015-12-23 01:15:52,345 NetUtil.py:112 - Server at https://sandbox.hortonworks.com:8440 is not reachable, sleeping for 10 seconds...\nINFO 2015-12-23 01:16:02,346 NetUtil.py:60 - Connecting to https://sandbox.hortonworks.com:8440\n</pre><p>Initially I thought it was openSSL version issue but that seems to be ok:</p><pre># rpm -qa | grep openssl\nopenssl-1.0.1e-42.el6_7.1.x86_64\n</pre><pre># yum update openssl\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirror.dattobackup.com\n * epel: linux.mirrors.es.net\n * extras: dallas.tx.mirror.xygenhosting.com\n * updates: centos.hostingxtreme.com\nSetting up Update Process\nNo Packages marked for Update\n</pre><pre># cat /etc/ambari-server/conf/ambari.properties | grep java\njava.home=/usr/java/default\njava.releases=jdk1.8,jdk1.7</pre>","tags":["help","ambari-2.2.0","hdp-2.3.4","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-28 20:24:05.0","id":8220,"title":"Can we get file metadata using org.apache.hadoop.fs.FileSystem?","body":"<p>We can use the org.apache.hadoop.fs.FileStatus class to get the file metadata like Block size, File permissions & Ownership, replication factor.</p><p>Can we get the same metadata using org.apache.hadoop.fs.FileSystem class. If yes whats the difference between FileSystem and FileSatus class?</p><p>Thanks!</p>","tags":["api","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-29 21:24:32.0","id":8226,"title":"How to install eclipse in hortonworks sandbox?","body":"","tags":["installation"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-28 15:43:27.0","id":8159,"title":"What is the \"hive\" password?","body":"<p>In <a href=\"http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#section_5\">http://hortonworks.com/hadoop-tutorial/hello-world...</a> it mentions that you could run the queries via the shell. I am using an Azure sandbox 2.3.2 and I can login using my username/password. However, when I \"su hive\" it will not accept any of the passwords I could think to try (\"hive\", \"admin\", \"hadoop\").</p>","tags":["Sandbox","permissions"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-30 03:17:28.0","id":8331,"title":"Could not run Oozie workflow with distcp-action","body":"<p>Hi,</p><p>I am trying to run Oozie workflow job with distcp-action but getting below error:</p><pre>2015-12-30 10:19:36,004  INFO CallbackServlet:520 - SERVER[centos10-82.letv.cn] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000013-151228182829447-oozie-oozi-W] ACTION[0000013-151228182829447-oozie-oozi-W@distcp-node] callback for action [0000013-151228182829447-oozie-oozi-W@distcp-node]\n2015-12-30 10:19:36,138  INFO DistcpActionExecutor:520 - SERVER[centos10-82.letv.cn] USER[oozie] GROUP[-] TOKEN[] APP[distcp-WF] JOB[0000013-151228182829447-oozie-oozi-W] ACTION[0000013-151228182829447-oozie-oozi-W@distcp-node] action completed, external ID [job_1450278216920_52402]\n2015-12-30 10:19:36,152  WARN DistcpActionExecutor:523 - SERVER[centos10-82.letv.cn] USER[oozie] GROUP[-] TOKEN[] APP[distcp-WF] JOB[0000013-151228182829447-oozie-oozi-W] ACTION[0000013-151228182829447-oozie-oozi-W@distcp-node] Launcher ERROR, reason: Main class [org.apache.oozie.action.hadoop.DistcpMain], exception invoking main(), java.lang.ClassNotFoundException: Class org.apache.oozie.action.hadoop.DistcpMain not found\n2015-12-30 10:19:36,152  WARN DistcpActionExecutor:523 - SERVER[centos10-82.letv.cn] USER[oozie] GROUP[-] TOKEN[] APP[distcp-WF] JOB[0000013-151228182829447-oozie-oozi-W] ACTION[0000013-151228182829447-oozie-oozi-W@distcp-node] Launcher exception: java.lang.ClassNotFoundException: Class org.apache.oozie.action.hadoop.DistcpMain not found\njava.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.oozie.action.hadoop.DistcpMain not found\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n        at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:234)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.oozie.action.hadoop.DistcpMain not found\n        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n        ... 9 more\n2015-12-30 10:19:36,179  INFO ActionEndXCommand:520 - SERVER[centos10-82.letv.cn] USER[oozie] GROUP[-] TOKEN[] APP[distcp-WF] JOB[0000013-151228182829447-oozie-oozi-W] ACTION[0000013-151228182829447-oozie-oozi-W@distcp-node] ERROR is considered as FAILED for SLA\n2015-12-30 10:19:36,196  INFO ActionStartXCommand:520 - SERVER[centos10-82.letv.cn] USER[oozie] GROUP[-] TOKEN[] APP[distcp-WF] JOB[0000013-151228182829447-oozie-oozi-W] ACTION[0000013-151228182829447-oozie-oozi-W@fail] Start action [0000013-151228182829447-oozie-oozi-W@fail] with user-retry state : userRetryCount [0], userRetryMax [0], userRetryInterval [10]</pre><p>and the workflow.xml file content as below:</p><pre>&lt;workflow-app xmlns=\"uri:oozie:workflow:0.4\" name=\"distcp-WF\"&gt;\n    &lt;start to=\"distcp-node\"/&gt;\n    &lt;action name=\"distcp-node\"&gt;\n        &lt;distcp xmlns=\"uri:oozie:distcp-action:0.1\"&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;prepare&gt;\n&lt;delete path=\"${to}\"/&gt;\n&lt;/prepare&gt;            &lt;configuration&gt;\n                &lt;property&gt;\n                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;\n                    &lt;value&gt;${queueName}&lt;/value&gt;\n                &lt;/property&gt;\n               &lt;property&gt;\n                   &lt;name&gt;oozie.use.system.libpath&lt;/name&gt;\n                   &lt;value&gt;true&lt;/value&gt;\n               &lt;/property&gt;            &lt;/configuration&gt;\n            &lt;arg&gt;${from}&lt;/arg&gt;\n            &lt;arg&gt;${to}&lt;/arg&gt;\n        &lt;/distcp&gt;\n        &lt;ok to=\"end\"/&gt;\n        &lt;error to=\"fail\"/&gt;\n    &lt;/action&gt;\n    &lt;kill name=\"fail\"&gt;\n        &lt;message&gt;DistCP failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n    &lt;/kill&gt;\n    &lt;end name=\"end\"/&gt;\n&lt;/workflow-app&gt;</pre><pre>        \n</pre><p>then, i checked the SharedLib in these path:</p><ul><li>ls /usr/hdp/2.3.2.0-2950/oozie/share/lib/distcp</li></ul><p><img src=\"/storage/attachments/1120-1.png\"></p><ul><li>hdfs oozie ShareLib:</li></ul><ul><li><p><img src=\"/storage/attachments/1118-2.png\"></p></li></ul><p>please help me, Thanks!</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-31 09:46:01.0","id":8464,"title":"Why some Blocks are not replicated ?","body":"<p>Hello,</p><p>In my clutser (4nodes) I have some blocks that does not meet replication number. I do not fnd any issue.</p><p><strong>About versions :</strong></p><p>Clutser installed with ambari</p><p>HDP 2.3.2.0.2950</p><p>HDFS 2.7.1.2.3</p><p>Replication factor : 3</p><p><strong>Here is the nodename log :</strong></p><pre>2015-12-31 10:09:33,004 INFO  BlockStateChange (UnderReplicatedBlocks.java:chooseUnderReplicatedBlocks(407)) - chooseUnderReplicatedBlocks selected 8 blocks at priority level 2;  Total=8\n2015-12-31 10:09:33,004 INFO  BlockStateChange (BlockManager.java:computeReplicationWorkForBlocks(1522)) - BLOCK* neededReplications = 6848, pendingReplications = 0.\n2015-12-31 10:09:33,004 INFO  blockmanagement.BlockManager (BlockManager.java:computeReplicationWorkForBlocks(1529)) - Blocks chosen but could not be replicated = 8; of which 8 have no target, 0 have no source, 0 are UC, 0 are abandoned, 0 already have enough replicas.\n2015-12-31 10:09:36,005 INFO  BlockStateChange (UnderReplicatedBlocks.java:chooseUnderReplicatedBlocks(407)) - chooseUnderReplicatedBlocks selected 8 blocks at priority level 2;  Total=8\n2015-12-31 10:09:36,005 INFO  BlockStateChange (BlockManager.java:computeReplicationWorkForBlocks(1522)) - BLOCK* neededReplications = 6848, pendingReplications = 0.\n2015-12-31 10:09:36,005 INFO  blockmanagement.BlockManager (BlockManager.java:computeReplicationWorkForBlocks(1529)) - Blocks chosen but could not be replicated = 8; of which 8 have no target, 0 have no source, 0 are UC, 0 are abandoned, 0 already have enough replicas.\n2015-12-31 10:09:38,635 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3574)) - BLOCK* allocate blk_1073964742_223940{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c616a45-8b95-4c41-b592-0821b07a8803:NORMAL:10.1.1.82:50010|RBW], ReplicaUC[[DISK]DS-0a078e05-8825-4ea4-952d-5dc617317989:NORMAL:10.1.1.28:50010|RBW], ReplicaUC[[DISK]DS-a81872bf-681a-41d9-b5bd-a4241536e2ed:NORMAL:10.1.1.26:50010|RBW], ReplicaUC[[DISK]DS-be0ad8a7-2413-493d-9706-b7bf46dad668:NORMAL:10.1.1.24:50010|RBW]]} for /apps/accumulo/data/tables/!0/table_info/F0001jq5.rf_tmp\n2015-12-31 10:09:38,677 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3494)) - DIR* completeFile: /apps/accumulo/data/tables/!0/table_info/F0001jq5.rf_tmp is closed by DFSClient_NONMAPREDUCE_-1241177763_29\n2015-12-31 10:09:38,872 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3574)) - BLOCK* allocate blk_1073964743_223941{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-e21e239f-1ad9-4aff-8a8c-419671f8f87a:NORMAL:10.1.1.82:50010|RBW], ReplicaUC[[DISK]DS-a47ab1c7-9f5a-48fb-8613-1557c412c20a:NORMAL:10.1.1.28:50010|RBW], ReplicaUC[[DISK]DS-be0ad8a7-2413-493d-9706-b7bf46dad668:NORMAL:10.1.1.24:50010|RBW], ReplicaUC[[DISK]DS-165a6e3c-9ded-496e-8c21-40f32f482e3e:NORMAL:10.1.1.26:50010|RBW]]} for /apps/accumulo/data/tables/+r/root_tablet/F0001jq6.rf_tmp\n2015-12-31 10:09:38,916 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3574)) - BLOCK* allocate blk_1073964744_223942{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-e21e239f-1ad9-4aff-8a8c-419671f8f87a:NORMAL:10.1.1.82:50010|RBW], ReplicaUC[[DISK]DS-165a6e3c-9ded-496e-8c21-40f32f482e3e:NORMAL:10.1.1.26:50010|RBW], ReplicaUC[[DISK]DS-9d1e974d-56e1-4f5f-9b2c-9437041cf148:NORMAL:10.1.1.24:50010|RBW], ReplicaUC[[DISK]DS-0a078e05-8825-4ea4-952d-5dc617317989:NORMAL:10.1.1.28:50010|RBW]]} for /apps/accumulo/data/tables/!0/table_info/A0001jq7.rf_tmp\n2015-12-31 10:09:38,919 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3494)) - DIR* completeFile: /apps/accumulo/data/tables/+r/root_tablet/F0001jq6.rf_tmp is closed by DFSClient_NONMAPREDUCE_-1241177763_29\n2015-12-31 10:09:38,969 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3494)) - DIR* completeFile: /apps/accumulo/data/tables/!0/table_info/A0001jq7.rf_tmp is closed by DFSClient_NONMAPREDUCE_-1241177763_29\n2015-12-31 10:09:39,006 INFO  BlockStateChange (UnderReplicatedBlocks.java:chooseUnderReplicatedBlocks(407)) - chooseUnderReplicatedBlocks selected 8 blocks at priority level 2;  Total=8\n2015-12-31 10:09:39,006 INFO  BlockStateChange (BlockManager.java:computeReplicationWorkForBlocks(1522)) - BLOCK* neededReplications = 6851, pendingReplications = 0.\n2015-12-31 10:09:39,006 INFO  blockmanagement.BlockManager (BlockManager.java:computeReplicationWorkForBlocks(1529)) - Blocks chosen but could not be replicated = 8; of which 8 have no target, 0 have no source, 0 are UC, 0 are abandoned, 0 already have enough replicas.\n2015-12-31 10:09:39,051 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3574)) - BLOCK* allocate blk_1073964745_223943{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-e21e239f-1ad9-4aff-8a8c-419671f8f87a:NORMAL:10.1.1.82:50010|RBW], ReplicaUC[[DISK]DS-165a6e3c-9ded-496e-8c21-40f32f482e3e:NORMAL:10.1.1.26:50010|RBW], ReplicaUC[[DISK]DS-0a078e05-8825-4ea4-952d-5dc617317989:NORMAL:10.1.1.28:50010|RBW], ReplicaUC[[DISK]DS-9d1e974d-56e1-4f5f-9b2c-9437041cf148:NORMAL:10.1.1.24:50010|RBW]]} for /apps/accumulo/data/tables/+r/root_tablet/A0001jq8.rf_tmp\n2015-12-31 10:09:39,102 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3494)) - DIR* completeFile: /apps/accumulo/data/tables/+r/root_tablet/A0001jq8.rf_tmp is closed by DFSClient_NONMAPREDUCE_-1241177763_29\n2015-12-31 10:09:42,007 INFO  BlockStateChange (UnderReplicatedBlocks.java:chooseUnderReplicatedBlocks(407)) - chooseUnderReplicatedBlocks selected 8 blocks at priority level 2;  Total=8\n2015-12-31 10:09:42,007 INFO  BlockStateChange (BlockManager.java:computeReplicationWorkForBlocks(1522)) - BLOCK* neededReplications = 6852, pendingReplications = 0.\n2015-12-31 10:09:42,007 INFO  blockmanagement.BlockManager (BlockManager.java:computeReplicationWorkForBlocks(1529)) - Blocks chosen but could not be replicated = 8; of which 8 have no target, 0 have no source, 0 are UC, 0 are abandoned, 0 already have enough replicas.</pre><p>Any help will be appreciate ?</p><p>Thanks</p><p>Stephane.</p>","tags":["help","replication"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-06 06:04:08.0","id":26109,"title":"how to define global variable in ambari","body":"<p>I'm want define global variable in ambari and faile in cluster-env.xml. how to do?</p>","tags":["Ambari"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-11-05 02:32:12.0","id":2845,"title":"Zeppelin + SparkR","body":"<p>does zeppelin support SparkR interpreter?</p>","tags":["zeppelin","sparkr","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-05-13 21:42:02.0","id":33295,"title":"How to run hbaase export snapshot as a non hbase user in a secured HDP 2.3.4 cluster ?","body":"<p>In a kerberised 2.3.4 cluster, a non hbase user can't seem to run hbase export snapshot since the snapshots are stored under /hbase directory that is only rwx by hbase user. </p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-17 22:33:10.0","id":33865,"title":"indexing doc from HDFS in Solr","body":"<p>Hi:</p><p>When you index file from HDFS into Solr, those files are stored into local FS like /use/local/de??? or indexing directly from HDFS, i mean if we will double copy of files HDFS and LOCAL FYLESYSTEM</p>","tags":["SOLR"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-31 17:30:44.0","id":36766,"title":"Cloudera cluster install hangs","body":"<p>Hi, When I login into Cloudera Manger it goes to the Cluster Installation wizard, where it says it is installing parcels, but just hangs and nothing happens.</p><p>I checked the log on the nodes and it gives those errors:</p><p>Anyone knows what this parcel.torrent file is suppose to do what, and how to resolve this python errors?</p><p>thank you, </p><p><a target=\"_blank\" href=\"https://lh3.googleusercontent.com/UvaEs7QOoK8hK_Fawycni8UgK4djKiK2WgPC0U89vZ1-DVN5dn3_fzQaiUPWVFf7cdVn_CTMEOpQvch25SW2Vgcnzpo-ib7XpROolYnConFzo0ZXj3M3is6AjGlFhe8hGtOXsU5KnSa2Zq1RnhYxnXMvIEMEEUPrevMjglW91MlDeMn1Ou3OdB-AWUDTiZUGETy-vDeuW_EtyCHu4t_BHCC7OPCm9jz6XHrkpST-kucZtx8lXL9n6DpL2bi_x6yZBapyJGNx0iI99CEhrZ3bqxi1WP7ldWvjt6Z3ml9QXGv9NXM96tZti62x425Wscho8Pv54WSRy0GUiTy_yUsDpJOtTi8DoOhZ_nc95u6xNEf5RCoL1A9RhzQaBm0zlXKsrYRLG-6nMv7fwpd8sMjg6ps0tY71xrSv56lBIfKHs17RhmM6QZjvl2xct6lFkcBLyqRWMaVyTF4_bvBuv7kFjOP0x_-c5e2smS4ur5mE1Gk_oSp83tGM2Mi69dXi7lk_WEFjoxXdsQlHAbgKUAox1EP7NDVEh8NipJPdgF6wzFxnsgkudx0O4Co_X8i8HAbJaJ76AJRVkG3DzsTV6kDL4n8tVq3dvA=w812-h239-no\">cluster package installation screenshot</a></p><pre>[31/May/2016 11:14:35 +0000] 4374 Thread-13 https        ERROR    Failed to retrieve/stroe URL: http://hadoop5.vm:7180/cmf/parcel/download/CDH-5.7.0-1.cdh5.7.0.p0.45-el6.parcel.torrent -&gt; /opt/cloudera/parcel-cache/CDH-5.7.0-1.cdh5.7.0.p0.45-el6.parcel.torrent &lt;urlopen error timed out&gt;\nTraceback (most recent call last):\n  File \"/usr/lib64/cmf/agent/build/env/lib/python2.6/site-packages/cmf-5.7.0-py2.6.egg/cmf/https.py\", line 176, in fetch_to_file\n    resp = self.open(req_url)\n  File \"/usr/lib64/cmf/agent/build/env/lib/python2.6/site-packages/cmf-5.7.0-py2.6.egg/cmf/https.py\", line 171, in open\n    return self.opener(*pargs, **kwargs)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 126, in urlopen\n    return _opener.open(url, data, timeout)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 391, in open\n    response = self._open(req, data)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 409, in _open\n    '_open', req)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 369, in _call_chain\n    result = func(*args)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 1161, in http_open\n    return self.do_open(httplib.HTTPConnection, req)\n  File \"/usr/lib64/python2.6/urllib2.py\", line 1136, in do_open\n    raise URLError(err)\nURLError: &lt;urlopen error timed out&gt;\n\n\n</pre>","tags":["cloudera"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-04 07:46:07.0","id":37691,"title":"apache drill queries","body":"<p>Hi:</p><p>I have one question, apache drill make the queries in memory??? and he keep the metastore in memory??</p><p>thanks</p>","tags":["drill"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-07 00:12:50.0","id":38029,"title":"Hbase Copy table fails for 250 Million Records from a HDP 2.2 ==> HDP 2.4 cluster. Please assist.","body":"<pre>Error: org.apache.hadoop.hbase.client.ScannerTimeoutException: 70218ms passed since the last invocation, timeout is currently set to 60000 at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:434) at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:364) at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:205) at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:147) at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1.nextKeyValue(TableInputFormatBase.java:216) at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556) at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) at</pre>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-07 14:48:13.0","id":38211,"title":"Through java how to i stop the processor? when ever the exception is there automatically stop the processor?","body":"","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-09 14:46:23.0","id":38807,"title":"hive overwrtite table","body":"<p>hi: i am receiving tihs error with this script:</p><pre>CREATE EXTERNAL TABLE IF NOT EXISTS  mi_cliente_fmes(\nid_interno_pe bigint,\ncod_nrbe_en int,\nmi_nom_cliente string,\nfec_ncto_const_pe string,\nfecha_prim_rl_cl string ,\nsexo_in string,\ncod_est_civil_indv string,\ncod_est_lab_indv string,\nnum_hijos_in int,\nind_autnmo_in string,\ncod_ofcna_corr string,\ncod_cpcdad_lgl_in int\n)\nCLUSTERED BY (cod_nrbe_en) INTO 60 BUCKETS\nstored as ORC\nLOCATION '/RSI/tables/desercion/mi_cliente_fmes'\n\n\nset hive.enforce.bucketing = true;\nset map.reduce.tasks = 25;\nSET hive.exec.parallel=true;\nSET hive.vectorized.execution.enabled=true;\nINSERT OVERWRITE TABLE mi_cliente_fmes\nselect id_interno_pe,\n       cod_nrbe_en,\n       mi_nom_cliente,\n       fec_ncto_const_pe,\n       fecha_prim_rl_cl,\n       sexo_in,\n       cod_est_civil_indv,\n       cod_est_lab_indv,\n       num_hijos_in,\n       ind_autnmo_in,\n       cod_ofcna_corr,\n       cod_cpcdad_lgl_in\n FROM mi_cliente_fmes_temp;\n</pre><p>and the error:</p><pre>Error: Error while compiling statement: FAILED: SemanticException [Error 10295]: INSERT OVERWRITE not allowed on table with OutputFormat that implements AcidOutputFormat while transaction manager that supports ACID is in use (state=42000,code=10295)\n</pre>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-10 02:53:22.0","id":38954,"title":"How to ingest oracle redo logs into the Kafka.Log miner or golden gate, need to devise some customised ways using open source technologies.Any help or  pointer.","body":"<p>How to ingest oracle redo logs into the Kafka.Log miner or golden gate, need to devise some customised ways using open source technologies.Any help or  pointer.</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-13 13:05:45.0","id":39369,"title":"Pheniox patch apply.","body":"<p>I am getting issue </p><h1>org.apache.spark.sql.catalyst.expressions.GenericMutableRow cannot be cast to org.apache.spark.sql.RoW. </h1><p>This issue is regarding with patch. Please help me to understand how to apply the patch to resolve this issue.</p>","tags":["hdp-2.3.4"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-13 15:47:10.0","id":39421,"title":"What is the x_service_config_map table actually used for in Ranger?","body":"<p>What is the x_service_config_map table actually used for in Ranger?</p><p>What does it harm if a value (like auth_to_local) can't be stored in this table because it is too long?</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-01 02:10:53.0","id":4959,"title":"Can oozie email actions include log files or standard output of other oozie actions?","body":"<p>I have an oozie java action that logs some basic information to standard out.  The next oozie action is an email to me indicating success or failure.  I would really like to include the prior action's standard out in that email.  Is there a straight forward way to do that?</p>","tags":["Oozie"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-01 13:38:05.0","id":4976,"title":"Is there any way to see running queries in phoenix? Any monitoring capability?","body":"<p>Customer is looking to monitor running querries in Phoenix.  Is there any monitoring capability available?</p>","tags":["Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-03 17:10:42.0","id":5130,"title":"Guidance around setting right number for 'hive.exec.max.created.files'","body":"<p>Hi, I'm looking for some guidance around setting <strong>hive.exec.max.created.files.</strong> Is there a formula or ratio to follow ahead of time to find a right number for my queries?</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-04 15:03:46.0","id":5213,"title":"How to prevent default Tez containers.","body":"<p>Is there a way to prevent default Tez containers when using the Hive CLI and the Hive version is &gt;= .14? </p><p>I would like Tez to be the default engine but not spin up any containers when launching the Hive CLI. </p>","tags":["yarn-container","Tez","Hive","cli"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-04 21:28:41.0","id":5252,"title":"Ambari - HDFS Rebalancer","body":"<p>I was testing out the HDFS rebalance operation from Ambari (2.0.2) and noticed that it seems to time out / stop after an hour.  Restarting it right after allows it to run another hour, but not something you want to watch all day.</p><p>Is that a configurable setting that can be changed?  </p>","tags":["HDFS","Ambari","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-07 17:32:00.0","id":5321,"title":"Ambari Ldap sync error:  javax.naming.InvalidNameException","body":"<pre>\"href\" : \"<a href=\"http://127.0.0.1:8080/api/v1/ldap_sync_events/17\">http://127.0.0.1:8080/api/v1/ldap_sync_events/17</a>\",\n  \"Event\" : {\n  \"id\" : 17,\n  \"specs\" : [\n  {\n  \"names\" : \"HDP_Admin,HDP_Developer,HDP_Users\",\n  \"sync_type\" : \"specific\",\n  \"principal_type\" : \"groups\"\n  },\n  {\n  \"names\" : \"user1,user2,user3\",\n  \"sync_type\" : \"specific\",\n  \"principal_type\" : \"users\"\n  }\n  ],\n  \"status\" : \"ERROR\",\n  \"status_detail\" : \"Caught exception running LDAP sync. Invalid name: /ldap.xxxxx.com:389; nested exception is javax.naming.InvalidNameException: Invalid name: /ldap.xxx.com:389\",\n  \"summary\" : {\n  \"groups\" : {\n  \"created\" : null,\n  \"removed\" : null,\n  \"updated\" : null\n  },\n  \"memberships\" : {\n  \"created\" : null,\n  \"removed\" : null\n  },\n  \"users\" : {\n  \"created\" : null,\n  \"removed\" : null,\n  \"updated\" : null\n  }\n  },\n  \"sync_time\" : {\n  \"end\" : 1449508883644,\n  \"start\" : 1449508883642\n  }</pre>","tags":["ambari-ldap-sync"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-07 21:24:11.0","id":5369,"title":"What is the best way to perform geospatial analysis using Spark?","body":"<p>I am working with a lot of geospatial data and big data sets that need analysis and interpretation. Can you point me to best practices on doing this.</p>","tags":["Spark","geospatial"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-21 11:33:49.0","id":7461,"title":"Error while Installing HDFS using Ambari tool.","body":"<p>Host registration aborted. Ambari Agent host cannot reach Ambari server</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-21 04:10:15.0","id":7399,"title":"How to : HTTP Stream in flink","body":"<p>Which methods/objects does Flink provide in order to enable users to read HTTP streams?</p>","tags":["flink","flink-streaming"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-28 06:42:50.0","id":8121,"title":"Why hive query with hint cannot be converted into map only join??","body":"<p>select  t1.student_id, t2.course_name from table_one t1 join table_two t2\n on (t1.course_id=t2.course_id);</p><p>The above one is the basic query i am dealing with.Where t1 and t2 are less in size 160 bytes and 50 bytes. When i run the above query ,Map only join is running as usual as \"hive.auto.convert.join=true\" by default in 0.14. Now, </p><p>I made \"set hive.auto.convert.join=false;\".I run the query .Then query chooses \"Default or map reduce join \" as i taught.</p><p>Now i changed query like </p><p>select    /*+ MAPJOIN(t2) */  t1.student_id,t2.course_name from table_one t1 join table_two t2\n on (t1.course_id=t2.course_id);</p><p>i.e I am giving hint through query.But In this case also,query chooses default join only.So i have 2 queries(still auto conversion set false)</p><p>1) How to give hint through query,so that query should select map only join.(when auto conversion set false).</p><p>2)What is the configuration parameter to know size where tables can fit it.So that map only join run.  or How to know the size where tables can fit with in to run map only join.</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-30 09:07:03.0","id":8347,"title":"I am getting below exception by using distcp to copy hdfs data into s3 using s3a protocol.","body":"<p>15/12/30 08:55:10 INFO mapreduce.Job: Task Id : attempt_1451465507406_0001_m_000001_2, Status : FAILED\nError: java.lang.IllegalArgumentException\n        at java.util.concurrent.ThreadPoolExecutor.&lt;init&gt;(ThreadPoolExecutor.java:1307)\n        at java.util.concurrent.ThreadPoolExecutor.&lt;init&gt;(ThreadPoolExecutor.java:1230)\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:274)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371)\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n        at org.apache.hadoop.tools.mapred.CopyMapper.setup(CopyMapper.java:112)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</p>","tags":["s3","distcp","hadoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-04 17:23:22.0","id":8624,"title":"Kernel Error  When Starting Sandbox 2.3.2","body":"<p>Hello,</p><p>  I'm receiving this error when I tried starting Sandbox 2.3.2 using Oracle's Virtual Box \"This kernel requires an x86-64 CPU, but only detected an i686 CPU. Unable to boot - please use a kernel appropriate for your CPU.\"</p><p>  I'm on a laptop running Windows 7 Home Premium, Service Pack 1, 64-bit Operating System. My processor is an AMD E-300 CPU with Radeon (tm) HD Graphics 1.30GHz.</p><p>  I have no choice between a 32-bit or a 64-bit version of the software. The download page must be retrieving my OS version and sending me the appropriate file. Why is the Sandbox thinking I don't have a 64-bit system?</p><p>Thank you,</p><p>  Richard</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-06 16:45:14.0","id":8970,"title":"Phoenix 4.6 on HDP 2.3 and 2.2","body":"<p>I would like to upgrade to the latest phoenix version on two clusters, one stack is 2.3 the other is 2.2. Is this supported and what's the recommended upgrade method? Thanks</p>","tags":["Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-07 10:03:36.0","id":9066,"title":"Hive JDBC : Could not open client transport with JDBC Uri","body":"<p>Hello,</p><p>I tried to connect to hive using jdbc but i had the following error:</p><p>Error: Could not open client transport with JDBC Uri: jdbc:hive2://server:10000/;principal=&lt;Server_Principal_of_HiveServer2&gt;\njava.net.ConnectException: Connection refused (state=08S01,code=0)</p><p>I got the same error from beeline : beeline -u 'jdbc:hive2://server:10000/;principal=&lt;Server_Principal_of_HiveServer2&gt;'</p><p>I will be thankfull for your help.</p><p>thanks </p><p>Hive version: 1.2.1</p><p>HDP: 2.3</p>","tags":["hiveserver2","jdbc","Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-09 07:32:43.0","id":9265,"title":"How can I add configuration files to a Spark job running in YARN?","body":"<p>After reading the Spark documentation and source code, I can find two ways to reference an external configuration file inside of a Spark (v1.4.1) job, but I'm unable to get either one of them to work.  </p><p><strong>Method 1</strong>: from <a href=\"http://spark.apache.org/docs/latest/submitting-applications.html\">Spark documentation</a> says to use <em>./bin/spark-submit --files /tmp/test_file.txt,</em> but doesn't specify how to retrieve that file inside of a Spark job written in Java.  I see it being added,<em> </em>but I don't see any configuration parameter in Java that will point me to the destination directory</p><pre>INFO Client: Uploading resource file:/tmp/test_file.txt -&gt; hdfs://sandbox.hortonworks.com:8020/user/guest/.sparkStaging/application_1452310382039_0019/test_file.txt</pre><p><strong>Method 2:</strong> <a href=\"https://github.com/apache/spark/blob/master/core%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2FSparkContext.scala#L1330\">from Spark source code</a> suggests to use <em>SparkContext.addFile(...) </em>and <em>SparkContext.</em><em>textFile(SparkFiles.get(...))</em>, but that doesn't work either as that directory doesn't exist in HDFS (only locally).  I see this in the output of <em>spark-submit --master yarn-client</em></p><pre>16/01/09 07:10:09 INFO Utils: Copying /tmp/test_file.txt to /tmp/spark-8439cc21-656a-4f52-a87d-c151b88ff0d4/userFiles-00f58472-f947-4135-985b-fdb8cf4a1474/test_file.txt\n16/01/09 07:10:09 INFO SparkContext: Added file /tmp/test_file.txt at http://192.168.1.13:39397/files/test_file.txt with timestamp 1452323409690\n.\n.\n16/01/09 07:10:17 INFO SparkContext: Created broadcast 5 from textFile at Main.java:72\nException in thread \"main\" org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://sandbox.hortonworks.com:8020/tmp/spark-8439cc21-656a-4f52-a87d-c151b88ff0d4/userFiles-00f58472-f947-4135-985b-fdb8cf4a1474/test_file.txt</pre>","tags":["hdp-2.3.2","Spark","java"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-10 16:59:58.0","id":9327,"title":"How to change the default Java version?","body":"<p>I am new to Hadoop and  started learning it, but have some beginners problems with the setup.  I had HDP 2.2 with JDK 1.7 and i tried to execute map reduce coded with 1.8.  And i realized that HDP 2.2 does not support JDK 1.8 and i have upgraded it to HDP 2.3.  When i execute Java -Version, i am still getting Java 1.7. As per the directions in HDP, i have used ambari-server setup to download JDK 1.8 and even after that Java -version is showing 1.7.  How to make it use JDK 1.8 as the default? </p>","tags":["upgrade","java","Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-12 10:02:52.0","id":9487,"title":"Is there any UI for fair scheduler of yarn?","body":"<p>Hello, </p><p>In hdp-2.3.2.0-2950, there are UIs for capacity scheduler in Ambari: </p><p>Yarn &gt; Configs &gt; Scheduler</p><p>Yarn Queue Manager menu on the Top</p><p>Is there any UI for fair scheduler? </p><p>If not, how can I set the fair scheduler? </p><p>Both value of yarn.resourcemanager.scheduler.class and fair-scheduler.xml are enough?</p><p>Regards, </p><p>park </p>","tags":["yarn-scheduler","ui","YARN"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-11 01:54:13.0","id":9371,"title":"Am I able to copy a file from my mac (/Users/rev/geolocation.csv) to the HDFS (/tmp/admin/data) via SSH?","body":"<p>Hi, I am interested in doing the HDP Certified Developer (HDPCD) Exam. As such I am doing some background work.\nWithin the link (http://hortonworks.com/training/class/hdp-certified-developer-hdpcd-exam/) there are a set of tasks that I was attempting.\nThe first task ‘Input a local file into HDFS using the Hadoop file system shell’ was one I attempted. I can do this via Ambari by going to the HDFS section and simply uploading the file by pressing the browse button and selecting the data file.\nHowever I want to do this via command line. Is there anyway I can do this? </p><p>My data is stored on my mac /Users/rev/geolocation.csv. I have created a directory in HDFS located: tmp/admin/data. This is where I will copy the geolocation.csv to. </p><p>\nVia SSH (http://127.0.0.1:4200/) I have tried commands like the one below which works fine. </p><pre>hadoop fs -ls tmp/admin/data </pre><p>However, when I type the below I get a no such file or directory message. </p><pre>hadoop fs -put localfile /Users/rev/geolocation.csv /tmp/admin/data </pre><p>Am I able to copy a file from my mac (/Users/rev/geolocation.csv) to the HDFS (/tmp/admin/data)? </p>","tags":["help","import","HDFS","localfile"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-12 13:44:16.0","id":9512,"title":"Future support for elastic search?","body":"<p>Hi all,</p><p>Is there anything in the roadmap to add a processor like PutSolrContentStream, but to send data to ElasticSearch?</p><p>Thank you,</p><p>D.</p>","tags":["Nifi","elasticsearch"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-13 10:51:16.0","id":9656,"title":"Hello Every One ! I have installed HDP_2.3.2_vmare ,it is asking Sandbox login .","body":"<p>I logged in by <strong>Username : root</strong> and <strong>password : hadoop</strong> and further still asking for a loggin.</p><p>And i am trying to browse on <strong>chrome/IE/Mozila </strong>with link http://10.0.2.15/                                                                                     it is displaying web page not found </p><p>Kindly reply me on this , I am stucked in installation</p><p>I have Followed the documentation  for installation of HDP on window :\"The last step Browse on chromeis not working \"</p><p>Thank you well in advance .....................!! </p>","tags":["login","virtualbox","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-14 08:57:20.0","id":9876,"title":"Is there any Out of the box Nifi processor for websocket streaming??","body":"<p>Iam looking to stream data from datasift websocket streaming api. Just wondering is there any processor in Nifi which does this currently??? thanks</p>","tags":["help","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-14 20:59:52.0","id":9984,"title":"Permission Denied using Sqoop from MySQL on sandbox","body":"<p>I am using Hortonworks Sandbox (a clean setup), and created a simple table in mysql which I want to sqoop into hdfs. </p><p>I'm used the MySql that was already installed on the sandbox, by running 'mysql' in the shell.</p><p>My first attempt at sqoop was: </p><p><strong>     sqoop import --connect jdbc:mysql://localhost/test  --table mytable</strong></p><p>This gave \"<em>..</em><em>access denied to localhost..</em>\" error, so based on an answer another post, and finding the MySQL port using \"<em>show variables like 'port'\"</em> within MySQL, I then tried:</p><p><strong>     sqoop import --connect jdbc:mysql://sandbox.hortonworks.com:3306/test --username root  --table mytable</strong></p><p><strong></strong>This gave errors about \".. streaming result sets are open...\", which was solved by another post by adding the 'driver' argument as follows:</p><p>     <strong>sqoop import --connect jdbc:mysql://sandbox.hortonworks.com:3306/test --username root  --table mytable --driver com.mysql.jdbc.Driver</strong></p><p>It now gets a bit further, but now fails with a RemoteException:  ...<strong>Permission denied: user=root, access=WRITE, inode=\"user/root/.staging\":hdfs:hdfs:drwxr-xr-x</strong></p><p>That's as far as I've got.  Am I doing something wrong, or do I need to set up some access permissions somewhere?  I would've expected that many people have been sqooping from MySql without having to change the set up, so is my sqoop command incorrect?</p>","tags":["Sqoop","permission-denied","Sandbox","mysql"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-15 18:13:05.0","id":10092,"title":"How can hadoop client libraries be added to a node which is not part of the cluster? Can this be done through Ambari?","body":"<p>How can hadoop client libraries be added to a node which is not part of the cluster? Can this be done through Ambari?</p>","tags":["HDFS","client","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-05 03:38:27.0","id":8720,"title":"Oozie Sqoop action throwing java.io.IOException: No columns to generate for ClassWriter","body":"<p>I'm trying to test Oozie's Sqoop action in the following environment:</p><ul>\n<li>HDP2.3.2</li><li>Sqoop 1.4.6</li><li>Oozie 4.2.0</li></ul><p>Via the command line, the following sqoop command works:</p><pre>sqoop import \\\n\t-D mapred.task.timeout=0 \\\n\t--connect jdbc:sqlserver://x.x.x.x:1433;database=CEMHistorical \\\n \t--table MsgCallArrival \\\n\t--username hadoop \\\n\t--password-file hdfs:///user/sqoop/.adg.password \\\n\t--hive-import \\\n\t--create-hive-table \\\n\t--hive-table develop.oozie \\\n\t--split-by TimeStamp \\\n\t--map-column-hive Call_ID=STRING,Stream_ID=STRING\n</pre><p>But when I try to execute the same command via Oozie, I'm running into <strong>java.io.IOException: No columns to generate for ClassWriter</strong></p><p>Below are my `job.properties` and `workflow.xml`:</p><pre>nameNode=hdfs://host.vitro.com:8020\njobTracker=host.vitro.com:8050\nprojectRoot=${nameNode}/user/${user.name}/tmp/sqoop-test/\noozie.use.system.libpath=true\noozie.wf.application.path=${projectRoot}\n\n\n\n&lt;workflow-app name=\"sqoop-test-wf\" xmlns=\"uri:oozie:workflow:0.4\"&gt;\n    &lt;start to=\"sqoop-import\"/&gt;\n\n    &lt;action name=\"sqoop-import\" retry-max=\"10\" retry-interval=\"1\"&gt;\n        &lt;sqoop xmlns=\"uri:oozie:sqoop-action:0.2\"&gt;\n            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n        &lt;command&gt;import -D mapred.task.timeout=0 --connect jdbc:sqlserver://x.x.x.x:1433;database=CEMHistorical --table MsgCallArrival --username hadoop --password-file hdfs:///user/sqoop/.adg.password --hive-import --create-hive-table --hive-table develop.oozie --split-by TimeStamp --map-column-hive Call_ID=STRING,Stream_ID=STRING&lt;/command&gt;\n        &lt;/sqoop&gt;\n        &lt;ok to=\"end\"/&gt;\n        &lt;error to=\"errorcleanup\"/&gt;\n    &lt;/action&gt;\n    &lt;kill name=\"errorcleanup\"&gt;\n      &lt;message&gt;Sqoop Test WF failed. [${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n    &lt;/kill&gt;\n    &lt;end name =\"end\"/&gt;\n&lt;/workflow-app&gt;</pre><p>I've attached the full log, but here's an excerpt:</p><pre>2016-01-05 11:29:21,415 ERROR [main] tool.ImportTool (ImportTool.java:run(613)) - Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter\n    at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1651)\n    at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)\n    at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)\n    at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)\n    at org.apache.sqoop.Sqoop.run(Sqoop.java:148)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:184)\n    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:226)\n    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:235)\n    at org.apache.sqoop.Sqoop.main(Sqoop.java:244)\n    at org.apache.oozie.action.hadoop.SqoopMain.runSqoopJob(SqoopMain.java:197)\n    at org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:177)\n    at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:47)\n    at org.apache.oozie.action.hadoop.SqoopMain.main(SqoopMain.java:46)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:236)\n    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</pre><p>I've been struggling with this problem for quite some time now, any help would be greatly appreciated!</p>","tags":["Sqoop","Oozie","hdp-2.3.2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-18 03:37:36.0","id":10295,"title":"Sqoop - Override log4j properties to add custom appender","body":"<p>HI All,</p><p>I am using Hortonoworks HDP cluster with 4 nodes.</p><p>I have a sqoop job which pulls the data from MySQL Db into HDFS.</p><p>Now, What I am trying to do is push the Sqoop Job logs into MongoDB by overriding log4j properties.</p><p>However, after trying out several options I am unable to override Log4j.</p><p>Could you please tell me how to do it?</p><p>Please note that I do not want to override the global log4j of Hadoop since it will affect other components too which I do not want.</p><p>Please let me know if you need any more details.</p><p>Regards,</p><p>Shiva</p>","tags":["logs","logging","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-18 17:25:40.0","id":10386,"title":"High NameNode Heap Usage","body":"<p>NameNode is running at 75% to 85% heap usage even when the cluster is at near idle state.</p><p>As per the following link, we should set the name node size at 1gb and Young Generation Size should be 128mb but we have set the heap size at 3gb and Young Generation Size at 384mb.</p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4/bk_installing_manually_book/content/ref-80953924-1cbf-4655-9953-1e744290a6c3.1.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4/bk_installing_manually_book/content/ref-80953924-1cbf-4655-9953-1e744290a6c3.1.html</a></p><p>Should i change it to 1gb and 128mb as per the Hortonworks recommendation? or leave the settings as is and change some other parameter to bring the usage down? Please advise</p><p>Some other info</p><p>Total HDFS Data Size &lt; 1 TB</p><p>Number of Files &lt; 160k</p><p>Block Size 128 mb</p><p>HDP 2.3.2</p>","tags":["performance","heap","namenode","hdp-2.3.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-18 19:26:40.0","id":10418,"title":"Error reading data from parquet file ( I am pretty new to Pig)","body":"<pre>Pig Stack Trace\n---------------\nERROR 1200: can't convert optional int96 uploadTime\nFailed to parse: can't convert optional int96 uploadTime\n  at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:201)\n  at org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1707)\n  at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1680)\n  at org.apache.pig.PigServer.registerQuery(PigServer.java:623)\n  at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1061)\n  at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:501)\n  at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)\n  at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\n  at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)\n  at org.apache.pig.Main.run(Main.java:560)\n  at org.apache.pig.Main.main(Main.java:170)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:606)\n  at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n  at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: parquet.pig.SchemaConversionException: can't convert optional int96 uploadTime\n  at parquet.pig.PigSchemaConverter.convertFields(PigSchemaConverter.java:108)\n  at parquet.pig.PigSchemaConverter.convert(PigSchemaConverter.java:84)\n  at parquet.pig.TupleReadSupport.getPigSchemaFromMultipleFiles(TupleReadSupport.java:70)\n  at parquet.pig.ParquetLoader.initSchema(ParquetLoader.java:204)\n  at parquet.pig.ParquetLoader.setInput(ParquetLoader.java:108)\n  at parquet.pig.ParquetLoader.getSchema(ParquetLoader.java:188)\n  at org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:175)\n  at org.apache.pig.newplan.logical.relational.LOLoad.&lt;init&gt;(LOLoad.java:89)\n  at org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:901)\n  at org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3568)\n  at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1625)\n  at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)\n  at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)\n  at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)\n  at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)\n  ... 16 more\nCaused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: NYI\n  at parquet.pig.PigSchemaConverter$1.convertINT96(PigSchemaConverter.java:148)\n  at parquet.pig.PigSchemaConverter$1.convertINT96(PigSchemaConverter.java:120)\n  at parquet.schema.PrimitiveType$PrimitiveTypeName$7.convert(PrimitiveType.java:219)\n  at parquet.pig.PigSchemaConverter.getSimpleFieldSchema(PigSchemaConverter.java:119)\n  at parquet.pig.PigSchemaConverter.getFieldSchema(PigSchemaConverter.java:222)\n  at parquet.pig.PigSchemaConverter.convertFields(PigSchemaConverter.java:99)\n  ... 30 more\n================================================================================</pre>","tags":["error"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-21 13:28:48.0","id":11059,"title":"Configuration parameter 'yarn.timeline-service.leveldb-timeline-store.path' was not found in configurations dictionary!","body":"<p>Hi,</p><p>I just upgraded Ambari from 2.1 to 2.2, my HDP version is 2.1.0, all on CentOS 6.7. I have a new strange issue with starting of MapReduce2 and YARN services. All is ending with this message:</p>stderr:  /var/lib/ambari-agent/data/errors-4003.txt<pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/resourcemanager.py\", line 221, in &lt;module&gt;\n    Resourcemanager().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/resourcemanager.py\", line 110, in start\n    import params\n  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/params.py\", line 28, in &lt;module&gt;\n    from params_linux import *\n  File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/params_linux.py\", line 153, in &lt;module&gt;\n    ats_leveldb_lock_file = os.path.join(ats_leveldb_dir, \"leveldb-timeline-store.ldb\", \"LOCK\")\n  File \"/usr/lib64/python2.6/posixpath.py\", line 67, in join\n    elif path == '' or path.endswith('/'):\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py\", line 81, in __getattr__\n    raise Fail(\"Configuration parameter '\" + self.name + \"' was not found in configurations dictionary!\")\nresource_management.core.exceptions.Fail: Configuration parameter 'yarn.timeline-service.leveldb-timeline-store.path' was not found in configurations dictionary!</pre>stdout:  /var/lib/ambari-agent/data/output-4003.txt<pre>2016-01-21 14:05:32,174 - Using hadoop conf dir: /etc/hadoop/conf\n2016-01-21 14:05:32,282 - Using hadoop conf dir: /etc/hadoop/conf\n2016-01-21 14:05:32,284 - Group['hadoop'] {}\n2016-01-21 14:05:32,286 - Group['users'] {}\n2016-01-21 14:05:32,286 - User['mapred'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-01-21 14:05:32,288 - User['ambari-qa'] {'gid': 'hadoop', 'groups': ['users']}\n2016-01-21 14:05:32,289 - User['zookeeper'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-01-21 14:05:32,290 - User['hdfs'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-01-21 14:05:32,291 - User['yarn'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-01-21 14:05:32,292 - User['ams'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-01-21 14:05:32,292 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-01-21 14:05:32,294 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2016-01-21 14:05:32,343 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2016-01-21 14:05:32,344 - User['hdfs'] {'ignore_failures': False}\n2016-01-21 14:05:32,345 - User['hdfs'] {'ignore_failures': False, 'groups': ['hadoop']}\n2016-01-21 14:05:32,345 - Directory['/etc/hadoop'] {'mode': 0755}\n2016-01-21 14:05:32,346 - Directory['/etc/hadoop/conf.empty'] {'owner': 'root', 'group': 'hadoop', 'recursive': True}\n2016-01-21 14:05:32,346 - Link['/etc/hadoop/conf'] {'not_if': 'ls /etc/hadoop/conf', 'to': '/etc/hadoop/conf.empty'}\n2016-01-21 14:05:32,393 - Skipping Link['/etc/hadoop/conf'] due to not_if\n2016-01-21 14:05:32,407 - File['/etc/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-01-21 14:05:32,408 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}\n2016-01-21 14:05:32,423 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}\n2016-01-21 14:05:32,477 - Skipping Execute[('setenforce', '0')] due to not_if\n2016-01-21 14:05:32,477 - Directory['/var/log/hadoop'] {'owner': 'root', 'mode': 0775, 'group': 'hadoop', 'recursive': True, 'cd_access': 'a'}\n2016-01-21 14:05:32,480 - Directory['/var/run/hadoop'] {'owner': 'root', 'group': 'root', 'recursive': True, 'cd_access': 'a'}\n2016-01-21 14:05:32,480 - Directory['/tmp/hadoop-hdfs'] {'owner': 'hdfs', 'recursive': True, 'cd_access': 'a'}\n2016-01-21 14:05:32,481 - File['/var/lib/ambari-agent/lib/fast-hdfs-resource.jar'] {'content': StaticFile('fast-hdfs-resource.jar'), 'mode': 0644}\n2016-01-21 14:05:32,538 - File['/etc/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'hdfs'}\n2016-01-21 14:05:32,540 - File['/etc/hadoop/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'hdfs'}\n2016-01-21 14:05:32,541 - File['/etc/hadoop/conf/log4j.properties'] {'content': ..., 'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}\n2016-01-21 14:05:32,549 - File['/etc/hadoop/conf/hadoop-metrics2.properties'] {'content': Template('hadoop-metrics2.properties.j2'), 'owner': 'hdfs'}\n2016-01-21 14:05:32,549 - File['/etc/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}\n2016-01-21 14:05:32,550 - File['/etc/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}\n2016-01-21 14:05:32,554 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop'}\n2016-01-21 14:05:32,602 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}\n2016-01-21 14:05:32,845 - Using hadoop conf dir: /etc/hadoop/conf\n2016-01-21 14:05:32,845 - Skipping get_hdp_version since hdp-select is not yet available\n2016-01-21 14:05:32,846 - Using hadoop conf dir: /etc/hadoop/conf</pre><p>Can you please recommend what shall I do? Thank you so much.</p><p>Regards,</p><p>Pavel</p>","tags":["MapReduce","YARN","yarn-ats"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-23 20:41:26.0","id":11382,"title":"HDP 2.3.4 - spark web UI","body":"<p>In ambari, i can only see link for Spark History server UI. what is the URL for opening Spark Web UI to debug Spark DAG?</p><p>I am running HDP 2.3.4 with Spark 1.5.2</p>","tags":["hdp-2.3.4","ui","Spark"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-23 10:51:52.0","id":7787,"title":"I am unable to access port 8000 .I am using sandbox hosted on azure....","body":"","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-26 11:49:00.0","id":11768,"title":"Unable to login to shell (Login incorrect)","body":"<p>The default username and password is not being accepted. Could anyone please help me out. </p><p><img src=\"/storage/attachments/1565-virtualbox-hortonworks-sandbox-with-hdp-232-26-01.png\"></p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-26 19:50:13.0","id":11838,"title":"AD ldap sync issue: Caught exception running LDAP sync. [LDAP: error code 32 - 0000208D: NameErr: DSID-031522C9, problem 2001 (NO_OBJECT), data 0, best match of:","body":"","tags":["ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-26 21:00:55.0","id":11837,"title":"Passing parameters to pig script from file issue","body":"<p>pig -x mapreduce /home/hadoop/test.pig -param_file=hdfs://ip-XXX-XX-XX-XXX.ec2.internal:8020/home/hadoop/adh_time</p><p>I am trying to run the above command and its not working saying undefined parameter i tired multiple ways and nothing works.Any help would be appreciated.</p><p>Value inside the adh_time is input = 2015-06-16T14:59:55.000Z</p><p>Error:</p><p>16/01/26 20:56:38 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\n16/01/26 20:56:38 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE\n16/01/26 20:56:38 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType\n46  [main] INFO  org.apache.pig.Main  - Apache Pig version 0.14.0-amzn-0 (r: unknown) compiled Nov 16 2015, 21:51:33\n16/01/26 20:56:38 INFO pig.Main: Apache Pig version 0.14.0-amzn-0 (r: unknown) compiled Nov 16 2015, 21:51:33\n47  [main] INFO  org.apache.pig.Main  - Logging error messages to: /mnt/var/log/pig/pig_1453841798872.log\n16/01/26 20:56:38 INFO pig.Main: Logging error messages to: /mnt/var/log/pig/pig_1453841798872.log\n789  [main] INFO  org.apache.pig.impl.util.Utils  - Default bootup file /home/hadoop/.pigbootup not found\n16/01/26 20:56:39 INFO util.Utils: Default bootup file /home/hadoop/.pigbootup not found\n810  [main] ERROR org.apache.pig.impl.PigContext  - Undefined parameter : input\n16/01/26 20:56:39 ERROR impl.PigContext: Undefined parameter : input\n828  [main] ERROR org.apache.pig.Main  - ERROR 2997: Encountered IOException. org.apache.pig.tools.parameters.ParameterSubstitutionException: Undefined parameter : input\n16/01/26 20:56:39 ERROR pig.Main: ERROR 2997: Encountered IOException. org.apache.pig.tools.parameters.ParameterSubstitutionException: Undefined parameter : input\nDetails at logfile: /mnt/var/log/pig/pig_1453841798872.log\n848  [main] INFO  org.apache.pig.Main  - Pig script completed in 952 milliseconds (952 ms)\n16/01/26 20:56:39 INFO pig.Main: Pig script completed in 952 milliseconds (952 ms)</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-27 10:27:51.0","id":11943,"title":"Why is the access mode of the usersync.log 600 ?","body":"<p>I built the hadoop cluster included the Ranger service with the Ambari.</p><p>When I checked the usersync.log, I needed the sudo command.</p><p>On the other hands, the logs for the ranger-admin allow anyone to read.</p><p>Why is the access mode of the usersync.log 600 ?</p>","tags":["ranger-0.5.0","Ranger"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-02 19:47:19.0","id":795,"title":"Expected State FINISHED but found RUNNING SQL Error in Hive View Query Editor","body":"<p>We are  getting the following error in Hive query editor. This error occurs inconsistently  with identical SQL and happens across different web browsers. I can tell when I will get the error after the query is started because it shows a successful status immediately, before the query would be finished. When the error does not occur there is a running status first and then a completed status. In this case, the duration of the overall process is 10 seconds or so.</p><p>We do not think it is a permission issue but maybe a timeout setting in Ambari?\n\n\"trace\":\"org.apache.ambari.view.hive.client.HiveErrorStatusException: H190 Unable to fetch results metadata. Expected state FINISHED, but found RUNNING [ERROR_STATUS]\\n\\norg.apache.ambari.view.hive.client.HiveErrorStatusException: H190 Unable to fetch results metadata. Expected state FINISHED, but found RUNNING [ERROR_STATUS]\\n\\tat org.apache.ambari.view.hive.client.Utils.verifySuccess(Utils.java:29)\\n\\tat org.apache.ambari.view.hive.client.Cursor.getSchema(Cursor.java:107)\\n\\tat org.apache.ambari.view.hive.resources.jobs.ResultsPaginationController.request(ResultsPaginationController.java:127)\\n\\tat org.apache.ambari.view.hive.resources.jobs.JobService.getResults(JobService.java:279)\\n\\tat sun.reflect.GeneratedMethodAccessor673.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:497)\\n\\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\\n\\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\\n\\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\\n\\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\\n\\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\\n\\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\\n\\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\\n\\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\\n\\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\\n\\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:182)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\\n\\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\\n\\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\\n\\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\\n\\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\\n\\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\\n\\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\\n\\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\\n\\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\\n\\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\\n\\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)\\n\\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:198)\\n\\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:132)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\\n\\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\\n\\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\\n\\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\\n\\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\\n\\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\\n\\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\\n\\tat org.eclipse.jetty.io.nio.SslConnection.handle(SslConnection.java:196)\\n\\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\\n\\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n\",\"message\":\"H190 Unable to fetch results metadata. Expected state FINISHED, but found RUNNING [ERROR_STATUS]\",\"status\":500}</p>","tags":["ambari-views","sql","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-01 21:13:38.0","id":712,"title":"Ranger KMS won't start","body":"<pre>2015-10-01 21:00:14,475 - File['/etc/ranger/kms/rangerkms.jceks'] {'owner': 'kms', 'group': 'kms'}\n2015-10-01 21:00:14,476 - XmlConfig['dbks-site.xml'] {'group': 'kms', 'conf_dir': '/usr/hdp/current/ranger-kms/conf', 'mode': 0644, 'configuration_attributes': {}, 'owner': 'kms', 'configurations': ...}\n2015-10-01 21:00:14,494 - Generating config: /usr/hdp/current/ranger-kms/conf/dbks-site.xml\n2015-10-01 21:00:14,495 - File['/usr/hdp/current/ranger-kms/conf/dbks-site.xml'] {'owner': 'kms', 'content': InlineTemplate(...), 'group': 'kms', 'mode': 0644, 'encoding': 'UTF-8'}\n2015-10-01 21:00:14,505 - Writing File['/usr/hdp/current/ranger-kms/conf/dbks-site.xml'] because contents don't match\n2015-10-01 21:00:14,506 - XmlConfig['ranger-kms-site.xml'] {'group': 'kms', 'conf_dir': '/usr/hdp/current/ranger-kms/conf', 'mode': 0644, 'configuration_attributes': {}, 'owner': 'kms', 'configurations': ...}\n2015-10-01 21:00:14,520 - Generating config: /usr/hdp/current/ranger-kms/conf/ranger-kms-site.xml\n2015-10-01 21:00:14,520 - File['/usr/hdp/current/ranger-kms/conf/ranger-kms-site.xml'] {'owner': 'kms', 'content': InlineTemplate(...), 'group': 'kms', 'mode': 0644, 'encoding': 'UTF-8'}\n2015-10-01 21:00:14,527 - Writing File['/usr/hdp/current/ranger-kms/conf/ranger-kms-site.xml'] because contents don't match\n2015-10-01 21:00:14,527 - XmlConfig['kms-site.xml'] {'group': 'kms', 'conf_dir': '/usr/hdp/current/ranger-kms/conf', 'mode': 0644, 'configuration_attributes': {}, 'owner': 'kms', 'configurations': ...}\n2015-10-01 21:00:14,541 - Generating config: /usr/hdp/current/ranger-kms/conf/kms-site.xml\n2015-10-01 21:00:14,541 - File['/usr/hdp/current/ranger-kms/conf/kms-site.xml'] {'owner': 'kms', 'content': InlineTemplate(...), 'group': 'kms', 'mode': 0644, 'encoding': 'UTF-8'}\n2015-10-01 21:00:14,556 - Writing File['/usr/hdp/current/ranger-kms/conf/kms-site.xml'] because contents don't match\n2015-10-01 21:00:14,556 - File['/usr/hdp/current/ranger-kms/conf/kms-log4j.properties'] {'content': ..., 'owner': 'kms', 'group': 'kms', 'mode': 0644} </pre><p>2015-10-01 21:01:17,660 - Error : [Errno 110] Connection timed out</p>","tags":["configuration","Ranger","ranger-kms"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-02 23:04:55.0","id":823,"title":"Where Ambari set hbase-env template variable values?","body":"<p>inside Ambari--&gt; Hbase conf --&gt; hbase-env template, there are variables such as \"hbase_max_direct_memory_size\", \"regionserver_heapsize\", where they are defined or where these values are set?</p>","tags":["Hbase","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-04 22:03:49.0","id":862,"title":"Can we use variable hostname for flume sink configuration inside flume config file?","body":"<p>Given a hfds sink configuration :</p><pre>ale.sinks.sink1.hdfs.path = hdfs://ham-dal-0001.corp.wayport.net:8020/prod/hadoop/smallsite/flume_ingest_ale2_hak_3/station/%Y/%m/%d/%H</pre><p>The question is that can we use {variable hostname}  in flume.conf file. For example:</p><pre>ale.sinks.sink1.hdfs.path = hdfs://ham-dal-0001.corp.wayport.net:8020/prod/hadoop/smallsite/{variable hostname}/station/%Y/%m/%d/%H \t\t\t\t\t\t\n</pre><p>Thank you.</p>","tags":["configuration","Flume","HDFS"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-06 02:26:38.0","id":937,"title":"Is there a Ranger KMS REST API?","body":"<p>Hi, the reference Hadoop KMS implementation has a REST API <a href=\"https://hadoop.apache.org/docs/current/hadoop-kms/index.html\">https://hadoop.apache.org/docs/current/hadoop-kms/index.html</a> Is there anything like that for Ranger KMS? Given that Ranger itself has a complete REST API, I would expect the same for KMS, but I don't see any mention in here <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_Ranger_KMS_Admin_Guide/content/ch_ranger_kms_overview.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_Ranger_KMS_Admin_Guide/content/ch_ranger_kms_overview.html</a></p>","tags":["api","ranger-kms","hadoop","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-05 14:35:32.0","id":885,"title":"Can I also Lower Case User Names in Apache Knox?","body":"<p>All usernames are lower cased fetching from AD/LDAP, can I also force Knox to lower case the name?</p>","tags":["Knox","active-directory","ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-07 00:02:09.0","id":1027,"title":"How do we plan Falcon deployments for replication, mirroring and data pipeline on prod and DR clusters?","body":"<p>My understanding is if replication or mirroring is required then falcon is installed only on destination cluster in standalone mode. For data pipeline, install falcon where pipeline will be executed. Is my understanding correct? What is falcon prism(distributed mode) use? I cant find any reference.  Any inputs will  be appreciated.</p>","tags":["disaster-recovery","mirroring","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-07 16:16:35.0","id":1069,"title":"How to deploy 2 HiveServer2 processes on same host; one with LDAP authentication and other one with Kerberos authentication","body":"<p>Hiveserver2 only supports 1 Authentication method at a time? if so, how to have 2 authentication methods for different clients? And How to deploy 2 HiveServer2 processes on same host; one with LDAP authentication and other one with Kerberos authentication.</p>","tags":["ldap","hiveserver2","kerberos"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-07 20:27:22.0","id":1094,"title":"Hbase Connectivity Fails","body":"<p>Getting following error while connecting to Hbase.</p><p>HConnection connection = HConnectionManager.createConnection(conf);</p><p>Error in Log file:</p><p>2015-10-07 20:19:33 o.a.z.ClientCnxn [INFO] Session establishment complete on server localhost.localdomain/127.0.0.1:2181, sessionid = 0x15043a5de090013, negotiated timeout = 4000</p><p>2015-10-07 20:19:33 STDIO [ERROR] java.io.IOException: java.lang.reflect.InvocationTargetException</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:426)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.client.ConnectionManager.createConnectionInternal(ConnectionManager.java:319)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:292)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at com.opensoc.enrichment.adapters.write.dhcp.DynamicLookupWriterBolt.initializeAdapter(DynamicLookupWriterBolt.java:64)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at com.opensoc.enrichment.adapters.write.dhcp.DynamicLookupWriterBolt.execute(DynamicLookupWriterBolt.java:115)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.daemon.executor$fn__5697$tuple_action_fn__5699.invoke(executor.clj:659)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.daemon.executor$mk_task_receiver$fn__5620.invoke(executor.clj:415)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.disruptor$clojure_handler$reify__1741.onEvent(disruptor.clj:58)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.daemon.executor$fn__5697$fn__5710$fn__5761.invoke(executor.clj:794)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at backtype.storm.util$async_loop$fn__452.invoke(util.clj:465)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at clojure.lang.AFn.run(AFn.java:24)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at java.lang.Thread.run(Thread.java:745)</p><p>2015-10-07 20:19:33 STDIO [ERROR] Caused by: java.lang.reflect.InvocationTargetException</p><p>2015-10-07 20:19:33 STDIO [ERROR] at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:424)</p><p>2015-10-07 20:19:33 STDIO [ERROR] ... 15 more</p><p>2015-10-07 20:19:33 STDIO [ERROR] Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.ClusterId.parseFrom(ClusterId.java:64)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:75)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:106)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:858)</p><p>2015-10-07 20:19:33 STDIO [ERROR] at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:662)</p><p>2015-10-07 20:19:33 STDIO [ERROR] ... 20 more</p>","tags":["Hbase","connection"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-07 13:13:36.0","id":1051,"title":"Where can we find the SQL syntax reference for Atlas Metadata search?","body":"<p>Atlas UI allows us to search for entities with SQL like syntax, for example 'table where name = \"sales_fact\"'. Is this SQL framework based on any existing libraries within Gremlin? Or was this custom built for Atlas?</p><p>Trying to understand if there is documentation on the proper SQL syntax for these searches?</p><p>Doc Reference: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_data_governance/content/section_metadata_store_mgmt_with_atlas_ui.html</p>","tags":["Atlas","sql"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-08 15:26:36.0","id":1125,"title":"Which tool is most popular for managing large number of NiFi worker/slave configuration files?","body":"<p>Which tools do you see as most popular for customers who wish to manage large number of NiFi worker/slave instances across many separate servers?</p><p>E.g. do you see people using Puppet, pdcp, or other tools?</p>","tags":["operations","Nifi","configuration"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-07 17:12:17.0","id":1079,"title":"Qlikview ODBC connection to Hive","body":"<p>Need your input.......Is there a quick way to move data from Hive?</p><ol>\n<li>Calling ‘Select * from &lt;table&gt;’ to qlick view dashboard</li></ol><p>1.4 million records in Hive, they need to load all these records in to Qlikview dash board .Extracting data from Hive to Qlikview takes around 3 hours. Any solution to reduce this time?</p><p>My initial thought was to increase the replication factor, pre-warm containers, or put data in memory but curious if folks may know of other things that can be done to increase performance.  Maybe there is something that can be done from ODBC side, or even Clikview?</p>","tags":["Hive","odbc","qlikview"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-10-14 20:04:41.0","id":1467,"title":"How to run Flume in HA ?","body":"<p>Would like know if there is a way to  run flume in HA mode.</p>","tags":["Flume","namenode-ha"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-14 16:43:12.0","id":1454,"title":"Hue CLI logging location","body":"<p>Does anyone know an easy way to configure where Hue writes the logs for the CLI operations (e.g sync_ldap_users_and_groups, import_ldap_group, and import_ldap_user)?</p><p>-Darwin</p>","tags":["logs","cli","hue"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-12 06:37:33.0","id":6138,"title":"Transactions quite possible in HBase, why people are moving towards Hive to implement with more fileformats","body":"","tags":["Hive","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 15:25:49.0","id":6373,"title":"java.io.IOException:java.io.EOFException: Read past end of RLE integer from compressed stream","body":"<p>Trying to run a simple select column statement on a hive table stored as ORC and I'm running into this error. I can't seem to find any solution to it, however, I have found similar questions around the internet regarding it. \n</p><pre>Failed with exception java.io.IOException:java.io.EOFException: Read past end of RLE integer from compressed stream Stream for column 2 kind DATA position: 20 length: 20 range: 0 offset: 20 limit: 20 range 0 = 0 to 20 uncompressed: 77 to 77</pre><p>The data looks like this (here it's pseudo-data, tab-separated):</p><pre>uuid\tfeed\tunixgmtfetchtime\naaa\treviews\t1385830800\nbbb\treviews\t1395830800\nzzz\treviews\t1405830800</pre><p>with schema:</p><pre>uuid                \tstring\nfeed                \tstring\nunixgmtfetchtime        int\n</pre><p>running select unixgmtfetchtime from tablename; returns the error mentioned above.</p><p><strong>More info: I have run alter table tablename concatenate; on the table. </strong></p>","tags":["Hive","concatenate","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-21 11:01:44.0","id":7442,"title":"Unable to connect to Ambari on x.x.x.x:8081 : HTTP Error 404: Not Found","body":"<p>Edge node's ambari-agent is running. Logs showing heartbeat issues. i tried to reboot the edgenode but still the error comes.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-23 16:44:31.0","id":7877,"title":"shared folder between windows and the guest system(cent os)","body":"<p>how to create a shared folder between windows and the guest system(cent os)  to copy data , executable jars from windows to hdfs ,</p>","tags":["help","mount","share","windows"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-06 20:14:44.0","id":26268,"title":"ambari-agent start failed","body":"<p>The agent does not start. Looks like OpenSSL is missing - it isn't.</p><p>Any ideas?</p><p>The rpm is: </p><p>ambari-agent-2.2.1.1-70.x86_64</p><p>&gt;&gt; ambari-agent start\n\nVerifying Python version compatibility...\n\nUsing python  /usr/bin/python\n\nChecking for previously running Ambari Agent...\n\n/var/run/ambari-agent/ambari-agent.pid found with no process. Removing 7128...\n\nStarting ambari-agent\n\nVerifying ambari-agent process status...\n\nERROR: ambari-agent start failed. For more details, see /var/log/ambari-agent/ambari-agent.out:\n\n====================\n\nCan't open library libX11.so.6: libssl.so: cannot open shared object file: No such file or directory\n\n====================\n\nAgent out at: /var/log/ambari-agent/ambari-agent.out\n\nAgent log at: /var/log/ambari-agent/ambari-agent.log</p><p>Red Hat Enterprise Linux Server release 6.7 (Santiago)\n\njava version \"1.8.0_77\"\n\nJava(TM) SE Runtime Environment (build 1.8.0_77-b03)\n\nJava HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)</p><p>Package openssl-1.0.1e-42.el6_7.2.x86_64 already installed and latest version\n\nPackage libXt-1.1.4-6.1.el6.x86_64 already installed and latest version\n\nPackage libX11-1.6.0-6.el6.x86_64 already installed and latest version</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-08 15:28:09.0","id":26657,"title":"nifi control rate processor template in 0.5.1 imported into 0.6 does not import property for autoterminate relationship","body":"<p>I have  a template built in 0.5.1, when I import it into 0.6 with ControlRate processor, auto-terminate relationship is not checked. </p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-08 19:08:35.0","id":26708,"title":"Ambari can run in a singlenode?","body":"<p>Hello,</p><p>In order to prepare a multi\nnode deployment using Ambari, I wanted first run the Ambari in a VM Centos 6 on\nmy laptop.</p><p>I'm running the server and\nthe agent in the same node and wanted to deploy Hortonworks using Ambari.</p><p>The preparing step takes\nseveral minutes and ended in error but without any error message in the Ambari\nServer log either in the agent log files. So I'm wondering if it's possible to\nrun Ambari in a single node?</p><p>Thanks</p><p>Regards</p>","tags":["Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-09 06:46:22.0","id":26706,"title":"sqoop-merge: class file not found","body":"<p>Hi,</p><p>I ran the following sqoop-codegen command followed by sqoop-merge command, but get </p><p>java.lang.ClassNotFoundException: Class /user/output/bin/customerInfo.class not found</p>\nTried with /user/output/bin/customerInfo and get the same error. The .java, .class and .jar file are in LFS in dir /user/output/bin.\n\n<p>sqoop codegen --connect jdbc:mysql://sandbox.hortonworks.com:3306/test --username root --password askfor1 --table customerInfo --map-column-java mtts=String,startdte=String --null-string \"\" --null-non-string \"\" --driver com.mysql.jdbc.Driver --bindir /user/output/bin --outdir /user/output/bin</p>then ran the sqoop merge:<p></p><p>sqoop merge --class-name /user/output/bin/customerInfo.class --jar-file /user/output/bin/customerInfo.jar --new-data /user/output/part-m-00000 --onto /user/output/part-m-00001 --target-dir /user/output --merge-key Name</p><p></p><p><img src=\"/storage/attachments/3299-kxx1d.png\"></p><p>Am I missing anything?</p><p>Thank you.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-12 13:07:41.0","id":27155,"title":"HDFS In-Memory Tier","body":"<p>Looking to understand if there are any limits to using Host Groups to define specific machines in a cluster to serve as HDFS in Memory Tier.  Cluster has machines that have RAM and SSD, in addition to machines that have RAM and SATA Drives.</p><p>Goal would be to use the in memory tier as storage for Spark data processing pipelines.</p>","tags":["HDFS","Spark"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-13 01:34:15.0","id":27245,"title":"Metron Ansible deployment failure in metron_pcapservice","body":"<p>TASK [metron_pcapservice : Check for hbase-site] *******************************\nok: [node1] </p><p>TASK [metron_pcapservice : include] ********************************************\nskipping: [node1] </p><p>TASK [metron_pcapservice : include] ********************************************\nincluded: /Users/nbalaji-elangovan/Downloads/incubator-metron-Metron_0.1BETA_rc7/deployment/roles/metron_pcapservice/tasks/pcapservice.yml for node1 </p><p>TASK [metron_pcapservice : Create Metron streaming directories] ****************\nok: [node1] =&gt; (item={u'name': u'lib'})\nok: [node1] =&gt; (item={u'name': u'config'}) </p><p>TASK [metron_pcapservice : Copy Metron pcapservice jar] ************************\nfatal: [node1]: FAILED! =&gt; {\"changed\": false, \"failed\": true, \"msg\": \"could not find src=/Users/nbalaji-elangovan/Downloads/incubator-metron-Metron_0.1BETA_rc7/metron-streaming/Metron-Pcap_Service/target/Metron-Pcap_Service-0.1BETA-jar-with-dependencies.jar\"}\nPLAY RECAP *********************************************************************\nnode1                      : ok=262  changed=140  unreachable=0    failed=1\nAnsible failed to complete successfully. Any error output should be\nvisible above. Please fix these errors and try again.</p>","tags":["Metron"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-04-13 09:47:09.0","id":27308,"title":"Team,Could you please let me know  how to use clip board ( Copy and Paste ) in HDPCD exam real ..","body":"","tags":["hdpcd"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-13 11:51:01.0","id":27327,"title":"About Java 8 support","body":"<p>In your documentation, you mention that Java 8 is also supported.  But according to hadoop documentation, they mention only Java 7, and there is a separate branch of Hadoop where they do porting to Java 8 (and hadoop-2.8 is expected to be first release with Java 8 support).</p><p>How is it possible you support Java 8 with (currently) hadoop-2.7.1?  Do you backport their changes from Java8 branch?</p><p>Thanks.</p>","tags":["java","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-13 12:23:58.0","id":27341,"title":"Ambari: to avoid root use account","body":"<p>Hi All,</p><p>I have 2 questions :</p><p>1) Is it possible to setup Ambari to avoid to use the root account durnig the installation step?</p><p>2) Can we change the installation path? I can see that Ambari installe some folders (kafka_log, hadoop) just under /. I want to avoid this in my production environement. And also som log file under /var. Which is not a right place from my opinion to put the log files.</p><p>So, I 'm interested by any suggestion and exeprience on how to customize an Ambari server behavior regarding the installation path.</p><p>Thanks and Regards</p><p>Farhad</p>","tags":["ambari-server"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-13 18:53:25.0","id":27442,"title":"Starting Services after Rebooting OS","body":"<p>Hi All,</p><p>After applying OS patches to Data nodes, the servers must be rebooted. Once the servers are rebooted, the hadoop services running on them are not automatically coming up. You have to manually go into Ambari and tell the services to start on that host. </p><p>What are some best practices and recommendations as to how to automatically bring up the hadoop services after an OS reboot?</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-13 16:53:25.0","id":27399,"title":"Flume - source exec and sink hdfs. File is not loaded to hdfs","body":"<p><a href=\"/storage/attachments/3423-exec-source.txt\">exec-source.txt</a>Hi,</p><p>I have used the below Flume program to read a file from LFS to HDFS, for learning.</p><p>But I see not folder created.</p><p>Do you see any issue in this file. I wanted to see how interceptor works.</p><p>Thank you.</p>","tags":["Flume"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-04 00:53:29.0","id":2740,"title":"Oozie job: What value should I use for jobTracker for Resource Manager HA?","body":"<p>I was always wondering what value I should use for \"jobtTacker\" in my job.properites for Resource Manager HA.</p><p>Now a customer asked same question, so I thought this might be a good opportunity to find out.</p><p>Does anyone know which string we should use to utilize YARN Resource Manager HA?</p><p>According to Google, Cloudera uses \"logicaljt\" but I don't see this string in HDP code so far. </p>","tags":["resource-manager","namenode-ha","YARN","Oozie"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-04 16:41:43.0","id":2791,"title":"REST API /controller/search-results query syntax?","body":"<p><a target=\"_blank\" href=\"https://nifi.apache.org/docs/nifi-docs/rest-api/index.html\">https://nifi.apache.org/docs/nifi-docs/rest-api/index.html</a></p><p>I can pull all of the data about a NiFi data flow using the <strong>/controller/search-results</strong> api call, but the request section also mentions a 'q' parameter (for query). There was no mention of the query syntax in the doc, however, any pointers? Would like to avoid pulling in a full config every time and parse on a constrained device, if possible.</p><p><a target=\"_blank\" href=\"https://nifi.apache.org/docs/nifi-docs/rest-api/index.html\"></a></p>","tags":["api","dataflow","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-09 01:21:56.0","id":3033,"title":"I get an IllegalArgumentException error when trying to read a file with Spark 1.4.1","body":"<p>Running a Spark command to read a file and get an illegalArgumentException. This is HDP 2.3.1 and Spark 1.4.1. Same error occurs with PySpark. The error appears to come from the SnappyCompressionCodec.</p><p>scala&gt; <strong>var file = sc.textFile(\"hdfs://HdpTest:8020/user/weli/README.md\")</strong></p><p><strong>java.lang.IllegalArgumentException</strong></p><p>  at org.apache.spark.io.SnappyCompressionCodec.&lt;init&gt;(CompressionCodec.scala:152)</p><p>  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</p>","tags":["error","pyspark","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-04 19:12:33.0","id":2806,"title":"3rd party packages in spark and zeppelin","body":"<p>Is there any differences  to load new 3rd party packages using cli or zeppelin if I am using zeppelin as the notebook.</p><p>1)cli: spark-shell\n--packages com.databricks:spark-csv_2.11:1.1.0</p><p>or using</p><p>2) zeppelin: // add artifact recursively except comma separated GroupID:ArtifactId list</p><pre>z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId,groupId:artifactId, ...\")</pre>\n<p><code></code></p>","tags":["zeppelin","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-07 16:00:48.0","id":9124,"title":"Ambari Alert REST API to filter out services on maintenance mode","body":"<p>we are using Ambari alerts Rest API to poll for components health. We would like to filter out the ones which are in Maintenance mode.  Any idea what how we can get that. The REST API call  throws all the services  that are critical- http://&lt;ambari host&gt;:8080/api/v1/clusters/tech_hdp/alerts?Alert/state=CRITICAL</p>","tags":["alerts","Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-07 19:32:08.0","id":9163,"title":"Is Sqoop robust over WAN for a production HDInsight cluster?","body":"<p> Need to pull data from MSSQL over WAN into Hive. Storing them as files on HDFS would be a bonus. </p><p>Curios to know if Sqoop is able to provide a robust import over WAN? Would Sqoop meet requirements of a production environment? </p><p>\nThanking in advance</p>","tags":["Sqoop","azure","production","hadoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-23 19:34:15.0","id":142,"title":"Hadoop Yarn Simple Application error","body":"<p>I am trying to run the simple yarn application listed here: <a href=\"https://github.com/hortonworks/simple-yarn-app\">https://github.com/hortonworks/simple-yarn-app</a></p><p>I am a beginner with both Java and Hadoop, and when I try to compile the simple yarn Client file using '<code>javac</code>', I get the following error:</p><p>Client.java:9: <code>error: package org.apache.hadoop.conf does not exist</code></p><pre>import org.apache.hadoop.conf.Configuration;</pre>\n<p>The command I am using to compile the file is:</p><pre>javac Client.java</pre>\n<p>I have Googled this error to see if I could find which JAR file is missing from my classpath, but I couldn't find anything helpful with respect to YARN. Most of the results were related to <code>HBASE</code>, <code>PIG</code> or <code>HIVE</code>.</p><p>Can someone please point me towards the relevant JAR file I am missing here? Thanks.</p>","tags":["YARN","hadoop","java"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-08 18:28:47.0","id":9221,"title":"How to manage Metadata for Files in HDFS?","body":"<p>We have files being pushed into HDFS using curl and WebHDFS interface. Files mostly contain structured data and there is lots of metadata available around fields, data type, file description etc. at the time of ingestion. There is no specific requirement to add these files to Hive. </p><p>As per my understanding Atlas is more aligned with Sqoop/Hive based data ingestion. Can we add the file specific metadata in Atlas, some how at the time ingestion using curl/WebHDFS? Example: HDFS location, file name, fields, datatype etc. </p><p>An alternate might be to use Falcon and using free form tags to  the metadata but that would require changes to the way data is currently ingested, unless we can schedule a curl script in Falcon. </p><p>Any ideas or suggestions... Thanks!</p>","tags":["metadata","Atlas","HDFS"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-07 21:47:38.0","id":9176,"title":"Atlas UI not working","body":"<p>an 07, 2016 9:54:29 AM com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator attachTypes\nINFO: Couldn't find JAX-B element for class javax.ws.rs.core.Response\nJan 07, 2016 9:54:29 AM com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator$8 resolve\nSEVERE: null\njava.lang.IllegalAccessException: Class com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator$8 can not access a member of class javax.ws.rs.core.Response with modifiers \"protected\"\n  at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:109)\n  at java.lang.Class.newInstance(Class.java:368)\n  at com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator$8.resolve(WadlGeneratorJAXBGrammarGenerator.java:467)\n  at com.sun.jersey.server.wadl.WadlGenerator$ExternalGrammarDefinition.resolve(WadlGenerator.java:181)\n  at com.sun.jersey.server.wadl.ApplicationDescription.resolve(ApplicationDescription.java:81)\n  at com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator.attachTypes(WadlGeneratorJAXBGrammarGenerator.java:518)\n  at com.sun.jersey.server.wadl.WadlBuilder.generate(WadlBuilder.java:179)\n  at com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl.getApplication(WadlApplicationContextImpl.java:125)\n  at com.sun.jersey.server.impl.wadl.WadlMethodFactory$WadlOptionsMethodDispatcher.dispatch(WadlMethodFactory.java:98)\n  at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n  at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n  at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n  at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n  at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n  at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n  at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n  at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n  at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n  at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n  at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n  at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n  at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n  at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\n  at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\n  at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\n  at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n  at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\n  at org.apache.atlas.web.filters.AuditFilter.doFilter(AuditFilter.java:67)\n  at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n  at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n  at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n  at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n  at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n  at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n  at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n  at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n  at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n  at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n  at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n  at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n  at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n  at org.mortbay.jetty.Server.handle(Server.java:326)</p>","tags":["Atlas","governance"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-08 19:32:54.0","id":9242,"title":"Customized service accounts with Ambari BluePrint","body":"<p>How can I Provide customized service accounts in Ambari Blueprint.Any references can help us.</p>","tags":["ambari-blueprint","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-08 18:49:47.0","id":9238,"title":"is it possible to upgrade Ranger to latest version while using HDP2.2.4 ?","body":"<p>Hello,</p><p>can Ranger be upgraded 'somehow' separately from 0.4 to 0.5 within HDP 2.2.4?</p><p>or is it just possible by upgrading the whole HDP stack to 2.3 ?</p><p>Thanks, Gerd</p>","tags":["Ranger","upgrade"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-04 17:50:54.0","id":8641,"title":"What is correct strategy for hbase Kerberos Login For Long running applications?","body":"<p>We run hbase client in long running application in kerberized cluster. </p><p>We login to hadoop and hbase using UserGroupInformation class.</p><p>Also, we launch as background thread to relogin.</p><p>However , this code does not work with versions of hbase-client ( 0.98-4-hadoop2 and hadoop-common 2.6.0).</p><p>Is there a better way to perform kerberos login usinf keytab and taking care of ticket expiry.</p>","tags":["Hbase","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-01-09 04:45:49.0","id":9278,"title":"Ingest tail a file using flume","body":"<p>Hi friends,I am beginner to flume and run my first job like this </p><pre>#  activate.\nagent.channels = memory-channel\nagent.sources = tail-source\nagent.sinks = log-sink hdfs-sink\n\n# Define a memory channel on agent called memory-channel.\nagent.channels.memory-channel.type = memory\n\n# Define a source on agent and connect to channel memory-channel.\nagent.sources.tail-source.type = exec\nagent.sources.tail-source.command = tail -F /sureshbonam/file \nagent.sources.tail-source.channels = memory-channel\n\n# Define a sink that outputs to logger.\nagent.sinks.log-sink.channel = memory-channel\nagent.sinks.log-sink.type = logger\n# Define a sink that outputs to hdfs.\nagent.sinks.hdfs-sink.channel = memory-channel\nagent.sinks.hdfs-sink.type = hdfs\nagent.sinks.hdfs-sink.hdfs.path = /flume_data\nagent.sinks.hdfs-sink.hdfs.fileType = DataStream\n</pre><p>I run the job like this</p><pre>flume-ng agent --conf conf --conf-file /sureshbonam/flume/tail1.conf --name agent -Dflume.root.logger=INFO,console</pre><p>This gives results like </p><pre>16/01/08 04:44:45 INFO node.Application: Starting Channel memory-channel\n16/01/08 04:44:45 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: memory-channel: Successfully registered new MBean.\n16/01/08 04:44:45 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: memory-channel started\n16/01/08 04:44:45 INFO node.Application: Starting Sink hdfs-sink\n16/01/08 04:44:45 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: hdfs-sink: Successfully registered new MBean.\n16/01/08 04:44:45 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: hdfs-sink started\n16/01/08 04:44:45 INFO node.Application: Starting Sink log-sink\n16/01/08 04:44:45 INFO node.Application: Starting Source tail-source\n16/01/08 04:44:45 INFO source.ExecSource: Exec source starting with command:tail -F /sureshbonam/file\n16/01/08 04:44:45 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: tail-source: Successfully registered new MBean.\n16/01/08 04:44:45 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: tail-source started </pre><p>In another terminal i tail the file like</p><pre>tail -F /sureshbonam/file\n\nBut result in terminal one won't changing or not moving forward.I don't need output to be hdfs(sink),even f it is console i am  ok. Any help??\n\n\n\n</pre>","tags":["Flume"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-09 23:05:03.0","id":9299,"title":"Using Nifi to stream data from Free streaming(not twitter) api to hdfs","body":"<p>Actually iam trying to use any api so that i can stream data using nifi and put it to hdfs. I know GetHttp processor is used to fetch data from a website. I want to know any free api thats streams data and also which processor to use in Nifi to connect to that api and stream data from that api using nifi to hdfs. Its a prototype iam trying to build using nifi. I just want to use an api other than twitter. Appreciate any help from friends.</p>","tags":["hdf","Nifi","nifi-streaming"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-11 05:05:15.0","id":9379,"title":"Insert overwrite query failing with Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask","body":"<p>I am a running a insert overwrite query as below.</p><p>Insert overwrite directory\n'/org/data/tmp/webapptempspace/UC3/log'\nselect\na.* from\na join b on ucase(a.name)=ucase(b.name);</p><p>It works fine when one of the table has smaller dataset, but when both tables have huge data, it throws the below error.</p><pre>Failed with exception Unable to move source /org/data/tmp/webapptempspace/UC3/log/.hive-staging_hive_2016-01-11_04-31-06_067_6297667876520454770-1/-ext-10000\nto destination </pre>","tags":["Hive","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-11 14:59:48.0","id":9407,"title":"HortonWorks HIVE ODBC Driver: Connecting to Hive Environment using MS Access","body":"<p>I am using an ODBC connection to connect to hive environment using the hortonworks hive odbc driver that was installed.  I have ran a test and able to connect to the environment without issues.  The problem I'm having is when I attempt to run a pass-through query joining two tables, I receive an error message </p><p><strong>\n \"finished with operation state:\nERROR_STATE\".</strong></p><p>I can run a pass-through query on a single table without issue its just when I perform a join on another table is when the issue appears.  Is it something to do with my configuration ODBC Administrator or is it something to do with Microsoft Access itself?  Thanks in advance.</p>","tags":["help","odbc","Hive","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-11 21:35:47.0","id":9472,"title":"Does Knox have protection against other web app vulnerabilities like injection, broken authentication, etc.?","body":"Security dept. is asking about the Knox REST API and the protections that are built into the service. ","tags":["security","Knox"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-10 22:10:58.0","id":9350,"title":"Could not resolve host: sandbox.hortonworks.com when trying to upload a file.","body":"<p>I try to upload a file to hdfs through webHDFS . Before file update i made a directory with webHDFS, \"testDIR\" in /tmp/testDIR and it works.</p><p>First command is :</p><pre> curl -i -X PUT \"http://192.168.1.112:50070/webhdfs/v1/tmp/testDIR/test.txt?op=CREATE\"</pre><p>For the second command i'll use url from \"Location\" as in the instructions.</p><p>Second commnd : </p><pre>curl -i -X PUT -T test.txt \"http://sandbox.hortonworks.com:50075/webhdfs/v1/tmp/testDIR/test.txt?op=CREATE&namenoderpcaddress=sandbox.hortonworks.com:8020&createflag=&createparent=true&overwrite=false\".</pre><p>This command response is : \"</p><pre>Could not resolve host: sandbox.hortonworks.com</pre><p>What can i do ?</p>","tags":["help","webhdfs","HDFS"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-12 18:20:55.0","id":9546,"title":"Beeline CLI fails compiling order by while Hue UI runs the query successfully","body":"<p>Hi,</p><p>If I run below query in Hue UI, it runs successful, but in beeline CLI it fails saying missing 'EOF'</p><pre>use DB_xxx;\nSET hive.execution.engine=tez;\nSET hive.vectorized.execution.enabled=true;\nSET hive.vectorized.execution.reduce.enabled= true;\nSET hive.cbo.enable=true;\nSET hive.stats.fetch.partition.stats=true;\nSET hive.auto.convert.join=true;\nSET hive.auto.convert.join.noconditionaltask.size=25000000;\nSET hive.compute.query.using.stats=true;\nSET hive.stats.fetch.column.stats =true;\ninsert into table db_xgl77sar.orc_claiminfo\nselect \nCONCAT(CLAIMNUMBER,'-',EXP_NUMBER) AS EXP_ID,\nClaimant_Age,\nCase \nwhen Claimant_Age &gt;=0 and Claimant_Age &lt; 3 then  'Infants'\nwhen Claimant_Age &gt;=3 and Claimant_Age &lt; 8 then   'Preschoolers'\nwhen Claimant_Age &gt;=8 and Claimant_Age &lt; 15 then   'General Schoolchildren'\nwhen Claimant_Age &gt;=15 and Claimant_Age &lt; 25 then  'Teens'\nwhen Claimant_Age &gt;=25 and Claimant_Age &lt; 40 then   'Young People'\nwhen Claimant_Age &gt;=40 and Claimant_Age &lt; 60 then   'Mature Age People'\nwhen Claimant_Age &gt;=60 and Claimant_Age &lt; 80 then   'Older People'\nwhen Claimant_Age &gt;=80 then  'Long-lived'\nelse  'N/A' END AS Claimant_Age_Group,\nDATEDIFF(to_date(EXP_ORIGINAL_CLOSE_DATE),to_date(CLAIM_REPORT_DATE)) AS Rpt2Clsd,\ncase \nwhen TOTAL_LOSS_ID is  null then '0'\nelse '1'\nend AS TOTAL_LOSS,\nClaimnumber, \nINCIDENT, \nEXP_NUMBER,\nClaim_Report_Date,   \nto_date(Date_of_Loss) As Date_of_Loss,   \nClaim_Close_Date,\nEXP_ORIGINAL_CLOSE_DATE,   \nEXP_CLOSEDATE,   \nEXP_REOPENDATE,  \nEXP_REOPENREASON, \nDescription,   \nInsured_Lability, \nACCIDENT_TYPE,   \nFAULT_RATING,   \nLOSS_CAUSE,   \nLOSS_TYPE,   \nLINE_OF_BUSINESS,   \nJurisdictionState, \nLOSS_STATE,\nPOLICY_STATE,\nSIU_STATUS,\nCLAIMANT_NAME,\nTO_DATE(CLAIMANT_DOB) as CLAIMANT_DOB,\nCLAIMANT_GENDER,\nCLAIMANT_MARITAL_STATUS,\nCLAIMANT_OCCUPATION,\nEXP_STATUS,   \nEXP_TYPE, \nEXP_Coverage,\nCOMPENSABILITYCOMMENTS,\nHitandRun,\nTOTAL_LOSS_ID,\nVEHICLE_STYLE,\nAmbulance_Used,\nDeath_Date,\nDisabledDueToAccident,\nGeneral_Injury_Type,\nImpairment,\nInjuredCheck_AAA,\nDetailed_Body_Part,\nDetailed_Injury_Type,\nPre_Exist_Cond,\ncase \nwhen LOSS_STATE=POLICY_STATE then '1'\nelse '0'\nend PSEQLS,\nDate_of_Loss AS DOL_DAY,\ncase \nwhen EXP_REOPENDATE is  null then '0'\nelse '1'\nend AS Reopened,\ncase \nwhen Death_Date is null then '0'\nelse '1' \nend AS DEATH,\nfrom_unixtime(unix_timestamp()) AS RUN_DATE\nfrom (SELECT C.Claimnumber,\n  C.IncidentReport                AS INCIDENT,\n  E.claimorder                    AS EXP_NUMBER,\n  to_date(C.createtime)           AS Claim_Report_Date,\n  to_date(C.lossdate)             AS Date_of_Loss,\n  to_date(C.closedate)            AS Claim_Close_Date,\n  to_date(E.INITIALCLOSEDATE_AAA) AS EXP_ORIGINAL_CLOSE_DATE,\n  to_date(E.CLOSEDATE)            AS EXP_CLOSEDATE,\n  to_date(E.REOPENDATE)           AS EXP_REOPENDATE,\n  E.ReopenedReason                AS EXP_REOPENREASON,\n  C.Description,\n  C.fault                                                                          AS Insured_Lability,\n  C.LOSSCAUSE                                                                      AS ACCIDENT_TYPE,\n  C.FAULTRATING                                                                    AS FAULT_RATING,\n  C.LOSSCAUSE                                                                      AS LOSS_CAUSE,\n  C.LOSSTYPE                                                                       AS LOSS_TYPE,\n  C.LOBCODE                                                                        AS LINE_OF_BUSINESS,\n  E.JURISDICTIONSTATE                                                              AS JurisdictionState,\n  ADR.STATE                                                                        AS LOSS_STATE,\n  P.POLICYSTATE_AAA                                                                AS POLICY_STATE,\n  C.SIUSTATUS                                                                      AS SIU_STATUS,\n  CONCAT(CT.FIRSTNAME, ' ', CT.LASTNAME )                                          AS CLAIMANT_NAME,\n  to_date(CT.DateOfBirth)                                                           AS CLAIMANT_DOB,\n  DATEDIFF (to_date(FROM_UNIXTIME( UNIX_TIMESTAMP())),to_date(CT.DateOfBirth))/365 AS Claimant_Age,\n  CT.Gender                                                                        AS CLAIMANT_GENDER,\n  CT.MaritalStatus                                                                 AS CLAIMANT_MARITAL_STATUS,\n  CT.OCCUPATION                                                                    AS CLAIMANT_OCCUPATION,\n  E.STATE                                                                          AS EXP_STATUS,\n  E.EXPOSURETYPE                                                                   AS EXP_TYPE,\n  E.COVERAGESUBTYPE                                                                AS EXP_Coverage,\n  BP.COMPENSABILITYCOMMENTS,\n  I.HitandRun,\n  I.ID                    AS TOTAL_LOSS_ID,\n  V.STYLE                 AS VEHICLE_STYLE,\n  I.AmbulanceUsed         AS Ambulance_Used,\n  I.DeathDate_AAA         AS Death_Date,\n  I.DisabledDueToAccident AS DisabledDueToAccident,\n  GeneralInjuryType       AS General_Injury_Type,\n  I.Impairment,\n  I.InjuredCheck_AAA,\n  BP.DetailedBodyPart  AS Detailed_Body_Part,\n  BP.InjuryType_AAA    AS Detailed_Injury_Type,\n  BP.Pre_ExistCond_AAA AS Pre_Exist_Cond \nFROM\n(SELECT id,Claimnumber,IncidentReport,createtime,lossdate,closedate,Description,fault,LOSSCAUSE,FAULTRATING,LOSSTYPE,LOBCODE,SIUSTATUS,LOSSLOCATIONID,POLICYID FROM CC_Claim WHERE RETIRED = 0) C\nINNER JOIN (SELECT claimorder,INITIALCLOSEDATE_AAA,CLOSEDATE,REOPENDATE,ReopenedReason,JURISDICTIONSTATE,STATE,EXPOSURETYPE,COVERAGESUBTYPE,CLAIMID,INCIDENTID,ClaimantDenormID FROM CC_Exposure WHERE RETIRED = 0) E\nON C.ID = E.CLAIMID\nLEFT JOIN CC_Incident I\nON E.INCIDENTID = I.ID\nLEFT JOIN cc_bodypart BP\nON I.ID = BP.INCIDENTID\nLEFT JOIN cc_contact CT\nON E.ClaimantDenormID = CT.ID\nLEFT JOIN CC_ADDRESS ADR\nON C.LOSSLOCATIONID = ADR.ID\nLEFT JOIN CC_VEHICLE V\nON I.VEHICLEID = V.ID\nLEFT JOIN (SELECT id,POLICYSTATE_AAA FROM CC_POLICY WHERE RETIRED = 0 ) P\nON C.POLICYID = P.ID\nWHERE C.claimnumber LIKE '1000-%'\nAND C.incidentreport    &lt;&gt; 1\nAND C.lobcode            = '10001'\nAND C.losscause NOT     IN ('10010', '10049')\nAND upper(C.Description) not like'%TEST%'\nORDER BY C.CLAIMNUMBER ASC\n) clminfo;</pre><p>What could be the possible issue??</p><p>Thanks</p><p>Mamta</p>","tags":["cli","beeline","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-13 16:27:10.0","id":9762,"title":"How to get rid of stale alerts in Ambari","body":"<p>I have restarted Ambari Server and all agents along with complete HDP stack multiple times in past 5 days for different activities but these alerts don't go away.</p><p><img src=\"/storage/attachments/1308-screen-shot-2016-01-13-at-102524-am.png\"></p>","tags":["ambari-server","Ambari","hdp-2.3.2","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-13 14:58:01.0","id":9695,"title":"HDP on Isilon - CPU Usage","body":"<p>We need to set up a HDP cluster based on Isilon storage and customer is asking how much impact would it have on the CPU usage on Isilon nodes ? Currently the Isilon cluster is shared with other work loads as well. What are our experiences around this ? Would the namenode operations in Isilon cause lot of CPU spikes which results in degradation in performance with other workloads on Isilon ?</p>","tags":["Hive","onefs","isilon"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-13 20:49:16.0","id":9797,"title":"Error with using 'Store' (Pig)","body":"<p>Hi,</p><p>Hoping someone can advise. I am running a few lines of Pig code where I get an error.</p><p>When I run the four lines of code below in Pig it works fine:</p><p>allairports = LOAD 'default.airport' USING org.apache.hive.hcatalog.pig.HCatLoader(); </p><p>gbairports = filter allairports by iso_country == 'GB'; </p><p>gbcols = foreach gbairports generate $13, $3, $6; </p><p>dump gbcols;\n</p><p>However, when I replace the last line (as below) with a store statement, I get an error.</p><p>allairports = LOAD 'default.airport' USING org.apache.hive.hcatalog.pig.HCatLoader();</p><p>gbairports = filter allairports by iso_country == 'GB';</p><p>gbcols = foreach gbairports generate $13, $3, $6;</p><p>store gbcols into '/user/hue/tutorials' using pigstorage(',');</p><p>The error is below. Does anyone know why I am getting this?</p><pre>WARNING: Use \"yarn jar\" to launch YARN applications.\n16/01/13 20:33:06 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\n16/01/13 20:33:06 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE\n16/01/13 20:33:06 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType\n2016-01-13 20:33:06,502 [main] INFO  org.apache.pig.Main - Apache Pig version 0.15.0.2.3.2.0-2950 (rexported) compiled Sep 30 2015, 19:39:20\n2016-01-13 20:33:06,502 [main] INFO  org.apache.pig.Main - Logging error messages to: /hadoop/yarn/local/usercache/hue/appcache/application_1452469761257_0020/container_e03_1452469761257_0020_01_000002/pig_1452717186500.log\n2016-01-13 20:33:07,183 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /home/yarn/.pigbootup not found\n2016-01-13 20:33:07,294 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://sandbox.hortonworks.com:8020\n2016-01-13 20:33:08,734 [main] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.metastore.local does not exist\n2016-01-13 20:33:08,770 [main] INFO  hive.metastore - Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083\n2016-01-13 20:33:08,823 [main] INFO  hive.metastore - Connected to metastore.\n2016-01-13 20:33:09,127 [main] ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. Could not resolve pigstorage using imports: [, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.]\nFailed to parse: Pig script failed to parse: \n&lt;file script.pig, line 9, column 46&gt; pig script failed to validate: org.apache.pig.backend.executionengine.ExecException: ERROR 1070: Could not resolve pigstorage using imports: [, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.]\n\tat org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:199)\n\tat org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1735)\n\tat org.apache.pig.PigServer$Graph.access$000(PigServer.java:1443)\n\tat org.apache.pig.PigServer.parseAndBuild(PigServer.java:387)\n\tat org.apache.pig.PigServer.executeBatch(PigServer.java:412)\n\tat org.apache.pig.PigServer.executeBatch(PigServer.java:398)\n\tat org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:171)\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:234)\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\n\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\n\tat org.apache.pig.Main.run(Main.java:502)\n\tat org.apache.pig.Main.main(Main.java:177)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: \n&lt;file script.pig, line 9, column 46&gt; pig script failed to validate: org.apache.pig.backend.executionengine.ExecException: ERROR 1070: Could not resolve pigstorage using imports: [, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.]\n\tat org.apache.pig.parser.LogicalPlanBuilder.validateFuncSpec(LogicalPlanBuilder.java:1339)\n\tat org.apache.pig.parser.LogicalPlanBuilder.buildFuncSpec(LogicalPlanBuilder.java:1324)\n\tat org.apache.pig.parser.LogicalPlanGenerator.func_clause(LogicalPlanGenerator.java:5184)\n\tat org.apache.pig.parser.LogicalPlanGenerator.store_clause(LogicalPlanGenerator.java:7782)\n\tat org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1669)\n\tat org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)\n\tat org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)\n\tat org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)\n\tat org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)\n\t... 17 more\nCaused by: org.apache.pig.backend.executionengine.ExecException: ERROR 1070: Could not resolve pigstorage using imports: [, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.]\n\tat org.apache.pig.impl.PigContext.resolveClassName(PigContext.java:677)\n\tat org.apache.pig.parser.LogicalPlanBuilder.validateFuncSpec(LogicalPlanBuilder.java:1336)\n\t... 25 more\n2016-01-13 20:33:09,131 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1070: Could not resolve pigstorage using imports: [, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.]\nDetails at logfile: /hadoop/yarn/local/usercache/hue/appcache/application_1452469761257_0020/container_e03_1452469761257_0020_01_000002/pig_1452717186500.log </pre><p>2016-01-13 20:33:09,153 [main] INFO  org.apache.pig.Main - Pig script completed in 2 seconds and 779 milliseconds (2779 ms)</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-13 22:05:07.0","id":9776,"title":"Can Ambari manage a cluster of only zookeeper and storm?","body":"<p>In addition to whatever infrastructure Ambari needs, e.g. mysql.</p>","tags":["Storm","Ambari","zookeeper"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-01-14 05:31:12.0","id":9852,"title":"DataXceiver error processing READ_BLOCK operation","body":"<p>Frequently getting these below error messages on datanode</p><p>ERROR datanode.DataNode (DataXceiver.java:run(250)) - X.X.X.X6:50010:DataXceiver error processing READ_BLOCK operation  src: /x.x.x.7:49636 dst: /x.x.x.6:50010\njava.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/x.x.x.6:50010 remote=/x.x.x.7:49636]\n  at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)\n  at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)\n  at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)\n  at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:547)\n  at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:716)\n  at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)\n  at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:110)\n  at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)\n  at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)\n  at java.lang.Thread.run(Thread.java:745)</p>","tags":["datanode","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-07 05:04:20.0","id":38047,"title":"Modified Ambari Disk alert Threshold is not getting into Effect","body":"<p>HDP@Muthukumar</p><p>How to make the disk alert threshold change to take effect? I have changed the disk threshold (description) via Ambari console and also through curl command on Ambari server i have changed the alert threshold (from 50 to 90% for warning and 80 to 95% for critical) and verified through curl GET option. Also changed the value in the file /var/lib/ambari-server/resources/host_scripts/alert_disk_space.py.. After that i have restarted the Ambari-server service and also tried restarting the Ambari agents and also tried disabling and enabling the host disk alert option. Now it is at 78% and the warning still exists and the new values are not getting into effect? Version is HDP 2.4. Any advice or steps which im missing? Appreciate your reply. </p><p>Also some stale alerts (in RED) remain and not getting removed. This is another issue.  </p><p>Thank you.</p>","tags":["Ambari"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-06-07 17:18:17.0","id":38262,"title":"Support for Cassandra ACL","body":"<p>Does ranger support Cassandra ACLs like the way it supports HIVE</p>","tags":["cassandra"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-05 09:17:46.0","id":14652,"title":"How to add Scheduler widgets to RM in Ambari UI ?","body":"<p>Hello friends,</p><p>How to add scheduler queue's name and its utilization widgets to RM in Ambari UI ?</p><p>Thanks in advance. </p>","tags":["yarn-scheduler"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-21 19:35:18.0","id":23958,"title":"Critical alerts server:port","body":"<p>We are on HDP 2.3.2 and Ambari 2.1.2. Services on the secondary name node not failed to start getting an error message connection refused on server:port</p>","tags":["ambari-2.1.2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-05 12:21:57.0","id":8744,"title":"How can i create rpm package with HDP source code project in Github?","body":"","tags":["hdp-2.2.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-11 06:13:59.0","id":9381,"title":"i am trying to load data from sqlserver to hive and getting an error","body":"<p>while i use the command </p><p>sqoop list-databases --connect jdbc:sqlserver://192.168.56.1:1433 --username hadoop --password hadoop1 --driver com.microsoft.sqlserver.SQLServerDriver </p><p>i get error</p><p>ERROR manager.sqlmanager :generic Sqlmanager.listdatabases() not implemented ,</p><p>ERROR tool.ListDatabasesTool: manager.ListDatabases() returned null</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-21 13:30:28.0","id":11061,"title":"Storm bolt is not connected with Kafka   look like some error","body":"<p>i am trying to connect my storm spout with Kafka  and having error.    Here is the code   and next section is the error i getting when i run this topology.    i am running on HDP 2.2.4.2-2     and compiling my code on windows 7 with eclipse </p><pre>package com.storm;\n\n\nimport java.util.UUID;\n\n\nimport storm.kafka.BrokerHosts;\nimport storm.kafka.KafkaSpout;\nimport storm.kafka.SpoutConfig;\nimport storm.kafka.StringScheme;\nimport storm.kafka.ZkHosts;\nimport backtype.storm.Config;\nimport backtype.storm.LocalCluster;\nimport backtype.storm.generated.AlreadyAliveException;\nimport backtype.storm.generated.InvalidTopologyException;\nimport backtype.storm.spout.SchemeAsMultiScheme;\nimport backtype.storm.topology.TopologyBuilder;\n\n\n\n\npublic class storm123 \n{\n\n\n\tpublic static void main(String[] args) throws AlreadyAliveException, InvalidTopologyException \n\t{\n\t\t// create an instance of TopologyBuilder class\n\t\tTopologyBuilder builder = new TopologyBuilder();\n\t\t\n\t\t\n\t\tBrokerHosts hosts = new ZkHosts(\"sandbox.hortonworks.com:2181\");\n\t\tSpoutConfig spoutConfig = new SpoutConfig(hosts, \"testing2\", \"/testing2\", UUID.randomUUID().toString());\n\t\tspoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());\n\t\tKafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);\t\t\n\t\tbuilder.setSpout(\"myKafkaSpout\", kafkaSpout, 1);\n\t\t\n\t\t// set the spout class\n\t\t//builder.setSpout(\"LearningStormSpout\", new stormspout(), 2);\n\t\t\n\t\t// set the bolt class\n\t\tbuilder.setBolt(\"LearningStormBolt\", new stormbolt(), 4).shuffleGrouping (\"myKafkaSpout\");\n\t\tConfig conf = new Config();\n\t\tconf.setDebug(true);\n\t\t\n\t\t\n\t\t// create an instance of LocalCluster class for\n\t\t// executing topology in local mode.\n\t\tLocalCluster cluster = new LocalCluster();\n\t\t\n\t\t// LearningStormTopolgy is the name of submitted topology\n\t\tcluster.submitTopology(\"LearningStormToplogy\", conf, builder.createTopology());\n\t\ttry {\n\t\t\tThread.sleep(70000);\n\t\t} catch (Exception exception) {\n\t\t\tSystem.out.println(\"Thread interrupted exception : \" + exception);\n\t\t}\n\t\t\n\t\t// kill the LearningStormTopology\n\t\tcluster.killTopology(\"LearningStormToplogy\");\n\t\t\n\t\t// shutdown the storm test cluster\n\t\tcluster.shutdown();\n\t}\n\n\n}\n</pre><p>and here is the error when i run this code in end </p><pre>20848 [Thread-16-__metricsorg.apache.hadoop.metrics2.sink.storm.StormTimelineMetricsSink] INFO  backtype.storm.daemon.executor - Prepared bolt __metricsorg.apache.hadoop.metrics2.sink.storm.StormTimelineMetricsSink:(6)\n20854 [Thread-18-myKafkaSpout] ERROR backtype.storm.util - Async loop died!\njava.lang.NoSuchMethodError: org.apache.zookeeper.server.quorum.flexible.QuorumMaj.&lt;init&gt;(Ljava/util/Map;)V\n        at org.apache.curator.framework.imps.EnsembleTracker.&lt;init&gt;(EnsembleTracker.java:54) ~[curator-framework-3.0.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.&lt;init&gt;(CuratorFrameworkImpl.java:156) ~[curator-framework-3.0.0.jar:na]\n        at org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:136) ~[curator-framework-3.0.0.jar:na]\n        at org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:107) ~[curator-framework-3.0.0.jar:na]\n        at storm.kafka.ZkState.newCurator(ZkState.java:45) ~[storm-kafka-0.9.2-incubating.jar:0.9.2-incubating]\n        at storm.kafka.ZkState.&lt;init&gt;(ZkState.java:61) ~[storm-kafka-0.9.2-incubating.jar:0.9.2-incubating]\n        at storm.kafka.KafkaSpout.open(KafkaSpout.java:85) ~[storm-kafka-0.9.2-incubating.jar:0.9.2-incubating]\n        at backtype.storm.daemon.executor$fn__4949$fn__4964.invoke(executor.clj:542) ~[storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at backtype.storm.util$async_loop$fn__452.invoke(util.clj:463) ~[storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79]\n20855 [Thread-18-myKafkaSpout] ERROR backtype.storm.daemon.executor -\njava.lang.NoSuchMethodError: org.apache.zookeeper.server.quorum.flexible.QuorumMaj.&lt;init&gt;(Ljava/util/Map;)V\n        at org.apache.curator.framework.imps.EnsembleTracker.&lt;init&gt;(EnsembleTracker.java:54) ~[curator-framework-3.0.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.&lt;init&gt;(CuratorFrameworkImpl.java:156) ~[curator-framework-3.0.0.jar:na]\n        at org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:136) ~[curator-framework-3.0.0.jar:na]\n        at org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:107) ~[curator-framework-3.0.0.jar:na]\n        at storm.kafka.ZkState.newCurator(ZkState.java:45) ~[storm-kafka-0.9.2-incubating.jar:0.9.2-incubating]\n        at storm.kafka.ZkState.&lt;init&gt;(ZkState.java:61) ~[storm-kafka-0.9.2-incubating.jar:0.9.2-incubating]\n        at storm.kafka.KafkaSpout.open(KafkaSpout.java:85) ~[storm-kafka-0.9.2-incubating.jar:0.9.2-incubating]\n        at backtype.storm.daemon.executor$fn__4949$fn__4964.invoke(executor.clj:542) ~[storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at backtype.storm.util$async_loop$fn__452.invoke(util.clj:463) ~[storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79]\n20900 [Thread-18-myKafkaSpout] ERROR backtype.storm.util - Halting process: (\"Worker died\")\njava.lang.RuntimeException: (\"Worker died\")\n        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:322) [storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]\n        at backtype.storm.daemon.worker$fn__5425$fn__5426.invoke(worker.clj:491) [storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at backtype.storm.daemon.executor$mk_executor_data$fn__4850$fn__4851.invoke(executor.clj:245) [storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at backtype.storm.util$async_loop$fn__452.invoke(util.clj:475) [storm-core-0.9.3.2.2.4.2-2.jar:0.9.3.2.2.4.2-2]\n        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]\n        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79]\n</pre>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-22 20:14:50.0","id":11315,"title":"Accessing Hive cli is hung forever with java.sql.SQLException","body":"<p>Hi,</p><p>I could not access Hive CLI with below error messages</p><p>16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.identifierFactory value null from  jpox.properties with datanucleus1\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.ConnectionURL value null from  jpox.properties with jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.DetachAllOnCommit value null from  jpox.properties with true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding hive.metastore.integral.jdo.pushdown value null from  jpox.properties with false\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.storeManagerType value null from  jpox.properties with rdbms\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding datanucleus.transactionIsolation value null from  jpox.properties with read-committed\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.PersistenceManagerFactoryClass value null from  jpox.properties with org.datanucleus.api.jdo.JDOPersistenceManagerFactory\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: Overriding javax.jdo.option.Multithreaded value null from  jpox.properties with true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.rdbms.useLegacyNativeValueStrategy = true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: hive.metastore.integral.jdo.pushdown = false\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.autoStartMechanismMode = checked\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.Multithreaded = true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.identifierFactory = datanucleus1\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.transactionIsolation = read-committed\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.validateTables = false\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.ConnectionURL = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.DetachAllOnCommit = true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.NonTransactionalRead = true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.fixedDatastore = false\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.validateConstraints = false\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.ConnectionDriverName = org.apache.derby.jdbc.EmbeddedDriver\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: javax.jdo.option.ConnectionUserName = APP\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.validateColumns = false\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.cache.level2 = false\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.plugin.pluginRegistryBundleCheck = LOG\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.cache.level2.type = none\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: javax.jdo.PersistenceManagerFactoryClass = org.datanucleus.api.jdo.JDOPersistenceManagerFactory\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.autoCreateSchema = true\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.storeManagerType = rdbms\n16/01/22 12:59:03 [main]: DEBUG metastore.ObjectStore: datanucleus.connectionPoolingType = BONECP\n16/01/22 12:59:03 [main]: INFO metastore.ObjectStore: ObjectStore, initialize called\n16/01/22 12:59:04 [main]: DEBUG bonecp.BoneCPDataSource: JDBC URL = jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT\n16/01/22 12:59:04 [BoneCP-pool-watch-thread]: ERROR bonecp.PoolWatchThread: Error in trying to obtain a connection. Retrying in 7000ms\njava.sql.SQLException: A read-only user or a user in a read-only database is not permitted to disable read-only mode on a connection.\n        at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n        at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n        at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n        at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n        at org.apache.derby.impl.jdbc.EmbedConnection.setReadOnly(Unknown Source)\n        at com.jolbox.bonecp.ConnectionHandle.setReadOnly(ConnectionHandle.java:1324)\n        at com.jolbox.bonecp.ConnectionHandle.&lt;init&gt;(ConnectionHandle.java:262)\n        at com.jolbox.bonecp.PoolWatchThread.fillConnections(PoolWatchThread.java:115)\n        at com.jolbox.bonecp.PoolWatchThread.run(PoolWatchThread.java:82)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.sql.SQLException: A read-only user or a user in a read-only database is not permitted to disable read-only mode on a connection.\n        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n        at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n        ... 13 more\nCaused by: ERROR 25505: A read-only user or a user in a read-only database is not permitted to disable read-only mode on a connection.</p><p>We are using Mysql for metastore but I see JDBC URl with Derby database. I am not sure if it can be ignored but highlighting if it gives any hint about the issue.</p><p>Please help</p>","tags":["Hive"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-22 01:46:36.0","id":11208,"title":"Why the Ranger Admin console is **not** coming in HDP 2.3 Sandbox ?","body":"<p>I am trying to open Ranger admin console in the Sandbox,  for some reason I donot see the page coming up.   Any ideas ?  I also do not see the Ranger as a service in the Ambari homepage.  </p><p>  However when I check the service running on the sandbox backend -- it is running .</p><p><em>[root@sandbox ~]# sudo service ranger-admin status </em></p><p><em>Ranger Admin Service is  running [pid={1679}]</em></p><p>Thanks</p><p>Siva</p>","tags":["ranger-admin"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-05 02:02:37.0","id":21307,"title":"Any idea why we are getting RA040 I/O error while trying to enable/Validating hive views?","body":"<p>I enabled hive views with HDP2.3.4 and Ambari2.2.0 but getting below error. </p><p>Can you provide me some hints why we are getting this error?</p><p>org.apache.ambari.view.utils.ambari.AmbariApiException: RA040 I/O error while requesting Ambari </p><p>org.apache.ambari.view.utils.ambari.AmbariApiException: RA040 I/O error while requesting Ambari\nat org.apache.ambari.view.utils.ambari.AmbariApi.requestClusterAPI(AmbariApi.java:176)\nat org.apache.ambari.view.utils.ambari.AmbariApi.requestClusterAPI(AmbariApi.java:142)\nat org.apache.ambari.view.utils.ambari.AmbariApi.getHostsWithComponent(AmbariApi.java:99)\nat org.apache.ambari.view.hive.client.ConnectionFactory.getHiveHost(ConnectionFactory.java:79)\nat org.apache.ambari.view.hive.client.ConnectionFactory.create(ConnectionFactory.java:68)\nat org.apache.ambari.view.hive.client.UserLocalConnection.initialValue(UserLocalConnection.java:42)\nat org.apache.ambari.view.hive.client.UserLocalConnection.initialValue(UserLocalConnection.java:26)\nat org.apache.ambari.view.utils.UserLocal.get(UserLocal.java:66)\nat org.apache.ambari.view.hive.resources.browser.HiveBrowserService.databases(HiveBrowserService.java:87)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\nat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)</p>","tags":["ambari-views"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-10 07:30:10.0","id":22289,"title":"ERROR UserGroupSync [UnixUserSyncThread]","body":"<p>Ranger usersync is throwing following error with AD authentication. </p><p>10 Mar 2016 01:59:36 ERROR UserGroupSync [UnixUserSyncThread] - Failed to initialize UserGroup source/sink. Will retry after 3600000 milliseconds. Error details: </p><p>com.sun.jersey.api.client.ClientHandlerException: java.net.ConnectException: Connection refused</p><p>at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:151)</p><p>at com.sun.jersey.api.client.filter.HTTPBasicAuthFilter.handle(HTTPBasicAuthFilter.java:104)</p><p>at com.sun.jersey.api.client.Client.handle(Client.java:648)</p><p>at com.sun.jersey.api.client.WebResource.handle(WebResource.java:680)</p><p>at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)</p><p>at com.sun.jersey.api.client.WebResource$Builder.get(WebResource.java:507)</p><p>at org.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder.buildUserList(PolicyMgrUserGroupBuilder.java:382)</p><p>at org.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder.buildUserGroupInfo(PolicyMgrUserGroupBuilder.java:157)</p><p>at org.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder.init(PolicyMgrUserGroupBuilder.java:152)</p><p>at org.apache.ranger.usergroupsync.UserGroupSync.run(UserGroupSync.java:51)</p><p>at java.lang.Thread.run(Thread.java:745)</p><p>Caused by: java.net.ConnectException: Connection refused</p><p>at java.net.PlainSocketImpl.socketConnect(Native Method)</p>","tags":["error"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-11 02:00:57.0","id":22423,"title":"How to reinstall an ambari-agent in HDP cluster？","body":"","tags":["agent"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-01 17:14:02.0","id":25561,"title":"Unable to see Files view in Ambari","body":"<p>I have installed latest versions of Ambari and HDP on a two node Dell server . Server has CentOS installed. </p><p>I am unable to see all the views </p><p>Please let me know what steps are required to see all the views .. </p><p>More details below</p><ol><li>Views screenshot from Ambari: <a href=\"https://community.hortonworks.com/storage/attachments/3136-ambari-views.jpg\">ambari-views.jpg</a>)</li><li>I restarted the server and checked the logs . Relevant info (according to me :) ) from logs :</li></ol><p>31 Mar 2016 17:48:13,613  INFO [main] ViewRegistry:1598 - Auto creating instance of view TEZ for cluster MuSigma_Cluster.\n31 Mar 2016 17:48:13,613  INFO [main] ViewRegistry:1566 - View deployed: TEZ{0.7.0.2.3.4.0-70}.\n31 Mar 2016 17:48:13,619  INFO [main] ViewRegistry:1538 - Reading view archive /var/lib/ambari-server/resources/views/capacity-scheduler-2.2.1.1.70.jar.\n31 Mar 2016 17:48:13,656  INFO [main] ViewRegistry:1598 - Auto creating instance of view CAPACITY-SCHEDULER for cluster MuSigma_Cluster.\n31 Mar 2016 17:48:13,656  INFO [main] ViewRegistry:1566 - View deployed: CAPACITY-SCHEDULER{1.0.0}.\n31 Mar 2016 17:48:13,661  INFO [main] ViewRegistry:1538 - Reading view archive /var/lib/ambari-server/resources/views/pig-2.2.1.1.70.jar.\n31 Mar 2016 17:48:13,702  INFO [main] ViewRegistry:1566 - View deployed: PIG{1.0.0}.\n31 Mar 2016 17:48:13,707  INFO [main] ViewRegistry:1538 - Reading view archive /var/lib/ambari-server/resources/views/slider-2.2.1.1.70.jar.\n31 Mar 2016 17:48:13,743  INFO [main] ViewRegistry:1566 - View deployed: SLIDER{2.0.0}.\n31 Mar 2016 17:48:13,747  INFO [main] ViewRegistry:1538 - Reading view archive /var/lib/ambari-server/resources/views/ambari-admin-2.2.1.1.70.jar.\n31 Mar 2016 17:48:13,756  INFO [main] ViewRegistry:1566 - View deployed: ADMIN_VIEW{2.2.1.1}.\n31 Mar 2016 17:48:13,761  INFO [main] ViewRegistry:1538 - Reading view archive /var/lib/ambari-server/resources/views/smartsense-ambari-view-1.2.1.0-70.jar.\n31 Mar 2016 17:48:13,788  INFO [main] ViewRegistry:1566 - View deployed: HORTONWORKS_SMARTSENSE{1.2.1.0-70}.\n31 Mar 2016 17:48:13,794  INFO [main] ViewRegistry:1538 - Reading view archive /var/lib/ambari-server/resources/views/files-2.2.1.1.70.jar.\n31 Mar 2016 17:48:13,834  INFO [main] ViewRegistry:1566 - View deployed: FILES{1.0.0}.\n31 Mar 2016 17:48:13,839  INFO [main] ViewRegistry:1538 - Reading view archive /var/lib/ambari-server/resources/views/hive-2.2.1.1.70.jar.\n31 Mar 2016 17:48:13,968  INFO [main] ViewRegistry:1598 - Auto creating instance of view HIVE for cluster MuSigma_Cluster.\n31 Mar 2016 17:48:13,969  INFO [main] ViewRegistry:1566 - View deployed: HIVE{1.0.0}.</p>","tags":["ambari-views"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-01-14 08:37:53.0","id":9873,"title":"Ambari custom install failure","body":"<p><a href=\"/storage/attachments/1348-failed-cluster-install.pdf\">failed-cluster-install.pdf</a>Hi all,</p><p>I have test driven the sandbox for a while and I decided to  take my knowledge to another level. I got 4 refurbished PowerEdge servers 2 Dell 2850 and 2 Dell 2950. The Ambari server preparation and host discovery was successful  see attached pdf. In the assign master I realised the Ambari server was overloaded so I reassigned some components to other servers. I was lost when it came to assign slave and client ONLY one of the servers have been check so I decided to go with the default. The Install start and smoke test failed.(Attached pdf)</p><p>I don't intend to create multiple users across the cluster ,how do I achieve this which file should I edit prior to the launch ? Below extract of the user creation error</p><p>2016-01-14 00:25:23,320 - Group['hadoop'] {'ignore_failures': False} </p><p>2016-01-14 00:25:23,320 - Group['users'] {'ignore_failures': False} </p><p>2016-01-14 00:25:23,320 - Group['knox'] {'ignore_failures': False} </p><p>2016-01-14 00:25:23,320 - User['hive'] {'gid': 'hadoop', 'ignore_failures': False, 'groups': ['hadoop']} </p><p>2016-01-14 00:25:23,321 - User['storm'] {'gid': 'hadoop', 'ignore_failures': False, 'groups': ['hadoop']} </p><p>2016-01-14 00:25:23,322 - User['zookeeper'] {'gid': 'hadoop', 'ignore_failures': False, 'groups': ['hadoop']}</p><p>Any advice is welcome </p>","tags":["installation"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-16 07:53:36.0","id":10143,"title":"Hadoop Services are not starting up after successfull installation of windows server 2012","body":"<p>Dear all,</p><p>i have installed Hortonworks HDP 2.3 on windows server 2012 R2 datacenter edition. i have three nodes NameNode1,DataNode1 and DataNode2</p><p>all the nodes are pinging each other and i have put the entries in etc\\hosts file as well. i install HDP on all three servers and installation was successfull.</p><p>now i want to startup the services on NameNode1, other services are starting except the following</p><p>Apache Hadoop namenode</p><p>Apache Hadoop derbyserver</p><p>could somebody guide me what is the reason? where are the logs to check? how to fix?</p><p>Regards.</p><p>thank you</p>","tags":["windows"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-21 11:27:05.0","id":11037,"title":"job history not available","body":"<p>When i submit jobs(from any one of the data nodes) within the cluster, i am able to see the job history. But when i submit a job from a remote client (it is not an edge node), the job gets completed. But the job history for the correspondign job is not available in the job history url. Can anyone help. IS there any particular setting that needs to be done on the client machine.</p>","tags":["spark-history-server"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-12 00:07:00.0","id":3296,"title":"anyone familiar with PepperData (http://pepperdata.com/)?","body":"","tags":["cluster"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-11 17:08:29.0","id":9414,"title":"Fair scheduler vs Capacity scheduler, need detailed explanation and which one is the best option to choose?","body":"","tags":["yarn-scheduler"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-18 01:26:17.0","id":10268,"title":"Facing alerts isuue with my sandbox Config","body":"<p>There are some alerts which I face whenever I start Ambari-Sandbox, I am using HDP 2.3.2   I dont understand why ? Can Anyone help me with this.. I am new to sandbox .</p><p>capture3.jpg   <a href=\"/storage/attachments/1425-capture4.jpg\">capture4.jpg</a></p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-02 19:54:31.0","id":13514,"title":"While importing the virtual appliance using Oracle VM VirtualBox, I receive the following error: Any input would be much appreciated.","body":"<p>Failed to import appliance <strong>C:\\Users\\along\\Documents\\Hadoop\\Sandbox\\HDP_2.3.2_virtualbox.ova</strong>.</p><p>Could not create the imported medium <strong>'C:\\Users\\along\\VirtualBox VMs\\Hortonworks Sandbox with HDP 2.3.2\\Hortonworks Sandbox with HDP 2.3.2-disk1.vmdk'</strong>.</p><p>VMDK: Compressed image is corrupted <strong>'C:\\Users\\along\\Hortonworks Sandbox with HDP 2.3.2-disk1.vmdk'</strong> (VERR_ZIP_CORRUPTED).</p><table><tbody><tr><td>Result Code: </td><td>VBOX_E_FILE_ERROR (0x80BB0004)</td></tr><tr><td>Component: </td><td>ApplianceWrap</td></tr><tr><td>Interface: </td><td>IAppliance {8398f026-4add-4474-5bc3-2f9f2140b23e}</td></tr></tbody></table>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-03 11:50:39.0","id":13833,"title":"We are facing issues in HDP 2.3 stack installation using Ambari in centOS 6. Under the \"Select Stack\" option, getting \"Could Not Find Base Url\" error under Advanced Repository options.","body":"<p>We are facing issues in HDP 2.3 stack installation using Ambari in centOS 6. Under the \"Select Stack\" option, getting \"Could Not Find Base Url\" error under Advanced Repository options.</p><p>HDP Stack : 2.3</p><p>CentOS 6</p><p>Machine has got internet connectivity as we could use wget to download the Ambari.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-03 19:41:05.0","id":14153,"title":"Widely used flavor & version of Linux","body":"<p>Hi, </p><p>Do we know what is the most popular flavor and version of Linux for HDP clusters (2.3)  with latest patch ?  I guess it would be RHEL 7, but any official information would be helpful. </p><p>Thanks </p><p>Sundar R</p>","tags":["linux"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-04 10:16:36.0","id":14346,"title":"Sqoop 1.4.6 shipped with HW v/s Apache","body":"<p>Hi,</p><p>I’ve one quick question on Sqoop (version 1.4.6,\nto be specific) that’s being shipped with HortonWorks. Can someone please confirm\nwhether this Sqoop jar file is same as what’s available at Apache’s location\nfor 1.4.6 (<a href=\"http://www.apache.org/dyn/closer.lua/sqoop/1.4.6\">http://www.apache.org/dyn/closer.lua/sqoop/1.4.6</a>)?\nOr does HortonWorks modifies/customizes Apache’s Sqoop before it ships it along\nwith the distribution. I do understand the Sqoop connectors will be more\nspecific with HortonWorks, I’m interested to know about Sqoop tool jar itself.</p><p>Thanks!</p><p>Rahul.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-04 20:24:23.0","id":14516,"title":"At final steps of the installation, it failed, error:    root is not in the sudoers file.  This incident will be reported.","body":"<p>however, I have this entry in my sudoers file</p><p>root    ALL=(ALL)   ALL</p><p>what I can do here to fix that error?</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-05 01:48:29.0","id":14616,"title":"run install by root, but always get error : \"root is not in the sudoers file.  This incident will be reported.\" why installer try to 'sudo'?","body":"<p>I checked all files requiered FQDN in /etc/sysconfig/network, /etc/hosts.., and add root ALL=(ALL) NOPASSWD: ALL at end of sudoers file. why it always complain about root is not in sudoers file? and error only occurs when you do 'sudo' as root user.</p><p>is it possible a bug in installer?  </p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-05 16:52:28.0","id":14788,"title":"HDP Developer Practice Exam Errors","body":"<p><a rel=\"user\" href=\"/users/164/rich.html\" nodeid=\"164\">@rich</a> Rich and team,</p><p>Is there a better way to contact you in regards to discovered errors in the Practice Exam?  Not sure this is the correct forum for raising such issues as it seems more intended for Q&A.</p><p>However, the issue noted <a href=\"https://community.hortonworks.com/questions/14733/hdp-developer-practice-exam-task-01.html#answer-14751\">here</a> also affects the answer for Task 03 item #1.  The correct answer should be 30,000.  However, the answer shown in the solutions folder is 30,267 which is including records from the 2 weather related files which shouldn't be used until Task 07.</p>","tags":["development","hdp-2.3.4","practice"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-08 06:30:25.0","id":15131,"title":"Falcon jobs are failing with EL_ERROR: variable [feedInstancePaths] cannot be resolved, though we are not defining this \"feedInstancePaths\" variable anywhere in our code.","body":"<p>We are running hive jobs with oozie and falcon, it was running fine but all of sudden it started failing with below error. </p><p>EL_ERROR: variable [feedInstancePaths] cannot be resolved,\n\n\n\nAlso we are not declaring or assigning this variable anywhere in our code. \n\n\n\nsitecatalyst-kpis-generation-daily-process] \nJOB[0011048-151208005457707-oozie-oozi-W] \nACTION[0011048-151208005457707-oozie-oozi-W@succeeded-post-processing] \nELException in ActionStartXCommand \n\njavax.servlet.jsp.el.ELException: variable [feedInstancePaths] cannot be resolved\n\n\tat org.apache.oozie.util.ELEvaluator$Context.resolveVariable(ELEvaluator.java:106)</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-02-05 23:43:03.0","id":14892,"title":"AVG function in pig","body":"<p>there are two column :qid( unique question id), at (epoch time ) </p><p>for example : looks like this : 563355, 1235000501</p><p>how to find average time to answer questions?</p><p>I converted epoch time to time in excel and when I run in pig ,shows error 1200: null.</p><p>another question, </p><p>number of questions which got answered within 1 hour?</p>","tags":["Pig"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-08 10:48:44.0","id":15163,"title":"Fail to create Falcon Cluster: Invalid Worklfow Server or  Port","body":"<p>Hi,</p><p>I have downloaded HDP Sandbox as well as provisioned one VM in the Azure. When following tutorial and creating the Cluster through the Falcon Web-UI I am getting the error:  </p><p>org.apache.falcon.FalconWebException::org.apache.falcon.FalconException</p><p>Invalid Workflow Server or Port</p><p>http://sandbox.hortonworks.com:11000/oozie</p><p>I have tried with using localhost as well as IP of the server - nothing helps. </p><p>Sounds like authentication problem. And here is something that doesn't comply with the tutorial - the WebUI is not authenticating with the password.</p><p>The suspect is somehow also supported by the following entry in the oozie log:</p><pre>2016-02-08 10:28:41,762 ERROR V2AdminServlet:517 - SERVER[sandbox.hortonworks.com] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] URL[GET &lt;a href=\"http://sandbox.hortonworks.com:11000/oozie/v2/admin/status?doAs=ambari\"&gt;http://sandbox.hortonworks.com:11000/oozie/v2/admin/status?doAs=ambari&lt;/a&gt;\n-qa&user.name=root] error, User [root] not defined as proxyuser                                                                                                                                                    \njava.security.AccessControlException: User [root] not defined as proxyuser                                                                                                                                         \n        at org.apache.oozie.service.ProxyUserService.validate(ProxyUserService.java:149)                                                                                                                           \n        at org.apache.oozie.servlet.JsonRestServlet.getUser(JsonRestServlet.java:567)                                                                                                                              \n        at org.apache.oozie.servlet.JsonRestServlet.service(JsonRestServlet.java:296)                                                                                                                              \n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                            \n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)                                                                                                       \n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)                                                                                                               \n        at org.apache.oozie.servlet.AuthFilter$2.doFilter(AuthFilter.java:171)                                                                                                                                     \n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:595)                                                                                           \n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:554)                                                                                           \n        at org.apache.oozie.servlet.AuthFilter.doFilter(AuthFilter.java:176)                                                                                                                                       \n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)                                                                                                       \n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)                                                                                                               \n        at org.apache.oozie.servlet.HostnameFilter.doFilter(HostnameFilter.java:86)                                                                                                                                \n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)                                                                                                       \n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)                                                                                                               \n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)                                                                                                                     \n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)                                                                                                                     \n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)                                                                                                                           \n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)                                                                                                                           \n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)                                                                                                                       \n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)                                                                                                                             \n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)                                                                                                                              \n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:620)                                                                                                        \n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)                                                                                                                                 \n        at java.lang.Thread.run(Thread.java:745)      \n</pre><p>Any idea how to solve this problem?</p><p>Thanks!</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-03-10 11:51:40.0","id":22298,"title":"we are not able to install HDP setup","body":"<p>we are not able to Install HDP setup. Can someone help us to install hdp setup?</p><p><a href=\"/storage/attachments/2705-hdp-setup-error.png\">hdp-setup-error.png</a></p>","tags":["installation"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-03-18 09:50:44.0","id":23587,"title":"Can we use hdfs acl and ranger hdfs plugin at the same time ?","body":"","tags":["HDFS","help"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-03 05:35:26.0","id":5099,"title":"NiFi equivalent to flume spooling directory source","body":"<p>I'm trying to use NiFi to replace a basic flume agent that uses Spooling Directory Source and send content to Kafka.</p><p><a target=\"_blank\" href=\"http://flume.apache.org/FlumeUserGuide.html#spooling-directory-source\">http://flume.apache.org/FlumeUserGuide.html#spooling-directory-source</a></p><p>The functionality of Flume Spooling Directory source is describe in flume documentation as:</p><p><em>\"This source lets you ingest data by placing files to be ingested into a “spooling” directory on disk. This source will watch the specified directory for new files, and will parse events out of new files as they appear. The event parsing logic is pluggable. After a given file has been fully read into the channel, it is renamed to indicate completion (or optionally deleted).\"</em></p><p>Looking for hints on how to do it with NiFi.</p>","tags":["Flume","Nifi","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-01 23:39:27.0","id":5012,"title":"Storage space consideration for deploying AMS","body":"<p>What are the guidelines around allocating storage space for AMS service?</p><p>A partner is looking for ballpark number for a small / medium  and large cluster so they can ensure that they don't run out of storage space for AMS</p>","tags":["operations","ambari-metrics","storage"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-12-03 21:30:39.0","id":5148,"title":"Atlas generating huge log files","body":"<p>HDP 2.3.2 cluster is generating huge log files Atlas, see this example:</p><pre>[root@APHDPMND01 atlas]# ls -la\ndrwxr-xr-x.  2 atlas hadoop       4096 Nov 19 00:00 .\ndrwxr-xr-x. 35 root  root         4096 Nov 15 03:07 ..\n-rw-r--r--.  1 atlas hadoop 1548523843 Oct 29 16:52 metadata.20151020-105724.err\n-rw-r--r--.  1 atlas hadoop          0 Oct 20 10:57 metadata.20151020-105724.out\n-rw-r--r--.  1 atlas hadoop 1520931501 Nov 11 11:58 metadata.20151102-100301.err\n-rw-r--r--.  1 atlas hadoop          0 Nov  2 10:03 metadata.20151102-100301.out\n-rw-r--r--.  1 atlas hadoop  978775361 Nov 19 17:48 metadata.20151113-153007.err\n-rw-r--r--.  1 atlas hadoop          0 Nov 13 15:30 metadata.20151113-153007.out\n</pre><p>I see about 15,000 lines like below <strong>per second</strong> in the log files. Is it a known issue? Any solution?</p><pre>Nov 12, 2015 3:40:04 PM com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator attachTypes\nINFO: Couldn't find JAX-B element for class javax.ws.rs.core.Response\nNov 12, 2015 3:40:04 PM com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator attachTypes\nINFO: Couldn't find JAX-B element for class java.lang.String\nNov 12, 2015 3:40:04 PM com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator$8 resolve\nSEVERE: null\njava.lang.IllegalAccessException: Class com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator$8 can not access a member of class javax.ws.rs.core.Response with modifiers \"protected\"\n\tat sun.reflect.Reflection.ensureMemberAccess(Reflection.java:109)\n\tat java.lang.Class.newInstance(Class.java:368)\n\tat com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator$8.resolve(WadlGeneratorJAXBGrammarGenerator.java:467)\n\tat com.sun.jersey.server.wadl.WadlGenerator$ExternalGrammarDefinition.resolve(WadlGenerator.java:181)\n\tat com.sun.jersey.server.wadl.ApplicationDescription.resolve(ApplicationDescription.java:81)\n\tat com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator.attachTypes(WadlGeneratorJAXBGrammarGenerator.java:518)\n\tat com.sun.jersey.server.wadl.WadlBuilder.generate(WadlBuilder.java:179)\n\tat com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl.getApplication(WadlApplicationContextImpl.java:125)\n\tat com.sun.jersey.server.impl.wadl.WadlMethodFactory$WadlOptionsMethodDispatcher.dispatch(WadlMethodFactory.java:98)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\n\tat org.apache.atlas.web.filters.AuditFilter.doFilter(AuditFilter.java:67)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n\n\n</pre>","tags":["logging","Atlas","hdp-2.3.2"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-07 14:05:17.0","id":5297,"title":"Storm JAR Version Conflicts","body":"<p>Hi all,</p><p>We have a Storm topology that has a bolt which is required to go over the proxy, to do so we are using httpcore and httpclient, but the versions we are using are newer then the version which Storm has. The latest version has some new methods that we are using which the old version packaged with Storm does not have. This in turn is causing our Bolt to fail repeatedly with a 'NoSuchMethod' error... We believe this is being caused because the Bolt is picking up the older version on the classpath and not the newest version that we packaged into the fat Jar with the topology. </p><p>In MapReduce we can set to respect the user classpath first... Is there any such feature in Storm that we can use to get around this other then implmenting our own ClassLoader? </p>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-08 15:22:58.0","id":5437,"title":"What does the Ranger plugin for Kerberized kafka currently support ?","body":"<p>It doesn't support creation and deletion of topics. I assume that adding a user to Ranger for kafka will provide producer and consumer authorization for kafka. If that is not case, I need to be enable native acls.</p>","tags":["kerberos","Kafka","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-10 05:43:27.0","id":5732,"title":"Error on tutorial \"how-to-process-data-with-apache-pig\"","body":"<p>\n\tHi  I am starting Tutorial on</p><p>\n\t<a href=\"http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-pig/\">http://hortonworks.com/hadoop-tutorial/how-to-proc...</a></p><p>\n\tI already upload Master.csv and Batting.csv. I Update file permission so everyone can read.</p><p>\n\tScript pig look like this and i have the error</p><p>\n\t<img src=\"/storage/attachments/729-z0v35.png\" alt=\"\"></p><p>\n\tI change the query </p><p>batting = load 'Batting.csv' using PigStorage(','); -&gt;batting = load 'user/admin/Batting.csv' using PigStorage(',');</p><p>here the stack tracer </p><pre>java.net.SocketTimeoutException: Read timed out\n\njava.net.SocketTimeoutException: Read timed out\n\tat java.net.SocketInputStream.socketRead0(Native Method)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:152)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:122)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n\tat sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:690)\n\tat sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1325)\n\tat java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)\n\tat org.apache.ambari.server.controller.internal.URLStreamProvider.processURL(URLStreamProvider.java:209)\n\tat org.apache.ambari.server.view.ViewURLStreamProvider.getHttpURLConnection(ViewURLStreamProvider.java:239)\n\tat org.apache.ambari.server.view.ViewURLStreamProvider.getInputStream(ViewURLStreamProvider.java:216)\n\tat org.apache.ambari.server.view.ViewURLStreamProvider.readFrom(ViewURLStreamProvider.java:103)\n\tat org.apache.ambari.server.view.ViewURLStreamProvider.readAs(ViewURLStreamProvider.java:117)\n\tat org.apache.ambari.view.pig.templeton.client.JSONRequest.readFrom(JSONRequest.java:273)\n\tat org.apache.ambari.view.pig.templeton.client.JSONRequest.post(JSONRequest.java:117)\n\tat org.apache.ambari.view.pig.templeton.client.JSONRequest.post(JSONRequest.java:128)\n\tat org.apache.ambari.view.pig.templeton.client.TempletonApi.runPigQuery(TempletonApi.java:96)\n\tat org.apache.ambari.view.pig.templeton.client.TempletonApi.runPigQuery(TempletonApi.java:103)\n\tat org.apache.ambari.view.pig.resources.jobs.JobResourceManager.submitJob(JobResourceManager.java:222)\n\tat org.apache.ambari.view.pig.resources.jobs.JobResourceManager.create(JobResourceManager.java:87)\n\tat org.apache.ambari.view.pig.resources.jobs.JobService.runJob(JobService.java:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:770)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:182)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:198)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:132)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)</pre><p>Thank you </p><p>YanAdiputra</p>","tags":["HDFS","Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-10 10:02:50.0","id":5802,"title":"Storm: Realtime Ingestion partitioned by data in Stream.","body":"<p>I have a streaming architecture with Kafka-&gt;Storm.</p><p>I want to load data into a cluster partitioned in folders by a timestamp field in the data. I.e. not the timestamp of the Storm Server. The standard HDFSBolt can only add a rotation policy based on file size or time ( in bolt ). Not a field of the data. This is not good enough since the data is supposed to be partitioned by the log timestamp in the source server.</p><p>I didn't find anything finished in google but I think there are two possibilities:</p><p>a) Load data in Storm and use a Hive/Pig load to partition the data every x minutes into the target table using the dynamic hive partitioning.</p><p>Safe way</p><p>b) Write an extension to the HDFSBolt that allows the parametrization of the path by a data field.</p><p>This storm bolt would rotate files every x minutes but would open up one file for each distinct value in the input stream coming through.</p><p>I.e. for the time of 07/12/2015:23:00 -&gt; 08/12/2015:01:00 he would have a Map of &lt;TimeStamp, HDFSFile&gt; which contains two entries:</p><p>&lt;07122015, /data/07122015/file.12374474747.del&gt;</p><p>&lt;08122015, /data/08122015/file.12374474747.del&gt;</p><p>And would be parametrized with:</p><p>MyHDFSBolt.setPath('/data/&lt;datefieldinstream&gt;/');</p><p>There should not be more than two files at a time open since the data is normally quasi-ordered. </p><p>Anybody done anything like this before? Or seen it? Wondering about development complexity and performance. </p><p>Any other suggestions welcome too</p>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-14 10:08:25.0","id":6328,"title":"Update java from 1.7 to 1.8","body":"<p>hello,</p><p>I would like to perform and upgrade of the java 1.7 to java 1.8 of my cluster. I find the following documentation:</p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.1/bk_ambari_reference_guide/content/ch_changing_the_jdk_version_on_an_existing_cluster.html\">http://docs.hortonworks.com/HDPDocuments/Ambari-2....</a></p><p>For the ambari-server, it's very clear, the document says:</p><ol><li> If you choose Oracle JDK 1.8 or Oracle JDK 1.7, the JDK you choose downloads and\n        installs automatically on the Ambari Server host. This option requires that you have an\n        internet connection. You must install this JDK on all hosts in the cluster to this same\n        path.</li></ol><p>Can someone explain me clearly what I have to do, and how, on the others hosts? I understand that I have to install manually the same jdk on the others hosts but what will be the path? Should I uninstall previous version? Should I also change any configuration file on the others hosts?</p><p>Many thanks in advance,</p><p>Michel</p>","tags":["upgrade","java"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-14 13:37:49.0","id":6352,"title":"Networking and edge nodes","body":"<p>Hello,</p><p>I am very new to networking and also very new to Hadoop. I have some questions regarding installing / running a small (2x namenode, 3x datanode) Hadoop cluster and how to connect the small cluster with the rest of our environment. We have 2 networks - a 'public' network (which is really not public at all, just allows desktops, etc. to connect to it), and a 'cluster' network which is where we connect our cluster computing together. We would like to put our Hadoop cluster in the 'cluster' network, but I'm unclear as to how the 'public' network connects to this.</p><p>Is it correct to think that this is where 'edge nodes' come in to play? We can have nodes (can they be virtualized?) which sit in both the 'public' and 'cluster' networks so that members of the public network can connect to these edge nodes and access the Hadoop cluster behind the scenes?</p>","tags":["network","edge"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-12-14 16:36:55.0","id":6389,"title":"Solr: How to index rich text document put in hdfs?","body":"<p>Hi Guys!</p><p>I'm working on Solr, exactly on rich text document indexing. </p><p>I'm able to index pdf document with DirectoryIngestMapper specifying hdfs folder and so, job map&reduce runs Solr indexing operation.</p><p>Now I want index my files when they are putted in hdfs folders automatically without start yarn job everytimes and for each file. </p><p>How I can it or can you indicate me the documentation to do it?</p><p>When a file with the same name is saved in hdfs but with different content, SOLR will update the index or will create a new index?</p><p>For each indexing operation, SOLR will proceed to index only new files or all files and so all index data?</p><p>Thank you in advance,\nRegards,\nGiuseppe</p>","tags":["SOLR"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-15 19:25:38.0","id":6578,"title":"Hive table load","body":"<p>Hello Friends,</p><p>I am new to Hadoop and Hive. Created a simple table with one column as ID and loaded data into this table from a file that's located in local filesyste with 6 (one with NULL)  records with command as \"load data local inpath '/home/edureka/Desktop/data' into table  emp;\"</p><p>Did select and it's show 5 records. Later manually changed the source file and removed all those 5 records and added 5 new records and loaded these new records without using the OVERWRITE with command  \"load data local inpath '/home/edureka/Desktop/data' into table  emp;\" Data load was successful.</p><p>This time if I do select * then I am getting 18 records. 1st set is repeating twice. I don't know why it's showing like this. Am I missing any command ? pls help me to understand.</p><p><strong>Pls refer the screenshot.</strong></p><p>Thanks<a href=\"/storage/attachments/839-capture.png\">capture.png</a></p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-15 10:12:06.0","id":6494,"title":"Cloudbreak for hybrid cloud (on-premise AND in-cloud)","body":"<p>Hello, could Cloudbreak be used to scale out a hdp plattform into the public cloud e.g. Azure in order to react to peak ressources request e.g. for Data Science projects or for a cloud DEV Sandbox?</p>","tags":["Cloudbreak","scale","hybrid"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-15 21:05:47.0","id":6592,"title":"Is there any way to do a fully automated rolling restart of an entire cluster?","body":"<p>How can I automate a rolling restart of all the nodes in a cluster so it can be done without human intervention (all HA features are enabled)?  I'm aware that you can do a rolling restart of the data nodes through Ambari, but we want to restart masters, edge nodes, etc. too. Organization requires restarting all nodes periodically and we want to avoid all manual steps.</p>","tags":["operations","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-16 10:25:09.0","id":6720,"title":"Namenode Txid Error","body":"<p>Dear Team,</p><p>Following error has been occur when Namenode service start.  </p><p>ERROR namenode.NameNode (NameNode.java:main(1657)) - Failed to start namenode.\njava.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 293929 but unable to find any edit logs containing txid 221561</p><p>Regards,</p><p>Nilesh</p>","tags":["namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-18 19:38:00.0","id":7297,"title":"How to optimize my spark cluster to work faster","body":"<p>Hi,</p><p>We have 6 node cluster and trying to run some complex computation on the 6 to 7 GB around data. We essentially do joins and some basic transformations at the best. However, the job is taking almost 2 to 3 hours and some times running memory issues as well. </p><p>I want to get clear understanding of how to optimize my cluster to get every ounce of its power and i am sure the computation that we are doing is nothing and it may not run in minutes, but i feel 2 hours is not what it takes. I have not dig much into optmimizing the cluster first of all, apologies for that. I am still doing R&D to understand how to get best out of my cluster. If i can get some valuable information from you it would be really helpful. We have 6 nodes cluster and each node has 24 cores i believe and has 16 GB RAM. </p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-19 20:30:55.0","id":7342,"title":"Solr Console log missing timestamp","body":"<p>It seems like the solr console log shows a number (milliseconds since start?) in the first column of the console log, but is missing a timestamp. </p><p>Does anyone know how to configure the log to print a timestamp? </p><pre>Java HotSpot(TM) 64-Bit Server VM warning: Failed to reserve shared memory (errno = 1).\nListening for transport dt_socket at address: 1044\n0    [main] INFO  org.eclipse.jetty.server.Server  – jetty-8.1.10.v20130312\n25   [main] INFO  org.eclipse.jetty.deploy.providers.ScanningAppProvider  – Deployment monitor /home/hdpsrvc/HDPSearch/solr-4.10.2/example/contexts at interval 0\n49   [main] INFO  org.eclipse.jetty.deploy.DeploymentManager  – Deployable added: /home/hdpsrvc/HDPSearch/solr-4.10.2/example/contexts/banana-context.xml\n343  [main] INFO  org.eclipse.jetty.webapp.StandardDescriptorProcessor  – NO JSP Support for /banana, did not find org.apache.jasper.servlet.JspServlet\n392  [main] INFO  org.eclipse.jetty.deploy.DeploymentManager  – Deployable added: /home/hdpsrvc/HDPSearch/solr-4.10.2/example/contexts/solr-jetty-context.xml\n2110 [main] INFO  org.eclipse.jetty.webapp.StandardDescriptorProcessor  – NO JSP Support for /solr, did not find org.apache.jasper.servlet.JspServlet\n2161 [main] INFO  org.apache.solr.servlet.SolrDispatchFilter  – SolrDispatchFilter.init()\n2178 [main] INFO  org.apache.solr.core.SolrResourceLoader  – JNDI not configured for solr (NoInitialContextEx)\n2178 [main] INFO  org.apache.solr.core.SolrResourceLoader  – using system property solr.solr.home: /home/hdpsrvc/HDPSearch/solr-4.10.2/example/example-schemaless/solr\n2179 [main] INFO  org.apache.solr.core.SolrResourceLoader  – new SolrResourceLoader for directory: '/home/hdpsrvc/HDPSearch/solr-4.10.2/example/example-schemaless/solr/'\n</pre>","tags":["solrcloud","SOLR"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-20 16:31:48.0","id":7375,"title":"Error while creating a DataStream using fromElements function","body":"<p>Below is the expeption -</p><p>Caused by: java.io.IOException: Failed to deserialize an element from the source. If you are using user-defined serialization (Value and Writable types), check the serialization functions. Serializer is org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer@599fcdda at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:121) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:58) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:55) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:218) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:584) at java.lang.Thread.run(Thread.java:745)</p>","tags":["flink","flink-streaming"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-21 21:21:52.0","id":7588,"title":"Falcon & Oozie in Hue","body":"<p>Falcon doesn’t seem to replace the Hue Oozie Editor functionality.  And it’s not clear how well it integrates with software build/deployment tools like SVN, Ant, or Maven.</p><p>There may be some issues with Hue 2* as well:</p><ol><li>We are running in Debian 7 OS</li><li>Hue 3 made further improvements to Oozie editor which are not applied in our supported version of HUE</li></ol><p>Also, has anyone tested HUE3 on HDP, including HBase editor etc. ?</p>","tags":["ambari-views","hue","Falcon","Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-23 09:11:33.0","id":7783,"title":"List the EMPLOYEES who are senior to king","body":"<p>Hi guys,</p><p>The question is looking very simple.But i don't why i unable to get the solution.Think like we emp table similar to scott schema(oracle).I just want to write a query whose hiredate is more that hiredate of employee KING.</p><p>Here is the Query:</p><p>select ename,hiredate from emp e1 where e1.hiredate &gt; (select hiredate from emp e2 where e2.ename='KING')</p><p>But unfortunately we have only support for 'IN' (Subqueries) in hive.How to achieve the requirement.</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-21 22:16:18.0","id":7606,"title":"In Hortonworks HDPCDeveloper_2.2 PracticeExam_v7 (ami-b47138de), Services not running despite restart using start script. Is there any issue with this version/","body":"","tags":["aws","hdpcd","hortonworks-university"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-22 07:52:33.0","id":7643,"title":"What is the value of ${wf:user()} and where it is defined ?","body":"<p>Q1. Need more clarification on below variable substitution, Is it $username who submits the workflow ?</p><pre>HADOOP_USER_NAME=${wf:user()}\n</pre>","tags":["configuration","Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-23 23:21:25.0","id":7910,"title":"upgrading from ambari 2.1.2 to 2.2","body":"<p>When I run ambari-server upgrade,   I get contrainst error:</p><p>Batch entry 1 INSERT INTO serviceconfigmapping (config_id, service_config_id) VALUES (3, 290) was aborted.  Call getNextException to see the cause.</p><p>this table in in the db, has no dups.</p><p>thanks </p>","tags":["ambari-2.1.2","ambari-2.2.0","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-24 12:11:46.0","id":7950,"title":"The given key was not present in the dictionary","body":"<p>I am using Hortonworks to access Hive tables:-</p><p>HDP version: 2.2.4</p><p>Hive version: 0.14</p><p>HortonWorks ODBC driver: 2.0.5</p>","tags":["Hive","hdp-2.2.4","odbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-24 17:03:58.0","id":8002,"title":"Bash_Profile : Cannot change directory to /var/lib/ranger : Permission denied","body":"<p>am getting the below warning message while connecting the VM using virtual box, can you please let me know the significance of this warning and how critical is this access ? how can I address this issue ?</p><p>Cannot change directory to /var/lib/ranger : Permission denied</p><p>-bash: /var/lib/ranger/.bash_profile: Permission Denied </p><p><img src=\"https://community.hortonworks.com/storage/attachments/1083-permission1.jpg\"></p>","tags":["permission-denied","permissions"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-28 12:20:05.0","id":8141,"title":"BucketMap Join or SMB MapJoin not generating or creating???","body":"<p>I have two table bucket_small and bucket_big as following</p><pre>hive&gt; show create table bucket_big;\nOK\nCREATE TABLE `bucket_big`(\n  `id` int,\n  `student_id` string,\n  `student_name` string,\n  `course_id` int)\nCLUSTERED BY (\n  course_id)\nINTO 4 BUCKETS\nROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS INPUTFORMAT\n  'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  'hdfs://hdp1.stratapps.com:8020/apps/hive/warehouse/bucket_big'\nTBLPROPERTIES (\n  'COLUMN_STATS_ACCURATE'='true',\n  'numFiles'='4',\n  'numRows'='10',\n  'rawDataSize'='148',\n  'totalSize'='158',\n  'transient_lastDdlTime'='1451302285')\nTime taken: 0.166 seconds, Fetched: 23 row(s)\nhive&gt;\n\n\n</pre><pre>hive&gt; show create table bucket_small;\nOK\nCREATE TABLE `bucket_small`(\n  `course_id` int,\n  `course_name` string)\nCLUSTERED BY (\n  course_id)\nINTO 2 BUCKETS\nROW FORMAT SERDE\n  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS INPUTFORMAT\n  'org.apache.hadoop.mapred.TextInputFormat'\nOUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  'hdfs://hdp1.stratapps.com:8020/apps/hive/warehouse/bucket_small'\nTBLPROPERTIES (\n  'COLUMN_STATS_ACCURATE'='true',\n  'numFiles'='2',\n  'numRows'='6',\n  'rawDataSize'='39',\n  'totalSize'='45',\n  'transient_lastDdlTime'='1451302349')\nTime taken: 0.172 seconds, Fetched: 21 row(s)\n\nAnd i inserted Data as following.\n\nhive&gt;insert overwrite table bucket_big select *from table_one;\nhive&gt;insert overwrite table bucket_small select *from table_two;\n\nNow tables have buckets as i expected.And i set the following configurations.\n\nhive&gt;set hive.auto.convert.join=true;\nhive&gt;set hive.auto.convert.sortmerge.join=true;\nhive&gt;set hive.optimize.bucketmapjoin = true;\nhive&gt;set hive.optimize.bucketmapjoin.sortedmerge = true;\n\nWhen i run the following query,\n\nhive&gt; explain select /*+ MAPJOIN(a) */ b.student_id,a.course_name FROM bucket_small a JOIN bucket_big b ON a.course_id = b.course_id;\nOK\nSTAGE DEPENDENCIES:\n  Stage-4 is a root stage\n  Stage-3 depends on stages: Stage-4\n  Stage-0 depends on stages: Stage-3\n\n\nSTAGE PLANS:\n  Stage: Stage-4\n    Map Reduce Local Work\n      Alias -&gt; Map Local Tables:\n        a\n          Fetch Operator\n            limit: -1\n      Alias -&gt; Map Local Operator Tree:\n        a\n          TableScan\n            alias: a\n            filterExpr: course_id is not null (type: boolean)\n            Statistics: Num rows: 6 Data size: 39 Basic stats: COMPLETE Column stats: NONE\n            Filter Operator\n              predicate: course_id is not null (type: boolean)\n              Statistics: Num rows: 3 Data size: 19 Basic stats: COMPLETE Column stats: NONE\n              HashTable Sink Operator\n                condition expressions:\n                  0 {course_name}\n                  1 {student_id}\n                keys:\n                  0 course_id (type: int)\n                  1 course_id (type: int)\n\n\n  Stage: Stage-3\n    Map Reduce\n      Map Operator Tree:\n          TableScan\n            alias: b\n            filterExpr: course_id is not null (type: boolean)\n            Statistics: Num rows: 10 Data size: 148 Basic stats: COMPLETE Column stats: NONE\n            Filter Operator\n              predicate: course_id is not null (type: boolean)\n              Statistics: Num rows: 5 Data size: 74 Basic stats: COMPLETE Column stats: NONE\n              Map Join Operator\n              -----------------\n                condition map:\n                     Inner Join 0 to 1\n                condition expressions:\n                  0 {course_name}\n                  1 {student_id}\n                keys:\n                  0 course_id (type: int)\n                  1 course_id (type: int)\n                outputColumnNames: _col1, _col6\n                Statistics: Num rows: 5 Data size: 81 Basic stats: COMPLETE Column stats: NONE\n                Select Operator\n                  expressions: _col6 (type: string), _col1 (type: string)\n                  outputColumnNames: _col0, _col1\n                  Statistics: Num rows: 5 Data size: 81 Basic stats: COMPLETE Column stats: NONE\n                  File Output Operator\n                    compressed: false\n                    Statistics: Num rows: 5 Data size: 81 Basic stats: COMPLETE Column stats: NONE\n                    table:\n                        input format: org.apache.hadoop.mapred.TextInputFormat\n                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n      Local Work:\n        Map Reduce Local Work\n\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        ListSink\n\n\nTime taken: 0.299 seconds, Fetched: 70 row(s)\nhive&gt;\n\nIt is generating only Map Only join not Bucket Map join or SMB Map join.What is the problem ???\n\n\t\t\t\t\t\t\n</pre>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-30 09:29:49.0","id":8358,"title":"Issue while installing Ambari using local repository option on VirtualBox using CentOS7","body":"<p>Hi Team - I am facing issue in installing Ambari server using local repository option on Virtual box using CentOS7. </p><p>error:</p><p><img src=\"/storage/attachments/1131-ambari-error.jpg\"></p><p>Error: Package:  ambari-server-2.1.2.1-418.x86_64 (Updates-ambari-2.1.2.1)</p><p>Requires :postgresql-server &gt;=8.1</p><p>Ambari. repo file details are attached.</p><p><a href=\"/storage/attachments/1132-ambarirepo.jpg\">ambarirepo.jpg</a></p>","tags":["centos","installation","virtualbox","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-01 20:13:47.0","id":8519,"title":"Register UDF in PIG","body":"<p>Following code gives error.  Please suggest how to register a udf in PIG Script.</p><p>register piggybank.jar;\nbatting = load '/tmp/admin/data/Batting.csv' using PigStorage(',');\nreverseplayerid = foreach batting generate REVERSE($0);\ndump reverseplayerid;</p><pre>ERROR org.apache.pig.tools.grunt.Grunt - ERROR 101: file 'piggybank.jar' does not exist.</pre>","tags":["Pig","hive-udf"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-05 10:49:38.0","id":8742,"title":"Is there any tool in Hadoop which can do the language translation on my data?","body":"<p>I have data in a SQL Server RDBMS. The data is in French and I need to save that data on hdfs. I also need the data translated into English.</p>","tags":["hadoop-ecosystem","language"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-06 00:45:41.0","id":8886,"title":"What is the best way to read a random line from hundreds of files millions of times in Spark?","body":"<p>I'm writing an iterative Spark application that needs to read a random line from hundreds of files and then aggregate the data in  each iteration.  The number of files is small: ~100, and each one is small in size: &lt;1MB, but both will grow in the future.  Each file has the exact same CSV schema and all of them live in the same directory in HDFS.  In pseudo-code, the application would look like this:</p><pre>for each trial in 1..1,000,000:\n    val total = 0\n    for file in all files:\n        val = open file and read random line\n        total += val\n    done\n    return total\ndone</pre><p>I see the following possibilities to accomplish this in Spark:</p><ol><li>Execute ~1M iterations and in each one open ~100 files, read one line, and perform aggregation (the approach in pseudo-code above). This is simple, but very I/O intensive because there will be 1M * 100 calls to open a file in HDFS.</li><li>Place the contents of each of the ~100 files into memory in the driver program and then broadcast that to each of the ~1M iterations.  Each iteration would read a random line from ~100 in-memory objects, then aggregate the results.  This is better, but each object has to be serialized and transferred over the network from the driver program.</li><li>Create an external Hive table and in each iteration execute <em>select </em>queries to fetch a random row, then aggregate the results. </li><li>Execute ~100 iterations and in each one open a single file and read ~1M lines from it at random.  Each iteration would return an list of values ~1M long and all of the aggregation would be performed in the driver program.  </li></ol><p> What is the best approach?</p>","tags":["Spark","HDFS"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-06 23:15:27.0","id":9010,"title":"High System CPU on Some Servers","body":"<p>Hi,</p><p>Ganglia is reporting that some servers in our cluster are using a lot more system CPU than others. According to Ganglia, most of the servers are using 1% to 10% of CPU for system, but some servers are reported to be using 100% CPU for system. I compared the servers, and they are identical. They are both HP Gen8, same OS, BIOS, and kernel. Even a rpm -qa shows identical software installations. I've used commands such as sar and vmstat, and doesn't look to be disk issue. Also tried bandwidth test, and doesn't seem to be networking issue either.</p><p>Does anyone have any suggestion in terms of how to troubleshoot this further?</p><p>Thanks very much.</p>","tags":["cpu"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-03 06:12:17.0","id":25688,"title":"Data Ingestion from mysql to HDFS - simple import","body":"<p>Hi,</p><p>I am using the sandbox to practice Sqoop.\nFrom the SSH shell, I entered to mysql prompt and have created a table customerInfo under the database test.</p><p>Now, I have from the command prompt I typed the following command:\nsqoop import \\ \n--connect jdbc:mysql://localhost/test \\\n\n--username root \\\n\n--password xxxx \\\n\n--table customerInfo\n\n--m 1</p><p>and got the error : access denied for user 'root'@'localhost'.\n\n1) Could you reply which password should I use?\nI tried with hadoop. I am using the mysql that comes along with the sandbox.</p><p>2) Where can I see the port configuration as mentioned in:\nhttps://alexeikh.wordpress.com/2012/05/03/using-sqoop-for-moving-data-between-hadoop-and-sql-server/</p><p>Thank you.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-04 00:29:18.0","id":25732,"title":"Cloudbreak/Ambari package install timeout","body":"<p>Hi,</p><p>We're seeing a strange issue where our Cloudbreak-orchestrated clusters fail to bootstrap.  We're using CloudBreak version 1.2.0 on AWS (us-west-2).  Looks like it's using ami-a106eec1 for the nodes.  </p><p>Our infrastructure spins up just fine, but any node installing the Hadoop packages times out here:</p><pre>2016-04-01 18:54:18,636 - Installing package hdp-select ('/usr/bin/yum -d 0 -e 0 -y install hdp-select')\n2016-04-01 18:54:29,023 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-04-01 18:54:29,024 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-04-01 18:54:29,029 - Package['rpcbind'] {}\n2016-04-01 18:54:29,103 - Installing package rpcbind ('/usr/bin/yum -d 0 -e 0 -y install rpcbind')\n2016-04-01 18:55:50,028 - Package['hadoop_2_4_*'] {}\n2016-04-01 18:55:50,037 - Installing package hadoop_2_4_* ('/usr/bin/yum -d 0 -e 0 -y install 'hadoop_2_4_*'')\n</pre><p>The odd thing is that the package is actually getting installed ok.  I ran the installer manually inside one of the ambari-agent docker containers on a host that was problematic, and this popped up:</p><pre>2016-04-03 19:57:06,528 - Skipping installation of existing package hadoop_2_4_*</pre><p>So what appears to be happening is the Python install script isn't seeing that the package has installed correctly.  I checked and package retrieval + installation take 30-40 seconds, so I don't think the successful install is exceeding the 30 min timeout.  </p><p>As Ambari is executing yum with \"-d 0 -e 0\", it's difficult to see what's actually happening.  Is there an easy way to tell Cloudbreak-orchestrated Ambari servers to log at a higher level?  Ideally this would be set before any servers get bootstrapped, maybe by making sure <a href=\"https://github.com/apache/ambari/blob/branch-2.2.1/ambari-common/src/main/python/resource_management/core/providers/package/yumrpm.py\">yumrpm.py</a> gets told to use the more verbose log settings.  This would be pretty trivial to do with a standalone Ambari server, but it'd be nice if there's a way to do it quickly that doesn't involve packaging our own RPMs.</p><p>And any thoughts on why this might be happening?  It looks to be an issue with Ambari's RPM handling, but we didn't see this problem in the non-Cloudbreak clusters we spun up on AWS, although we used our own AMIs there.  </p><p>Thanks!</p>","tags":["Cloudbreak","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-04 16:06:10.0","id":25833,"title":"Advantages of Ranger Admin HA","body":"<p>Hi, </p><p>My question is what are advantages of using High Availability in Ranger Admin? The only advantage is that I can log in to Ranger Admin GUI from 2 hosts, are there any others?</p>","tags":["ha","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-05 09:58:43.0","id":25939,"title":"Interpreters not working on Zeppelin","body":"<p>Hi,</p><p>I downloaded zeppelin 0.5.6 (http://www.apache.org/dyn/closer.cgi/incubator/zeppelin/0.5.6-incubating/zeppelin-0.5.6-incubating.tgz), extracted, and started zeppelin-daemon.sh.</p><p>localhost:8080 shows the zeppelin interface, but nothing is working. it looks like the interpreters are not defined properly.</p><p>shift+enter on the cells just return the cell context, even when trying sc.appName or sc.master to see SparkContext configuration.</p><p>The documentation specify that spark should be available without any config., and I couldn't find the required configuration to enable spark (export SPARK_HOME in bin/zeppelin-env.sh didn't help either).</p><p>Please advice.</p><p>Best regards,</p><p>Keren</p>","tags":["zeppelin","configuration","zeppelin-notebook"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-05 11:54:34.0","id":25923,"title":"Email from hadoop environment","body":"<p>is it possible to send mail from hadoop environment. If so, any shell commands is there ?</p>","tags":["Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-05 23:34:04.0","id":26067,"title":"Connection Pool Management from Storm -> to -> External RDBMS","body":"<p>Connection Pooling from a single app is pretty easy and straight forward but what is the best way to manage connection pool for a storm topology. Since every bolt may potentially run in a separate node & JVM, creating a connection pool for each container could result in many open connections which  could be an issue. </p><p>Is there a way to share connection pool for a topology? Are there any best practices / lessons learns, anyone would like to share? </p>","tags":["Storm","rdbms"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-06 12:09:38.0","id":26171,"title":"Is it possible to define automatic broker failover in storm topology?","body":"<p>2 brokers are running Veritas cluster agents to allow failover with listening on single VIP. Storm topology connects to the VIP.</p><p>At this time, storm topology needs restart to pickup the change, when the failover happens.</p><p>Is it possible in the storm topology to auto discover (possibly continuous polling) the broker change and resume without restarting the topology?</p>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-06 18:01:00.0","id":26244,"title":"\"Upgrade\" table missing from Ambari database","body":"<p>Hello- I the process of upgrading Ambari to 2.2.1.1 I hit the following error after $ ambari-server upgrade:</p><pre>06 Apr 2016 13:22:52,129 ERROR [main] SchemaUpgradeHelper:315 - Exception occurred during upgrade, failed\norg.apache.ambari.server.AmbariException: Errors found while populating the upgrade table with values for columns upgrade_type and upgrade_package.\nat org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executePreDMLUpdates(SchemaUpgradeHelper.java:215)\nat org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:302)\nCaused by: org.apache.ambari.server.AmbariException: Errors found while populating the upgrade table with values for columns upgrade_type and upgrade_package.</pre><p>Took a look at my (external) Postgres database and found the ambari database does not have an \"upgrade\" table. I checked another cluster with a similar external Postgres instance and it does have that table.</p><p>Should I create the table manually? If so, anyone have the appropriate create statement for the table? If there's a more ambari-friendly way get the tables straight, that would be preferable. Thanks.</p>","tags":["Ambari","postgres","upgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-06 12:02:18.0","id":26170,"title":"Does ExecuteSQL processor allow to execute stored procedure ?","body":"<p>Can I execute stored procedure on database using ExecuteSQL proessor ? Is there any sample ?</p>","tags":["Nifi","sql"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-15 12:42:11.0","id":27764,"title":"Hello I installed Sandbox 2.2. after installation when I enter the adress they hv given me  in browser it says connection time out. there were some warnings during installation.","body":"<p><img src=\"/storage/attachments/3470-untitled.png\"></p>","tags":["Sandbox","installation"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-15 14:06:34.0","id":27780,"title":"Where exactly classpaths for hadoop are present in hortonworks sandbox and also in cluster. My knowledge in hortonworks sandbox I have seen in /etc/hadoop/conf/0/hadoopenv.sh.","body":"","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-26 16:43:14.0","id":35885,"title":"Using global parameters inside oozie.xml (namenode,jobtracker)","body":"<p>We have 100's of oozie jobs which are called from different places. Is there a way I can provide namenode,jobtracker and similar parameters when oozie starts up and that's being used inside oozie.xml</p><p>example:</p><p> &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;</p><p>I want the value of job-tracker and namenode set up as properties when oozie starts up; so that I don't need to define these for every job</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-26 22:26:52.0","id":35976,"title":"PROBLEM  with refresh rate of hive server audit db destination to HDFS","body":"<p>Hive server audit db is configured to go the HDFS.\nProblem is that AUDIT events are not replicated to the HDFS folder on a continuous basis but rather once a day.We need to enable frequent refresh during the day like once a minute or two.</p><p>Is there a way to increase the refresh rate?</p>","tags":["Ranger","Hive"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-30 18:51:39.0","id":36485,"title":"Ambari server installation fails at Confirm Hosts","body":"<p>Hi all,</p><p>My installation of Amabari server fails at the Confirm Hosts step at the registration phase. I have 4 hosts I want to use for this cluster. Of the 4 hosts, hadoop02 to hadoop04 successfully complete the registration step. The failure comes at hadoop01 when it tries to complete registration. So the whole step of Confirm Hosts fails.</p><p>Hadoop01 is the one I intend to use as the master, and that's where I'm running the GUI from on 8080. I'm using the 'root' user to ssh to all the hosts. I also tried using the 'hadoop' user to to ssh to all the hosts. I've verified that I can do that to all hosts (including hadoop01) by ssh'ing into each host as those users by hand.</p><p>This is what I have in the logs when hadoop01 decides to fail:\n</p><pre>INFO:root:BootStrapping hosts ['hadoop01.example.com',\n 'hadoop02.example.com',\n 'hadoop03.example.com',\n 'hadoop04.example.com'] using /usr/lib/python2.6/site-packages/ambari_server cluster primary OS: redhat7 with user 'root' sshKey File /var/run/ambari-server/bootstrap/4/sshKey password File null using tmp dir /var/run/ambari-server/bootstrap/4 ambari: hadoop01.example.com; server_port: 8080; ambari version: 2.1.0; user_run_as: root\nINFO:root:Executing parallel bootstrap\nWARNING:root:Bootstrap at host hadoop01.example.com timed out and will be interrupted\nINFO:root:Finished parallel bootstrap\n30 May 2016 18:43:12,461  INFO [pool-9-thread-1] BSHostStatusCollector:55 - Request directory /var/run/ambari-server/bootstrap/4\n30 May 2016 18:43:12,461  INFO [pool-9-thread-1] BSHostStatusCollector:62 - HostList for polling on [hadoop01.example.com, hadoop02.example.com, hadoop03.example.com, hadoop04.example.com]\n30 May 2016 18:43:12,909  INFO [pool-9-thread-1] BSHostStatusCollector:55 - Request directory /var/run/ambari-server/bootstrap/4\n30 May 2016 18:43:12,910  INFO [pool-9-thread-1] BSHostStatusCollector:62 - HostList for polling on [hadoop01.example.com, hadoop02.example.com, hadoop03.example.com, hadoop04.example.com]</pre><p>Can I get some help in getting past this step?</p>","tags":["installer"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-31 06:57:47.0","id":36545,"title":"Falcon HiveIntegration","body":"<p>Hi,\nI am trying https://falcon.apache.org/HiveIntegration.html with HDP 2.3.4 in both source and target cluster. </p><p>There is know limitation mentioned in the link as follows: </p><p><strong>Oozie 4.x with Hadoop-2.x </strong></p><p>Replication jobs are submitted to oozie on the destination cluster. </p><p>Oozie runs a table export job on RM on source cluster. </p><p>Oozie server on the target cluster must be configured with source hadoop configs else jobs fail with errors on secure and non-secure clusters as below: </p><p>org.apache.hadoop.security.token.SecretManager$InvalidToken: Password not found for ApplicationAttempt appattempt_1395965672651_0010_000002 </p><p>So I have configured my oozie as given below: </p><pre>&lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt;\n&lt;value&gt;*=/etc/hadoop/conf,nr1.hwxblr.com:8020=/etc/primary_conf/conf,nr3.hwxblr.com:8030=/etc/primary_conf/conf,nr21.hwxblr.com:8020=/etc/hadoop/conf,nr23.hwxblr.com:8030=/etc/hadoop/conf&lt;/value&gt;</pre><p>Still I am facing below error. </p><pre>2016-05-31 06:31:34,170 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33\n2016-05-31 06:31:34,204 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at nr23.hwxblr.com/10.0.1.25:8030\n2016-05-31 06:31:34,235 WARN [main] org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : \norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): appattempt_1464674342681_0001_000002 not found in AMRMTokenSecretManager.\nat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)\nat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:558)\nat org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:373)\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:727)\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:723)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422) </pre><p>However note that this job is running on source cluster RM from target cluster oozie and as per the log it looks like AM is trying to connect to target RM instead of source RM. </p><p>Anyone can share any details on what could be wrong?</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-31 22:09:26.0","id":36839,"title":"Ambari lockout on default authentication","body":"<p>If I'm using the defaulted authentication/authorization for Ambari \"By default, Ambari uses an internal database as the user store for authentication and authorization\", are there any plugins or future plans to add lockouts (on too many failed login attempts)?  I just recently watched a video that stated \"in the cloud - Ambari can be a target for hackers with default credentials\".  Aside from changing the admin password - brute force techniques can still be used, correct?  To my knowledge, unless I configure Ambari with LDAP (that uses a lockout time), there is no way to currently set lockouts in Ambari - is this correct?  </p>","tags":["Ambari","authentication"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-06-12 16:53:04.0","id":39254,"title":"pbag in PIG count","body":"<p>Hi:</p><p>i need to count in PIG the frecuency of every element on the bag:</p><pre>(1,2010-01-31 00:00:00.0,3159,{(VIST),(REC),(EMISTR),(TD)},13)\n(1,2010-02-31 00:00:00.0,3159,{(TD),(TD),(TD),(TD)},4)\n(2,2010-01-31 00:00:00.0,3159,{(VIST),(REC),(DOMINGREC),(EMISTR)},4)\n(2,2010-02-31 00:00:00.0,3159,{(VIST),(REC),(DOMINGREC),(EMISTR)},4)\n</pre><p>I see that in internet:</p><pre>http://datafu.incubator.apache.org/docs/datafu/guide/bag-operations.html</pre><p>but i dont know if is the solution because with PIG i dont know how to manage it.</p><p>Please could you help me??</p><p>Thanks</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-13 09:56:47.0","id":39325,"title":"Multiple tables export with Sqoop in a Oozie workflow","body":"<p>Hi,</p><p>What is the best way to have an Oozie action performing a multiple tables export using Sqoop?</p><p>Context:</p><ul><li>I have a HDFS directory containing one file per destination table but the number of files is not constant over time</li><li>The filename allows me to know what is the name of the target table</li></ul><p>I'd like to have one single Oozie action calling Sqoop to export all the files in the target database.</p><p>Is it something we can do? What are the available options?</p>","tags":["Sqoop","Oozie"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-15 05:09:12.0","id":39820,"title":"HBase server fails to start - File does not exist: MasterProcWALs","body":"<p>Hi All,</p><p>HBase server fails to start with the error below.</p><p>A resolution suggested in another post of Hbase start issue - Deleting files under WALS path didn't help.</p><p>IF you could advise what can be done or investigated that would be a great help.</p><p>Regards,</p><p>-----------------------------</p><p><strong></strong></p><p><em><strong>\n</strong></em></p><p><em><strong>2016-06-15 15:02:38,692 FATAL [ip-172-31-25-3:16000.activeMasterManager] master.HMaster: Failed to become active master\njava.io.FileNotFoundException: File does not exist: /apps/hbase/data/MasterProcWALs/state-************00000001.log</strong></em></p><p>2016-06-15 15:02:38,636 INFO  [ip-172-31-25-3:16000.activeMasterManager] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase-unsecure/flush-table-proc/acquired /hbase-unsecure/flush-table-proc/reached /hbase-unsecure/flush-table-proc/abort\n2016-06-15 15:02:38,643 INFO  [ip-172-31-25-3:16000.activeMasterManager] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase-unsecure/online-snapshot/acquired /hbase-unsecure/online-snapshot/reached /hbase-unsecure/online-snapshot/abort\n2016-06-15 15:02:38,664 INFO  [ip-172-31-25-3:16000.activeMasterManager] master.MasterCoprocessorHost: System coprocessor loading is enabled\n2016-06-15 15:02:38,676 INFO  [ip-172-31-25-3:16000.activeMasterManager] procedure2.ProcedureExecutor: Starting procedure executor threads=5\n2016-06-15 15:02:38,677 INFO  [ip-172-31-25-3:16000.activeMasterManager] wal.WALProcedureStore: Starting WAL Procedure Store lease recovery\n2016-06-15 15:02:38,680 INFO  [ip-172-31-25-3:16000.activeMasterManager] util.FSHDFSUtils: Recovering lease on dfs file hdfs://ip-172-31-25-3.ap-southeast-2.compute.internal:8020/apps/hbase/data/MasterProcWALs/state-************00000001.log\n<em><strong>2016-06-15 15:02:38,692 FATAL [ip-172-31-25-3:16000.activeMasterManager] master.HMaster: Failed to become active master\njava.io.FileNotFoundException: File does not exist: /apps/hbase/data/MasterProcWALs/state-************00000001.log</strong></em>\n   at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n   at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLease(FSNamesystem.java:2877)\n   at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.recoverLease(NameNodeRpcServer.java:753)\n   at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.recoverLease(ClientNamenodeProtocolServerSideTranslatorPB.java:671)\n   at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n   at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n   at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)\n   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)\n   at java.security.AccessController.doPrivileged(Native Method)\n   at javax.security.auth.Subject.doAs(Subject.java:422)\n   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)\n   at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-13 16:31:33.0","id":1350,"title":"Config inconsistency in ambari-server.log after HDP migration from 2.1.5 to 2.3.2, what should we do ?","body":"","tags":["hdp-2.3.0","configuration","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-13 17:47:51.0","id":1357,"title":"Are their NiFi nightly builds published anywhere?","body":"<p>Hi, I couldn't find direct links to published dev builds on the Apache site, are they being produced regularly? E.g. we are seeing some new features streaming in and don't always have access to a full local build environment to iterate with the NiFi engineering team. Having automated build infrastructure helps here.</p>","tags":["help","Nifi","builds"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-13 22:07:33.0","id":1387,"title":"What is a suggested offsite/cold backup method for HDFS? besides AWS S3","body":"","tags":["HDFS","aws"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-13 18:56:25.0","id":1374,"title":"Any Consolidated Document on HBase Design Best Practices ?","body":"<p>Can someone point me in the right direction or share a consolidated document on HBase Design Best Practices? Thank you in advance !</p>","tags":["performance","best-practices","Hbase"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-14 04:23:53.0","id":1405,"title":"Can you please advise about how best to use this SSD storage to boost performance in HDP on Azure?","body":"<p>We're running HDP 2.3 on Windows Azure virtual machines. These machines come with a 400GB temporary SSD disk (which gets wiped after restart). I wanted to ask for advice about how best to use this SSD storage to boost performance? e.g. which config params should we change to point to locations on the SSD disk to boost HDFS / Tez / Yarn / Hive performance?</p>","tags":["performance","hdp-2.3.0","azure","ssd"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-15 01:23:28.0","id":1475,"title":"Restart scripts for ambari managed services on server reboot - Supported in Ambari 2.1.2?","body":"","tags":["Ambari","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-14 09:16:49.0","id":1419,"title":"metadata enrichment, data dictionary and data lineage in Hive","body":"<p>Hive tables can store comments on columns when the table is created.</p><p>Oozie is used to load tables by copying data from remote nodes to edge node first and then loading to hive tables. </p><p>What are the recommended practices for metadata enrichment, building a data dictionary and maintaining data lineage ?</p>","tags":["data-lineage","provenance","dictionary","metadata"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-14 14:06:06.0","id":1438,"title":"In support portal under tool tab customer is not seeing the list of OS that is supported and cannot download rpm file","body":"<p><a href=\"/storage/attachments/244-smartsense.jpg\">smartsense.jpg</a></p>","tags":["smartsense"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-12 04:43:04.0","id":1280,"title":"ODBC driver 2.0.5 for HDP 2.3 posted","body":"<p>ODBC driver 2.0.5 with support for Windows, RedHat 6, Ubuntu 12/14, SLES SP3 and OS X is posted.</p><p>You can find the download at <a href=\"http://hortonworks.com/hdp/addons/\">http://hortonworks.com/hdp/addons/</a></p><p>A driver for RedHat 7 is expected in 6-8 weeks.</p><p>Also: Those of us using <a href=\"https://github.com/hortonworks/structor\">Structor</a> may be interested in <a href=\"https://github.com/cartershanklin/structor/blob/structor2/profiles/2node-secure-odbc-sampledata.profile\">this profile</a> which shows how to create a 2 node secure cluster, install redundant HiveServer2 instances and access them through Zookeeper failover using this new ODBC driver. A sample Python program to query Hive using PyODBC is included.\n<a href=\"https://github.com/cartershanklin/structor/blob/structor2/profiles/2node-secure-odbc-sampledata.profile\"></a></p>","tags":["hdp-2.3.0","Hive","odbc"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-15 16:18:18.0","id":1522,"title":"How to Setup HiveServer2 Authentication with LDAP SSL (No Knox)","body":"","tags":["ldap","hiveserver2","Hive","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-15 18:09:31.0","id":1534,"title":"How do you cancel the Resource Manager HA wizard if it is stuck?","body":"<p>The Resource Manager HA wizard was stuck on the last part of the last step because of an issue with starting of the Oozie server.  Hitting the \"X\" did close the Wizard and allow me to place Oozie in maintenance mode.  I then cycled the Ambari server and went back into the wizard.  The first step completed (again) but the wizard stopped progress at that point.  </p>","tags":["resource-manager","high-availability","YARN","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-16 13:32:50.0","id":1583,"title":"Hive View Not Populating Database Explorer","body":"<p><img src=\"/storage/attachments/252-2015-10-16-09-29-17.png\"></p><p>I've downloaded the latest sandbox and I'm running through a tutorial. It seems that the Hive view in Ambari can execute commands, but the Database Explorer will not show any of the tables that are present. </p><p>However I can still execute statements and receive the results. I just can't see the list of tables. It's not throwing any errors up on the screen so I'm not sure what's wrong.</p><p>Is there any way to fix this?</p>","tags":["hdp-2.3.0","ambari-views","Hive","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-10-16 18:12:48.0","id":1601,"title":"Ambari AD Sync On Groups Not Working","body":"<p>I have synced AD users to Ambari and they all work. But i added a group in AD and assigned users their respective groups then re-sync Ambari with AD. It brings in the groups but it didn't link the users to their respective groups in Ambari. how do I link them since Ambari won't let you do it - add user to group function is greyed out. </p><p>Is it possible to just go into postgresql and delete all the ldap user entries and their groups from users and groups table respectively the resync afterwards?</p>","tags":["active-directory","ambari-ldap-sync","user-groups"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-15 19:49:58.0","id":1544,"title":"Seeing PSYoungGen GC on YARN nodes aggressively every 8-10 secs","body":"<p>Seeing PSYoungGen GC on YARN/RM nodes aggressively every 8-10 secs: </p><pre>6171.971: [GC [PSYoungGen: 136875K-&gt;512K(144896K)] 252995K-&gt;116647K(1542656K), 0.0070630 \nsecs] [Times: user=0.05 sys=0.00, real=0.01 secs] \n6180.179: [GC [PSYoungGen: 115578K-&gt;480K(142336K)] 231714K-&gt;116703K(1540096K), 0.0065030 \nsecs] [Times: user=0.04 sys=0.01, real=0.00 secs] \n6190.316: [GC [PSYoungGen: 141974K-&gt;736K(142336K)] 258198K-&gt;116975K(1540096K), 0.0059270 \nsecs] [Times: user=0.03 sys=0.01, real=0.01 secs] \n6200.145: [GC [PSYoungGen: 137784K-&gt;672K(140800K)] 254024K-&gt;116935K(1538560K), 0.0059910 \nsecs] [Times: user=0.04 sys=0.00, real=0.01 secs] \n6209.983: [GC [PSYoungGen: 138483K-&gt;704K(139776K)] 254746K-&gt;117079K(1537536K), 0.0060980 \nsecs] [Times: user=0.04 sys=0.00, real=0.01 secs] \n6220.037: [GC [PSYoungGen: 137973K-&gt;544K(138752K)] 254349K-&gt;117157K(1536512K), 0.0036380 \nsecs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n6228.739: [GC [PSYoungGen: 117752K-&gt;768K(137728K)] 234366K-&gt;117437K(1535488K), 0.0061930 \nsecs] [Times: user=0.03 sys=0.02, real=0.00 secs] \n6237.006: [GC [PSYoungGen: 113053K-&gt;768K(136704K)] 229723K-&gt;117493K(1534464K), 0.0037210 \nsecs] [Times: user=0.03 sys=0.00, real=0.01 secs] \n6244.956: [GC [PSYoungGen: 107217K-&gt;672K(135680K)] 223942K-&gt;117589K(1533440K), 0.0080520 \nsecs] [Times: user=0.03 sys=0.03, real=0.01 secs] \n6255.009: [GC [PSYoungGen: 133601K-&gt;512K(134656K)] 250518K-&gt;117517K(1532416K), 0.0070160 \nsecs] [Times: user=0.04 sys=0.00, real=0.01 secs] \n6265.007: [GC [PSYoungGen: 131612K-&gt;672K(134144K)] 248618K-&gt;117821K(1531904K), 0.0039770 \nsecs] [Times: user=0.04 sys=0.00, real=0.00 secs] \n6274.880: [GC [PSYoungGen: 129679K-&gt;815K(132608K)] 246828K-&gt;118132K(1530368K), 0.0036740 \nsecs] [Times: user=0.03 sys=0.00, real=0.01 secs] \n6284.782: [GC [PSYoungGen: 129427K-&gt;608K(132096K)] 246744K-&gt;118164K(1529856K), 0.0039850 \nsecs] [Times: user=0.04 sys=0.00, real=0.01 secs] \n6294.809: [GC [PSYoungGen: 129803K-&gt;576K(130560K)] 247360K-&gt;118260K(1528320K), 0.0037790 \nsecs] [Times: user=0.03 sys=0.00, real=0.00 secs] \n6297.736: [GC [PSYoungGen: 39607K-&gt;544K(129536K)] 157291K-&gt;118316K(1527296K), 0.0035240 secs] \n[Times: user=0.02 sys=0.00, real=0.00 secs] </pre><p>Is this normal? </p><p>Is there a way to properly tweak it? </p><p>Looks like the new size is set to 144M, should this be increased? </p><p>Thank you ! </p>","tags":["garbage-collector","java","YARN","rm"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-19 04:33:00.0","id":1652,"title":"How can I query HBase from Hive?","body":"<p>hive&gt; CREATE TABLE test3(id string, val decimal) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,0:val\");</p><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:MetaException(message:java.io.IOException: java.lang.reflect.InvocationTargetException</p><p>at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)</p><p>at org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:420)</p><p>at org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:413)</p><p>at org.apache.hadoop.hbase.client.ConnectionManager.getConnectionInternal(ConnectionManager.java:291)</p><p>at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:222)</p><p>at org.apache.hadoop.hive.hbase.HBaseStorageHandler.getHBaseAdmin(HBaseStorageHandler.java:120)</p><p>at org.apache.hadoop.hive.hbase.HBaseStorageHandler.preCreateTable(HBaseStorageHandler.java:200)</p><p>at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:665)</p><p>at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:658)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</p><p>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>at java.lang.reflect.Method.invoke(Method.java:497)</p><p>at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)</p><p>at com.sun.proxy.$Proxy5.createTable(Unknown Source)</p><p>at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:717)</p><p>at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4170)</p><p>at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:307)</p><p>at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)</p><p>at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)</p><p>at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)</p><p>at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)</p><p>at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)</p><p>at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)</p><p>at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)</p><p>at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)</p><p>at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)</p><p>at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)</p><p>at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)</p><p>at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)</p><p>at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</p><p>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>at java.lang.reflect.Method.invoke(Method.java:497)</p><p>at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</p><p>at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p><p>Caused by: java.lang.reflect.InvocationTargetException</p><p>at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</p><p>at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</p><p>at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</p><p>at java.lang.reflect.Constructor.newInstance(Constructor.java:422)</p><p>at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)</p><p>... 36 more</p><p>Caused by: java.lang.UnsupportedOperationException: Unable to find org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</p><p>at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:36)</p><p>at org.apache.hadoop.hbase.ipc.RpcControllerFactory.instantiate(RpcControllerFactory.java:58)</p><p>at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.createAsyncProcess(ConnectionManager.java:2242)</p><p>at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:690)</p><p>at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:630)</p><p>... 41 more</p><p>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</p><p>at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</p><p>at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</p><p>at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)</p><p>at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</p><p>at java.lang.Class.forName0(Native Method)</p><p>at java.lang.Class.forName(Class.java:264)</p><p>at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:32)</p><p>... 45 more</p><p>)</p><p>at org.apache.hadoop.hive.hbase.HBaseStorageHandler.getHBaseAdmin(HBaseStorageHandler.java:124)</p><p>at org.apache.hadoop.hive.hbase.HBaseStorageHandler.preCreateTable(HBaseStorageHandler.java:200)</p><p>at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:665)</p><p>at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:658)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</p><p>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>at java.lang.reflect.Method.invoke(Method.java:497)</p><p>at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)</p><p>at com.sun.proxy.$Proxy5.createTable(Unknown Source)</p><p>at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:717)</p><p>at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4170)</p><p>at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:307)</p><p>at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)</p><p>at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)</p><p>at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)</p><p>at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)</p><p>at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)</p><p>at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)</p><p>at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)</p><p>at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)</p><p>at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)</p><p>at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)</p><p>at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)</p><p>at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)</p><p>at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</p><p>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>at java.lang.reflect.Method.invoke(Method.java:497)</p><p>at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</p><p>at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</p><p>)</p>","tags":["Hive","Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-20 11:10:20.0","id":1705,"title":"Storm Multi Tenancy - Best practices","body":"<p>I've got a customer who is going to have multiple storm topology running ( different business area ) on top of a single cluster. What's the best way to guarantee multi-tenancy?  ( they are not using Slider )\n</p><p>Thanks,</p><p>Olivier</p>","tags":["Storm","best-practices"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-20 16:52:44.0","id":1733,"title":"ambari-server upgrade fails from 1.7 to 2.1.2","body":"<p>Ambari server upgrade failing from 1.7 to 2.1.2  during ambari-server-upgrade command</p><pre>[root@xxxxxxx ~]# ambari-server upgrade \nUsing python /usr/bin/python2.6 \nUpgrading ambari-server \nUpdating properties in ambari.properties ... \nWARNING: Can not find ambari.properties.rpmsave file from previous version, skipping import of settings \nWARNING: Can not find ambari-env.sh.rpmsave file from previous version, skipping restore of environment settings \nFixing database objects owner \nAmbari Server configured for MySQL. Confirm you have made a backup of the Ambari Server database [y/n] (y)? \nUpgrading database schema \nERROR: Error executing schema upgrade, please check the server logs. \nERROR: Ambari server upgrade failed. Please look at /var/log/ambari-server/ambari-server.log, for more details. \nERROR: Exiting with exit code 11. \nREASON: Schema upgrade failed. </pre><p>ambari-server.log</p><pre>20 Oct 2015 09:59:33,974  INFO [main] Configuration:463 - Reading password from existing file\n20 Oct 2015 09:59:33,984  INFO [main] Configuration:674 - Hosts Mapping File null\n20 Oct 2015 09:59:33,984  INFO [main] HostsMap:60 - Using hostsmap file null\n20 Oct 2015 09:59:34,341 ERROR [main] SchemaUpgradeHelper:255 - Unexpected error, upgrade failed\njava.lang.NoSuchMethodError: org.eclipse.jetty.server.SessionManager.setSessionPath(Ljava/lang/String;)V\nat org.apache.ambari.server.controller.ControllerModule.configure(ControllerModule.java:245)\nat org.apache.ambari.server.upgrade.SchemaUpgradeHelper$UpgradeHelperModule.configure(SchemaUpgradeHelper.java:161)\nat com.google.inject.AbstractModule.configure(AbstractModule.java:59)\nat com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:223)\nat com.google.inject.spi.Elements.getElements(Elements.java:101)\nat com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:133)\nat com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:103)\nat com.google.inject.Guice.createInjector(Guice.java:95)\nat com.google.inject.Guice.createInjector(Guice.java:72)\nat com.google.inject.Guice.createInjector(Guice.java:62)\nat org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:221)</pre>","tags":["upgrade","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-19 11:38:55.0","id":28253,"title":"ClassNotFoundException for Hbase","body":"<p>Hi,</p><p>I have created the code as below to write a record into hbase table of 1.1.2 version. I have included all the jars present in 1.1.2 hbase lib folder but while executing that I am getting the <strong>Error Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration</strong></p><p><strong>Code</strong></p><p><strong>package com.hp; </strong></p><pre>import java.io.IOExceptionimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Connection;\nimport org.apache.hadoop.hbase.client.ConnectionFactory;\nimport org.apache.hadoop.hbase.client.Get;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Result;\nimport org.apache.hadoop.hbase.client.ResultScanner;\nimport org.apache.hadoop.hbase.client.Scan;\nimport org.apache.hadoop.hbase.client.Table;\nimport org.apache.hadoop.hbase.util.Bytes;\npublic class HbasePut \n{\npublic static void main(String[] args) throws IOException\n{// You need a configuration object to tell the client where to connect.    \n// When you create a HBaseConfiguration, it reads in whatever you've set    \n// into your hbase-site.xml and in hbase-default.xml, as long as these can    \n// be found on the CLASSPATH  \nConfiguration config = HBaseConfiguration.create();  \n// Next you need a Connection to the cluster. Create one. When done with it,    \n// close it (Should start a try/finally after this creation so it gets closed    \n// for sure but leaving this out for readibility's sake).  \nConnection connection = ConnectionFactory.createConnection(config); try \n{      \n// This instantiates a Table object that connects you to      \n// the \"myLittleHBaseTable\" table (TableName.valueOf turns String into TableName instance).      \n// When done with it, close it (Should start a try/finally after this creation so it gets      \n// closed for sure but leaving this out for readibility's sake).      \nTable table = connection.getTable(TableName.valueOf(\"myLittleHBaseTable\"));      \ntry \n{        \n// To add to a row, use Put.  A Put constructor takes the name of the row        \n// you want to insert into as a byte array.  In HBase, the Bytes class has        \n// utility for converting all kinds of java types to byte arrays.  \n//In the   below, we are converting the String \"myLittleRow\" into a byte array to\n// use as a row key for our update. Once you have a Put instance, you can\n// adorn it by setting the names of columns you want to update on the row,        \n// the timestamp to use in your update, etc.If no timestamp, the server        \n// applies current time to the edits.        \nPut p = new Put(Bytes.toBytes(\"myLittleRow\"));        \n// To set the value you'd like to update in the row 'myLittleRow', specify\n// the column family, column qualifier, and value of the table cell you'd \n// like to update.  The column family must already exist in your table\n// schema.  The qualifier can be anything.  All must be specified as byte \n// arrays as hbase is all about byte arrays.  Lets pretend the table        \n// 'myLittleHBaseTable' was created with a family 'myLittleFamily'.        \np.add(Bytes.toBytes(\"myLittleFamily\"), Bytes.toBytes(\"cf1\"),Bytes.toBytes(\"SomeValue\"));        \n// Once you've adorned your Put instance with all the updates you want to\n// make, to commit it do the following (The HTable#put method takes the        \n// Put instance you've been building and pushes the changes you made into        // hbase)      \ntable.put(p);        \n// Now, to retrieve the data we just wrote. The values that come back are        \n// Result instances. Generally, a Result is an object that will package up        \n// the hbase return into the form you find most palatable.        \n/*Get g = new Get(Bytes.toBytes(\"myLittleRow\"));        \nResult r = table.get(g);        \nbyte [] value = r.getValue(Bytes.toBytes(\"myLittleFamily\"), Bytes.toBytes(\"cf1\"));        \n// If we convert the value bytes, we should get back 'Some Value', the    \n// value we inserted at this location.        \nString valueStr = Bytes.toString(value);        \nSystem.out.println(\"GET: \" + valueStr);        \n// Sometimes, you won't know the row you're looking for. In this case, you        \n// use a Scanner. This will give you cursor-like interface to the contents        \n// of the table.  To set up a Scanner, do like you did above making a Put        \n// and a Get, create a Scan.  Adorn it with column names, etc.        \nScan s = new Scan();        \ns.addColumn(Bytes.toBytes(\"myLittleFamily\"), Bytes.toBytes(\"cf1\"));        \nResultScanner scanner = table.getScanner(s);        \ntry \n{          \n// Scanners return Result instances.          \n// Now, for the actual iteration. One way is to use a while loop like so:          \nfor (Result rr = scanner.next(); rr != null; rr = scanner.next()) \n{             // print out the row we found and the columns we were looking for            \nSystem.out.println(\"Found row: \" + rr);          \n}          \n// The other approach is to use a foreach loop. Scanners are iterable!           // \nfor (Result rr : scanner) \n{           //  \nSystem.out.println(\"Found row: \" + rr);           //\n}        \n} \nfinally \n{          \n// Make sure you close your scanners when you are done!          \n// Thats why we have it inside a try/finally clause          \nscanner.close();        \n} \n*/        \n// Close your table and cluster connection.      \n} \nfinally \n{\nif (table != null) \ntable.close();      \n}    \n} \nfinally \n{      \nconnection.close();    \n}  }\n\n</pre><ol><li><strong></strong></li></ol><p><strong>\n</strong></p><p><strong>\n</strong></p><p><strong>\n</strong></p><p><strong>\n</strong></p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-19 13:05:20.0","id":28256,"title":"ClassNotFoundException org.apache.flume.instrumentation.kafka.KafkaSinkCounter","body":"<p>Hi all,</p><p>We are\ntrying to write in a Kafka queue with Flume. We have HDP 2.2.4, with this Flume\n1.5.2.2.2  is installed.</p><p>Below the kafka sink configuration:\n</p><pre>occLogTcp.sinks.KAFKA.type = org.apache.flume.sink.kafka.KafkaSink\noccLogTcp.sinks.KAFKA.topic = occTest\noccLogTcp.sinks.KAFKA.brokerList = &lt;broker_host_1&gt;:9092,&lt;broker_host_2&gt;:9092\noccLogTcp.sinks.KAFKA.requiredAcks = 1\noccLogTcp.sinks.KAFKA.batchSize = 20\noccLogTcp.sinks.KAFKA.channel = c1</pre><p>Starting the flume agent\nthrows the following error:</p><pre>java.lang.NoClassDefFoundError:\norg/apache/flume/instrumentation/kafka/KafkaSinkCounter \nat org.apache.flume.sink.kafka.KafkaSink.configure(KafkaSink.java:218) \nat org.apache.flume.conf.Configurables.configure(Configurables.java:41) \nat\norg.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:418) \nat org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:103) \nat\norg.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:140) \nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) \nat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) \nat\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) \nat\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) \nat\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) \nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) \nat java.lang.Thread.run(Thread.java:745)Caused by:\njava.lang.ClassNotFoundException:\norg.apache.flume.instrumentation.kafka.KafkaSinkCounter \nat java.net.URLClassLoader$1.run(URLClassLoader.java:366) \nat java.net.URLClassLoader$1.run(URLClassLoader.java:355) \nat java.security.AccessController.doPrivileged(Native Method) \nat java.net.URLClassLoader.findClass(URLClassLoader.java:354) \nat java.lang.ClassLoader.loadClass(ClassLoader.java:425) \nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) \nat java.lang.ClassLoader.loadClass(ClassLoader.java:358) \n... 12 more</pre><p>Thank you in advance,\nChristian</p>","tags":["Flume","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-24 12:04:19.0","id":29243,"title":"Guarda-HD+>MotoGP Jerez di-retta S-t.reaming 24.04.2016","body":"","tags":["jms"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-25 15:10:20.0","id":29387,"title":"Import data directly in as-parquetfile format","body":"<p>Hello,\nPlease I will like to know how to import the data with Sqoop while compressing with as- parquetfile directly . Is it possible?\nhere's what I use:</p><p>sqoop import \\\n-libjars jtds-1.3.1.jar \\\n--connect \"jdbc:jtds:sqlserver://xxxxxxx:xxxx;databaseName=xxxxxx;user=xxxxx;password=xxxxxxx;instance=xxxxx\" \\\n--driver net.sourceforge.jtds.jdbc.Driver \\\n--username xxxx \\\n--table xxxx \\\n--hive-import \\\n--hive-database xxxxxx \\\n--hive-table xxxx --as-parquetfile --m 1</p><pre>Here the error it returns me:\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /apps/hive/warehouse/\"name_table\"/.metadata/schemas/1.avsc\n\nThank you !\n\n\n</pre>","tags":["formatting"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-04 16:19:49.0","id":31488,"title":"Namenode, ranger and hive audit logs","body":"<p>where can I find Namenode, ranger and hive audit logs on edge node</p>","tags":["logs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-05 02:19:24.0","id":31613,"title":"Default Spark env config error in HDP2.4 installation","body":"<p>Default Spark env config error in HDP2.4 installation:</p><p>#SPARK_DRIVER_MEMORY=\"512 Mb\"</p><p>This line include a wrong config param \"512 Mb\", it should be \"512M\" without space. This will cause spark-shell yarn client start error:</p><p>Invalid initial heap size.</p><p>Suggest to update the template.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-04 20:50:36.0","id":31590,"title":"Kafka CPU busy with GC","body":"<p>Anyone has seen kafka broker running into GC related CPU high ? We see it being 100%.  And, we have no load on kafka. And, we see the following activity going on throughout that time : </p><p>[GC (Allocation Failure) 4.651: [ParNew: 272640K-&gt;7265K(306688K), 0.0277514 secs]272640K-&gt;7265K(1014528K), 0.0281243 secs] [Times: user=0.03 sys=0.05, real=0.03 secs]</p><p>There is a ticket here : https://issues.apache.org/jira/browse/KAFKA-2627</p><p>Heap memory setting on broker : java -Xmx1G -Xms1G -server -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-05 08:54:39.0","id":31667,"title":"Metron TP1 self destructs after approx 10 days of running","body":"<p>Just so everyone is aware, we need to put a logrotate rule in for /var/log/metron_pcapservice.log as after less than 10 days of running that log file filled up my root partition on my Metron node making all kinds of bad stuff happen.</p><p>I'm running the \"First Steps in the Cloud\" AWS style deployment.</p>","tags":["Metron","operations"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-05-18 14:32:12.0","id":34028,"title":"YARN yarn.nodemanager.resource.cpu-vcore","body":"<p>Hi All, </p><p>i don't know what value is needed if I have heterogeneous hardware, datanode1 : (2 Physical CPU, 8 cores, 64Go RAM)  datanode2 : (4 Physical CPU, 16 cores, 128Go RAM)</p><pre>yarn.nodemanager.resource.cpu-vcore</pre><p>thanks</p>","tags":["yarn-scheduler"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-18 21:36:52.0","id":34149,"title":"Cant restart oozie after recreating the oozie.war","body":"<p>I recreated the oozie.war without stopping it and now I just cant seem to start it back again. I can run 'service oozie start/restart' but then oozie status says the below. I also deleted the pid file and started back but I get the same issue. Any pointers on how to fix it ? </p><pre>ubuntu@bcpc-vm2:/etc/init.d$ sudo service oozie status      \nnot running but /var/run/oozie/oozie.pid exists.            \nubuntu@bcpc-vm2:/etc/init.d$   \nubuntu@bcpc-vm2:/etc/init.d$ cat /var/run/oozie/oozie.pid                  \n30858                                                                      \nubuntu@bcpc-vm2:/etc/init.d$ ps -ef | grep 30858                           \nubuntu   32491 29828  0 17:40 pts/3    00:00:00 grep --color=auto 30858    \nubuntu@bcpc-vm2:/etc/init.d$                                               \n                             \n</pre>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-20 03:05:18.0","id":34449,"title":"Running namenode using 2 ip addresses with the same port","body":"<p>Hello experts,</p><p>I have this situation where I have installed namenode on ip address A port 50070 and it works perfect. However I need to view from web browser using ip address B port 50070. Is there anyway I can do this? Map ip address B to A or anything similar? However any namenode activity has to go through ip address A port 50070. Only browsing from web browser will use ip address B port 50070.</p><p>Thanks for any advice.</p>","tags":["namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-20 13:04:30.0","id":34532,"title":"Can multiple LDAP servers be configured on apache knox/ranger","body":"<p>We use multiple LDAP servers separated based on geographic location. Is it possible to configure multiple LDAP servers in apache knox/Ranger.</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-20 19:05:22.0","id":34619,"title":"Why am getting to get web UI for Apache Atlas in Technical Perview Machine?","body":"<p>Hello,</p><p>I have downloaded Technical Perview Machine(Atlas-Ranger).All services are running fine and able to see their web UI too.But am getting error when I hit for Atlas UI.</p><p>Error:</p><h2>HTTP ERROR: 503</h2><p>Problem accessing /. Reason:</p><pre>    Service Unavailable</pre>","tags":["Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-21 11:03:20.0","id":34672,"title":"Does Anyone know,How to define data pipeline to load data into hive table using apache falcon?","body":"<p>Assumtion,</p><p>1) Hive table is created and partitioned already.</p><p>2)data is present somewhere on hdfs with correct date-time pattern and landing on hourly basis.</p><p>My Questions are:</p><p>1) Do you have link for such example?.if yes,please share with me.</p><p>2) Do we require to write hive script.</p><p>If you have implemented such a example then it's good and you can share that with me.</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-23 13:15:12.0","id":34924,"title":"ambari metrics getting down frequently","body":"<p>After enabling HA I foung following error on /var/log/ambari-metrics-collector</p><p>Caused by: java.io.IOException: Can't get master address from ZooKeeper; znode data == null</p><p>Please help me out</p>","tags":["ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-06 16:56:06.0","id":37915,"title":"Looking for Oozie usecases:","body":"<p>Hi,</p><p>I am looking for usecases for Oozie.</p><p>when I look around I get information from https://oozie.apache.org/docs/4.2.0/ but no usecases in here.</p><p>Can you please point me to documentation with steps for using Oozie.</p><p>Any help is highly appreciated.</p><p>Many Thanks,</p><p>Sujitha</p>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-06 18:15:39.0","id":37951,"title":"Can Atlas fetch metadata from Informatica governance tool and integrate with the local metadata?","body":"","tags":["Atlas"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-05-31 22:36:08.0","id":36844,"title":"Hortonworks Tutorials - Created/Modified Dates","body":"<p>I've noticed on the website, the tutorials don't have dates (created/modified).  http://hortonworks.com/hadoop-tutorial/  Is this on purpose?  As an end-user, the first thing I look for is a date on a tutorial.  Do we not put the date on purpose?  Is the end-user to assume the tutorials are newer (within the last year or two)? </p><p>Also - as an end-user if I followed a tutorial and something changed, I would like see a date (modified) to know something has changed - so I can either re-do the tutorial or apply the changes.</p>","tags":["how-to-tutorial"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-08 09:13:50.0","id":38420,"title":"wrong table name?","body":"<p>It is not clear how the 1st step of creating the hive table via hiveContext is related to the later step of saving the \"results\" into that table. It seems it is merely being saved to a file in hive's warehouse directory but how will hive know that this file is for the previous table. Looks like you missed a step!</p>","tags":["tutorial-400","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-09 11:41:48.0","id":38766,"title":"ambari-shell - can I start/stop only one service?","body":"<p>Installed ambari-shell according to great post here:</p><p>http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/</p><p>can I start/stop one particular service through ambari-shell?</p>","tags":["ambari-shell"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-06-09 22:49:03.0","id":38923,"title":"Spark ML confidence interval","body":"<p>When running a Spark ML model, the precision of the model was low, around 65%.  Could this be due to the small set of training data we are working with. Does the size of the training data influence the confidence interval? </p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-10 18:39:00.0","id":39116,"title":"Predictive Analysis Tool","body":"<p>Hello everyone,</p><p>I already use HDP in large scale, but I need to know what tool people are using to do Predictive Analysis. I know that are a lot of tools in the market, but I am looking for something that allows me not to have a specific person with a lot of Data Science knowledge, statistics, etc...But i want my business user to build his own analysis.</p><p>I know that are some tools in the market that aren´t Open Source like SAP Predictive Analysis that has this purpose.</p><p>Does anyone knows another software like this?</p><p>Thank you </p>","tags":["analytics"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-09-30 22:36:43.0","id":657,"title":"Installing HDP via Ambari without having Sudo/Root privileges","body":"<p>I have a locked down environment where I can't have sudo/root privileges.  Am I still available to install Ambari and HDP? If so, how?</p><p>I was able to create a<a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.1.0/bk_Installing_HDP_AMB/content/_using_a_local_repository.html\"> local repo</a> following the docs. Furthermore I have created a <a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.1.0/bk_Installing_HDP_AMB/content/_set_up_password-less_ssh.html\">user called 'Ambari'</a> to run the Ambari service as.   </p>","tags":["Ambari","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-30 13:17:57.0","id":606,"title":"How do you add nodes to a Cloudbreak cluster?","body":"<p>I have deployed a cluster in Azure using Cloudbreak and want to permanently add another two data nodes and, ideally, still have it managed through cloudbreak. How would I go about doing this?</p>","tags":["aws","Cloudbreak","azure"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-01 13:46:09.0","id":683,"title":"Solr - avoid 'multi-valued field can't be sorted' issue?","body":"<p>Hi All,</p><p>The use case is a Banana dashboard working with a SolrCloud instance/cluster. If we follow default steps we end up using the 'data_driven_schema' in Solr, which makes it easy for it to accept any random data and try index it.</p><p>However, the problem is down the line. Banana <em>table</em> widget can't sort on many columns as Solr complains about those fields being multi-valued. In fact, they are not, and unique (checked via admin section), but rather declared multi-valued.</p><p>What is the approach to address this? Ideally, without having to specify a complete new schema for a Solr index. Can one have a benefit of flexible fields, but default to non-multi-valued maybe?</p>","tags":["index","SOLR","banana"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-02 12:15:52.0","id":739,"title":"(hive) How to dynamically set hive.aux.jars.path environment variable after HiveServer2 is started.","body":"<p>Getting the below question in the context of an application that creates tables with a custom storage handler:</p><p>When I tried setting jar locations on the fly using “set hive.aux.jars.path=&lt;file urls&gt;” on beeline prompt after connecting to HS2, it did not work.</p><p>It works only if the property is set in hive-site.xml before starting HS2, OR set the env var HIVE_AUX_JARS_PATH to a dir containing my jars OR start HS2 with --hiveconf hive.aux.jars.path=… and it then becomes a global setting. I would like it to become a session specific setting as setting it through a global property for the process needs HS2 restart through admin, and looks like not favored by many of our users.</p><p>Is this the way the property is supposed to work or I am doing something wrong?</p><p>I need this for our storage handlers, serde, and UDFs to work. Please note that even the “add jar” does not help here.</p><p>See <a href=\"http://mail-archives.apache.org/mod_mbox/hive-user/201509.mbox/%3CBY1PR03MB1339916A485A79382623A40DCD430@BY1PR03MB1339.namprd03.prod.outlook.com%3E\">Hive ML</a> for full question details</p>","tags":["dynamic","Hive","configuration"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-30 20:50:46.0","id":646,"title":"How does BrokerList in Kafka Sink work ?","body":"<p>Customer's Flume Kafka Sink,</p><p>we have defined agent.sinks.kafka-netflow-ci.brokerList=edge01:9092,edge02:9092,edge03:9092</p><p>Question is How does sink uses the broker list ?</p><p>Are edge02 and edge03 used in case edge01 is failing or will they be used randomly ?</p><p>\nfrom the internet i could below , But still not clear on this.</p><p>The brokers in the Kafka sink uses to discover topic partitions, formatted as a comma-separated list of hostname:port entries. You do not need to specify the entire list of brokers, but Cloudera recommends that you specify at least two for high availability.</p>","tags":["Flume","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-05 16:17:22.0","id":899,"title":"How can you query using JSON against HDP?","body":"<p>The customer wants to use something like Apache Drill to query HDP using JSON due to the fact that it's self-describing.</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-29 10:41:21.0","id":449,"title":"How to reset Ambari Admin password?","body":"<p>I have forgot Ambari Admin password after I had changed it from default, not able to login to Ambari UI now. Please suggest.</p>","tags":["Sandbox","Ambari","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-08 19:23:48.0","id":1163,"title":"Falcon Mirroring: HDFS To S3 AWS Credentials","body":"<p>Where do you define your AWS access key and secret key credentials for mirroring data from a local Falcon cluster to S3? I have the job setup but it is failing because those values are not defined.</p>","tags":["mirroring","Falcon","s3","aws"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-12 14:46:29.0","id":1288,"title":"With Hive View what causes: \"H100 Unable to submit statement show databases like '*': org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe\"?","body":"<p>This an simlar exception come up with Hive View. Also see:</p><p>https://hortonworks.com/community/forums/topic/amabari-hive-view-error-broken-pipe/\n\nHow to resolve and best debug such an issue?</p>","tags":["ambari-views","metastore","Hive","thrift"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-09 13:11:53.0","id":1192,"title":"What were the main factors to replace Nagios and Ganglia with Ambari Metrics ?","body":"<p>Wondering what were the driving factors to replace Nagios and Ganglia with Ambari Metrics.</p>","tags":["ganglia","operations","ambari-metrics","nagios"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-12 22:27:42.0","id":1320,"title":"Required NiFi Ports","body":"<p>Hi, I'd like clarification on the ports required to run NiFi. I was reading through the admin guide here<a href=\"https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html\">https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html</a></p><p>Questions:</p><ol><li>In a non-clustered local mode, what are the minimum ports required for a runtime (on top of data protocol ones, which will naturally differ based on the flow)? Anything beyond the web port for UI and API?</li><li>In a clustered mode, things are a little more interesting. Appears that ports will differ based on the casting protocol (if any). Comments on that? The guide mentions many are left blank by default.</li><li>Site-to-site protocol. Any additional ports for a protocol itself? Does a sender node have to have access to every individual node as well? Any special ports or is it just data ports?</li></ol>","tags":["Nifi","hdf","operations"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-11 12:15:53.0","id":1277,"title":"What is the Best consolidated Guide for HBase Tuning?","body":"<p>In your opinion, what is the best consolidated resource for performance tuning an HBase instance? </p><p>Are the HBase guides the best reference we have for performance tuning or are there others which are more comprehensive?</p><ul>\n<li><a target=\"_blank\" href=\"http://hbase.apache.org/book.html#performance\">http://hbase.apache.org/book.html#performance</a></li><li><a target=\"_blank\" href=\"http://hbase.apache.org/book.html#important_configurations\">http://hbase.apache.org/book.html#important_configurations</a></li></ul>","tags":["performance","Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-21 16:47:19.0","id":1821,"title":"HBase Quota Management","body":"<p>In HDP 2.2.8 is there any way to enable quota management within HBase that would limit namespaces to a specific size for example:</p><p>For Example:</p><p>Create two namespaces and apply a Quota limit:</p><p>Namespace1:  50 TB</p><p>  Table1, Table2</p><p>Namespace2  150 TB</p><p>          Table3, Table4, Table5</p><p>But the sum of the tables would be allowed to use up to that limit.</p><p>Example:</p><p>Table1 + Table2 &lt;= 50 TB</p><p>Table3 + Table4 + Table5 &lt;= 150TB </p><p>Thank you for any advice and guidance.</p>","tags":["HDFS","Hbase","sizing","hdp-2.2.8"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-22 20:24:47.0","id":1929,"title":"Changing Ranger UI password causing Namenode issues","body":"<p>Changing Ranger UI password causes Namenode to stop because it seems that the password needs to be updated in HDFS plugin within HDFS config. However, I am not sure which config needs to be updated in HDFS plugin. Please advise. </p>","tags":["HDFS","Ranger","ui"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-21 22:04:45.0","id":1861,"title":"Can Voltage or Safenet be used as an Alternative Key Mangement Store for Transparent Data Encryption (TDE)","body":"<p>Can I use Voltage or Safenet / Key Secure as the Key Management Solution for the Encrypted Zone Keys needed for Transparent Data Encryption.</p>","tags":["encryption","kms"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-21 20:11:58.0","id":1844,"title":"SSSD Wont Work with AD After Its Installed and Configured","body":"<p>After integrating HDP 2.3.2 with AD, Kerberized it successfully I installed SSSD across all node and applied the configuration below, SSSD is not able to communicate with AD.</p><pre>[sssd]\nconfig_file_version = 2\ndomains = AD-HDP.COM\nservices = nss\n[nss]\nfilter_groups = root\nfilter_users = root\nreconnection_retries = 3\nentry_cache_timeout = 300\nentry_cache_nowait_percentage = 75\ndebug_level = 5\n[domain/AD-HDP.COM]\nid_provider = ldap\nldap_default_bind_dn = CN=adadmin,CN=Rommel_Garcia_Accounts,DC=AD-HDP,DC=COM\nldap_default_authtok_type = password\nldap_default_authtok = ldappw\nauth_provider = none\nmin_id = 1000\nad_server = ad-hdp-com.cloud.hortonworks.com\nldap_uri = ldaps://ad-hdp-com.cloud.hortonworks.com\nldap_schema = ad\nldap_id_mapping = true\ncache_credentials = true\nldap_referrals = false\n</pre><p>When I try to run in any of the HDP node 'su - hr1', hr1 is not recognized as a user but it exists in AD.</p><p>Here's the sssd log entries.</p><p>/var/log/sssd/sssd.log:</p><pre>(Wed Oct 21 08:59:40:805458 2015) [sssd] [get_monitor_config] (0x0010): Invalid service ns\n\n(Wed Oct 21 08:59:40:805637 2015) [sssd] [main] (0x0020): SSSD couldn't load the configuration database.\n</pre><p>/var/log/sssd/sssd_nss.log</p><pre>(Wed Oct 21 09:03:52 2015) [sssd[nss]] [monitor_common_send_id] (0x0100): Sending ID: (nss,1)\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [sss_names_init_from_args] (0x0100): Using re [(?P&lt;name&gt;[^@]+)@?(?P&lt;domain&gt;[^@]*$)].\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [sss_fqnames_init] (0x0100): Using fq format [%1$s@%2$s].\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [dp_common_send_id] (0x0100): Sending ID to DP: (1,NSS)\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [sysdb_domain_init_internal] (0x0200): DB File for AD-HDP.COM: /var/lib/sss/db/cache_AD-HDP.COM.ldb\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [sss_parse_name_for_domains] (0x0200): name 'root' matched without domain, user is root\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [sss_parse_name_for_domains] (0x0200): name 'root' matched without domain, user is root\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [responder_set_fd_limit] (0x0100): Maximum file descriptors set to [8192]\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [sss_names_init_from_args] (0x0100): Using re [(?P&lt;name&gt;[^@]+)@?(?P&lt;domain&gt;[^@]*$)].\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [sss_fqnames_init] (0x0100): Using fq format [%1$s@%2$s].\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [dp_id_callback] (0x0100): Got id ack and version (1) from DP\n\n(Wed Oct 21 09:03:52 2015) [sssd[nss]] [id_callback] (0x0100): Got id ack and version (1) from Monitor\n</pre><p>All of the SSSD services across all nodes runs fine. Do I need to configure SSSD to work with Kerberos? How?</p>","tags":["security","kerberos","sssd"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-22 19:33:25.0","id":1922,"title":"YARN Memory","body":"<p>When a certain amount of memory is given to ResourceManager (Memory allocated for all YARN containers on a node), is it immediately blocked or gradually/progressively used on as-needed basis until that capacity is reached? </p>","tags":["YARN","memory"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-22 23:17:42.0","id":1942,"title":"Spark to Phoenix","body":"<p>Trying to connect spark with phoenix using JDBC. Appended location of phoenix-client.jar to the SPARK_CLASSPATH in spark_env.sh. </p><p>When I launch Spark shell, I get the following errors:</p><pre>&lt;console&gt;:10: error: not found: value sqlContext\n       import sqlContext.implicits._\n              ^\n&lt;console&gt;:10: error: not found: value sqlContext\n       import sqlContext.sql</pre>","tags":["Phoenix","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-10-23 21:38:34.0","id":2035,"title":"ambari-agent failing to come up....","body":"<p>While starting the agent on a new node, I get the following:</p><pre>Checking for previously running Ambari Agent...\nStarting ambari-agent\nVerifying ambari-agent process status...\nERROR: ambari-agent start failed. For more details, see /var/log/ambari-agent/ambari-agent.out:\n====================\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/NetUtil.py\", line 21, in &lt;module&gt;\n    from HeartbeatHandlers import HeartbeatStopHandlers\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/HeartbeatHandlers.py\", line 21, in &lt;module&gt;\n    from ambari_commons.exceptions import FatalException\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/__init__.py\", line 21, in &lt;module&gt;\n    \"\"\"\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_check.py\", line 133, in &lt;module&gt;\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_check.py\", line 115, in __init__\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_check.py\", line 112, in initialize_data\nException: Couldn't load 'os_family.json' file</pre>","tags":["Ambari","ambari-2.1.2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-26 22:27:39.0","id":2156,"title":"How do I configure to not include REALM when logging in to Kerberized machine integrated with AD?","body":"<p>I have SSSD working with AD on a kerberized cluster. When i login as the AD user, it requires me to append the REALM i.e. su - hr1@AD-HDP.COM. I'd like to remove the REALM appended to the username. How do I configure the users that when they login the REALM won't be required? Listed my configurations below.</p><p><strong>[sssd.conf]</strong></p><pre>[sssd]\n\nconfig_file_version = 2\n\ndomains = AD-HDP.COM\n\nservices = nss, pam\n\noverride_space = _\n\ndebug_level = 2\n\n# [nss]: This is where we configure the NSS service\n\n[nss]\n\n# Filter out the users and groups that we don't want Hadoop to see. Not important. But feel free to add more if you like.\n\nfilter_groups = root\n\nfilter_users = root\n\nreconnection_retries = 3\n\nentry_cache_timeout = 300\n\nentry_cache_nowait_percentage = 75\n\n# debug levels 5 to 7 seem to be appropriate while testing. I suggest starting with level five.\n\ndebug_level = 2\n\n[domain/AD-HDP.COM]\n\n# Uncomment if you need offline logins\n\n# cache_credentials = true\nenumerate = true\n\nid_provider = ad\n\nauth_provider = ad\n\n#access_provider = ad\n\ndebug_level = 2\n\n# Uncomment if service discovery is not working\n\nad_server = [host_name_taken_out]\n\n# Uncomment if you want to use POSIX UIDs and GIDs set on the AD side\n\n# ldap_id_mapping = False\n\n# Comment out if the users have the shell and home dir set on the AD side\n\ndefault_shell = /bin/bash\n\nfallback_homedir = /home/%d/%u\n\n# Uncomment and adjust if the default principal SHORTNAME$@REALM is not available\n\n# ldap_sasl_authid = host/client.ad.example.com@AD.EXAMPLE.COM\n\n# Comment out if you prefer to user shortnames.\n\nuse_fully_qualified_names = true\n</pre><p><strong>[nsswitch.conf]</strong></p><pre>#\n\n# /etc/nsswitch.conf\n\n#\n\n# An example Name Service Switch config file. This file should be\n\n# sorted with the most-used services at the beginning.\n\n#\n\n# The entry '[NOTFOUND=return]' means that the search for an\n\n# entry should stop if the search in the previous entry turned\n\n# up nothing. Note that if the search failed due to some other reason\n\n# (like no NIS server responding) then the search continues with the\n\n# next entry.\n\n#\n\n# Valid entries include:\n\n#\n\n#\tnisplus\t\t\tUse NIS+ (NIS version 3)\n\n#\tnis\t\t\tUse NIS (NIS version 2), also called YP\n\n#\tdns\t\t\tUse DNS (Domain Name Service)\n\n#\tfiles\t\t\tUse the local files\n\n#\tdb\t\t\tUse the local database (.db) files\n\n#\tcompat\t\t\tUse NIS on compat mode\n\n#\thesiod\t\t\tUse Hesiod for user lookups\n\n#\t[NOTFOUND=return]\tStop searching if not found so far\n\n#\n\n\n\n\n# To use db, put the \"db\" in front of \"files\" for entries you want to be\n\n# looked up first in the databases\n\n#\n\n# Example:\n\n#passwd:    db files nisplus nis\n\n#shadow:    db files nisplus nis\n\n#group:     db files nisplus nis\n\n\n\n\npasswd:     files sss\n\nshadow:     files sss\n\ngroup:      files sss\n\n\n\n\n#hosts:     db files nisplus nis dns\n\nhosts:      files dns\n\n\n\n\n# Example - obey only what nisplus tells us...\n\n#services:   nisplus [NOTFOUND=return] files\n\n#networks:   nisplus [NOTFOUND=return] files\n\n#protocols:  nisplus [NOTFOUND=return] files\n\n#rpc:        nisplus [NOTFOUND=return] files\n\n#ethers:     nisplus [NOTFOUND=return] files\n\n#netmasks:   nisplus [NOTFOUND=return] files     \n\n\n\n\nbootparams: nisplus [NOTFOUND=return] files\n\n\n\n\nethers:     files\n\nnetmasks:   files\n\nnetworks:   files\n\nprotocols:  files\n\nrpc:        files\n\nservices:   files sss\n\n\n\n\nnetgroup:   files sss\n\n\n\n\npublickey:  nisplus\n\n\n\n\nautomount:  files sss\n\naliases:    files nisplus\n</pre>","tags":["active-directory","kerberos","security","sssd"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-26 16:42:06.0","id":2125,"title":"query YARN application via REST","body":"<p>Customer is using following REST call to obtain a list of app ids: </p><p>http://&lt;rm http address:port&gt;/ws/v1/cluster/apps</p><p>Their question is how frequently should they make REST API call so that they do not miss any jobs submitted? -- Are there any recommendations on this? </p><p>Thanks </p>","tags":["resource-manager","api","YARN"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-27 20:48:17.0","id":2230,"title":"Recommended size for yarn.nodemanager.resource.local-dirs?","body":"<p>Folks,</p><p>What is the recommended value for \"yarn.nodemanager.resource.local-dirs\"?</p><p>We only have one value (directory) configured for the above property, which has a size of 200GB.</p><p>Our hive jobs' map/reduce fill this folder up, and yarn places this node in the blocklist. Moving to tez engine and/or increasing the quota size may fix this, but we'd like to know the recommended value.</p>","tags":["YARN","Hive","MapReduce"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-28 06:44:41.0","id":2281,"title":"Does upgrading HDP delete Tez history?","body":"<p>After upgrading Ambari form 1.6 to 2.1.2 and HDP from 2.1.7 to 2.2.4.8, Tez View doesn't show old history (shows jobs which were created after upgrade).</p><p>Is this by-design?</p><p>Tez Version:</p><pre>rpm -qa | grep -i tez \ntez_2_2_4_8_24-0.5.2.2.2.4.8-24.el6.noarch</pre><p>Is this something to do with the new property \"yarn.timeline-service.leveldb-state-store.path\"?</p>","tags":["spark-history-server","ambari-views","Tez","jobs"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-30 12:12:23.0","id":36395,"title":"How many nodes cluster we need to run in HDPCA exam ?","body":"","tags":["hdpca"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-31 13:24:01.0","id":36676,"title":"ToDate java.lang.IllegalArgumentException","body":"<p>Running HDP 2.4 Sandbox, I'm running into an java.lang.IllegalArgumentException while using a ToDate on a chararray argument. The ToDate(DT_HR_VAL, 'yyyyMMddHHmmss') AS DTHR instruction works in another script that has identically formed datetime stamps like 20131231234559 so I am really puzzled...</p><p>1) My input data looks like this:</p><p>ID_VAL,DT_HR_VAL,ID_TICKET,LOCATION,LINE\nA1B2D3C4,20131231234559,ABCD1234,121,54\nA1B2D3D5,20131231235659,ABCD1234,134,34</p><p>2) My Pig script is the following:</p><p>raw_ticket_validations = LOAD 'RAW_VALIDATIONS.csv' USING PigStorage(',') AS (   ID_VAL:chararray, DT_HR_VAL:chararray, ID_TICKET:chararray, LOCATION:chararray,  LINE:chararray); \nDUMP raw_ticket_validations;\nvalidations = FOREACH raw_ticket_validations GENERATE    ID_VAL, ToDate(DT_HR_VAL, 'yyyyMMddHHmmss') AS DTHR, ID_TICKET,  LOCATION, LINE;</p><p>3) The error I'm getting is the following:</p><p>2016-05-31 13:02:48,819 [PigTezLauncher-0] INFO  org.apache.pig.backend.hadoop.executionengine.tez.TezJob - DAG Status: status=FAILED, progress=TotalTasks: 1 Succeeded: 0 Running: 0 Failed: 1 Killed: 0 FailedTaskAttempts: 4, diagnostics=Vertex failed, vertexName=scope-39, vertexId=vertex_1464401179728_0022_2_00, diagnostics=[Task failed, taskId=task_1464401179728_0022_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: validations: Store(hdfs://sandbox.hortonworks.com:8020/tmp/temp1416169477/tmp145896623:org.apache.pig.impl.io.InterStorage) - scope-38 Operator Key: scope-38): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.builtin.ToDate2ARGS)[datetime] - scope-26 Operator Key: scope-26) children: null at []]: java.lang.IllegalArgumentException: Invalid format: \"DT_HR_VAL\"\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:316)\n   at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:119)\n   at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:332)\n   at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:197)\n   at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)\n   at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181)\n   at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)\n   at java.security.AccessController.doPrivileged(Native Method)\n   at javax.security.auth.Subject.doAs(Subject.java:415)\n   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n   at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:172)\n   at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:168)\n   at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\n   at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n   at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.builtin.ToDate2ARGS)[datetime] - scope-26 Operator Key: scope-26) children: null at []]: java.lang.IllegalArgumentException: Invalid format: \"DT_HR_VAL\"\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:366)\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:400)\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:317)\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)\n   ... 16 more\nCaused by: java.lang.IllegalArgumentException: Invalid format: \"DT_HR_VAL\"\n   at org.joda.time.format.DateTimeFormatter.parseDateTime(DateTimeFormatter.java:945)\n   at org.apache.pig.builtin.ToDate2ARGS.exec(ToDate2ARGS.java:45)\n   at org.apache.pig.builtin.ToDate2ARGS.exec(ToDate2ARGS.java:33)\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:326)\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextDateTime(POUserFunc.java:416)\n   at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:357)\n   ... 19 more</p><p>Any help or insight appreciated.</p><p>MT</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-01 17:51:27.0","id":37102,"title":"How to pull  data out of HAWQ for use in Live wed D3 dashboard?","body":"<p>@allaboutbdata</p><p>@abawja</p><p>I have followed this post to  setup  single node  HDP HAWQ cluster setup. Now that after all processing is done I have stored final  results in HAWQ table that directly uses the HBASE table as External Table. </p><p>With the final  results i want to create a live D3 dashboard using DC.js and leaflet. Therefore I am looking for a connector api to expose the HAWQ table data to D3 preferable in JSON format. I know I can write the connector using Ajax and Javascript, but I am hoping if someone knows a prebuilt method to  do  this. I am bit reluctant to  reinvent the wheel. :P</p><p>Thanks,</p><p>Raghvendra Singh</p>","tags":["hawq"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-03 04:55:44.0","id":37453,"title":"Hi, I have a query in Spark.How to set up email notification when yarn job failed?. Which REST API i can use?. Thanks.","body":"<p>I have a query in Spark.How to set up email notification when yarn job failed?. Which REST API i can use?. Thanks.</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-03 12:56:28.0","id":37553,"title":"Yarn application logs error","body":"<p>In our customer env we are using kerberos and kinit'ed as AD user and Yarn Teragen job fails with PERMISSION issues. We see the yarn application Id in RM UI but when we try to access the Application logs via UI or via yarn logs -applicationId we do not see any contents. We have log aggregation enabled. Also further checking /app-logs/&lt;user&gt;/logs/applicationId directory we do not see the newly submitted application Id. Any help would be appreciated.</p>","tags":["YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-03 18:43:27.0","id":37632,"title":"Importing table structure changes from MySQL to Hive","body":"<p>I'm using Sqoop import to pull data from MySQL (source) to Hive (destination). I'm running into issue when the table structure is altered (datatype changes, columns added/deleted, etc...) at the source. The Sqoop incremental import works fine and the data shows up in the target directory but the hive load does not gets all the newly added columns. </p><p>Do we need to manually change the table structure at the destination to ensure the Hive data load successfully uploads the structure and data changes or can it be automated to ensure that the table structure and data shows in Hive. The idea is to reduce the administrative overheads and reduce the impact to end users especially where there are multiple sources.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-16 00:29:21.0","id":40047,"title":"Kafka CSV Metrics is not getting enabled","body":"<p>I am getting following error while configuring KafkaCSVMetrics</p><p>Config file:</p><p>auto.create.topics.enable=true\nauto.leader.rebalance.enable=true\ncompression.type=producer\ncontrolled.shutdown.enable=true\ncontrolled.shutdown.max.retries=3\ncontrolled.shutdown.retry.backoff.ms=5000\ncontroller.message.queue.size=10\ncontroller.socket.timeout.ms=30000\ndefault.replication.factor=1\ndelete.topic.enable=false\nexternal.kafka.metrics.exclude.prefix=kafka.network.RequestMetrics,kafka.server.DelayedOperationPurgatory,kafka.server.BrokerTopicMetrics.BytesRejectedPerSec\nexternal.kafka.metrics.include.prefix=kafka.network.RequestMetrics.ResponseQueueTimeMs.request.OffsetCommit.98percentile,kafka.network.RequestMetrics.ResponseQueueTimeMs.request.Offsets.95percentile,kafka.network.RequestMetrics.ResponseSendTimeMs.request.Fetch.95percentile,kafka.network.RequestMetrics.RequestsPerSec.request\nfetch.purgatory.purge.interval.requests=10000\nkafka.csv.metrics.dir=/var/log/kafka/kafka_metrics\nkafka.csv.metrics.reporter.enabled=true\nkafka.ganglia.metrics.group=kafka\nkafka.ganglia.metrics.host=localhost\nkafka.ganglia.metrics.port=8671\nkafka.ganglia.metrics.reporter.enabled=true\nkafka.metrics.reporters=kafka.metrics.KafkaCSVMetricsReporter\nkafka.timeline.metrics.host=storm-master.plantronics.com\nkafka.timeline.metrics.maxRowCacheSize=10000\nkafka.timeline.metrics.port=6188\nkafka.timeline.metrics.reporter.enabled=true\nkafka.timeline.metrics.reporter.sendInterval=5900\nleader.imbalance.check.interval.seconds=300\nleader.imbalance.per.broker.percentage=10\nlisteners=PLAINTEXT://kafka.plantronics.com:6667\nlog.cleanup.interval.mins=10\nlog.dirs=/data/kafka-logs\nlog.index.interval.bytes=4096\nlog.index.size.max.bytes=10485760\nlog.retention.bytes=-1\nlog.retention.hours=168\nlog.roll.hours=168\nlog.segment.bytes=1073741824\nmessage.max.bytes=1000000\nmin.insync.replicas=1\nnum.io.threads=8\nnum.network.threads=3\nnum.partitions=1\nnum.recovery.threads.per.data.dir=1\nnum.replica.fetchers=1\noffset.metadata.max.bytes=4096\noffsets.commit.required.acks=-1\noffsets.commit.timeout.ms=5000\noffsets.load.buffer.size=5242880\noffsets.retention.check.interval.ms=600000\n\"</p><p>Exception:</p><p>java.io.IOException: Unable to create /var/log/kafka/kafka_metrics/LogStartOffset.csv\n        at com.yammer.metrics.reporting.CsvReporter.createStreamForMetric(CsvReporter.java:141)\n        at com.yammer.metrics.reporting.CsvReporter.getPrintStream(CsvReporter.java:257)\n        at com.yammer.metrics.reporting.CsvReporter.access$000(CsvReporter.java:22)\n        at com.yammer.metrics.reporting.CsvReporter$1.getStream(CsvReporter.java:156)\n        at com.yammer.metrics.reporting.CsvReporter.processGauge(CsvReporter.java:229)\n        at com.yammer.metrics.reporting.CsvReporter.processGauge(CsvReporter.java:22)\n        at com.yammer.metrics.core.Gauge.processWith(Gauge.java:28)\n        at com.yammer.metrics.reporting.CsvReporter.run(CsvReporter.java:163)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-16 07:35:34.0","id":40077,"title":"hive very big table","body":"<p>Hi:</p><p>I have a hive table that contain 51.639.071  millions on row, and the easy query is very slow, I use ORC fomart ZLIB and bucket but still slow, any suggestion??</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-28 13:07:07.0","id":354,"title":"Hive on Avro - Reading 0 byte file throws IOException: Not a data file.","body":"<p>When using Hive (v.14) on Avro, org.apache.avro.file.DataFileReader throws java.io.IOException: Not a data file. - when encountering a 0 byte file.  This 0 byte file is the result of file rotation during Storm bolt writes to HDFS. </p><p>\"This issue is that org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader creates a new org.apache.avro.file.DataFileReader and DataFileReader throws an exception when trying to read an empty file (because the empty file lacks the magic number marking it as avro).  It seems like it be straight forward to modify AvroGenericRecordReader to detect an empty file and then behave sensibly.  For example, next() would always return false; getPos() would return zero, etc.\"</p><p>Is alterting AvroGenericRecordReader feasible here? </p><p>Kris</p>","tags":["Hive","avro"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-25 18:24:32.0","id":305,"title":"How to promote a principal in IPA to a user for a use case with Ranger, KMS, TDE and HBase?","body":"<p>Recently use TDE to encrypt an HBase installation and found some interesting request for Key access by the Region Servers.</p><p>Out of the box, we locked down the Key permissions to allow only the \"hbase\" user, since this was the user accessing the files by way of the Region Servers.  During normal operations, we saw additional requests from the \"nn\" user and later from \"hdfs\".</p><p>Well, \"hdfs\" is a user, that's fine.  But \"nn\" is not.  \"nn\" was setup as a principal per host for Kerberos (in IPA).</p><p>We got around this by actually creating an \"nn\" user in IPA and granting them rights to the Key in Ranger KMS.  Was that the best way?</p><p>And I'm a little curious \"how\" the \"nn\" principal expressed itself as a user in hdfs operations.</p>","tags":["encryption","security","Hbase","HDFS","kms"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 12:38:01.0","id":455,"title":"Does anyone have example code of how to use the MR input format to call a python script?","body":"","tags":["java","python","MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-29 00:49:28.0","id":390,"title":"HBase ODBC Driver","body":"<p>Does HBase and Phoenix have a supported ODBC driver that we can use to connect Tableau to HBase?</p><p>Simba has a driver, however its not a free driver: <a href=\"http://www.simba.com/connectors/apache-hbase-odbc\">apache-hbase-odbc</a></p>","tags":["odbc","Hbase","tableau"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-29 01:02:18.0","id":399,"title":"How to move Hue Database from default Sqlite database to MySQL/Postgres?","body":"","tags":["upgrade","hue","hadoop","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 18:24:35.0","id":532,"title":"In Ambari, the Quick Links for the service UI pages are returning a blank page with an address of about:blank.  What is going on?","body":"<p>Manually entering a service URL in the browser will render the correct page.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-01 15:28:34.0","id":694,"title":"Can I disable the default Ambari and Ranger admin user accounts?","body":"<p>I would like to disable the default Ranger and Ambari admin accounts after I have integrated with LDAP and assigned AD users to be admins. Is this possible? The UI does not seem to allow me disable these accounts even if there are other admins identified.</p>","tags":["Ranger","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-30 15:28:18.0","id":629,"title":"Kerberos is adding control (hidden) characters and a CNF (objectGUID) to the CN","body":"<p>Trying to delete old Principals throws the following ERROR: </p><pre>\n2015-09-29 09:55:41,330 - Failed to remove identity for HTTP/somenode.mycompany.com@MYCOMPANY.COM from the KDC - Can not remove principal HTTP/somenode.mycompany.com@MYCOMPANY.COM: [LDAP: error code 1 - 000020D6: SvcErr: DSID-031007DB, problem 5012 (DIR_ERROR), data 0 \n] \n..(bunch of these) \nStatus: \n2015-09-29 09:55:40,851 - Processing identities... \n2015-09-29 09:55:41,268 - Destroying identity, HTTP/somenode.mycompany.com@MYCOMPANY.COM </pre><p>Checking the AD, the principals that I tried to delete were still there, so obviously failed to be removed. \nHowever, in AD, it looks like for the bad principals, there is an additional CNF field and a control hidden character: </p><pre>distinguishedNameDN1CN=nm/somenode.mycompany.com\\0ACNF:0158d56b-6e58-48f8-adf3-3429f820e6c5,OU=Hadoop,OU=DataCenter,OU=ServersV2,DC=mycompany,DC=com </pre><p>\nThis CNF field is the objectGUID. \nIs it normal to have an embedded CNF field (with a hidden character) in the CN? \nThank you, </p>","tags":["active-directory","kerberos","Ambari"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-02 16:22:48.0","id":760,"title":"What are the steps for installing a kerberized Kafka cluster?","body":"","tags":["kerberos","Kafka","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 18:16:13.0","id":531,"title":"Ambari Alerts still show after node downscaled by Cloudbreak?","body":"<p>I have a customer that is getting persistant alerts for machines that cloudbreak has down scaled.  They want to know why cloudbreak does not handle removing the node from the alerts.</p><p>I set up a time based autoscale to reduce node slave node count by 1. After the node is removed Ambari continues to alert on HBase Region server missing and Data Node health.</p>","tags":["Ambari","Cloudbreak","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-02 20:45:04.0","id":801,"title":"Oozie Hive action hive-site.xml permission denied","body":"<p>Oozie Hive Action throwing an error </p><pre>Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.HiveMain], main() threw exception, hive-site.xml (Permission denied)\njava.io.FileNotFoundException: hive-site.xml (Permission denied)</pre>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-02 20:35:01.0","id":799,"title":"Hbase- Eliminating empty regions with an export-import approach","body":"<p>We are trying to reduce the number of empty regions in a table (informs_search). This table has around 5900 regions (includes thousands of empty regions) and 8TB worth data.</p><p>With an export – import approach on a sample data (16,819,569 rows).</p><p><strong>Backup informs serach</strong></p><p>disable 'informs_search'</p><p>snapshot 'informs_search', 'informs_search_snpsht'</p><p>clone_snapshot 'informs_search_snpsht', 'informs_search_backup'</p><p>delete_snapshot 'informs_search_snpsht'</p><p>enable ‘informs_search’</p><p><strong>Export informs_search</strong></p><p>/usr/hdp/current/hbase-client/bin/hbase org.apache.hadoop.hbase.mapreduce.Export 'informs_search' /db/support/hexport/inform_search_bk  1 1 1443738964000</p><p><strong>Truncate informs search</strong></p><p>truncate ‘informs_search’</p><p><strong>Import informs_search</strong></p><p>hbase org.apache.hadoop.hbase.mapreduce.Import 'informs_search' /db/support/hexport/inform_search_bk</p><p>Observations:-</p><ul>\n<li>Before we ran these steps , we had 9 regions (6+3) across two region servers</li><li>After we ran these steps, we have 2 regions across 1 Region server</li></ul><p>----------------------------------------------------------------------------------------------------------------------------------</p><p>* In Production, after running the same, would that reduce to 2 regions as well?</p><p>* IS there anyway to predict/configure the resultant number of regions and regions servers?</p><p>* Also, how many major compactions will it take so that data will be distributed across the region servers (and regions)?</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-04 18:57:34.0","id":856,"title":"Issue with Sqoop","body":"<p>Hi,\nI took the HDPCD practice test on AWS but I am facing few problems with sqoop.</p><p>For the Task 10, i used “sqoop export –connect jdbc:mysql://namenode:3306/flightinfo –table weather –export-dir /user/horton/weather –input-fields-terminated-by ‘,’ –username root –password hadoop” and got the following:</p><p>Warning: /usr/hdp/2.2.0.0-2041/sqoop/sqoop/bin/../../hcatalog does not exist! HCatalog jobs will fail.\nPlease set $HCAT_HOME to the root of your HCatalog installation.\n15/10/03 18:46:56 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5.2.2.0.0-2041\n15/10/03 18:46:56 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n15/10/03 18:46:56 INFO manager.SqlManager: Using default fetchSize of 1000\n15/10/03 18:46:56 INFO tool.CodeGenTool: Beginning code generation\n15/10/03 18:46:57 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Access denied for user ‘root’@’%’ to database ‘flightinfo’\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Access denied for user ‘root’@’%’ to database ‘flightinfo’</p><p>And also, I am not able to copy the pig scripts into the solutions folder as mentioned.</p><p>cp -f flightdelays_clean.pig /home/horton/solutions/\ncp: cannot create regular file ‘/home/horton/solutions/flightdelays_clean.pig’: Permission denied</p><p>Am I missing something? Please help.</p>","tags":["security","Sqoop","Pig"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-05 15:04:06.0","id":888,"title":"Eclipse to Sandbox","body":"<p>Hi,</p><p>I am new to HDP and hadoop.I managed to install HDP 2.3 sandbox on Virtual box.</p><p>I tried a few sample programs and they are working fine from the sandbox.</p><p>I have installed Eclipse with Scala in my Windows machine.</p><p>At present ,I use SBT and package my application and deploy the jar in the HDP Sandbox for execution.</p><p>I would like to execute programs from my Eclipse against the HDP sandbox directly instead of packaging it each and every time.</p><p>A sample code which I am trying to modify</p><p>val conf = new SparkConf().setAppName(“Simple Application”).setMaster(“local[2]”).set(“spark.executor.memory”,”1g”)</p><p>I guess , I have to change the local[2] to the master node / yarn cluster url.</p><p>How do I get the url from the sandbox ?</p><p>Any other configurations which has to be done on the Virtual box or on my code ?</p>","tags":["development","eclipse","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-10-07 13:24:19.0","id":1055,"title":"What's the way to change service accounts and group name after the install? Ambari 2.1.2","body":"","tags":["service-account","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-06 09:47:36.0","id":951,"title":"Ingestion : How to ingest data from Oracle Database to Kafka  ?","body":"<p>Customer wants to ingest data from Oracle Database to Kafka. it appears that sqoop2 is supported to ingest the data to Kafka. Since we dont have sqoop2 support yet. Kafka customer is looking for using logstash to ingest the data. Is there any other better options available ?</p>","tags":["data-ingestion","Sqoop","logstash","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-07 00:07:37.0","id":1028,"title":"performance impact of wire encryption and TDE?","body":"<p>How much impact on performance is expected if we do both end to end wire encryption and TDE?</p>","tags":["encryption","performance","wire-encryption"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-07 13:12:10.0","id":1050,"title":"How to view Alert History via Ambari ?","body":"<p>I want to view how many times Kafka died in the last 10 days . Is there any ways to view it from Ambari ?</p>","tags":["alerts","Ambari","operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-07 13:01:23.0","id":1047,"title":"What type of disk (RAID 1, RAID 0, etc) should be used for the following yarn directories:","body":"","tags":["YARN","operations","best-practices","raid"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-09 20:09:20.0","id":1243,"title":"Decommission a regionserver, got complaint \"hbase_client_jaas\" missing","body":"<p>Regionserver is hanging on \"decommissioning\", Any idea?</p>","tags":["regionserver","Ambari","Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-09 11:13:08.0","id":1187,"title":"Rewrite of Absolute URL in Knox?","body":"<p>I am trying to define a rewrite pattern in Knox that would rewrite a URL like https://sandbox:8443/myLogo.png to https://sandbox:8443/gateway/default/myservice/myLogo.png. Is that even possible? An example would be helpful.</p>","tags":["Knox","rewrite","examples"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-10 00:06:06.0","id":1252,"title":"What is the format of the (IDs to Follow) field in the Nifi GetTwitter processor ?","body":"<p>Tried both using the Twitter username and @username notation but Nifi errors out with \"invalid because Must be comma separated list of user IDs\"</p>","tags":["Nifi","twitter"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-08 16:51:24.0","id":1134,"title":"Ambari keeps old version as current after rolling upgrade 100%","body":"<p>Rolling upgrade is 100%, but Ambari still keeps the older version as current, and new version as \"upgrade in progress\", any idea?\nit is Ambari 2.1.1, upgrading from HDP 2.2.0 to 2.2.8.</p>","tags":["upgrade","operations","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-21 13:59:09.0","id":1783,"title":"Does Knox support active directory searches using nested OUs and/or multiple AD search strings?","body":"<p>1.)Does Knox support active directory searches using nested OUs? I’m reading in some of the documentation that it does not. The main.ldapRealm.userDnTemplate value we are trying to use is <strong><em>samaccountname={0},ou=corp,ou=associates,OU=MY_COMPANY Accounts,DC=amer,DC=qa_my_company,DC=com </em></strong>but the users are not being found.</p><p>2.)Does Knox support multiple AD search strings? Not all users that need access to Knox protected services can be found using the single search string above.</p><p>Would these require multiple Knox Topology files to be applied at once?</p>","tags":["active-directory","Knox","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-21 15:19:00.0","id":1803,"title":"Implementing custom grouping on Storm throws an error","body":"<p>I'm trying to implement custom grouping feature as seen in OpenSOC for HBaseStreamPartitioner</p><p>I'm declaring it like so</p><pre>declarer.customGrouping(\n                        MAPPER_BOLT_ID,\n                        new HBaseStreamPartitioner(\"clicks_tbl\", 0, 60, zkQuorum, zkParent, zkPort));</pre><p>What would cause the error below? What is the default stream id when you implement a custom grouping? My HBase bolt is 2nd bolt in chain after a mapper bolt that just deserializes byte array stream from Kafka to parsed values.</p><pre>2015-10-21 11:06:56.829 b.s.d.executor [ERROR]\njava.lang.RuntimeException: java.lang.IllegalArgumentException: No matching clause: default\nat backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.daemon.executor$fn__6214$fn__6227$fn__6278.invoke(executor.clj:808) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.util$async_loop$fn__543.invoke(util.clj:475) [storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]\nat java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]\nCaused by: java.lang.IllegalArgumentException: No matching clause: default\nat backtype.storm.daemon.acker$mk_acker_bolt$reify__1380.execute(acker.clj:59) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.daemon.acker$_execute.invoke(acker.clj:101) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.daemon.acker.execute(Unknown Source) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.daemon.executor$fn__6214$tuple_action_fn__6216.invoke(executor.clj:670) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.daemon.executor$mk_task_receiver$fn__6137.invoke(executor.clj:426) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.disruptor$clojure_handler$reify__5713.onEvent(disruptor.clj:58) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]\nat backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:120) ~[storm-core-0.10.0.2.3.2.0-2950.jar:0.10.0.2.3.2.0-2950]</pre>","tags":["Storm","Hbase","OpenSoc"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-23 15:15:45.0","id":1999,"title":"​Anyone seen something like a ‘calculator’ for figuring out the approximate time for a distcp?","body":"<p>I don’t think it is just   #bytes/bandwidth     because of multiple mappers. Is it just bytes per DN /  (bandwidth - overhead) ?  I suspect this is still too simple...?</p>","tags":["distcp"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-28 21:58:50.0","id":2327,"title":"Sample Java code for working with HDP components on secured cluster","body":"<p>Looking for input from the field teams on <em>sample code</em> (preferably Java) for interacting with HDP components on kerberos enabled cluster</p><ol><li>Kafka</li><li>Storm</li><li>HBase</li><li>Solr</li><li>Hive</li><li>Knox</li><li>YARN</li><li>HDFS</li><li>...</li></ol><p>Here are some of the code resources I have found so far:</p><ul><li>Hive code: <a href=\"http://community.hortonworks.com/questions/1807/connecting-to-kerberos-enabled-hive-via-jdbc-direc.html\">http://community.hortonworks.com/questions/1807/connecting-to-kerberos-enabled-hive-via-jdbc-direc.html </a></li><li>HBase code: <a href=\"http://community.hortonworks.com/articles/1452/sample-application-to-write-to-a-kerberised-hbase.html\">http://community.hortonworks.com/articles/1452/sample-application-to-write-to-a-kerberised-hbase.html</a> </li></ul><p>Found other resources but they are around configuration of components on kerborized env:</p><ul><li>Kafka config: <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_secure-kafka-ambari/content/ch_secure-kafka-overview.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_secure-kafka-ambari/content/ch_secure-kafka-overview.html</a></li><li>Storm config: <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_secure-storm-ambari/content/ch_secure-storm-overview.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_secure-storm-ambari/content/ch_secure-storm-overview.html</a></li><li><a href=\"http://community.hortonworks.com/questions/721/acessing-the-storm-ui-with-hdp-23-kerberized-clust.html\">http://community.hortonworks.com/questions/721/acessing-the-storm-ui-with-hdp-23-kerberized-clust.html</a></li><li>Solr config: <a href=\"https://cwiki.apache.org/confluence/display/solr/Kerberos+Authentication+Plugin\">https://cwiki.apache.org/confluence/display/solr/Kerberos+Authentication+Plugin</a></li></ul><p>If you have any useful resources, please post here so we can have a consolidated list </p>","tags":["hdp-2.3.4","kerberos","code"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-28 14:56:38.0","id":2299,"title":"How to Integrate IBM MQ with Storm?","body":"<p>What is recommended way to connect IBM MQ with Storm. Is this supported ?</p>","tags":["Storm","ibm","mq","streaming"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-27 13:25:30.0","id":36077,"title":"HBase master audit logging failure","body":"<p>I have configured HBase on HDP 2.3 to use HDFS for logging.  The regionservers are correctly logging to files, but the master is not.  Instead, the logs are filling with messages in this form:</p><p>2016-05-27 09:23:04,020 INFO  [hbaseMaster.async.summary.batch_hbaseMaster.async.summary.batch.hdfs_destWriter] queue.AuditFileSpool: Destination is down. sleeping for 30000 milli seconds. indexQueue=0, queueName=hbaseMaster.async.summary.batch, consumer=hbaseMaster.async.summary.batch.hdfs</p><p>I have restarted the master after configuration change.  Why would the regionservers be happy with the audit destination and the master fail?  They are reading from the same file, are they not?</p>","tags":["Hbase","Ranger"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-27 12:14:16.0","id":36059,"title":"Error: E0701 : E0701: XML schema error, cvc-complex-type.2.4.c: The matching wildcard is strict, but no declaration can be found for element 'sqoop'.? how can i resolve this error please help me???","body":"","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-05-29 06:07:57.0","id":36287,"title":"resource_management.core.exceptions.Fail: Execution of 'conf-select set-conf-dir --package zookeeper --stack-version 2.4.0.0-169 --conf-version 0' returned 1. /usr/hdp/2.4.0.0-169/zookeeper/conf does not exist","body":"<p>i have the problem </p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper_server.py\", line 179, in &lt;module&gt;\n    ZookeeperServer().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper_server.py\", line 70, in install\n    self.configure(env)\n  File \"/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper_server.py\", line 49, in configure\n    zookeeper(type='server', upgrade_type=upgrade_type)\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py\", line 89, in thunk\n    return fn(*args, **kwargs)\n  File \"/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper.py\", line 40, in zookeeper\n    conf_select.select(params.stack_name, \"zookeeper\", params.current_version)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/conf_select.py\", line 252, in select\n    shell.checked_call(get_cmd(\"set-conf-dir\", package, version), logoutput=False, quiet=False, sudo=True)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'conf-select set-conf-dir --package zookeeper --stack-version 2.4.0.0-169 --conf-version 0' returned 1. /usr/hdp/2.4.0.0-169/zookeeper/conf does not exist</pre><p>but, i have files</p><p><img src=\"/storage/attachments/4639-qq图片20160529135149.png\"></p><p>some guys can help me please!</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-30 07:36:31.0","id":36351,"title":"Kafka Error: Could not find or load main class kafka.Kafka . Please help","body":"","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-30 09:27:45.0","id":36355,"title":"AWS Multi AZ hadoop cluster issue","body":"<p>I have created a hadoop cluster.My cluster 1 Master NN lies in 1a and  Standby in 1b zone.My security group allows all communications.I have 5 DN.3 DN in 1a and 2 DN in 1b.</p><p>The issue i am facing is when my namenode 1a is active i only see 1a 3 datanodes instances,when my 1b namenode is active i only see 1b 2 datanode as active(live).</p>","tags":["aws"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-30 13:23:29.0","id":36417,"title":"NiFi queue how much time will hold the data, i saw the nifi.queue.swap.threshold=20000 property but what it does this property?","body":"","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-06 01:55:16.0","id":37770,"title":"Technical Approach (POV) to be followed in moving on premise HDP solution to Azure HDInsigh","body":"<p>I am looking for POV/Technical steps/approach to be followed for moving in premise HDP solution to Azure HDInsight</p>","tags":["hdinsight"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-06 23:16:56.0","id":38007,"title":"Hi..Is anything changed on TTL => 'FOREVER' on HDP 2.4?","body":"","tags":["Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-07 10:03:24.0","id":38115,"title":"After installing Hortonworks sandbox 2.4 I can not find out how to restart service like hbase Hdfs from Ambari.","body":"<p>I reset the password with  ambari-admin-password-reset command and tried to login with admin user but not able to login. Got the </p><p>Error on ambari web UI amabari services are not running kindly check whether ambari is running or not.</p><p>I verify  Ambai service is running.</p>","tags":["Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-07 14:38:38.0","id":38210,"title":"Sqoop Connect to DB with Truststore","body":"<p>\n\tHello,</p><p>\n\tI am trying to access a VIrtual Database from JBoss Data Virtualization using Sqoop. This database is defined to connect using a Truststore (JKS format), using another application like Squirrel i can connect using the following argument:</p><pre>\t-Djavax.net.ssl.trustStore=/home/user/truststore-xxxxx.jks</pre><p>Does anyone knows how can I use this argument with Sqoop? </p><p>Ps.: I already deploy my .jar file on $SQOOP_HOME/lib.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-08 15:02:46.0","id":38528,"title":"How can u use an attribute of any flowfile to  other flowfile?","body":"<p>Hi,</p><p>I invoque a service with GETHttp processor . The service return a json data. I use a evaluateJsonPath processor to extract value.  I want update attribute of other flowfile with this value. </p><p>Thanks.</p>","tags":["Nifi","nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-08 17:11:39.0","id":38568,"title":"Questions on waterline?","body":"<p>Hello,</p><p>I have downloaded sandbox for waterline and already gone through tutorial for waterline which are present on hortonworks website.I just have threotical knowladge about waterline and now I am going to explore that tool practically but haven't seen anywhere about </p><p><strong>how to use that sandbox machine?</strong></p><p><strong>What are the credentials for it's ?</strong></p><p><strong>Does waterline will give me hive column and table level lineage?</strong></p><p><strong>Can we implement data classification in waterli ne?</strong></p><p><strong>How to use waterline web ui? Anyone please send me link which explains end to end story  of  waterline?</strong></p>","tags":["waterline"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-30 18:27:05.0","id":2537,"title":"Do Ambari Metrics support snmp for Metric values?","body":"<p>Ambari support SNMP for alerts and notifications but do Ambari Metrics also support SNMP for metric values? </p>","tags":["ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-02 19:31:24.0","id":2629,"title":"Spark Transformations for Multiple RDDs in One Pass","body":"<p>We have a large text file which we want to read in spark , augment each line with some extra information and persist it in HDFS.  <strong>At the same time or in same pass</strong> we want to perform some aggregations across all lines like number of invalid SSNs , Invalid DOB etc.</p><p>For mapreduce it can be achieved with MultiOutPuts saving augmented lines from map task and sending all aggregations from cleanup method in map task to single reducer to perform global aggregations.</p><p>But I need a way in spark to achieve this.</p><p>Accumulators may help but really not sure about their reliability in case of failures or speculative jobs execution? \nIs there a way we can create two different/multiple RDDs from single transformation in spark ? </p><p>Any other way to achieve this in Spark ?</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-03 21:12:43.0","id":2721,"title":"Running a Sqoop script in the background appears to 'hang' but works when running in the foreground","body":"<p>Consdering the following bash script for Sqoop:</p><p>#!/bin/sh </p><p>connection_string=\"jdbc:sqlserver://remoteserver.somehwere.location-asp.com:1433;database=idistrict\" </p><p>user_name=\"OPSUser\" </p><p>db_password=\"OPSReader\" </p><p>sqoop_cmd=\"list-databases\" </p><p>sqoop $sqoop_cmd --connect $connection_string --username $user_name --password $db_password</p><p>We can run this just fine in the foreground, i.e.:</p><p>./sqoop_test.sh</p><p>But running it in the background like so:</p><p>./sqoop_test.sh &</p><p>The script appears to 'hang' when kicking off the actual sqoop command...i.e. nothing happens at all.</p><p>Using -x on the #!/bin/sh line shows that we end up at the last line of the script and then nothing...</p><p>We have tried all kinds of iterations of different commands like:</p><p>nohup bash sqoop.sh &gt; results.txt 2&gt;&1 &</p><p>./sqoop.sh &&gt; /dev/null &</p><p>switched to #!/bin/bash</p><p>Any ideas?  The odd thing is that the same exact script works fine both foregrounded and backgrounded on a different cluster.  /etc/profile, and .bash_profile don't look to have any major differences. </p>","tags":["Sqoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-04 18:24:38.0","id":2802,"title":"YARN Node Labels Not Working ?? Two Issues","body":"<p>Background: I have configured two\nnode labels - HiCPU (exclusive=false) and GPU (exclusive=true). I have attached\nHiCPU to a queue named Engineering with 100% capacity. Label GPU is attached to\na queue named Marketing with 100% capacity. No default label has been\nconfigured for either queue at the beginning of the test.</p><p>Issue 1:</p><p>When I run the following commands as\nthe hdfs user, the command will only run on an unlabeled node, and if no\nunlabeled nodes are available, the job simply hangs:</p><pre>yarn jar\n/usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar -shell_command \"sleep 25\" -jar\n/usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar -num_containers 3 -queue Engineering\nResourceRequest.setNodeLabelExpression HiCPU</pre><p><strong>or</strong></p><pre>yarn jar\n/usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar -shell_command \"sleep 25\" -jar\n/usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-distributedshell.jar -num_containers 3 -queue Marketing\nResourceRequest.setNodeLabelExpression GPU</pre><p>However, if I set the node label as\ndefault, the commands *DO* execute on the appropriate machine, even without the\nResourceRequest.setNodeLabelExpression attribute (as would be expected).</p><p>Bottom line - I can only get node\nlabels to work for a YARN job if they are set as the default, which means\nnon-labeled nodes are not available to that queue any longer for YARN jobs.</p><p>Issue 2: </p><p>Our documentation here:</p><p><a target=\"_blank\" href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_yarn_resource_mgt/content/using_node_labels.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_yarn_resource_mgt/content/using_node_labels.html</a></p><p>...states the following:</p><p>\"...if you submit a MapReduce\njob to a queue that has a default node label expression, the default node label\nwill be applied to the MapReduce job.\"</p><p>To test this, I executed the\nfollowing command using a user who was default-queue-mapped to the Engineering\nqueue:</p><pre>yarn jar \n/usr/hdp/2.3.0.0-2557/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 5 10</pre><p>While the job did get assigned to\nthe correct queue, in no instance could I get the MapReduce job to run on a\nlabeled node. It would run on an unlabeled node if available, and if no\nunlabeled nodes were available, it would just hang.</p><p>Thus, unless I am missing something,\nit appears that the *only* way to get a functional use of node labels is to set\nthem as a default for a queue, in which case YARN jobs assigned to that queue\nwill ONLY run on the labeled nodes (including unlabeled YARN jobs).\nFurthermore, under no circumstance will a MapReduce job run on a labeled node,\nregardless of default node label settings.</p><p>If someone wants to take a look at\nmy settings on my cluster and troubleshoot, let me know. Thanks!</p><p>@Neeraj</p>","tags":["yarn-node-labels","hdp-2.3.0","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-05 16:34:07.0","id":2879,"title":"Bug in Sandbox 2.3.2 hitting SQOOP-1400","body":"<p>Hitting https://issues.apache.org/jira/browse/SQOOP-1400 on Sandbox 2.3.2 when executing</p><p>sqoop export --connect jdbc:mysql://127.0.0.1/export --username hive --password hive --table exported --export-dir /apps/hive/warehouse/export_table</p><p>to overcome the issue add the following</p><p>--driver com.mysql.jdbc.Driver</p><p>mysql-connector-java-5.1.17 needs to be updated</p>","tags":["mysql","Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-06 14:28:09.0","id":2939,"title":"HDP 2.3/Ambari integration with AD managed by Centrify","body":"<p>We need to install a Non-Kerberized HDP 2.3 cluster and below are the requirements:</p><ol><li>There is an existing Active directory maintained by Centrify and all authentication should be done thru that.</li><li>The service users with custom names are already created in AD under custome group name. </li><li>We DON'T want ambari to create any local service accounts during HDP installation and want ambari to refer to AD accounts. The main concern is If ambari creates the service accounts locally, then that might mess up group permissions for the files when tried to login with the AD accounts.</li><li>The requirement is NOT to work with any local accounts and all authentication needs to be done from AD managed by Centrify.</li><li>From install perspective, what needs to be done to achieve this ?</li></ol>","tags":["centrify","Ambari","active-directory","ambari-2.1.2","user-groups"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-18 14:32:08.0","id":4229,"title":"What's the best way to do Monte Carlo simulation on Hadoop","body":"<p>Monte Carlo and is one of many simulation types that execute a huge amount of repetitive tasks that use relatively little data. The \"data\" is usually little more than sets of parameters to a function that must be executed a zillion times. Often this is followed by some kind of summarizing process.  Clearly a custom MR job can be written for this, but is there any kind of standard frameworks that HDP recommends, or a published set of best practices? </p>","tags":["MapReduce","scientific-computing"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-20 12:30:21.0","id":4408,"title":"is there any way to reset ranger admin UI password ?","body":"<p>I have forgotten my ranger UI admin password. could you please help ?</p>","tags":["Ranger","ui","password-reset"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-24 22:27:12.0","id":4744,"title":"Hive increase map join local task memory","body":"<p>Is there a way in HDP &gt;= v2.2.4 to increase the local task memory? I'm aware of disabling/limiting map-only join sizes, but we want to increase, not limit it.</p><p>Depending on the environment, the memory allocation will shift, but it appears to be entirely to Yarn and Hive's discretion.</p><p>\"Starting to launch local task to process map join;maximum memory = <strong>255328256 =&gt; ~ 0.25 GB\"</strong></p><p>I've looked at/tried:</p><ul>\n<li>hive.mapred.local.mem</li><li>hive.mapjoin.localtask.max.memory.usage - this is simply a percentage of the local heap. I want to increase, not limit the mem.\n</li><li>mapreduce.map.memory.mb - only effective for non-local tasks\n</li></ul><p>I found documentation suggesting 'export HADOOP_HEAPSIZE=\"2048\"' to change from the default, but this applied to the nodemanager. </p><p>Any way to configure this on a per-job basis?</p><p>EDIT</p><p>To avoid duplication, the info I'm referencing comes from here: https://support.pivotal.io/hc/en-us/articles/207750748-Unable-to-increase-hive-child-process-max-heap-when-attempting-hash-join</p><p>Sounds like a per-job solution is not currently available with this bug.</p>","tags":["memory","localtask","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-30 08:47:09.0","id":8354,"title":"at which directory ambari installed","body":"<p></p><p>I tried to install HDP using ambari. </p><p>At my computer, <a href=\"http://node1:8080/#/main\">http://node1:8080/#/main</a> works.</p><p>Actually I want to see the real page that the site address works.</p><p>Becuase I have to modify pages.</p><p>At which directory, I can see the pages.</p><p>If anybody knows that, please tell me. Thanks.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-30 09:06:17.0","id":8357,"title":"Server registration failed even with local repositories","body":"<p>One of my server failed to register. I tried even installing with local repositories in my web server, but the error persist. </p><p>This happened on my second try when I almost finish the installation and failed due to Ambary agent not running in this </p><p>server in the installation last stage. </p><p>Here is my entire registration log:</p><pre>==========================\nCreating target directory...\n==========================\n\nCommand start time 2015-12-30 09:46:55\n\nConnection to slave.dev.local closed.\nSSH command execution finished\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:55\n\n==========================\nCopying common functions script...\n==========================\n\nCommand start time 2015-12-30 09:46:55\n\nscp /usr/lib/python2.6/site-packages/ambari_commons\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:56\n\n==========================\nCopying OS type check script...\n==========================\n\nCommand start time 2015-12-30 09:46:56\n\nscp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.py\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:56\n\n==========================\nRunning OS type check...\n==========================\n\nCommand start time 2015-12-30 09:46:56\nCluster primary/cluster OS family is ubuntu14 and local/current OS family is ubuntu14\n\nConnection to slave.dev.local closed.\nSSH command execution finished\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:56\n\n==========================\nChecking 'sudo' package on remote host...\n==========================\n\nCommand start time 2015-12-30 09:46:56\nsudo\t\t\t\t\t\tinstall\n\nConnection to slave.dev.local closed.\nSSH command execution finished\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:57\n\n==========================\nCopying repo file to 'tmp' folder...\n==========================\n\nCommand start time 2015-12-30 09:46:57\n\nscp /etc/apt/sources.list.d/ambari.list\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:57\n\n==========================\nMoving file to repo dir...\n==========================\n\nCommand start time 2015-12-30 09:46:57\n\nConnection to slave.dev.local closed.\nSSH command execution finished\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:57\n\n==========================\nChanging permissions for ambari.repo...\n==========================\n\nCommand start time 2015-12-30 09:46:57\n\nConnection to slave.dev.local closed.\nSSH command execution finished\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:58\n\n==========================\nUpdate apt cache of repository...\n==========================\n\nCommand start time 2015-12-30 09:46:58\n0% [Working]\n            \nErr http://public-repo-1.hortonworks.com Ambari InRelease\n  \n\n0% [Working]\n            \nErr http://public-repo-1.hortonworks.com HDP-UTILS InRelease\n  \n\n            \nErr http://public-repo-1.hortonworks.com HDP InRelease\n  \n\n0% [Working]\n            \nErr http://public-repo-1.hortonworks.com Ambari Release.gpg\n  Could not resolve 'public-repo-1.hortonworks.com'\n\n0% [Working]\n            \nErr http://public-repo-1.hortonworks.com HDP-UTILS Release.gpg\n  Could not resolve 'public-repo-1.hortonworks.com'\n\n0% [Working]\n            \nErr http://public-repo-1.hortonworks.com HDP Release.gpg\n  Could not resolve 'public-repo-1.hortonworks.com'\n\n0% [Working]\n            \n\nReading package lists... 0%\n\nReading package lists... 0%\n\nReading package lists... 0%\n\nReading package lists... 0%\n\nReading package lists... 0%\n\nReading package lists... 0%\n\nReading package lists... 6%\n\nReading package lists... Done\n\nW: Failed to fetch http://public-repo-1.hortonworks.com/ambari/ubuntu14/2.x/updates/2.1.2/dists/Ambari/InRelease  \n\nW: Failed to fetch http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/ubuntu14/dists/HDP-UTILS/InRelease  \n\nW: Failed to fetch http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.4.0/dists/HDP/InRelease  \n\nW: Failed to fetch http://public-repo-1.hortonworks.com/ambari/ubuntu14/2.x/updates/2.1.2/dists/Ambari/Release.gpg  Could not resolve 'public-repo-1.hortonworks.com'\n\nW: Failed to fetch http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/ubuntu14/dists/HDP-UTILS/Release.gpg  Could not resolve 'public-repo-1.hortonworks.com'\n\nW: Failed to fetch http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.4.0/dists/HDP/Release.gpg  Could not resolve 'public-repo-1.hortonworks.com'\n\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nW: Duplicate sources.list entry http://public-repo-1.hortonworks.com/ambari/ubuntu14/2.x/updates/2.1.2/ Ambari/main amd64 Packages (/var/lib/apt/lists/public-repo-1.hortonworks.com_ambari_ubuntu14_2.x_updates_2.1.2_dists_Ambari_main_binary-amd64_Packages)\nW: Duplicate sources.list entry http://public-repo-1.hortonworks.com/ambari/ubuntu14/2.x/updates/2.1.2/ Ambari/main i386 Packages (/var/lib/apt/lists/public-repo-1.hortonworks.com_ambari_ubuntu14_2.x_updates_2.1.2_dists_Ambari_main_binary-i386_Packages)\n\nConnection to slave.dev.local closed.\nSSH command execution finished\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:59\n\n==========================\nCopying setup script file...\n==========================\n\nCommand start time 2015-12-30 09:46:59\n\nscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:46:59\n\n==========================\nRunning setup agent script...\n==========================\n\nCommand start time 2015-12-30 09:46:59\n('INFO 2015-12-30 09:47:13,707 AlertSchedulerHandler.py:323 - [AlertScheduler] Scheduling datanode_process with UUID bb05c01d-a891-4ea2-aca6-282b919dd96b\nINFO 2015-12-30 09:47:13,707 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts\nINFO 2015-12-30 09:47:13,707 AlertSchedulerHandler.py:323 - [AlertScheduler] Scheduling ambari_agent_disk_usage with UUID dedb798d-2ed3-44e3-b8df-97d55eb2d84d\nINFO 2015-12-30 09:47:13,708 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts\nINFO 2015-12-30 09:47:13,708 AlertSchedulerHandler.py:323 - [AlertScheduler] Scheduling zookeeper_server_process with UUID fd4ad98d-35d6-432c-98e3-3a4f5f03a246\nINFO 2015-12-30 09:47:13,708 AlertSchedulerHandler.py:134 - [AlertScheduler] Starting &lt;ambari_agent.apscheduler.scheduler.Scheduler object at 0x7f6305f02cd0&gt;; currently running: False\nINFO 2015-12-30 09:47:13,714 hostname.py:87 - Read public hostname \\'slave.dev.local\\' using socket.getfqdn()\nWARNING 2015-12-30 09:47:13,865 Facter.py:354 - Could not run /usr/sbin/sestatus: OK\nINFO 2015-12-30 09:47:14,100 Controller.py:140 - Registering with slave.dev.local (192.168.0.2) (agent=\\'{\"hardwareProfile\": {\"kernel\": \"Linux\", \"domain\": \"dev.local\", \"physicalprocessorcount\": 2, \"kernelrelease\": \"3.19.0-42-generic\", \"uptime_days\": \"0\", \"memorytotal\": 7918076, \"swapfree\": \"7.75 GB\", \"memorysize\": 7918076, \"osfamily\": \"ubuntu\", \"swapsize\": \"7.75 GB\", \"processorcount\": 2, \"netmask\": \"255.255.255.0\", \"timezone\": \"CET\", \"hardwareisa\": \"x86_64\", \"memoryfree\": 6802196, \"operatingsystem\": \"ubuntu\", \"kernelmajversion\": \"3.19\", \"kernelversion\": \"3.19.0\", \"macaddress\": \"00:19:66:88:A0:DE\", \"operatingsystemrelease\": \"14.04\", \"ipaddress\": \"192.168.0.2\", \"hostname\": \"slave\", \"uptime_hours\": \"0\", \"fqdn\": \"slave.dev.local\", \"id\": \"root\", \"architecture\": \"x86_64\", \"selinux\": false, \"mounts\": [{\"available\": \"3915384\", \"used\": \"4\", \"percent\": \"1%\", \"device\": \"udev\", \"mountpoint\": \"/dev\", \"type\": \"devtmpfs\", \"size\": \"3915388\"}, {\"available\": \"790656\", \"used\": \"1152\", \"percent\": \"1%\", \"device\": \"tmpfs\", \"mountpoint\": \"/run\", \"type\": \"tmpfs\", \"size\": \"791808\"}, {\"available\": \"900199676\", \"used\": \"4656112\", \"percent\": \"1%\", \"device\": \"/dev/sda1\", \"mountpoint\": \"/\", \"type\": \"ext4\", \"size\": \"953303940\"}, {\"available\": \"4\", \"used\": \"0\", \"percent\": \"0%\", \"device\": \"none\", \"mountpoint\": \"/sys/fs/cgroup\", \"type\": \"tmpfs\", \"size\": \"4\"}, {\"available\": \"5120\", \"used\": \"0\", \"percent\": \"0%\", \"device\": \"none\", \"mountpoint\": \"/run/lock\", \"type\": \"tmpfs\", \"size\": \"5120\"}, {\"available\": \"3958884\", \"used\": \"152\", \"percent\": \"1%\", \"device\": \"none\", \"mountpoint\": \"/run/shm\", \"type\": \"tmpfs\", \"size\": \"3959036\"}, {\"available\": \"102368\", \"used\": \"32\", \"percent\": \"1%\", \"device\": \"none\", \"mountpoint\": \"/run/user\", \"type\": \"tmpfs\", \"size\": \"102400\"}], \"hardwaremodel\": \"x86_64\", \"uptime_seconds\": \"1651\", \"interfaces\": \"eth0,lo\"}, \"currentPingPort\": 8670, \"prefix\": \"/var/lib/ambari-agent/data\", \"agentVersion\": \"2.1.2\", \"agentEnv\": {\"transparentHugePage\": \"\", \"hostHealth\": {\"agentTimeStampAtReporting\": 1451465234096, \"activeJavaProcs\": [], \"liveServices\": [{\"status\": \"Healthy\", \"name\": \"ntp\", \"desc\": \"\"}]}, \"reverseLookup\": true, \"alternatives\": [], \"umask\": \"18\", \"firewallName\": \"ufw\", \"stackFoldersAndFiles\": [{\"type\": \"directory\", \"name\": \"/etc/hadoop\"}], \"existingUsers\": [{\"status\": \"Available\", \"name\": \"mapred\", \"homeDir\": \"/home/mapred\"}, {\"status\": \"Available\", \"name\": \"ambari-qa\", \"homeDir\": \"/home/ambari-qa\"}, {\"status\": \"Available\", \"name\": \"zookeeper\", \"homeDir\": \"/home/zookeeper\"}, {\"status\": \"Available\", \"name\": \"hdfs\", \"homeDir\": \"/home/hdfs\"}, {\"status\": \"Available\", \"name\": \"yarn\", \"homeDir\": \"/home/yarn\"}, {\"status\": \"Available\", \"name\": \"spark\", \"homeDir\": \"/home/spark\"}, {\"status\": \"Available\", \"name\": \"ams\", \"homeDir\": \"/home/ams\"}, {\"status\": \"Available\", \"name\": \"mahout\", \"homeDir\": \"/home/mahout\"}], \"firewallRunning\": false}, \"timestamp\": 1451465233867, \"hostname\": \"slave.dev.local\", \"responseId\": -1, \"publicHostname\": \"slave.dev.local\"}\\')\nINFO 2015-12-30 09:47:14,101 NetUtil.py:59 - Connecting to https://master.dev.local:8440/connection_info\nINFO 2015-12-30 09:47:14,226 security.py:99 - SSL Connect being called.. connecting to the server\nINFO 2015-12-30 09:47:14,350 security.py:60 - SSL connection established. Two-way SSL authentication is turned off on the server.\nINFO 2015-12-30 09:47:14,711 Controller.py:166 - Registration Successful (response id = 0)\nINFO 2015-12-30 09:47:14,711 ClusterConfiguration.py:123 - Updating cached configurations for cluster clusterone\nINFO 2015-12-30 09:47:14,740 AmbariConfig.py:260 - Updating config property (agent.auto.cache.update) with value (true)\nINFO 2015-12-30 09:47:14,740 AmbariConfig.py:260 - Updating config property (agent.check.remote.mounts) with value (false)\nINFO 2015-12-30 09:47:14,741 AmbariConfig.py:260 - Updating config property (agent.check.mounts.timeout) with value (0)\nINFO 2015-12-30 09:47:14,787 AlertSchedulerHandler.py:189 - [AlertScheduler] Reschedule Summary: 0 rescheduled, 0 unscheduled\nINFO 2015-12-30 09:47:14,787 Controller.py:387 - Registration response from master.dev.local was OK\nINFO 2015-12-30 09:47:14,787 Controller.py:392 - Resetting ActionQueue...\n', None)\n('INFO 2015-12-30 09:47:13,707 AlertSchedulerHandler.py:323 - [AlertScheduler] Scheduling datanode_process with UUID bb05c01d-a891-4ea2-aca6-282b919dd96b\nINFO 2015-12-30 09:47:13,707 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts\nINFO 2015-12-30 09:47:13,707 AlertSchedulerHandler.py:323 - [AlertScheduler] Scheduling ambari_agent_disk_usage with UUID dedb798d-2ed3-44e3-b8df-97d55eb2d84d\nINFO 2015-12-30 09:47:13,708 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts\nINFO 2015-12-30 09:47:13,708 AlertSchedulerHandler.py:323 - [AlertScheduler] Scheduling zookeeper_server_process with UUID fd4ad98d-35d6-432c-98e3-3a4f5f03a246\nINFO 2015-12-30 09:47:13,708 AlertSchedulerHandler.py:134 - [AlertScheduler] Starting &lt;ambari_agent.apscheduler.scheduler.Scheduler object at 0x7f6305f02cd0&gt;; currently running: False\nINFO 2015-12-30 09:47:13,714 hostname.py:87 - Read public hostname \\'slave.dev.local\\' using socket.getfqdn()\nWARNING 2015-12-30 09:47:13,865 Facter.py:354 - Could not run /usr/sbin/sestatus: OK\nINFO 2015-12-30 09:47:14,100 Controller.py:140 - Registering with slave.dev.local (192.168.0.2) (agent=\\'{\"hardwareProfile\": {\"kernel\": \"Linux\", \"domain\": \"dev.local\", \"physicalprocessorcount\": 2, \"kernelrelease\": \"3.19.0-42-generic\", \"uptime_days\": \"0\", \"memorytotal\": 7918076, \"swapfree\": \"7.75 GB\", \"memorysize\": 7918076, \"osfamily\": \"ubuntu\", \"swapsize\": \"7.75 GB\", \"processorcount\": 2, \"netmask\": \"255.255.255.0\", \"timezone\": \"CET\", \"hardwareisa\": \"x86_64\", \"memoryfree\": 6802196, \"operatingsystem\": \"ubuntu\", \"kernelmajversion\": \"3.19\", \"kernelversion\": \"3.19.0\", \"macaddress\": \"00:19:66:88:A0:DE\", \"operatingsystemrelease\": \"14.04\", \"ipaddress\": \"192.168.0.2\", \"hostname\": \"slave\", \"uptime_hours\": \"0\", \"fqdn\": \"slave.dev.local\", \"id\": \"root\", \"architecture\": \"x86_64\", \"selinux\": false, \"mounts\": [{\"available\": \"3915384\", \"used\": \"4\", \"percent\": \"1%\", \"device\": \"udev\", \"mountpoint\": \"/dev\", \"type\": \"devtmpfs\", \"size\": \"3915388\"}, {\"available\": \"790656\", \"used\": \"1152\", \"percent\": \"1%\", \"device\": \"tmpfs\", \"mountpoint\": \"/run\", \"type\": \"tmpfs\", \"size\": \"791808\"}, {\"available\": \"900199676\", \"used\": \"4656112\", \"percent\": \"1%\", \"device\": \"/dev/sda1\", \"mountpoint\": \"/\", \"type\": \"ext4\", \"size\": \"953303940\"}, {\"available\": \"4\", \"used\": \"0\", \"percent\": \"0%\", \"device\": \"none\", \"mountpoint\": \"/sys/fs/cgroup\", \"type\": \"tmpfs\", \"size\": \"4\"}, {\"available\": \"5120\", \"used\": \"0\", \"percent\": \"0%\", \"device\": \"none\", \"mountpoint\": \"/run/lock\", \"type\": \"tmpfs\", \"size\": \"5120\"}, {\"available\": \"3958884\", \"used\": \"152\", \"percent\": \"1%\", \"device\": \"none\", \"mountpoint\": \"/run/shm\", \"type\": \"tmpfs\", \"size\": \"3959036\"}, {\"available\": \"102368\", \"used\": \"32\", \"percent\": \"1%\", \"device\": \"none\", \"mountpoint\": \"/run/user\", \"type\": \"tmpfs\", \"size\": \"102400\"}], \"hardwaremodel\": \"x86_64\", \"uptime_seconds\": \"1651\", \"interfaces\": \"eth0,lo\"}, \"currentPingPort\": 8670, \"prefix\": \"/var/lib/ambari-agent/data\", \"agentVersion\": \"2.1.2\", \"agentEnv\": {\"transparentHugePage\": \"\", \"hostHealth\": {\"agentTimeStampAtReporting\": 1451465234096, \"activeJavaProcs\": [], \"liveServices\": [{\"status\": \"Healthy\", \"name\": \"ntp\", \"desc\": \"\"}]}, \"reverseLookup\": true, \"alternatives\": [], \"umask\": \"18\", \"firewallName\": \"ufw\", \"stackFoldersAndFiles\": [{\"type\": \"directory\", \"name\": \"/etc/hadoop\"}], \"existingUsers\": [{\"status\": \"Available\", \"name\": \"mapred\", \"homeDir\": \"/home/mapred\"}, {\"status\": \"Available\", \"name\": \"ambari-qa\", \"homeDir\": \"/home/ambari-qa\"}, {\"status\": \"Available\", \"name\": \"zookeeper\", \"homeDir\": \"/home/zookeeper\"}, {\"status\": \"Available\", \"name\": \"hdfs\", \"homeDir\": \"/home/hdfs\"}, {\"status\": \"Available\", \"name\": \"yarn\", \"homeDir\": \"/home/yarn\"}, {\"status\": \"Available\", \"name\": \"spark\", \"homeDir\": \"/home/spark\"}, {\"status\": \"Available\", \"name\": \"ams\", \"homeDir\": \"/home/ams\"}, {\"status\": \"Available\", \"name\": \"mahout\", \"homeDir\": \"/home/mahout\"}], \"firewallRunning\": false}, \"timestamp\": 1451465233867, \"hostname\": \"slave.dev.local\", \"responseId\": -1, \"publicHostname\": \"slave.dev.local\"}\\')\nINFO 2015-12-30 09:47:14,101 NetUtil.py:59 - Connecting to https://master.dev.local:8440/connection_info\nINFO 2015-12-30 09:47:14,226 security.py:99 - SSL Connect being called.. connecting to the server\nINFO 2015-12-30 09:47:14,350 security.py:60 - SSL connection established. Two-way SSL authentication is turned off on the server.\nINFO 2015-12-30 09:47:14,711 Controller.py:166 - Registration Successful (response id = 0)\nINFO 2015-12-30 09:47:14,711 ClusterConfiguration.py:123 - Updating cached configurations for cluster clusterone\nINFO 2015-12-30 09:47:14,740 AmbariConfig.py:260 - Updating config property (agent.auto.cache.update) with value (true)\nINFO 2015-12-30 09:47:14,740 AmbariConfig.py:260 - Updating config property (agent.check.remote.mounts) with value (false)\nINFO 2015-12-30 09:47:14,741 AmbariConfig.py:260 - Updating config property (agent.check.mounts.timeout) with value (0)\nINFO 2015-12-30 09:47:14,787 AlertSchedulerHandler.py:189 - [AlertScheduler] Reschedule Summary: 0 rescheduled, 0 unscheduled\nINFO 2015-12-30 09:47:14,787 Controller.py:387 - Registration response from master.dev.local was OK\nINFO 2015-12-30 09:47:14,787 Controller.py:392 - Resetting ActionQueue...\n', None)\n\nConnection to slave.dev.local closed.\nSSH command execution finished\nhost=slave.dev.local, exitcode=0\nCommand end time 2015-12-30 09:47:14\n\nRegistering with the server...\nRegistration with the server failed.</pre>","tags":["Ambari","hdp-2.3.0","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-30 18:26:44.0","id":8403,"title":"Can Hue's syncdb be run after Hue install?","body":"<p>In this case, Hue is using MySQL as a backing store.  It is not obvious to me what all syncdb is doing but I need to solve a race condition in Chef that would potentially see syncdb executed with multiple times after Hue is running. </p>","tags":["database","syncdb","hue"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-12-31 11:56:48.0","id":8470,"title":"Is it mandatory to install Postgresql before installing ambari server using local repository","body":"<p>Hi Team - I am facing problem while installing ambari server using local repository method. I had downloaded ambari from below mentioned URL's. </p><p>When providing the following following command \"yum install ambari-server\" it is failing with dependency of postgresql.</p><p><a href=\"http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.2.0.0/ambari.repo\">http://public-repo-1.hortonworks.com/ambari/centos...</a></p><p><a href=\"http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.2.0.0/ambari-2.2.0.0-centos7.tar.gz\"></a></p><p><a href=\"http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.2.0.0/ambari-2.2.0.0-centos7.tar.gz\">http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.2.0.0/ambari-2.2.0.0-centos7.tar.gz</a></p><p>Please help me out in resolving this issue.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-04 08:47:00.0","id":8589,"title":"send mail after cluster deployment  is completed through Ambari","body":"<p>The Hadoop cluster installation is started through Ambari. Is it possible for Ambari to send an e-mail about cluster deployment, once the process completes?</p>","tags":["Ambari","operations","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-07 07:26:19.0","id":26329,"title":"Ambari agent abort abnormally","body":"<p>I installed ambari 2.2.1.1 and HDP 2.4, the ambari agent will abort abnormally. The ambari agent can be restart success by command ambari-agent restart, but after a while it will abort again. Check the ambari agent status:</p><p><em># ambari-agent status</em></p><p><em>Found ambari-agent PID: 73289</em></p><p><em>ambari-agent not running. Stale PID File at: /var/run/ambari-agent/ambari-agent.pid</em></p><p>I cannot see any error or warning in ambari agent log file (/var/log/ambari-agent/ambari-agent.log).</p><p>I want know why this happen and how to avoid it, thanks.</p>","tags":["ambari-agent"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-07 20:04:16.0","id":26498,"title":"Do Hiveserver2 direct JDBC Calls Support TLS1.2 ?","body":"<p>Does hiveserver2 JDBC calls support TLSv1.2  ?  We are having external applications configured to use TLS1.2 for stronger security.</p>","tags":["hiveserver2","sme-security","Hive"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-04-08 19:57:41.0","id":26718,"title":"ORC Schema evolution in hive","body":"<p>How does schema evolution work in orc table in hive0.14?  I can't seem to get it to respect adding column without breaking data already in the table.  Is column type change int-&gt; long supported in hive0.14? </p>","tags":["Hive","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-10 23:15:50.0","id":26856,"title":"Cannot create cluster on GCP because of invalid bucket name","body":"<p>I cannot create cluster using Cloudbreak on Google Compute Engine, the error is Invalid bucket name, though the GFS was not selected as an option when creating the Cluster. The bucket name which Cloudbreak is trying to create seems to be randomly generated by concatenating the project name and a random id. I tried selecting GFS as the file system and specifying the bucket name, but still get the same error.</p><p>Cloudbreak version is 1.2.4</p>","tags":["gce","Cloudbreak","bucketing"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-12 21:08:16.0","id":27230,"title":"No Hue in AWS VM,How to check logs?","body":"<p>Hi,</p><p>     There is no Hue in practice test VM,how can we check logs.Also how is it possible to print something on the console or log it in the log.Tried using Logging but didn't work.Please give a code snippet if possible.</p><p>Thanks</p>","tags":["hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-14 07:18:03.0","id":27525,"title":"Non-root user fail to deploy hadoop with Ambari 2.2.1 on CentOS 7","body":"<p>I try to deploy hadoop with non-root user through Ambari. I have already setup Ambari server successfully and deploying hadoop on Ambari web interface.On step 'Install Options', if I set the SSH User Account with ROOT,then I can go to the next step,but when I replace with my non-root user, then waiting for quite a time and finally shows the status 'Failed' message as below:</p><p>========================== Creating target directory... ========================== Command start time 2016-04-14 14:44:13 Automatic Agent registration timed out (timeout = 300 seconds). Check your network connectivity and retry registration, or use manual agent registration.</p><p>I have already set SSH free password login on root and non-root users and other essential configuration as well.Besides, I can pass the step  'Install Options' with root. Thus ,I'm quite sure that Non-root user fail the step because of lacking authority but don't know how to fix it.</p><p>Anyone has some suggestions?</p><p>Thanks.  </p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-14 01:06:58.0","id":27464,"title":"i am installing single node on google cloud, i have lot of confusion where can i generate ssh-keygen? in root user or normal user and how can i connect the root@FQDN?","body":"","tags":["permission-denied","Ambari","ssh"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-14 12:36:06.0","id":27580,"title":"Unable to reach Ambari on sandbox port 8080","body":"<p>I downloaded the sandbox and can reach Zepplin, Ranger etc, but not Ambari on port 8080 and am getting 'connection refused'. I have tried with the firewall off but this hasn't helped. Any suggestions to help would be great, thanks.</p><p>Cheers James</p>","tags":["Sandbox","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-15 06:04:47.0","id":27718,"title":"phoenix query server installation","body":"<p>I am planning to install Hbase on my cluster along with phoenix through Ambari. In the add service section (slaves and clients), i do see an option of Phoenix query server. By default it is not enabled. Should i install this component mandatorily or just a phoenix client is enough.  If i need to install it mandatorily, where should i install this component ( which all nodes).</p>","tags":["Hbase","Phoenix"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-21 11:20:33.0","id":28686,"title":"Tweet streams with Storm","body":"<p>Hello,</p><p>I am pretty new to Storm and I am getting started by trying to process some tweet streams with it. What would be the basic steps to start it?</p><p>I am aware of there is a stream api for it (https://dev.twitter.com/streaming/overview), but how would I integrate it with my Storm elements to start making it work. </p><p>Any insights appreciated.</p><p>Thanks!</p>","tags":["Storm"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-26 21:21:12.0","id":35954,"title":"How to re-run Metron installer after failure?","body":"","tags":["test"],"track_id":111,"track_name":"CyberSecurity"}
{"creation_date":"2016-05-28 18:22:44.0","id":36271,"title":"Install Ranger throws Error.","body":"<p>We ran into an error during Ranger installation on Mysql. The console error log as below:</p><pre>Error executing: call insert_public_group_in_x_group_table(); \ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'GROUP_SRC' in 'field list'\nSQLException : SQL state: 42S22 com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'GROUP_SRC' in 'field list' ErrorCode: 1054</pre><p>And the software version is:</p><p>MySQL: 5.7</p><p>HDP: 2.4.0.0-169</p><p>Ambari: 2.2.1</p><p>Anybody ran into this before?</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-29 11:34:39.0","id":36303,"title":"Hive query","body":"<p>I have following data :</p><p><img src=\"/storage/attachments/4641-screen-shot-2016-05-29-at-50244-pm.png\"></p><p>How to select the non-manager employees ?</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-30 10:03:24.0","id":36376,"title":"files view blank","body":"<p>http://192.168.100.5:8080/#/main/views/FILES/1.0.0/AUTO_FILES_INSTANCE</p><p>i have acces my local standalone HDP.  but it's blank and in descripton instance : \"This view instance is auto created when the HDFS service is added to a cluster.\"</p><p>i'm trying to upload image but Error parsing the uploaded file.</p>","tags":["ambari-views","HDFS","ambari-2.2.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-08 08:59:14.0","id":38415,"title":"Possible to exclude 'descriptor' field from NiFi REST API?","body":"<p>When working with the NiFi REST API and no UI is it possible to exclude the processor's config -&gt; descriptors field from the JSON response? Simply looking to avoid the large volume of useless data during development. </p>","tags":["api","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-08 11:29:52.0","id":38435,"title":"R package installtion","body":"<p>Hi in my new project i have HDP 2.1.5 and R package is installed already they want me to add some more R packages i have download all required R packages..how can i add this new packages to existing R..actually i am not aware of R..plz suggest me..</p><p>tq</p>","tags":["hadoop-maintenance"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-09 09:16:33.0","id":38723,"title":"Deleting History of Spark run applications","body":"<p>Guys,</p><p>I want to cleanup all the history of the applications run on Spark. Because of some issues previously still some of the applications started are being shown \"incomplete\" and when I check the /spark-history directory I see those applications with .in progress extension for example : /spark-history/application_14653124594105_0007.inprogress. </p><p>When I try killing it using yarn application kill command it shows that </p><p>Application application_14653124594105_0007 has already finished. </p><p>I delete the appname.inprogress directory from /spark-history but it doesn't stop appearing from the UI and when I click the link on the UI, it shows error saying : Application not found. </p><p>What is the safe way to delete the history and want to avoid displaying on the History servers.</p><p>Many thanks.</p>","tags":["delete","spark-history-server","operations"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-09 12:28:30.0","id":38756,"title":"Missing History when running as non Spark User in kerberized Cluster","body":"<p>Guys,</p><p>When I am running Spark Applications in YARN Cluster mode on HDP 2.4.0 using spark-submit (App runs fine without any errors) ,but I do not see application logs when I try to check in the History Server - Resource manager UI or through yarn logs command. ( Also checked that log directories are empty)  </p><p><img src=\"/storage/attachments/4904-history-issue.png\"></p><p>When I do kdestroy and then get TGT for Spark user using </p><p>kinit -kt /keytabs/spark-headless-keytab.. spark </p><p>and then run the Spark application using spark-submit and run Spark Application (For example SparkPI ) and see the logs through History Server UI or using yarn logs command I see the logs. </p><p style=\"margin-left: 20px;\">I am not sure if I am missing some configurations here. Or Am I supposed to run all the Spark applications as Spark user?</p><p style=\"margin-left: 20px;\"></p><p>Thanks.</p><p>SN</p><p><a href=\"https://community.hortonworks.com/questions/38756/missing-history-when-running-as-non-spark-user-in.html#\">Jitendra Yadav</a></p>","tags":["spark-history-server","spark-1.6.0"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-09 16:59:27.0","id":38853,"title":"We are working on a Use Case to load PDF Documents on HDFS and Extract the contents from PDF Documents and save them in Hive Tables. We would like the know the Best approach/solution to solve the use case.","body":"","tags":["parsers","documents"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-09 20:43:20.0","id":38906,"title":"Wiping Kafka Topics and starting fresh","body":"<p>Hi,</p><p>I currently have an HDP 2.3 cluster with Kafka running. Kafka topics look to be corrupt so I want to wipe all data in the topics. Delete Kafka from the cluster and re-add it. What is the best way to do this? </p><p>The cluster is currently being used for other tasks (Hive, Spark, etc) that I don't want to impact.</p><p>Thanks,</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-10 04:25:46.0","id":38969,"title":"How to install kafka through ambari without HDFS?","body":"<p>I want to install kafka on my cluster and use local storage for kafka instead of hdfs.  Is this setup possible through ambari?</p>","tags":["Kafka","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-14 08:43:54.0","id":39598,"title":"Iterate a dataframe","body":"<p>Hello,\nPlease I will like to iterate and perform calculations accumulated in a column of my dataframe but I can not. Can you help me?\nThank you</p><p>Here the creation of my dataframe. I would like to calculate an accumulated blglast the column and stored in a new column</p><p>from pyspark.sql import HiveContext\nfrom pyspark import SparkContext\nfrom pandas import DataFrame as df\n\nsc =SparkContext()\n\nhive_context = HiveContext(sc)\ntab = hive_context.table(\"table\")\n\ntab.registerTempTable(\"tab_temp\")\n\ndf=hive_context.sql(\"SELECT  blglast FROM tab_temp AS b limit 50\")</p><p>df.show()</p>","tags":["dataframe","Spark"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-26 15:28:59.0","id":2108,"title":"Ranger group policy not being applied to the users with in the group","body":"<p>Customer has synced the users and groups from AD/LDAP into Ranger using Usersync and is having issues with assigning group privileges on Hive tables usder \"group permissions\" section. It is not granting any users within the group to run any queries on the Hive table. If he explicitly specified the username and assign the selelct permission user \"user permissions\" section in Ranger then it works for him. Any ideas on how to get around this issue ?</p>","tags":["configuration","Ranger","ldap","user-groups"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-28 10:59:07.0","id":2288,"title":"How to configure storage policy in Ambari?","body":"<p>Setting the following value in the Ambari box corresponding to the property dfs.datanode.data.dir does not seem to work:</p><p>/hadoop/hdfs/data,[SSD]/mnt/ssdDisk/hdfs/data</p><p>I get a warning \"Must be a slash or drive at the start\" and I cannot save the new configuration.</p><p>Is there a way to define those disk storages in Ambari (in the past I tried to do it in the hdfs-site.xml file and it worked fine)?</p><p>My Ambari version is 2.1.0 and I use HDP 2.3.0 (Sandbox).</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-28 20:29:41.0","id":2321,"title":"Apache Oozie documentation shows how to get status in localtime which works but not an example on how to run a job using localtime.. Is there a way to run the job  local time? We are trying to avoid daylight saving's time.","body":"","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-29 15:55:26.0","id":2377,"title":"using HCatalog with Pig action in Oozie","body":"<p>if you'd like to take advantage of HCatalog in your Pig scripts, you need to add the following property in your job.properties OR workflow.xml OR oozie-site.xml for global change.</p><p><img src=\"/storage/attachments/357-hcat.png\"></p><p>if you don't include hive you will get an error message like so:</p><p><img src=\"/storage/attachments/358-error-message-sandbox2.png\"></p>","tags":["Pig","Oozie","hcatalog","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-29 17:23:49.0","id":2411,"title":"ambari security-setup setup tust store","body":"<p>After running security-setup on ambari, and then choose option [3]  - setup trustore, it successfull say its completed, but yet no trust store created in the path.</p>","tags":["active-directory","security","ambari-2.1.2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-29 16:16:26.0","id":2394,"title":"Apache Flink Tutorials, Examples and HDP Install Guides","body":"<p>Hi All,</p><p>Does anyone have any steps for installing Flink on HDP? And any tutorials/examples of using Flink? Maybe with Hive ORC tables?</p><p>Thanks,</p>","tags":["sql","Spark","flink"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-29 16:23:07.0","id":2396,"title":"Node Label Configuration with HDP 2.3.x and Ambari 2.1.x","body":"<p>Does anyone have (or can you point me to) a link that explains the steps of using YARN node labels in HDP 2.3 being managed by Ambari 2.1? Our existing documentation is for manual configuration only. I've clicked the enabled button, and have already figured out that Ambari creates slightly different directory structures, etc. than are listed. I can keep going with trial and error, but if someone has already documented the process, caveats, etc. that would be fantastic. Thanks!</p>","tags":["YARN","Ambari","yarn-node-labels"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-02 15:39:27.0","id":2610,"title":"Cloudbreak with Docker as Virtual Layer","body":"<p>I know CB creates docker containers with HDP components to provision HDP cluster but can CB also work with a cloud infrastructure that uses Docker as Virtualization layer? </p>","tags":["Cloudbreak","docker","virtualization"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-30 04:42:46.0","id":2498,"title":"Best practices for zookeeper placement?","body":"<p>What other services are best to colocate on a host with Zookeeper, and how does this change as number of hosts increases?</p><p>Does it make sense not to run it on a host with HA services, since those are what it protects? If running on a NodeManager, what adjustments should be made to memory available for YARN containers?</p>","tags":["YARN","best-practices","architecture","zookeeper"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-30 19:29:28.0","id":2538,"title":"Test Question for Docs Team","body":"<p>The is the right configuration for Nimbus HA. The docs is not correct</p>","tags":["notify-docs"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-30 23:26:23.0","id":2562,"title":"Appending to existing partition with Pig","body":"<p>Pig does not support <a href=\"https://issues.apache.org/jira/browse/HIVE-6405\">appending to an existing partition</a> through HCatalog.</p><p>What workarounds are there to perform the append and get a behavior similar to Hive's INSERT INTO TABLE with Pig ?</p>","tags":["partitioning","Pig","hcatalog"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-02 06:10:50.0","id":2580,"title":"Accessing HDP web UI from Windows PC causes \"GSSHeader did not find the right tag\"","body":"<p>HDP version : 2.3.0 \nAmbari version: 2.1.0</p><p>Enabled Kerberos with Windows Active Directory (not cross-realm) from Ambari.</p><p>Confirmed kinit and curl to WebHDFS worked with an AD user.</p><pre>Followed http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.0/bk_Ambari_Security_Guide/content/_configuring_http_authentication_for_HDFS_YARN_MapReduce2_HBase_Oozie_Falcon_and_Storm.html</pre><p>Again, confirmed kdestroy & kinit and curl to WebHDFS and NameNode UI worked with an AD user.</p><p>Now, tried to access HDP NameNode UI page from Firefox on Windows PC. </p><pre>Copied /etc/krb5.conf to C:\\Windows\\krb5.ini.\nFollowed some instruction found on internet to set up Firefox.\nAnd didn't work. The error was \"<strong>GSSHeader did not find the right tag</strong>\"</pre><p>For troubleshooting purpose, downloaded curl.exe from http://curl.haxx.se/download.html</p><p>Trying to access HDP, for example, NameNode:50070 with curl --negotiate, and got same error \"<strong>GSSHeader did not find the right tag</strong>\"</p><p>Copied \"/etc/security/keytabs/spnego.service.keytab\" into Windows and did kinit -k -t and curl --negotiate but same error.</p><p>Does anyone know what would be missing to make Windows PC work to access secured web page?</p>","tags":["spnego","windows","active-directory","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-02 21:28:17.0","id":2637,"title":"Do we support hdfs fuse?","body":"<p>Do we support Fuse?</p><p>One of customer is asking about this and saying other vendor support Fuse. They are already aware of NFS.</p><p>If customer set this up on his own, are there any gotchas?</p>","tags":["HDFS","nfs","fuse"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-05 20:30:49.0","id":2923,"title":"Do we need to add Knox self-signed cert into Ranger keystore?","body":"<p>It seem that invoking the test from the KNox repository in Ranger  failes due to miss path to the cert.</p><p>Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target</p>","tags":["Ranger","Knox","ssl"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-06 17:32:35.0","id":2972,"title":"Kerberos TCP vs HTTP Efficiency?","body":"<p>Hello experts.  Does anyone have any information on the efficiency of Kerberos TCP vs HTTP?</p>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-05 01:51:15.0","id":2840,"title":"Is it possible to Clone Interpreter  in zeppelin","body":"<p>Is it possible to clone the zeppelin interpreter with all settings programmatically(REST?) or using UI(I did not see that as an option). I created a new one manually but wanted to check if there was a better way?. I want to have 2 or more interpreters setup pointing @ different versions of spark 1.4 and 1.5. But i can easily see this extending to other components. </p>","tags":["zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-05 03:19:58.0","id":2852,"title":"Can we create a NiFi workflow and operate using CLI instead of GUI?","body":"<p>One of our prospects is looking at NiFi, they cant have GUI operated tools as they need everything to be scripted for ease of operations, can we create and manage NiFi flows with CLI without the need for GUI?</p>","tags":["hdf","Nifi","cli"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-11-06 20:37:07.0","id":2978,"title":"No ranger access logs Ranger HDFS/HIVE Plugins, os access_log file getting updates.","body":"<p>ENV:  Ambari 2.1.2, Ranger:0.5.0.2.3</p><p>HDFS/HIVE plugin configured, tried Audit logs to HDFS and changed to DB(Oracle 12c)</p><p>/var/log/ranger/admin/xa_portal.log  reports following error.</p><pre>2015-11-06 15:30:05,987 [http-bio-6080-exec-3] INFO  org.apache.ranger.common.RESTErrorUtil (RESTErrorUtil.java:311) - Operation error. response=VXResponse={org.apache.ranger.view.VXResponse@2a7e763statusCode={1} msgDesc={Error running query} messageList={[VXMessage={org.apache.ranger.view.VXMessage@6d64c8e4name={ERROR_SYSTEM} rbKey={xa.error.system} message={System Error. Please try later.} objectId={null} fieldName={null} }]} }\njavax.ws.rs.WebApplicationException</pre>","tags":["ranger-0.5.0","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-09 17:12:54.0","id":3068,"title":"SmartSense install fails with 500 status on Kerberized cluster","body":"<p>During the install of SmartSense we are getting the following errors:</p><p>500 status code received on GET method for API: /api/v1/clusters/test/services?ServiceInfo/state=INSTALLED&ServiceInfo/service_name.in(SMARTSENSE)&params/run_smoke_test=true&params/reconfigure_client=false</p><p>Error message: org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Invalid transition for service, clusterName=test, clusterId=2, serviceName=SMARTSENSE, currentDesiredState=INIT, newDesiredState=STARTED</p><p>{ \"status\" : 500, \"message\" : \"org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Invalid 'kdc_type' value: Existing MIT KDC\" }</p>","tags":["kerberos","installation","smartsense","security"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-12 03:17:17.0","id":3278,"title":"Data Node Not Starting","body":"<p>The following error is generated when adding a new data node to the cluster:</p><p>WARN  datanode.DataNode (DataNode.java:checkStorageLocations(2407)) - Invalid dfs.datanode.data.dir </p>","tags":["datanode","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-12 16:00:21.0","id":3330,"title":"Hive Config variables Session vs XML","body":"<p>Does anyone know of how to track what Hive config variables are able to be set at the session level vs which are required to be set at the config xml level (and require a restart of the component)?</p><p><a href=\"https://github.com/hortonworks/hive/blob/2.3-M15/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\">Hive Config Java</a> provides a list of most Hive variables, but there seems to be no easy way to tell.</p><p>e.g. There are multiple hive.metastore.* variables that <strong>only</strong> can be set in the config XML and require a Hive Metastore restart.</p>","tags":["configuration","help","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-12 19:47:45.0","id":3356,"title":"What rules set priority of recovery from lost disks or nodes?","body":"<p>When a disk, node, or rack fails, the missing blocks are eventually noticed by the NameNode and enqueued to be replicated.  What rules dictate the priority of this operation and are they controllable by the user? </p>","tags":["replication","HDFS","disaster-recovery"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-11 17:43:18.0","id":3271,"title":"Help me understand DHadoop and RHadoop versions","body":"<p>I often find references to DHadoop or RHadoop when setting env variables or building various projects. Can someone help explain these variables and how to identify what their setting should be.  hdp-select hadoop version etc.</p>","tags":["Spark","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-17 16:30:26.0","id":4120,"title":"What factors warrant going to a higher hdfs block size (dfs.blocksize) than the default 128MB?","body":"<p>Also, did we recommend any customers going higher block size? if so, what were the observations to provide recommendations?</p>","tags":["namenode","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-19 03:03:41.0","id":4292,"title":"How do I create an ORC Hive table from Spark?","body":"<p>I'm currently using Spark 1.4 and I'm loading some data into a DataFrame using jdbc:</p><p>val jdbcDF = sqlContext.load(\"jdbc\", options)</p><p>How can I save the jdbcDF DataFrame to a Hive table using the ORC file format?</p>","tags":["java","orc","Hive","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-17 23:37:50.0","id":4189,"title":"What services need to be restarted if  ipc.server.tcpnodelay has to be changed?","body":"<p>ipc.server.tcpnodelay has been changed to true by default in hadoop 2.6. We are on hadoop 2.4 and would like to change it to true. What services if any require a restart for this change? Can it be set at job level for all jobs and not restart services? </p><p>With a big cluster where NN restart takes more than 60 minutes, we would like to avoid all possible restarts. </p>","tags":["core-site"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-13 17:51:02.0","id":6251,"title":"Practical limits on number of simultaneous open HDFS file streams","body":"<p>I am building an ingest pipeline (Java program) that pulls from lots of relatively small CSV stats files. Every run cycle, I might find say 1000's of new files where further, say 100 of them are the same type of stats that all should land in the same Hive table. Because I am also partitioning them into yyyy/mm/dd Hive partitions, it is likely that all 100 belong in the same day partition. So, my logic will open a file stream to the appropriate file path, like \"/basepath/hivetabledir/year=2015/month=12/day=10/filename_nnnnnnn.avro\" and start writing. This is nice because I end up with one HDFS file containing all the data from 100 smaller source files which is part of my small file reduction strategy. However, I also have some very large files that contain stragger records from back in time - way back in time (like possibly seeing straggler data records for every day of the year or more, meaning potentially 100's of partition directories and files). And, due to policy, I want to retain them even though one might ask, why? If I was to have to delay ingest (like for maintain or something) then start up again to catch up, I could end up in a situation where I process lots of different types of stats files X many different days of timestamps = 100's or even 1000 or more simultaneously open HDFS file streams. So, my questions are: </p><p>(1) What resource limits should I be considering/monitoring in the design such that I limit the number of simultaneous open HDFS file streams to some practical value to avoid trouble? The balance is, the more I keep open, the more small files I can pack into one HDFS file. I could certainly modify that logic to close files at some threshold and start new ones to keep the open file count in check. </p><p>(2) A related question is regarding buffering. I know that HDFS shows a zero size file for the duration of the time each file is open and being written to then, when I close the stream, a see a small delay and the file size then updates to reflect the bytes written. But, I'm writing 100's of MB to GB's of data to some of these files. That is buffering somewhere and I have 100's in this state at once. Maybe this is just no problem for HDFS - not sure? So, what should I be concerned with regarding that \"buffered data\" resource load as opposed to an \"open file stream\" resource load?</p><p>Thanks, Mark</p>","tags":["help","files","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-15 16:32:12.0","id":6541,"title":"How can I fix incorrect file permissions for set-hdfs-plugin-env.sh when restarting from ambari","body":"<p>Ambari sets up set-hdfs-plugin-env.sh with root:hadoop 700 permissions when it restarts HDFS, this causes ranger integration to break as the hdfs user cannot execute this script when the namenode starts.</p><p>I can fix the problem if I restart the namenode manually, but that means to deploy config changes I need to restart from Ambari, then correct permissions and restart manually.</p><p>How </p>","tags":["Ranger","hdp-2.2.6","configuration","ambari-2.1.2","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-12 01:01:49.0","id":6114,"title":"Problem starting VirtualBox Sandbox on Microsoft Azure VT-x is not available (VERR_VMX_NO_VMX)","body":"<p>I installed a Oracle VM VitualBox Version 5.0.10 for running Hortonworks Sandbox with HDP 2.3.2.</p><p>When I try to start failed to open a session for it with the following message:</p><p>VT-x is not available (VERR_VMX_NO_VMX).</p>\n\nResult Code: \nE_FAIL (0x80004005)\n\nComponent: \nConsoleWrap\n\nInterface: \nIConsole {872da645-4a9b-1727-bee2-5585105b9eed}<p>Best regards,</p><p>JAG</p><p>PD: </p><p>Windows Azure Machine</p><p>Windows Server 2012 R2</p><p>Intel (R) Xeon (R) CPU E5-2673 v3 @ 2.40 GHz  2.39 GHz</p><p>Installed Memory 14 GB</p>","tags":["Sandbox","virtualbox","azure"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-16 18:56:19.0","id":6877,"title":"Viewing Hive Column or Table level Statistics","body":"<p>Does anyone know of a way to view the statistics which are created after the command \"analyze table [myTable] compute statistics;\" is executed?</p><p>Referenced from here: http://hortonworks.com/blog/5-ways-make-hive-queries-run-faster/</p>","tags":["statistics","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-16 16:57:27.0","id":6848,"title":"Is there a way to have pig default to python 3.4","body":"<p>we got python 3.4 installed and is there a way to have pig use 3.4 as default python instead of 2.6?</p>","tags":["Pig","python"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 07:28:00.0","id":7012,"title":"Install hue in HDP 2.3.2","body":"<p>Hi,</p><p>I am confused about hue ..</p><p>1.This <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_upgrading_hdp_manually/content/start-hue-22.html\">link </a> says have to use hue which shipped with HDP 2.3.2 .</p><p>2.When I google it I  get this <a href=\"http://gethue.com/hadoop-hue-3-on-hdp-installation-tutorial/\">link </a></p><p>Which option is correct ?</p><p>Would really appreciate if somebody can helps me with the steps ?</p><p><strong>My cluster is on EC2 having RHEL .</strong></p><p><a href=\"http://gethue.com/hadoop-hue-3-on-hdp-installation-tutorial/\"></a></p>","tags":["hdp-2.3.2","installation","aws","hue"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-17 00:29:10.0","id":6942,"title":"What is the minimum version of hadoop for httpFS ?","body":"","tags":["httpfs"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-17 03:46:24.0","id":6977,"title":"amazon ec2  4 node cluster shows lost heart beat for all services and not able to start any services","body":"<p>amazon ec2  4 node cluster shows lost heart beat for all services and not able to start any services</p>","tags":["heartbeat"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-17 10:33:16.0","id":7041,"title":"How to find week of the year of hive column(String)","body":"<p>Hi everyone, In my hive table i have values in one column are like  \" Thu Dec 17 15:55:08 IST 2015 \" .For this particular column i would like to find \"week of the year\".How can i do that one .Assume column name is survey_date and data type is string.</p>","tags":["date","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-18 11:32:19.0","id":7224,"title":"how to go from spark dataframe to hive readable metastore table","body":"<p>Currently we use the \"create table as select from temp_table\" trick to store a dataframe into HIVE, to avoid storing a spark only readable table.</p><p>Now I am trying to use hivecontext.createExternalTable(...) to store a dataframe in HIVE metastore using an externally stored file, but I run in the well known issues again: The table is only readable by spark, even if we provide a schema</p><p>How to make the table usable outside a spark context (so proper sql queries using an ODBC connection, etc. etc.)?</p><p>FYI We use pyspark</p>","tags":["spark-sql","Spark","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-18 22:21:50.0","id":7313,"title":"'saveAsOrcFile' error in tutorial lab-4 on Spark","body":"<p>in the sandbox tutorial Lab-4 on Spark, when running this statement:</p><p>            risk_factor_spark.saveAsOrcFile(\"risk_factor_spark\")</p><p>   it gives me the following error:</p><p>        error: value saveAsOrcFile is not a\nmember of org.apache.spark.sql.DataFrame </p><p>I\nhave done the 'import org.apache.spark.sql.hive.orc._' earlier in the lab.  Also tried capital '..ORC..'.  And I am using the  built-in web client for the SSH client.</p><p>Any\nsuggestions?</p>","tags":["how-to-tutorial","Sandbox","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-18 16:28:47.0","id":7264,"title":"How do you write a Spark SQL DataFrame using ORC format with SNAPPY compression","body":"<p>df.write.format(\"orc\").saveAsTable(...) works fine for the default ZLIB compression, but I would prefer to use SNAPPY. Anybody out there had success doing this?</p>","tags":["sparksql","orc","Spark","snappy"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-21 09:38:32.0","id":7432,"title":"How do I  pass headers (username and password) via Apache knox to my custom service?","body":"<p>I need to access username and password in my custom rest service which is passed via knox URL. Can anybody suggest changes needs to be done in rewrite.xml or service.xml?</p>","tags":["help","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-21 13:40:28.0","id":7455,"title":"NameNode can not be started","body":"<p>In hdp sandbox, I got the following error in name node log, and name node can not be started</p><pre>2015-12-21 12:51:27,924 INFO  common.Storage (Storage.java:tryLock(715)) - Lock on /hadoop/hdfs/namenode/in_use.lock acquired by nodename 2311@sandbox.hortonworks.com\n2015-12-21 12:51:28,120 INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(362)) - Recovering unfinalized segments in /hadoop/hdfs/namenode/current\n2015-12-21 12:51:28,175 ERROR namenode.FSImage (FSImage.java:loadFSImage(679)) - Failed to load image from FSImageFile(file=/hadoop/hdfs/namenode/current/fsimage_0000000000000006843, cpktTxId=0000000000000006843)\njava.io.IOException: Premature EOF from inputStream\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)\n        at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:221)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:957)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:941)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:740)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:676)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:294)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:976)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:607)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:667)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:896)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:880)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1586)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1652)\n2015-12-21 12:51:28,223 WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(683)) - Encountered exception loading fsimage</pre>","tags":["Sandbox","namenode"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-22 04:54:59.0","id":7595,"title":"Ambari URL is not loading","body":"<p>I installed Sandbox(HDP 2.3.2 virtual box.ova) on Yosemite MACos and virtual box 5.0.10. It doesn't seem to come up on 127.0.01:8080 as per documentation. <a href=\"/storage/attachments/991-screen-shot-2015-12-18-at-40845-pm.png\">screen-shot-2015-12-18-at-40845-pm.png</a>. I saw some warnings related to linux permissions while initialization of the appliance's servers. <a href=\"/storage/attachments/992-screen-shot-2015-12-18-at-40623-pm.png\">screen-shot-2015-12-18-at-40623-pm.png</a>. Any guidance will help...</p><p>Thanks! </p>","tags":["Sandbox","Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-22 16:15:30.0","id":7686,"title":"SSH login issue","body":"<p>I am running the sandbox on Azure.  I am able to login to Ambari, hue and 8080 page.  However, the SSH using putty or using web version at port 4200, does not accept any credentials.</p><p>Tried root/hadoop  Also the user name and password set for this vm at azure.  Getting Access denied message.  Thoughts?  How do reset root password if it helps?</p>","tags":["ssh"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-23 01:16:15.0","id":7760,"title":"Sqoop Export  Error: Unable to call a stored proc which is part a package.","body":"<p>Hi, </p><p>I am trying to export data to Oracle using \"sqoop export\". I am calling a stored proc in db using --call option and it works without any issues.</p><p>Here is the example:</p><p>sqoop export --connect \"jdbc:oracle:thin:@xxxx:1521:AAG1\" --username \"ABC\" --password \"QPR\" --call TESTPROC --export-dir /user/testuser/validdata</p><p>The other stored proc is created in a package and when I call the stored proc with package name it throws an error stating \"No class writter\"</p><p>Here is the example:</p><p>sqoop export --connect \"jdbc:oracle:thin:@xxxx:1521:AAG1\" --username \"ABC\" --password \"QPR\" --call <strong>schemaName.PackageName.TESTPROC1</strong> --export-dir /user/testuser/validdata</p><p>Here is the error:</p><p>Warning: /opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\n15/12/22 17:15:52 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.4\n15/12/22 17:15:52 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n15/12/22 17:15:52 INFO teradata.TeradataManagerFactory: Loaded connector factory for 'Cloudera Connector Powered by Teradata' on version 1.4c5\n15/12/22 17:15:53 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.\n15/12/22 17:15:53 INFO manager.SqlManager: Using default fetchSize of 1000\n15/12/22 17:15:53 INFO tool.CodeGenTool: Beginning code generation\n15/12/22 17:15:55 INFO manager.OracleManager: Time zone has been set to GMT\n15/12/22 17:15:56 ERROR tool.ExportTool: Encountered IOException running export job: java.io.IOException: No columns to generate for ClassWriter</p><p>Please let me know if you need additional details.</p><p>Thanks</p>","tags":["help","Sqoop","export"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-24 12:45:23.0","id":7958,"title":"Unable to Kill Session in Hive","body":"<p>have tried to run a select script in Hive, since it took time to fetch result tried modifying the query for limiting the result set. I have tried stopping the query/kill session, but the session remains unresponsive, can you let me know how to kill the session ? </p><p><img src=\"https://community.hortonworks.com/storage/attachments/1074-hive-session-1.png\"></p>","tags":["Hive","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-23 05:41:23.0","id":7774,"title":"Ambari Wizard just stuck at Step3 (KERBEROS_SERVICE_CHECK) of Ambari Security Wizard.. Attached snapshot and log from Ambari-Server","body":"<p>Its Ambari2.1.2.1 and HDP2.3.2</p><p><img src=\"/storage/attachments/1035-screen-shot-2015-12-22-at-114055-pm.png\"></p><p>Ambari-Server log:</p><p>23 Dec 2015 05:21:42,494  INFO [qtp-client-10627] AmbariManagementControllerImpl:3312 - Received action execution request, clusterName=HDP_QA_MAIN, request=isCommand :true, action :null, command :KERBEROS_SERVICE_CHECK, inputs :{}, resourceFilters: [RequestResourceFilter{serviceName='KERBEROS', componentName='null', hostNames=[]}], exclusive: false, clusterName :HDP_QA_MAIN\n23 Dec 2015 05:22:58,546  INFO [qtp-client-10626] PersistKeyValueService:82 - Looking for keyName hostPopup-pagination-displayLength-admin</p><p>23 Dec 2015 05:31:38,128  INFO [qtp-client-10626] PersistKeyValueService:82 - Looking for keyName hostPopup-pagination-displayLength-admin\n23 Dec 2015 05:35:24,586  INFO [qtp-client-10627] PersistKeyValueService:82 - Looking for keyName hostPopup-pagination-displayLength-admin\n23 Dec 2015 05:39:33,623  INFO [qtp-client-10627] PersistKeyValueService:82 - Looking for keyName hostPopup-pagination-displayLength-admin</p><p>23 Dec 2015 05:48:53,802  INFO [qtp-client-10627] PersistKeyValueService:82 - Looking for keyName hostPopup-pagination-displayLength-admin\n23 Dec 2015 05:49:26,322  INFO [qtp-client-10720] PersistKeyValueService:82 - Looking for keyName admin-settings-show-bg-admin</p>","tags":["kerberos","security","Ambari","ambari-server","ambari-2.1.2.1"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-22 07:35:30.0","id":7642,"title":"Oozie shell action - Permission denied: user=xyz, access=EXECUTE, inode=\"/user/yarn/.staging\":yarn:supergroup:drwx------","body":"<p>I have a shell action defined in my workflow which moves data from one hdfs location to another, my action is failing because of below error</p><pre>Permission denied: user=xyz, access=EXECUTE, inode=\"/user/yarn/.staging\":yarn:supergroup:drwx------</pre><p>Below are some useful configurations (got these from application log)</p><pre>yarn.app.mapreduce.am.staging-dir=/user\nuser.name=yarn\nmapreduce.job.user.name=xyz\n</pre><p>I also have added below variable in my workflow.xml file under shell action</p><pre>&lt;env-var&gt;HADOOP_USER_NAME=${wf:user()}&lt;/env-var&gt;</pre><p>I'm not sure why user \"xyz\" is trying to read/write at /user/yarn/.staging location. </p><p>Oozie experts, any idea whats going on here ?</p>","tags":["Oozie","permission-denied","hdfs-permissions","permissions"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-24 09:30:35.0","id":7942,"title":"Problem while Mirroring Dataset between hadoop cluster using Apache Falcon","body":"<p>Hello I am new to Apache Falcon & I am mirroring a dataset as shown here <a href=\"http://hortonworks.com/hadoop-tutorial/mirroring-datasets-between-hadoop-clusters-with-apache-falcon/\">Mirroring Dataset between Hadoop Clusters using Apache Falcon</a> ...... I saw the logs of Apache Falcon in \"falcon.application.log\"  & it is throwing this exception ::</p><pre>org.apache.falcon.FalconException: Entity schedule failed for PROCESS: MirrorTest\n\tat org.apache.falcon.resource.AbstractSchedulableEntityManager.scheduleInternal(AbstractSchedulableEntityManager.java:96)\n\tat org.apache.falcon.resource.AbstractSchedulableEntityManager.schedule(AbstractSchedulableEntityManager.java:73)\n\tat org.apache.falcon.resource.SchedulableEntityManager.schedule(SchedulableEntityManager.java:131)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.falcon.resource.channel.IPCChannel.invoke(IPCChannel.java:49)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$9.doExecute(SchedulableEntityManagerProxy.java:403)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$EntityProxy.execute(SchedulableEntityManagerProxy.java:575)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.schedule_aroundBody12(SchedulableEntityManagerProxy.java:405)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure13.run(SchedulableEntityManagerProxy.java:1)\n\tat org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n\tat org.apache.falcon.aspect.AbstractFalconAspect.logAroundMonitored(AbstractFalconAspect.java:51)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.schedule(SchedulableEntityManagerProxy.java:388)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n\tat org.apache.falcon.security.FalconAuthorizationFilter.doFilter(FalconAuthorizationFilter.java:106)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.falcon.security.FalconAuthenticationFilter$2.doFilter(FalconAuthenticationFilter.java:184)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:595)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:554)\n\tat org.apache.falcon.security.FalconAuthenticationFilter.doFilter(FalconAuthenticationFilter.java:193)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.falcon.security.FalconAuditFilter.doFilter(FalconAuditFilter.java:64)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n\n\nCaused by: org.apache.falcon.FalconException: E0501 : E0501: Could not perform authorization operation, Unauthorized connection for super-user: oozie \n\tat org.apache.falcon.workflow.engine.OozieWorkflowEngine.dryRunInternal(OozieWorkflowEngine.java:234)\n\tat org.apache.falcon.workflow.engine.OozieWorkflowEngine.schedule(OozieWorkflowEngine.java:172)\n\tat org.apache.falcon.resource.AbstractSchedulableEntityManager.scheduleInternal(AbstractSchedulableEntityManager.java:94)\n\n\nAm I missing any configuration for oozie or falcon in any configuration files ?? And also how should we deal with this following exception ?? </pre>","tags":["mirroring","Oozie","hadoop","Falcon","freeipa"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-29 15:44:10.0","id":8277,"title":"Flume + Knox","body":"<p>Is it possible to execute flume agent outside hadoop network using Knox gateway + WebHDFS?</p><p>I found this JIRA (<a target=\"_blank\" href=\"https://issues.apache.org/jira/browse/FLUME-2701\">https://issues.apache.org/jira/browse/FLUME-2701</a>), but it's not resolved yet.</p><p>A workaround I found would be to mount hdfs/nfs in flume-agent remote node and use File Roll Sink to this nfs/hdfs directory, but it does not seem be a good approach.</p>","tags":["Knox","webhdfs","Flume"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-30 04:30:41.0","id":8334,"title":"Is there way to run a single Ambari python unit test using mvn test command?","body":"<p>e.g. mvn -Dtest=TestHDP23StackAdvisor test</p>","tags":["development","python","ambari-server","Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-03 19:06:38.0","id":8579,"title":"Is there a Spark Maven archetype for Java?","body":"<p>Is there a Maven Archetype that does the following?</p><ol><li>Setup HDP Maven repository</li><li>Create Maven profiles to download different versions of Spark and Hadoop jars based on HDP version</li><li>Setup for building an uber jar</li><li>Add a sample Java application and test case to the project</li><li>Add winutils binaries to run test-cases on Windows</li></ol><p>I found this: https://github.com/spark-in-action/scala-archetype-sparkinaction, which does #4, but for Scala.</p>","tags":["maven","java","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-04 08:56:00.0","id":8591,"title":"Field with empty or no data causing error in pig","body":"<p>Apache Pig version 0.12.1.2.1.7.0-784</p><p>I have data where one of the field doesn't have data like </p><pre>2015,,08\n2015,,09\n2015,,11\n2015,,04\n2015,,05</pre><p>Now i run the pig command like </p><pre>grunt&gt; given_input = load '/pigtest/flightdelays/' using PigStorage(',') as (year,month,day);\ngrunt&gt; ori = foreach given_input generate month;\ngrunt&gt; illustrate ori;\n\ngenerating error like :  Caused by: java.lang.RuntimeException: No (valid) input data found!</pre><p>when i replace the loader with CSVExcelStorage like</p><pre>grunt&gt; given_input = load '/pigtest/flightdelays/' using org.apache.pig.piggybank.storage.CSVExcelStorage(',') as (year,month,day);\ngrunt&gt; ori = foreach given_input generate month;\ngrunt&gt; illustrate ori;</pre><p>getting output like</p><pre>-------------------------------------------------------------------------------\n| given_input     | year:bytearray    | month:bytearray    | day:bytearray    |\n-------------------------------------------------------------------------------\n|                 | 2015              |                    | 05               |\n-------------------------------------------------------------------------------\n--------------------------------\n| ori     | month:bytearray    |\n--------------------------------\n|         |                    |\n--------------------------------</pre><p>So,I would like to know </p><p>1)What is the problem with Pigstorage.</p><p>2)Is it loader problem or pig version problem.</p><p>3)If i want to use PigStoarage in this,How is should???</p><p>  Not only illustrate even dump behaves the same.</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-30 16:16:12.0","id":8390,"title":"Ambari 2.1.2 snmptrap","body":"<p>we got ambari 2.1.2 and we are able to send snmptraps to externals monitoring servers. however our monitoring folks says they don't see complete message to format it on their side.</p><p>is it related to <a href=\"https://issues.apache.org/jira/browse/AMBARI-13205\">https://issues.apache.org/jira/browse/AMBARI-13205...</a> bug?</p>","tags":["ambari-2.1.2","snmptrap"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-04 17:51:57.0","id":8642,"title":"Puppet Module for Ambari Install","body":"<p>Hi All,</p><p>Does anyone have an updated Puppet Module for preparing servers and installing Ambari? </p><p>I found some links from a couple years ago and I wasn't sure if anyone had an updated one - <a href=\"https://forge.puppetlabs.com/vzach/ambari\">https://forge.puppetlabs.com/vzach/ambari</a></p><p>Thanks,</p>","tags":["installation","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-31 08:13:12.0","id":8443,"title":"Hive/HBASE Integration","body":"<p>#Data I'm using</p><pre>ID,fname,lname,age,Career\n4000001,Kristina,Chung,55,Pilot\n4000002,Paige,Chen,74,Teacher\n4000003,Sherri,Melton,34,Firefighter\n4000004,Gretchen,Hill,66,Computer hardware engineer\n4000005,Karen,Puckett,74,Lawyer\n4000006,Patrick,Song,42,Veterinarian\n4000007,Elsie,Hamilton,43,Pilot</pre><p># Table Creation:</p><pre>hive&gt; create table foo(id string,fname string,lname string,age string,career string) \n  &gt;  ROW FORMAT DELIMITED   \n  &gt; FIELDS TERMINATED BY ' ';  </pre><p># Copying Data:</p><pre>hive&gt; load data inpath '/API/demo1/customers.txt' overwrite into table foo;\nLoading data to table default.foo\nDeleted hdfs://172.16.56.136:10001/user/hive/warehouse/foo\nTable default.foo stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 554, raw_data_size: 0]\nOK\nTime taken: 0.282 seconds</pre><p># Conversion</p><pre>create table Jac(id string,fname string,lname string,age string,career string)   \n  &gt;  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'   \n  &gt; WITH SERDEPROPERTIES ('hbase.columns.mapping' = ':key,cf1:fname,cf1:lname,cf1:age,cf1:career')\n  &gt; TBLPROPERTIES('hbase.table.name' = 'j');  </pre><p>#error:</p><pre>hive&gt; INSERT OVERWRITE TABLE Jac SELECT * FROM foo;   \nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks is set to 0 since there's no reduce operator\nStarting Job = job_201512091118_0019, Tracking URL = <a href=\"http://172.16.56.136:50030/jobdetails.jsp?jobid=job_201512091118_0019\">http://172.16.56.136:50030/jobdetails.jsp?jobid=j...</a>\nKill Command = /usr/local/hadoop/libexec/../bin/hadoop job  -kill job_201512091118_0019\nHadoop job information for Stage-0: number of mappers: 1; number of reducers: 0\n2015-12-24 16:11:27,258 Stage-0 map = 0%,  reduce = 0%\n2015-12-24 16:11:44,312 Stage-0 map = 100%,  reduce = 100%\nEnded Job = job_201512091118_0019 with errors\nError during job, obtaining debugging information...\nJob Tracking URL: <a href=\"http://172.16.56.136:50030/jobdetails.jsp?jobid=job_201512091118_0019\">http://172.16.56.136:50030/jobdetails.jsp?jobid=j...</a>\nExamining task ID: task_201512091118_0019_m_000002 (and more) from job job_201512091118_0019\nException in thread \"Thread-90\" java.lang.RuntimeException: Error while reading from task log url\n   at org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.getStackTraces(TaskLogProcessor.java:247)\n   at org.apache.hadoop.hive.ql.exec.JobDebugger.showJobFailDebugInfo(JobDebugger.java:285)\n   at org.apache.hadoop.hive.ql.exec.JobDebugger.run(JobDebugger.java:118)\n   at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: <a href=\"http://hadoop:50060/tasklog?attemptid=attempt_201512091118_0019_m_000000_0\">http://hadoop:50060/tasklog?attemptid=attempt_201...</a>\n   at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1626)\n   at java.net.URL.openStream(URL.java:1041)\n   at org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.getStackTraces(TaskLogProcessor.java:198)\n   ... 3 more\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask\nMapReduce Jobs Launched: \nJob 0: Map: 1  HDFS Read: 0 HDFS Write: 0 FAIL\nTotal MapReduce CPU Time Spent: 0 msec</pre>","tags":["Hive","hdfs-policies","Hbase"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-04 19:12:27.0","id":8658,"title":"Spark 1.6 Technical Preview","body":"<p>Team,</p><p>Congratulations. We just published Spark 1.6 technical preview (http://hortonworks.com/hadoop-tutorial/apache-spark-1-6-technical-preview-with-hdp-2-3/)</p><p>Please take it out for a spin.</p>","tags":["how-to-tutorial","help","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-05 00:02:05.0","id":8683,"title":"Does Ranger authenticate HDFS users","body":"<p>Ok. I am very confused with this Ranger product. There is no proper documentation on this aspect. </p><p>Dose Ranger authenticate a user before accessing HDFS content?</p><p>Is it just for authorization purpose only?</p><p>I got some consultant say, if you just use Ranger for Hdfs, you can fake as someone else and connect to the HDFS.</p><p>For example, you have a user called phil and john. They have  /tenent/users/phil and /users/john respectively. Both directories has directory level permission to only that particular user and  for group owner hdfs.</p><p>Is it possible for Phil to create a unix account as john on a linux box. Sudo as john on that machine and access hdfs as john. There by faking himself as john. </p><p>Appreciate any insights on this</p>","tags":["HDFS","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-12 19:52:09.0","id":1307,"title":"Upgrade Path from HDP 2.1 to HDP 2.3.2","body":"<p>Hi,</p><p>What is the best upgrade path from HDP 2.1 to HDP 2.3.2 ? </p>","tags":["upgrade","operations","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-25 18:02:07.0","id":2086,"title":"What does NiFi use non-heap memory for?","body":"<p>I'm trying to make sense of the below screenshot. Questions:</p><ol><li>What is non-heap memory used for? I couldn't find any reference in the admin guide.</li><li>Does -1 mean uncapped memory usage? What is a correlation between Total and Max here?</li><li>Should I be concerned there's only ~11MB left of free non-heap memory? Should we bump it? How?</li></ol><p><img src=\"/storage/attachments/331-screenshot.png\"></p>","tags":["performance","hdf","memory","dataflow","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-15 22:01:05.0","id":6610,"title":"Can I ensure that my own jars have classpath priority for oozie java actions?","body":"<p>I want my own jar files searched first before, for example, the standard jar files included in oozie actions like \"/hadoop/yarn/local/filecache/22/mapreduce.tar.gz/hadoop/share/hadoop/common/lib/\"</p>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 16:51:15.0","id":7103,"title":"Falcon retention deadlock","body":"<p>Hi,</p><p>we are having serious problems with Falcon feed retention. Current retention implementation creates one Oozie coordinator job for each feed. This coordinator periodically (e.g. daily) starts Oozie workflow job to take care of the feed's retention. This means that in one short interval a large number of YARN jobs is scheduled. The number of jobs exceeds capacity of our queue and jobs are stuck and queue is quickly filled with pending jobs. </p><p>After some investigation we believe there is a deadlock happening in the retention processing, since both the oozie launchers and oozie actions are running in the single queue. This queue is quickly filled with launchers, so that no containers are left for actions, but the launchers wait for actions so nothing is actually done and whole queue is frozen.</p><p>This is known issue with Oozie that is normally solved be reserving a separate queue just for the launchers so they do not compete with actions, using the 'oozie.launcher.mapred.job.queue.name' property. </p><p>However I could not find a way how to tell Falcon to use this separate queues for the retention workflow.</p><p>Has anyone seen this?  Any ideas?</p><p>Thanks,</p><p>Pavel</p>","tags":["retention","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-17 18:32:59.0","id":7119,"title":"Web client sandbox login not working","body":"<p>Hi,</p><p>I have just installed HDP on Vmplayer and have started the sandbox. The sandbox login on the vmplayer console works fine when giving the credentials but as I read there is a web client also to login to sandbox , the url for my sandbox is </p><p><a href=\"http://192.168.79.129:4200/\">http://192.168.79.129:4200/</a> but somehow the same credentials that I enter on vmplayer console is not working for the web client but working for vmplayer console.</p><p>Can someone help ? Thanks</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-21 03:53:06.0","id":7385,"title":"How to run Spark job from Oozie Workflow on HDP/hue","body":"<p>I have created a small java program for Spark. It works with \"spark-submit\" command. I like to run it from Oozie workflow. It seems HDP 2.3 has a capability to run Spark job from Oozie workflow, but on Hue's GUI, I don't have a choice of Spark job to include into a workflow. How do I do? </p>","tags":["Spark","hue","Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-20 11:55:46.0","id":7345,"title":"i have install ambari-server(Version 2.0.) ,but i can't Create a Cluster,In the \"host comfirm\" when the error occurred.","body":"<p><strong>this  is ambari_log\n</strong></p><pre>==========================\nCreating target directory...\n==========================\nCommand start time 2015-12-20 19:22:42\ntcgetattr: Invalid argument\nConnection to cluster02.hdp closed.\nSSH command execution finished\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:43\n==========================\nCopying common functions script...\n==========================\nCommand start time 2015-12-20 19:22:43\nscp /usr/lib/python2.6/site-packages/ambari_commons\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:44\n==========================\nCopying OS type check script...\n==========================\nCommand start time 2015-12-20 19:22:44\nscp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.py\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:44\n==========================\nRunning OS type check...\n==========================\nCommand start time 2015-12-20 19:22:44\nCluster primary/cluster OS family is redhat6 and local/current OS family is redhat6\ntcgetattr: Invalid argument\nConnection to cluster02.hdp closed.\nSSH command execution finished\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:45\n==========================\nChecking 'sudo' package on remote host...\n==========================\nCommand start time 2015-12-20 19:22:45\nsudo-1.8.6p3-7.el6.i686\ntcgetattr: Invalid argument\nConnection to cluster02.hdp closed.\nSSH command execution finished\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:58\n==========================\nCopying repo file to 'tmp' folder...\n==========================\nCommand start time 2015-12-20 19:22:58\nscp /etc/yum.repos.d/ambari.repo\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:58\n==========================\nMoving file to repo dir...\n==========================\nCommand start time 2015-12-20 19:22:58\ntcgetattr: Invalid argument\nConnection to cluster02.hdp closed.\nSSH command execution finished\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:58\n==========================\nCopying setup script file...\n==========================\nCommand start time 2015-12-20 19:22:58\nscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py\nhost=cluster02.hdp, exitcode=0\nCommand end time 2015-12-20 19:22:59\n==========================\nRunning setup agent script...\n==========================\nCommand start time 2015-12-20 19:22:59\n/bin/sh: /usr/sbin/ambari-agent: No such file or directory\nRepository epel is listed more than once in the configuration\nError: No matching Packages to list\nRepository epel is listed more than once in the configuration\nError: No matching Packages to list\n('', None)\ntcgetattr: Invalid argument\nConnection to cluster02.hdp closed.\nSSH command execution finished\nhost=cluster02.hdp, exitcode=1\nCommand end time 2015-12-20 19:23:10\nERROR: Bootstrap of host cluster02.hdp fails because previous action finished with non-zero exit code (1)\nERROR MESSAGE: tcgetattr: Invalid argument\nConnection to cluster02.hdp closed.\nSTDOUT: /bin/sh: /usr/sbin/ambari-agent: No such file or directory\nRepository epel is listed more than once in the configuration\nError: No matching Packages to list\nRepository epel is listed more than once in the configuration\nError: No matching Packages to list\n('', None)\ntcgetattr: Invalid argument\nConnection to cluster02.hdp closed.</pre>","tags":["installation","ambari-server","cluster","error"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-22 13:01:07.0","id":7654,"title":"Overriding Sqoop datatype mapping","body":"<p>Hi,</p><p>I want to override the existing data type mapping , which sqoop does while processing RDBMs to Hive and vice versa.</p><p>i know this can be done with -map-column-hive tag but before is there any link or document which illustrate the complete datatype mapping of RDBMS and HIVE.</p><p>Thanks in advance..</p>","tags":["data-migration","Sqoop","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-22 17:29:25.0","id":7698,"title":"After /etc/krb5.conf ticket renewal changes.  Do we required to regenerate all keytabs?","body":"<p>Planned to update krb5.conf with following settings.</p><p>ticket_lifetime = 270m\nrenew_lifetime = 7d</p><p>Do I need to run regenerate keytabs wizard and recreate all keytabs or restart HDP services will be sufficient ?</p><p>Executed kerberos wizard with default krb5.conf settings.  krb5.conf not managed by ambari.</p><p>ticket_lifetime = 24h\nrenew_lifetime = 7d</p><p>While troubleshooting ticket renewal issue found following kerberos settings in sssd.conf</p><p>krb5_lifetime = 10h\nkrb5_renewable_lifetime = 7d\nkrb5_renew_interval = 270m</p><p>Thanks:</p><p>ENV:  Ambari 2.1.2.1/HDP 2.3.2/RHEL 7.1/Oracle JDK 1.8 U65 / AD 2008 R2 KDC</p>","tags":["Ambari","kerberos","active-directory"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-22 02:42:09.0","id":7624,"title":"Load data and drop table commands  failed with metastore connection failed error","body":"<p>Beeline Error:</p><pre> LOAD  DATA  INPATH  '/tmp/actian_test.txt' OVERWRITE INTO TABLE t1_actian_table;\nINFO  : Loading data to table default.t1_actian_table from hdfs://HDPUAT/tmp/actian_test.txt\nNo rows affected (143.199 seconds)\nINFO  : [Warning] could not update stats.Failed with exception Unable to alter table. For direct MetaStore DB connections, we don't support retries at the client level.\norg.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. For direct MetaStore DB connections, we don't support retries at the client level.\n  at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:459)\n  at org.apache.hadoop.hive.ql.exec.StatsTask.aggregateStats(StatsTask.java:184)\n  at org.apache.hadoop.hive.ql.exec.StatsTask.execute(StatsTask.java:118)\n  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:86)\n  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1630)\n  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1389)\n  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1197)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1024)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1019)\n  at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:153)\n  at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)\n  at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:205)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n  at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)\n  at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:217)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:745)\nCaused by: MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:262)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:88)\n  at com.sun.proxy.$Proxy12.alter_table(Unknown Source)\n  at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:457)\n  ... 22 more</pre><p>Hiveserver2.log shows following error.</p><pre>==&gt; hiveserver2.log &lt;==\n2015-12-21 21:18:00,377 WARN  [HiveServer2-Background-Pool: Thread-4240]: metastore.RetryingMetaStoreClient (RetryingMetaStoreClient.java:invoke(115)) - MetaStoreClient lost connection. Attempting to reconnect.\nMetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:262)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:88)\n  at com.sun.proxy.$Proxy12.getDatabase(Unknown Source)\n  at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1253)\n  at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1242)\n  at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:2316)\n  at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:416)\n  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:86)\n  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1630)\n  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1389)\n  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1197)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1024)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1019)\n  at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:153)\n  at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)\n  at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:205)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n  at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)\n  at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:217)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:745)</pre><p>Thanks</p><p>Srinivas</p>","tags":["Hive","hiveserver2"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-23 10:54:06.0","id":7817,"title":"Random Alerts for Hive in Ambari","body":"<p>Hi,</p><p>After restarting the cluster <strong>randomly </strong>we see couple of red alerts in Ambari. Earlier also I remember seeing them sometime back and now again I see them again.</p><p>Can you suggest what could be going wrong?</p><p>I checked the ports 10001 and 9083 are open/in use.</p><p>HiveHive Metastore Process </p><p>====================================</p><pre>Connection failed on host HIVE_HOST:10001 (Execution of 'ambari-sudo.sh su ambari-qa -l -s /bin/bash -c 'export PATH='\"'\"'/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/var/lib/ambari-agent:/var/lib/ambari-agent:/bin/:/usr/bin/:/usr/lib/hive/bin/:/usr/sbin/'\"'\"' ; ! beeline -u '\"'\"'jdbc:hive2://HIVE_HOST:10001/;transportMode=http;httpPath=cliservice;principal=hive/_HOST@REALM.COM'\"'\"' -e '\"'\"''\"'\"' 2&gt;&1| awk '\"'\"'{print}'\"'\"'|grep -i -e '\"'\"'Connection refused'\"'\"' -e '\"'\"'Invalid URL'\"'\"''' was killed due timeout after 30 seconds)</pre><p>HiveHiveServer2 Process</p><p>\n=============================</p><pre>Metastore on HIVE_HOST failed (Execution of 'ambari-sudo.sh su ambari-qa -l -s /bin/bash -c 'export PATH='\"'\"'/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/var/lib/ambari-agent:/var/lib/ambari-agent:/bin/:/usr/bin/:/usr/sbin/:/usr/hdp/current/hive-metastore/bin'\"'\"' ; export HIVE_CONF_DIR='\"'\"'/usr/hdp/current/hive-metastore/conf/conf.server'\"'\"' ; hive --hiveconf hive.metastore.uris=thrift://HIVE_HOST:9083 --hiveconf hive.metastore.client.connect.retry.delay=1 --hiveconf hive.metastore.failure.retries=1 --hiveconf hive.metastore.connect.retries=1 --hiveconf hive.metastore.client.socket.timeout=14 --hiveconf hive.execution.engine=mr -e '\"'\"'show databases;'\"'\"''' was killed due timeout after 30 seconds)\n</pre>","tags":["Ambari","hiveserver2","metastore","Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-23 10:24:02.0","id":7810,"title":"How to integrate Apache Nutch2.3 with HDP2.3..please provide reference","body":"<p>How to integrate Apache Nutch2.3 with HDP2.3..please provide reference</p>","tags":["nutch","hdp-2.3.0","nutch-2.3"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-12-23 19:06:54.0","id":7883,"title":"ZK registry entries cleanup for slider app","body":"<p>Hi,</p><p>Setup - slider version .80, secure hadoop cluster, registry enabled with security.</p><p>Noticed using *** zookeeper client (zkCli.sh) *** that when slider application is stopped, the registry entries in zookeeper /registry/users/xxx/services/org-apache-slider/abc/components do not get deleted. However, they do get updated when application is started again and new container IDs get allocated. In this case, the components/containers_ do reflect new container IDs. Also noticed that ZK registry entries do get deleted if \"destroy\" applications is done. </p><p>Is this expected behavior ? Shouldn't the ZK entries be deleted when application and its containers are not present after application is stopped ? </p><p>Thanks in advance,</p><p>Manoj</p>","tags":["slider"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-27 14:01:19.0","id":8103,"title":"What are the simple ways to know Hadoop Environment in AWS working as configured???","body":"<p>I know some of the admin commands to check cluster detail.I am planning to write HDP practical as well as real exam.Other than admin commands ,Is there any way to check AWS ???</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-28 10:34:35.0","id":8136,"title":"Ambari  fails to register with nodes","body":"<p>I am trying to install HDP using ambari,however its failing to even register with the node at confirm host step.</p><p>Following is the error :-</p><pre>=========================\nCreating target directory...\n==========================\n\nCommand start time 2015-12-28 02:33:40\nTraceback: Traceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py\", line 218, in try_to_execute\n    retcode = action()\n  File \"/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py\", line 655, in createTargetDir\n    retcode = ssh.run()\n  File \"/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py\", line 138, in run\n    stderr=subprocess.PIPE)\n  File \"/usr/lib64/python2.6/subprocess.py\", line 642, in __init__\n    errread, errwrite)\n  File \"/usr/lib64/python2.6/subprocess.py\", line 1238, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\nERROR: Bootstrap of host datanode13.h2.dev.bigdata.sv2.247-inc.net fails because previous action finished with non-zero exit code (177)\nERROR MESSAGE: Execute of '&lt;bound method BootstrapDefault.createTargetDir of &lt;BootstrapDefault(Thread-1, started daemon 139713426605824)&gt;&gt;' failed\nSTDOUT: Try to execute '&lt;bound method BootstrapDefault.createTargetDir of &lt;BootstrapDefault(Thread-1, started daemon 139713426605824)&gt;&gt;'</pre>","tags":["Ambari","ambari-alerts","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-24 10:50:54.0","id":7951,"title":"Oozie HIveAction throwing permission denied exception","body":"<p>\n\tHello, i have file hive.hql, and it's working well then i run from ssh hive. But then i run this file on Oozie action it failed with this exception. This user have access to this directory. This problem started from updating Oozie and HDP. </p><p>\n\tEnviroment:</p><ul>\n\t\n<li>HDP 2.2.8</li>\t\n<li>Hive 0.14</li>\t\n<li>Oozie 4.1.0</li></ul>\n<pre>FAILED: HiveException java.security.AccessControlException: Permission denied: user=tech_dmon, access=WRITE, inode=\"/user/p/data\":tech_p:bgd_p:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:185)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6868)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPathAccess(FSNamesystem.java:6793)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:9676)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:1642)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1433)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2127)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2123)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2121)\nIntercepting System.exit(40000)\nFailing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.HiveMain], exit code [40000]\n</pre><p>hive.hql</p><pre>set mapreduce.job.queuename=prod;\nuse ${MY_VAR1};\nCREATE TABLE IF NOT EXISTS `${MY_VAR1}.d_sites` ...\nINSERT OVERWRITE DIRECTORY '/user/tech_dmon/D/DB' select '${MY_VAR2}' as ...\n\n</pre><p>\n\tWhat could be causing the problem?</p><p>\n\tThank you!</p>","tags":["hadoop","Hive","Oozie","permission-denied"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-28 15:33:07.0","id":8158,"title":"Permission Denied for user=hive on LOAD DATA INPATH","body":"<p>When trying to go through the tutorials on <a href=\"http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#section_4\">http://hortonworks.com/hadoop-tutorial/hello-world...</a> I get a permission denied error when trying to LOAD DATA INPATH '/tmp/admin/data/trucks.csv' OVERWRITE INTO TABLE trucks_stage;</p><p>This is using a brand new sandbox 2.3.2 instance in Azure. I did give the trucks.csv file WRITE permission for all 3. I am logged into Ambari as \"admin\". It appears the operation is running as \"hive\", which makes sense since that shows as the service account.</p><p>LOG SNIPPET:</p><p>INFO : Loading data to table default.trucks_stage from hdfs://sandbox.hortonworks.com:8020/tmp/admin/data/trucks.csv\nERROR : Failed with exception Unable to move source hdfs://sandbox.hortonworks.com:8020/tmp/admin/data/trucks.csv to destination hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/trucks_stage/trucks.csv\norg.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://sandbox.hortonworks.com:8020/tmp/admin/data/trucks.csv to destination hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/trucks_stage/trucks.csv</p><p>...</p><p>Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=hive, access=WRITE, inode=\"/tmp/admin/data/trucks.csv\":admin:hdfs:drwxr-xr-x</p>","tags":["Sandbox","permissions","Hive","permission-denied"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-26 13:07:01.0","id":8081,"title":"pig -useHCatalog not loading all the jars required","body":"<p>In latest practice exam in AWS, pig -useHCatalog not loading all the jars required. I am trying to fix these by adding classpath or using REGISTER. My question here is do we need to deal/spend time with such environmental issues in real exam?. Other day when I started AWS instance, services were not running at first instance. @rich helped me to address this.</p>","tags":["aws","hortonworks-university","Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-28 19:08:48.0","id":8190,"title":"How can I stop the entire HDP stack via the CLI?","body":"<p>How can I stop the entire HDP stack of services from the CLI? I seem to recall that there was a command to accomplish this but I can not find it. It would seem like this facility would be associated with the ambari-agent service but I did not see any such method/action on the usage for this script/service.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-24 04:32:25.0","id":7939,"title":"Unnecessary daemons for spark-shell","body":"<p>I'm trying use latest sandbox in a Virtualbox on Win7(32bit)+2CPU. It is rather poor machine that the sandbox is very very slow, e.g. load average is constantly &gt;20. I would like to stop all unnecessary daemons just to try spark-shell CLI.</p><p>YARN, Hive, Oozie and HDFS are most heavy daemons but I cannot find out if spark-shell need them or not.</p>","tags":["daemon","Sandbox","virtualbox","configuration","cli","Spark"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-05 11:11:07.0","id":8733,"title":"Hive issue with HDP 2.3.2 on Sandbox","body":"<p>Hello,</p><p>I am facing problem in hive with sandbox 2.3.2. When i type Hive in the CLI i am getting the error message.Sometimes it is giving me potential configuration detected error.</p><p>Please look into attached image for the error<a href=\"/storage/attachments/1203-hive-error.png\"></a></p><p><img src=\"/storage/attachments/1204-1203-hive-error.png\"></p><p>\n<a href=\"/storage/attachments/1203-hive-error.png\"></a></p><p>\n<a href=\"/storage/attachments/1203-hive-error.png\"></a></p>","tags":["Sandbox","hdp-2.3.2","Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-05 14:21:25.0","id":8765,"title":"Error on Tutorial \"How to Process Data with Apache Hive\" - Cannot load data into Hive tables","body":"<p>Problem loading data to table in this tutorial </p><p><a href=\"http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-hive/\">http://hortonworks.com/hadoop-tutorial/how-to-process-data-with-apache-hive/</a></p><p>This command </p><pre>&lt;code&gt;LOAD DATA INPATH '/user/admin/Batting.csv' OVERWRITE INTO TABLE temp_batting;</pre><p>produces error</p><p>H110 Unable to submit statement. Error while compiling statement: FAILED: \n HiveAccessControlException Permission denied: user [admin] does not have [READ] privilege on \n [hdfs://sandbox.hortonworks.com:8020/user/admin/elecMonthly_Orc] [ERROR_STATUS]</p><p>I created both user/admin and temp/admin folders. I used hdfs superuser to make admin owner of file, folder, and even parent folder.  I gave full permissions in HDFS. and this is clearly shown in Ambari. Error persists.</p><p>Can anyone help?  Thanks </p>","tags":["how-to-tutorial","Sandbox","Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-05 14:32:16.0","id":8767,"title":"Hive - why doesn't truncate support a DD/Day value?","body":"<p>From the <a target=\"_blank\" href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions\">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions</a></p><pre>trunc(string date, string format)</pre><p>I'm executing these commands in beeline:</p><pre>0: jdbc:hive2://&gt; select trunc(current_timestamp(), 'MM');\nOK\n+-------------+--+\n|     _c0     |\n+-------------+--+\n| 2016-01-01  |\n+-------------+--+\n1 row selected (0.97 seconds)\n0: jdbc:hive2://&gt; select trunc(current_timestamp(), 'DD');\nOK\n+-------+--+\n|  _c0  |\n+-------+--+\n| NULL  |\n+-------+--+\n1 row selected (0.241 seconds)\n</pre><p>Now, the question is why doesn't it support a day value? This is such a natural function for truncating timestamps to a day only.</p>","tags":["help","Hive","date"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-05 15:29:29.0","id":8787,"title":"status of groovy custom udfs in beeline","body":"<p>I'm experimenting with Groovy scripts as custom UDFs in Hive and I noticed that I can't use the same syntax in beeline as in hive shell for executing custom UDFs. Is it a supported feature and syntax is different or is it not supported altogether?</p><p>The following works as is in hive shell, in beeline it throws error</p><p>compile `import <a href=\"http://org.apache.hadoop.hive.ql.exec.udf\">org.apache.hadoop.hive.ql.exec.UDF</a> \\;</p><p>import <a href=\"http://groovy.json.jsonslurper\">groovy.json.JsonSlurper</a> \\;</p><p>import <a href=\"http://org.apache.hadoop.io.text\">org.apache.hadoop.io.Text</a> \\;</p><p>public class JsonExtract extends UDF {</p><p>  public int evaluate(Text a){</p><p>    def jsonSlurper = new JsonSlurper() \\;</p><p>    def obj = <a href=\"http://jsonslurper.parsetext\">jsonSlurper.parseText</a>(<a href=\"http://a.tostring\">a.toString</a>())\\;</p><p>    return  obj.val1\\;</p><p>  }</p><p>} ` AS GROOVY NAMED json_<a href=\"http://extract.groovy\">extract.groovy</a>;</p><p>hive&gt; CREATE TEMPORARY FUNCTION json_extract as 'JsonExtract';</p><p>hive&gt; select json_extract('{\"val1\": 2}') from date_dim limit 1;</p><p>select json_extract('{\"val1\": 2}') from date_dim limit 1</p><p>OK</p><p>2</p>","tags":["hive-udf","Hive","groovy"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-05 16:44:10.0","id":8802,"title":"What is the best way to upgrade Ambari blueprints for an Ambari upgrade?","body":"<p>I use a detailed Ambari blueprint to install a HDP cluster. When a new version of Ambari is released my blueprint upgrade technique is to manually install a similar cluster, export the new blueprint, diff old and new, then update the new blueprint for anything missed. </p><p>This process is time consuming and prone to error. Is there a better way to maintain blueprints across Ambari versions? </p>","tags":["Ambari","ambari-blueprint"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-05 22:10:26.0","id":8880,"title":"Hive on Tez Pushdown Predicate doesn't work in view using window function on partitioned table","body":"<p>Using Hive on Tez running this query against this view causes a full table scan even though there is a Partition on regionid and id. This query in Cloudera Impala takes 0.6s to complete and using Hortonworks Data Platform and Hive on Tez it takes 800s. I've come to the conclusion that in Hive on Tez using a window function prevents the predicate to be pushed down to the inner select causing the full table scan. </p><pre>    CREATE VIEW latestposition AS\n    WITH t1 AS (\n      SELECT *, ROW_NUMBER() OVER ( PARTITION BY regionid, id, deviceid order by ts desc) AS rownos FROM positions \n    )\n    SELECT *\n    FROM t1\n    WHERE rownos = 1;\n \n    SELECT * FROM latestposition WHERE  regionid='1d6a0be1-6366-4692-9597-ebd5cd0f01d1' and id=1422792010 and deviceid='6c5d1a30-2331-448b-a726-a380d6b3a432';</pre><p>\nI've tried joining this table to itself using the MAX function to get the latest record, it works and finishes in a few seconds but it still is too slow for my use case. Also if I remove the window function the predicate gets pushed down and this will return in milliseconds. \nIf anyone has any ideas it would be much appreciated.</p>","tags":["Tez","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-06 03:32:30.0","id":8900,"title":"SQOOP - MYSQL - Practice test instance","body":"<p>I am working on HDPCD practice test instance. Can anybody let me know -</p><p>a) Is there anything I should do before issue sqoop command on AWS practice instance.</p><p>b) what is the parameter I should use for --connect jdbc:?????</p>","tags":["hortonworks-university","Sqoop"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-06 11:57:21.0","id":8928,"title":"ERROR: Ambari Metrics Collector Start Failed","body":"<pre>ERROR: ambari-metrics-collector start failed. For more details, see /var/log/ambari-metrics-collector/ambari-metrics-collector.out:\n====================\nError occurred during initialization of VM\nCould not reserve enough space for object heap\nError: Could not create the Java Virtual Machine.\nError: A fatal exception has occurred. Program will exit.\n==================== </pre><p>Collector out at: /var/log/ambari-metrics-collector/ambari-metrics-collector.out</p>","tags":["ambari-metrics","error","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-21 20:17:05.0","id":7570,"title":"One of the Oracle table has data that has new line char in it. After importing the data using sqoop, I am not able to represent that data in Hive. The row that has new line char is getting split into two rows.","body":"","tags":["oracle","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-06 18:07:19.0","id":8982,"title":"security component Ranger installation issues","body":"","tags":["Ranger","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-31 15:54:06.0","id":25382,"title":"HDP Express upgrade failing with  ImportError: cannot import name UPGRADE_TYPE_NON_ROLLING","body":"<p>Quite a few tasks are failing with errors similar to the following:</p><pre>2016-03-31 15:29:48,526 - Task. Type: EXECUTE, Script: scripts/ru_set_all.py - Function: actionexecute\nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/custom_actions/scripts/ru_set_all.py\", line 26, in &lt;module&gt;\n    from resource_management.libraries.script import Script\n  File \"/usr/lib/python2.6/site-packages/resource_management/__init__.py\", line 23, in &lt;module&gt;\n    from resource_management.libraries import *\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/__init__.py\", line 23, in &lt;module&gt;\n    from resource_management.libraries.functions import *\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/__init__.py\", line 25, in &lt;module&gt;\n    from resource_management.libraries.functions.default import *\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/default.py\", line 24, in &lt;module&gt;\n    from resource_management.libraries.script import Script\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/__init__.py\", line 23, in &lt;module&gt;\n    from resource_management.libraries.script.script import *\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 32, in &lt;module&gt;\n    from ambari_commons.constants import UPGRADE_TYPE_NON_ROLLING, UPGRADE_TYPE_ROLLING </pre><p>ImportError: cannot import name UPGRADE_TYPE_NON_ROLLING</p><p>Any pointers? I'm upgrading from HDP 2.2.9 to HDP 2.4 using Ambari 2.2.1.1</p>","tags":["Ambari","upgrade","hdp-2.4"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-03 17:47:26.0","id":25695,"title":"Cannot login in Shell Web Client with root/password","body":"<p>Hi,</p><p>I'm trying to log into my VM via Shell Web Client: _port_:4200 </p><p>I can log with my user and password but I cannot with root/hadoop.</p><p>As I see this is the first step needed to access Ambari, so if I cannot access with root, I cannot access Ambari.</p><p>Could anyone help? Thanks!</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-03 01:58:33.0","id":25663,"title":"location of hadoop and pig logs","body":"<p>My pig script failed when I am trying to do order by. I want to check the logs but I can't see the location of the logs. I got to know from the <a href=\"http://stackoverflow.com/questions/22856958/hadoop-logs-value-of-environment-variable-hadoop-log-dir\">post</a> that it should be set in the file (<strong>/etc/hadoop/hadoop-env.sh</strong>) but the file has everything commented as shown below. Does that mean it is not being written? how can I set those values to look into the logs as my scripts is failing again and again and I want to check the logs to troubleshoot it.\nalso my file ( <strong>hadoop-env.sh</strong>)seems to be at different path(/etc/hadoop/conf.install) is there anything wrong ??</p><p># On secure datanodes, user to run the datanode as after dropping privileges. </p><p># This **MUST** be uncommented to enable secure HDFS if using privileged ports\n# to provide authentication of data transfer protocol.  This **MUST NOT** be </p><p># defined if SASL is configured for authentication of data transfer protocol\n# using non-privileged ports. </p><p>#export HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}\n# Where log files are stored.  $HADOOP_HOME/logs by default.\n#export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}/$USER\n</p>","tags":["hadoop","Pig","logs"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-04 09:20:46.0","id":25768,"title":"org.apache.hadoop.hbase.PleaseHoldException: Master is initializing error","body":"<p>I am getting following error while listing table from hbase shell. Below is error list-</p><p>[root@data1 hbase]# hbase shell\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nHBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.\nType \"exit&lt;RETURN&gt;\" to leave the HBase Shell\nVersion 1.1.2.2.4.0.0-169, r61dfb2b344f424a11f93b3f086eab815c1eb0b6a, Wed Feb 10 07:08:51 UTC 2016\nhbase(main):001:0&gt; list\nTABLE                                                                                                                                          \nERROR: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing\nat org.apache.hadoop.hbase.master.HMaster.checkInitialized(HMaster.java:2314)\nat org.apache.hadoop.hbase.master.MasterRpcServices.getTableDescriptors(MasterRpcServices.java:853)\nat org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:53136)\nat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2114)\nat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)\nat org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)\nat org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)\nat java.lang.Thread.run(Thread.java:745)</p><p>Hdfs directory structure---</p><p>[root@data1 hbase]# hdfs dfs -ls /apps/hbase\nFound 2 items\ndrwxr-xr-x   - hbase hdfs          0 2016-04-04 05:07 /apps/hbase/data\ndrwx--x--x   - hbase hdfs          0 2016-03-31 08:12 /apps/hbase/staging</p><p>Please let me know how to fix this?</p>","tags":["hbase-namespace","Hbase"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-04-04 09:31:17.0","id":25780,"title":"Issue in storm-hive topology when run locally","body":"<p>Hello,</p><p>I'm working on storm topology that read data from kafka and write on hive database, installed on Linux machine.</p><p>When I run the topology in debug mode locally (windows machine, with Netbeans), the topology is not able to write on hive, build fails and the following errors appear. I have also included hive-site.xml in project and open hive's port in order to connect of it.</p><p>When topology runs on linux, it works as well. Do you have any idea to solve this issue?</p><p>Thanks,</p><p>Giuseppe</p><p>com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException\n   at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2234) ~[hive-exec-0.14.0.jar:0.14.0]\n   at com.google.common.cache.LocalCache.get(LocalCache.java:3965) ~[hive-exec-0.14.0.jar:0.14.0]\n   at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969) ~[hive-exec-0.14.0.jar:0.14.0]\n   at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829) ~[hive-exec-0.14.0.jar:0.14.0]\n   at org.apache.hadoop.security.Groups.getGroups(Groups.java:182) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1518) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:374) ~[hive-metastore-0.14.0.jar:0.14.0]\n   at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:215) ~[hive-metastore-0.14.0.jar:0.14.0]\n   at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:161) ~[hive-metastore-0.14.0.jar:0.14.0]\n   at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.getMetaStoreClient(HiveEndPoint.java:448) ~[hive-hcatalog-streaming-0.14.0.jar:0.14.0]\n   at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.&lt;init&gt;(HiveEndPoint.java:274) ~[hive-hcatalog-streaming-0.14.0.jar:0.14.0]\n   at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.&lt;init&gt;(HiveEndPoint.java:243) ~[hive-hcatalog-streaming-0.14.0.jar:0.14.0]\n   at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:180) ~[hive-hcatalog-streaming-0.14.0.jar:0.14.0]\n   at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:157) ~[hive-hcatalog-streaming-0.14.0.jar:0.14.0]\n   at org.apache.storm.hive.common.HiveWriter$5.call(HiveWriter.java:229) ~[storm-hive-0.10.0.jar:0.10.0]\n   at org.apache.storm.hive.common.HiveWriter$5.call(HiveWriter.java:226) ~[storm-hive-0.10.0.jar:0.10.0]\n   at org.apache.storm.hive.common.HiveWriter$9.call(HiveWriter.java:332) ~[storm-hive-0.10.0.jar:0.10.0]\n   at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_31]\n   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_31]\n   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_31]\n   at java.lang.Thread.run(Thread.java:745) [?:1.8.0_31]\nCaused by: java.lang.NullPointerException\n   at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012) ~[?:1.8.0_31]\n   at org.apache.hadoop.util.Shell.runCommand(Shell.java:483) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.util.Shell.run(Shell.java:456) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.util.Shell.execCommand(Shell.java:815) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.util.Shell.execCommand(Shell.java:798) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:84) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:239) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:220) ~[hadoop-common-2.7.2.jar:?]\n   at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:208) ~[hadoop-common-2.7.2.jar:?]\n   at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568) ~[hive-exec-0.14.0.jar:0.14.0]\n   at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350) ~[hive-exec-0.14.0.jar:0.14.0]\n   at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313) ~[hive-exec-0.14.0.jar:0.14.0]\n   at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228) ~[hive-exec-0.14.0.jar:0.14.0]\n   ... 20 more</p>","tags":["Hive","Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-04 16:28:24.0","id":25834,"title":"How to import snapshot under Hadoop Directory into Hbase Shell","body":"<p>Listed below are the steps taken for copying a table from one host onto another. </p><p>1. We took a snapshot of our table on a single node machine. </p><p>2. Move snapshot from HBase Shell to Hadoop Directory.</p><p>3. CopyToLocal Directory from Hadoop Directoy to Secure copy onto another host on a different subnet. </p><p>4. CopyFromLocal Directory to Hadoop Directory. </p><p>We are now on the last step which is to bring the snapshot folder back into HBase and restore it in HBase Shell. </p><p>Any suggestions or insight would be greatly appreciated!</p>","tags":["Hbase"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-04 14:02:12.0","id":25805,"title":"Can a datanode be shared between 2 separate Ambari Servers ?","body":"<p>Is it possible to share the same datanode between 2 different clusters monitored by separate Ambari Server for each cluster ?</p>","tags":["cluster","ambari-server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-04 22:05:41.0","id":25896,"title":"Ambari server timed out waiting for metrics","body":"<p>Whenever I tried to assign more than 3-5 hosts to a config group, ambari server would timed out with this error \"14:57:40,393 ERROR [qtp2032601831-333] BaseProvider:143 - Timed out waiting for metrics.\" then stopped responding until I restart the server.  This makes it impossible to assign hosts to different config groups.</p><p>Any idea on how to resolve this?</p><p>Ambari Version1.7.0</p><p>HDP 2.2.4</p>","tags":["ambari-1.7","hdp-2.2.4"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-04-05 01:55:53.0","id":25904,"title":"PHD cluster nodemanager memory issues","body":"<p>I want to run hdp memory script on PHD cluster, aside from directory naming convention, what else can be different? Pivotal has a single node sandbox and I can easily test it but it's an urgent issue and I do not have access to that.</p>","tags":["phd","YARN","nodemanager","pivotal"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-05 23:13:51.0","id":26065,"title":"Server registration failed","body":"<p>Trying to install two node server. First node (same as Ambari serve) went through fine. Having issues with the second server. Below is a snippet of the error message;</p><p>==========================\nRunning setup agent script...\n==========================\nCommand start time 2016-04-05 15:50:50\nsudo: /etc/sudoers.d/opscenter-agent-sudoers is owned by uid 492, should be 0\n('INFO 2016-04-05 14:42:17,741 AlertSchedulerHandler.py:330 - [AlertScheduler] Scheduling flume_agent_status with UUID 649506b0-d101-47ad-8907-848d204bbedb\nINFO 2016-04-05 14:42:17,741 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts\nINFO 2016-04-05 14:42:17,742 AlertSchedulerHandler.py:330 - [AlertScheduler] Scheduling kafka_broker_process with UUID 1d2f5533-6962-4514-9239-fffbd1721cb8\nINFO 2016-04-05 14:42:17,742 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts\nINFO 2016-04-05 14:42:17,742 AlertSchedulerHandler.py:330 - [AlertScheduler] Scheduling ams_metrics_monitor_process with UUID 206629bc-f731-4ed9-8416-b97f5c8b7982\nINFO 2016-04-05 14:42:17,742 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts</p><p>Not clear what exactly the issue is. Can anyone help?</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-05 17:41:23.0","id":26013,"title":"Is there a data encryption option for Spark Thrift Server","body":"<p>Is there any data encryption option for Spark Thrift Server ?</p>","tags":["Spark","spark-thriftserver","encryption"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-08 15:00:46.0","id":26642,"title":"Unable to submit statement, Hive internal error when I go to Hive view","body":"<p>I follow your <a href=\"http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#section_4\">tutorial </a>with the sandbox 2.4</p><p>When I go to Hive view, the following error appears :</p><pre> H110 Unable to submit statement. Error while processing statement: FAILED: Hive Internal Error: com.sun.jersey.api.client.ClientHandlerException(java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Authentication failed, status: 503, message: Service Unavailable) [ERROR_STATUS]</pre><p><strong>StackTrace</strong> :</p><pre> org.apache.ambari.view.hive.client.HiveErrorStatusException: H110 Unable to submit statement. Error while processing statement: FAILED: Hive Internal Error: com.sun.jersey.api.client.ClientHandlerException(java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Authentication failed, status: 503, message: Service Unavailable) [ERROR_STATUS]\n\norg.apache.ambari.view.hive.client.HiveErrorStatusException: H110 Unable to submit statement. Error while processing statement: FAILED: Hive Internal Error: com.sun.jersey.api.client.ClientHandlerException(java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Authentication failed, status: 503, message: Service Unavailable) [ERROR_STATUS]\n\tat org.apache.ambari.view.hive.client.Utils.verifySuccess(Utils.java:48)\n\tat org.apache.ambari.view.hive.client.Connection.execute(Connection.java:614)\n\tat org.apache.ambari.view.hive.client.Connection.executeSync(Connection.java:629)\n\tat org.apache.ambari.view.hive.client.DDLDelegator.getDBListCursor(DDLDelegator.java:76)\n\tat org.apache.ambari.view.hive.client.DDLDelegator.getDBList(DDLDelegator.java:65)\n\tat org.apache.ambari.view.hive.resources.browser.HiveBrowserService.databases(HiveBrowserService.java:88)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:196)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:216)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:205)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:152)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)</pre><p>Can I continue the tutorial or I need to fix that before ?</p>","tags":["Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-08 21:12:34.0","id":26723,"title":"Spark streaming trying to persist the object in hive, throws exception – Not able to find the tables in hive","body":"<p>I'm Performing activities as below.</p><ol>\n \n<li>Spark\n     streaming program started. Its polling periodically to read new messages\n     from Kafka topic--Success</li> \n<li>Put\n     a JSON message onto the topic --Success 3. Spark\n     streaming reading the message from Kafka topic--Success;Spark\n     streaming parsing the JSON and creating individual RDD objects --Success;</li><li>Spark\n     streaming trying to persist the object in hive, throws exception – Not\n     able to find the tables in hive. The tables are present and accessible via\n     hive CLI</li></ol><p>What could be the reason of not finding hive table?  I have spark 1.4.1 installed  on my machine</p>","tags":["spark-streaming"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-08 16:29:01.0","id":26646,"title":"Failed to execute tez graph","body":"<p>I follow your <a href=\"http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#section_4\">tutorial </a>with the sandbox 2.4</p><p>When I go to Hive view, the following error appears during the creation of geolocation table.</p><p>Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask</p><p><img src=\"/storage/attachments/3295-tezhive.png\"></p><pre>INFO : Tez session hasn't been created yet. Opening sessionERROR : Failed to execute tez graph.org.apache.tez.dag.api.TezException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1460127060985_0010 to YARN : Application application_1460127060985_0010 submitted by user hive to unknown queue: defaultat org.apache.tez.client.TezClient.start(TezClient.java:413)at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:196)at org.apache.hadoop.hive.ql.exec.tez.TezTask.updateSession(TezTask.java:271)at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:151)at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1720)at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1477)at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1254)at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1118)at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1113)at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:415)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)at java.util.concurrent.FutureTask.run(FutureTask.java:262)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1460127060985_0010 to YARN : Application application_1460127060985_0010 submitted by user hive to unknown queue: defaultat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:271)at org.apache.tez.client.TezYarnClient.submitApplication(TezYarnClient.java:72)at org.apache.tez.client.TezClient.start(TezClient.java:408)\n</pre>","tags":["Hive","Tez"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-09 22:23:18.0","id":26809,"title":"NiFi with inputs of different frequencies","body":"<p>Is there a way to read multiple inputs that each have a different frequency into NiFi and have NiFi align the values for those inputs using the least common multiple of the frequencies?  I have a heart rate input that has a frequency of 1/s, a blood pressure input that has a frequency of 1/min, and an oxygen saturation input that has a frequency of x/hr.  I want NiFi to give me heart rate, blood pressure, and oxygen saturation readings every lcm(1,60,x) seconds.  </p><p><img src=\"/storage/attachments/3324-screen-shot-2016-04-09-at-61955-pm.png\" alt=\"\" style=\"float: left; width: 571px; margin: 0px 10px 10px 0px;\"></p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-11 12:41:05.0","id":26933,"title":"error ambari-action-scheduler","body":"<p>Hi:</p><p>When start the ambari-server i can see this error:</p><pre>2016 14:02:15,986  INFO [qtp-ambari-client-1397] MetricsReportPropertyProvider:153 - METRICS_COLLECTOR is not live. Skip populating resources with metrics, next message will be logged after 1000 attempts.\n11 Apr 2016 14:02:30,746  WARN [ambari-action-scheduler] ActionScheduler:654 - Host component information has not been found.  Details:cluster=rsicluster01; host=lnxbig04.cajarural.gcr; service=YARN; component=APP_TIMELINE_SERVER;\n11 Apr 2016 14:02:30,747  WARN [ambari-action-scheduler] ActionScheduler:682 - Host lnxbig04.cajarural.gcr has been detected as non-available. Host not found when trying to schedule an execution command. The most probable reason for that is that host or host component has been deleted recently. The command has been aborted and dequeued.Execution command details: cmdId: 894-11; taskId: 5031; roleCommand: CUSTOM_COMMAND\n11 Apr 2016 14:02:31,777  WARN [ambari-action-scheduler] ActionScheduler:654 - Host component information has not been found.  Details:cluster=rsicluster01; host=lnxbig04.cajarural.gcr; service=YARN; component=APP_TIMELINE_SERVER;\n11 Apr 2016 14:02:31,777  WARN [ambari-action-scheduler] ActionScheduler:682 - Host lnxbig04.cajarural.gcr has been detected as non-available. Host not found when trying to schedule an execution command. The most probable reason for that is that host or host component has been deleted recently. The command has been aborted and dequeued.Execution command details: cmdId: 894-37; taskId: 5085; roleCommand: CUSTOM_COMMAND\n11 Apr 2016 14:03:05,354  INFO [qtp-ambari-agent-1417] HeartBeatHandler:683 - State of service component METRICS_MONITOR of service AMBARI_METRICS of cluster rsicluster01 has changed from STARTED to INSTALLED at host lnxbig05.cajarural.gcr\n\n\n</pre><p>but i dont have any APP_TIMELINE_SERVER in the host lnxbig04.cajarural.gcr, so how i cant delete this alert??? also the upgrade doesnt finish for this error.</p><p>Thanks</p>","tags":["Ambari","app_timeline_server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-11 14:50:56.0","id":26961,"title":"Sqoop --split-by on a string /varchar column","body":"<p>Can a non-numeric column be specified for a --split-by key parameter? What are the potential issues in doing so?</p>","tags":["Sqoop","ingestion","spliting"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-11 15:28:52.0","id":26972,"title":"Not able to connect phoenix  via java jdbc","body":"<p>i'm  trying to connect to phoenix using java  jdbc , but i not succeed to etablish connection  ,there is  exemple show me how to write url of connection ??, i succeed to connect to phoenix by  using sqlline.py </p><p>\". /sqlline.py node4.bigdatau:2181 \"   this is my code java  , thx</p><pre>  try {\nClass.forName(\"org.apache.phoenix.jdbc.PhoenixDriver\");\n    }catch (ClassNotFoundException e) {\n\t\tSystem.out.println(\"Where is your  JDBC Driver?\");\n\t\te.printStackTrace();\n\t}\n   conn =  DriverManager.getConnection(\" jdbc:phoenix:195.154.55.93:2181\");\n    System.out.println(\"got connection\");\n</pre>","tags":["Hbase","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-20 09:55:12.0","id":34502,"title":"import tables from sqlserver to hbase with out --hbase-row-key? is it possible??","body":"","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-21 02:53:37.0","id":34650,"title":"Persistent Hive variables possible?","body":"<p>I want to set a hive variable which will be used by all hive sessions.  Any ideas how I can do this?</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-24 16:23:01.0","id":35298,"title":"How to restrict the groups seen in Ranger?","body":"<p>Guys,</p><p>We have setup a Kerberized and A/D integrated HDP 2.3 Cluster. On the same cluster, after setting up Ranger, when I try to define policies for any components, I see all the groups available in A/D. For a larger organization, I suspect it would go in terms of hundreds.In such scenario, how can I restrict the number of groups appearing in the drop down when defining policies?</p><p>Thanks.</p>","tags":["How-To/Tutorial","ranger-usersync","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-25 07:17:26.0","id":35433,"title":"hdf version","body":"<p>Hi,</p><p>Can anyone suggest which version of HDF/NiFi I can use for HDP2.2.8.0?</p><p>Is HDF 1.1 compatible for HDP2.2.x when I wanted to interact with HDP component stacks?</p><p><strong>Thanks in advance.</strong></p>","tags":["hdp-2.2.8","hdf","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-25 02:14:15.0","id":35405,"title":"I am trying to setup Phoenix for hbase","body":"<p>I am trying to setup phoenix for hbase but i get the below message when i try to run sqlline.py in unsecure mode and hbase master is shutting down automatically. I see the below error messages on logs. Appreciate your help and TIA.</p><pre>Setting property: [isolation, TRANSACTION_READ_COMMITTED]\nissuing: !connect jdbc:phoenix:localhost:2181:/hbase-unsecure none none org.apache.phoenix.jdbc.PhoenixDriver\nConnecting to jdbc:phoenix:localhost:2181:/hbase-unsecure\n16/05/24 17:55:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n16/05/24 17:55:13 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-phoenix.properties,hadoop-metrics2.properties\n16/05/24 17:55:14 WARN client.ConnectionManager$HConnectionImplementation: Checking master connection\ncom.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to host:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to host:16000 is closing. Call id=6, waitTime=1\n  at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:223)\n  at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:287)\n  at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.isMasterRunning(MasterProtos.java:50918)\n  at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$MasterServiceState.isMasterRunning(ConnectionManager.java:1437)\n  at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.isKeepAliveMasterConnectedAndRunning(ConnectionManager.java:2047)\n  at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getKeepAliveMasterService(ConnectionManager.java:1701)\n  at org.apache.hadoop.hbase.client.MasterCallable.prepare(MasterCallable.java:38)\n  at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:124)\n  at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3917)\n  at org.apache.hadoop.hbase.client.HBaseAdmin.access$700(HBaseAdmin.java:187)\n  at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.getProcedureResult(HBaseAdmin.java:4190)\n  at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.waitProcedureResult(HBaseAdmin.java:4142)\n  at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.get(HBaseAdmin.java:4098)\n  at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:560)\n  at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:842)\n  at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1215)\n  at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable(DelegateConnectionQueryServices.java:112)\n  at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:1902)\n  at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:744)\n  at org.apache.phoenix.compile.CreateTableCompiler$2.execute(CreateTableCompiler.java:186)\n  at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:303)\n  at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:295)\n  at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)\n  at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:293)\n  at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:1236)\n  at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:1893)\n  at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:1862)\n  at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:77)\n  at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:1862)\n  at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:180)\n  at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.connect(PhoenixEmbeddedDriver.java:132)\n  at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:151)\n  at sqlline.DatabaseConnection.connect(DatabaseConnection.java:157)\n  at sqlline.DatabaseConnection.getConnection(DatabaseConnection.java:203)\n  at sqlline.Commands.connect(Commands.java:1064)\n  at sqlline.Commands.connect(Commands.java:996)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at sqlline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:36)\n  at sqlline.SqlLine.dispatch(SqlLine.java:804)\n  at sqlline.SqlLine.initArgs(SqlLine.java:588)\n  at sqlline.SqlLine.begin(SqlLine.java:656)\n  at sqlline.SqlLine.start(SqlLine.java:398)\n  at sqlline.SqlLine.main(SqlLine.java:292)\nCaused by: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to host:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to host:16000 is closing. Call id=6, waitTime=1\n  at org.apache.hadoop.hbase.ipc.RpcClientImpl.wrapException(RpcClientImpl.java:1239)\n  at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1210)\n  at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:213)\n  ... 45 more\nCaused by: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to host:16000 is closing. Call id=6, waitTime=1\n  at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.cleanupCalls(RpcClientImpl.java:1037)\n  at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.close(RpcClientImpl.java:844)\n  at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.run(RpcClientImpl.java:572)\n16/05/24 17:55:14 WARN client.ConnectionManager$HConnectionImplementation: Checking master connection\ncom.google.protobuf.ServiceException: java.net.ConnectException: Connection refused\n  at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:223)\n  at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:287)\n  at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.isMasterRunning(MasterProtos.java:50918)\n  at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$MasterServiceState.isMasterRunning(ConnectionManager.java:1437)\n  at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.isKeepAliveMasterConnectedAndRunning(ConnectionManager.java:2047)\n  at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getKeepAliveMasterService(ConnectionManager.java:1701)\n  at org.apache.hadoop.hbase.client.MasterCallable.prepare(MasterCallable.java:38)\n  at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:124)\n  at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3917)\n  at org.apache.hadoop.hbase.client.HBaseAdmin.access$700(HBaseAdmin.java:187)\n  at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.getProcedureResult(HBaseAdmin.java:4190)\n  at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.waitProcedureResult(HBaseAdmin.java:4142)\n  at org.apache.hadoop.hbase.client.HBaseAdmin$ProcedureFuture.get(HBaseAdmin.java:4098)\n  at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:560)\n  at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:842)\n  at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1215)\n  at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable(DelegateConnectionQueryServices.java:112)\n  at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:1902)\n  at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:744)\n  at org.apache.phoenix.compile.CreateTableCompiler$2.execute(CreateTableCompiler.java:186)\n  at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:303)\n  at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:295)</pre><p>Logs from hbase-master</p><pre>2016-05-24 17:55:14,824 FATAL [WALProcedureStoreSyncThread] master.HMaster: Master server abort: loaded coprocessors are: []\n2016-05-24 17:55:14,824 INFO  [WALProcedureStoreSyncThread] regionserver.HRegionServer: STOPPED: The Procedure Store lost the lease\n2016-05-24 17:55:14,825 INFO  [master/host:16000] regionserver.HRegionServer: Stopping infoServer\n2016-05-24 17:55:14,851 INFO  [master/host:16000] mortbay.log: Stopped SelectChannelConnector@0.0.0.0:16010\n2016-05-24 17:55:14,862 INFO  [master/host:16000] procedure2.ProcedureExecutor: Stopping the procedure executor\n2016-05-24 17:55:14,862 INFO  [master/host:16000] wal.WALProcedureStore: Stopping the WAL Procedure Store\n2016-05-24 17:55:14,863 WARN  [master/host:16000] wal.WALProcedureStore: Unable to write the trailer: File /apps/hbase/data/MasterProcWALs/state-00000000000000000022.log could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and no node(s) are excluded in this operation.\n  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1551)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3108)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3032)\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:422)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)\n2016-05-24 17:55:14,863 ERROR [master/host:16000] wal.WALProcedureStore: Unable to close the stream\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /apps/hbase/data/MasterProcWALs/state-00000000000000000022.log could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and no node(s) are excluded in this operation.\n  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1551)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3108)</pre>","tags":["Phoenix"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-26 22:24:04.0","id":35975,"title":"where to hdfs apache drill","body":"<p>hi:</p><p>using queries to apache drill i found something strange</p><pre>0: jdbc:drill:&gt; SELECT * FROM hdfs.`f6214t.json` WHERE NUM-PARTICION-F='046';\n\n+------------------+-----------------+----------------+----------------+--------------------+-------------------+-----------------------+---------------+----------------+-----------------------+-----------------------+------------+-----------+--------------+----------------+----------------+---------------+-----------------------+-------------------+--------------+--------+-----------+\n\n| NUM-PARTICION-F  | NOMBRE-REGLA-F  | FECHA-OPRCN-F  | COD-NRBE-EN-F  | COD-NRBE-EN-FSC-F  | COD-INTERNO-UO-F  | COD-INTERNO-UO-FSC-F  | COD-CSB-OF-F  | COD-CENT-UO-F  | ID-INTERNO-TERM-TN-F  | ID-INTERNO-EMPL-EP-F  | NUM-SEC-F  | COD-TX-F  | COD-TX-DI-F  | ID-EMPL-AUT-F  | FECHA-CTBLE-F  | HORA-OPRCN-F  | COD-IDENTIFICACION-F  | IDENTIFICACION-F  | VALOR-IMP-F  | CANAL  | VARIABLE  |\n\n+------------------+-----------------+----------------+----------------+--------------------+-------------------+-----------------------+---------------+----------------+-----------------------+-----------------------+------------+-----------+--------------+----------------+----------------+---------------+-----------------------+-------------------+--------------+--------+-----------+\n\n+------------------+-----------------+----------------+----------------+--------------------+-------------------+-----------------------+---------------+----------------+-----------------------+-----------------------+------------+-----------+--------------+----------------+----------------+---------------+-----------------------+-------------------+--------------+--------+-----------+\n\nNo rows selected (0.248 seconds)\n</pre><p>and the second contain data withouth where</p><pre>0: jdbc:drill:&gt; SELECT * FROM hdfs.`f6214t.json`;\n\n+-----------------+----------------+---------------+---------------+-------------------+------------------+----------------------+--------------+---------------+----------------------+----------------------+-----------+----------+-------------+---------------+---------------+--------------+----------------------+------------------+-------------+-------+----------+\n\n| NUM-PARTICION-F | NOMBRE-REGLA-F | FECHA-OPRCN-F | COD-NRBE-EN-F | COD-NRBE-EN-FSC-F | COD-INTERNO-UO-F | COD-INTERNO-UO-FSC-F | COD-CSB-OF-F | COD-CENT-UO-F | ID-INTERNO-TERM-TN-F | ID-INTERNO-EMPL-EP-F | NUM-SEC-F | COD-TX-F | COD-TX-DI-F | ID-EMPL-AUT-F | FECHA-CTBLE-F | HORA-OPRCN-F | COD-IDENTIFICACION-F | IDENTIFICACION-F | VALOR-IMP-F | CANAL | VARIABLE |\n\n+-----------------+----------------+---------------+---------------+-------------------+------------------+----------------------+--------------+---------------+----------------------+----------------------+-----------+----------+-------------+---------------+---------------+--------------+----------------------+------------------+-------------+-------+----------+\n\n| 046 | TR_AF_CONS_APUNTE_TRN | 2016-02-14 12:50:45 | 3187 | 3187 | 0903 | 0903 | 0964 |  | A8790323 | U875407 | 7925 | GAE05COU | TUX |  | 2016-02-15 | 12.50.45 | 01 | 1390490918 | 0.00 | 01 | {\"COD-NRBE-EN-F\":\"3187\",\"PRPDAD-CTA-F\":\"A\",\"NUM-SEC-AC-F\":\"1390490918\",\"COD-CTA-F\":\"01\",\"COD-NUMRCO-MONEDA-F\":\"978\",\"NUM-SEC-F\":\"970\",\"SGN-F\":\"D\",\"IMP-APNTE-F\":\"68000.00\",\"FECHA-CTBLE-F\":\"2013-06-11\",\"FECHA-VALOR-F\":\"2013-06-11\",\"IND-1-F\":\"0\",\"IND-4-F\":\"C\",\"COD-ORGN-APNTE-F\":\"AC\",\"ID-ORGN-APNTE-F\":\"3187  ??°jð\",\"CONCPT-APNTEL\":\"60\",\"CONCPT-APNTEC\":\"REINTEGRO EN EFECTIVO\",\"COD-ORIGEN-F\":\"010001\",\"COD-INTERNO-UO-F\":\"0118\"} |\n\n| 045 | TR_AF_CONS_APUNTE_TRN | 2016-02-14 12:44:48 | 3187 | 3187 | 0903 | 0903 | 0964 |  | A8790323 | U875407 | 7925 | GAE05COU | TUX |  | 2016-02-15 | 12.44.48 | 01 | 1390490918 | 0.00 | 01 | {\"COD-NRBE-EN-F\":\"3187\",\"PRPDAD-CTA-F\":\"A\",\"NUM-SEC-AC-F\":\"1390490918\",\"COD-CTA-F\":\"01\",\"COD-NUMRCO-MONEDA-F\":\"978\",\"NUM-SEC-F\":\"970\",\"SGN-F\":\"D\",\"IMP-APNTE-F\":\"68000.00\",\"FECHA-CTBLE-F\":\"2013-06-11\",\"FECHA-VALOR-F\":\"2013-06-11\",\"IND-1-F\":\"0\",\"IND-4-F\":\"C\",\"COD-ORGN-APNTE-F\":\"AC\",\"ID-ORGN-APNTE-F\":\"3187  ??°jð\",\"CONCPT-APNTEL\":\"60\",\"CONCPT-APNTEC\":\"REINTEGRO EN EFECTIVO\",\"COD-ORIGEN-F\":\"010001\",\"COD-INTERNO-UO-F\":\"0118\"} |\n\n| 056 | TR_AF_CONS_APUNTE_TRN | 2016-02-14 11:23:43 | 3059 | 3059 | 0005 | 0005 | 0005 |  | A3000504 | U303863 | 258913 | GAE05COU | TUX |  | 2016-02-15 | 11.23.43 | 01 | 1556192415 | 0.00 | 01 | {\"COD-NRBE-EN-F\":\"3059\",\"PRPDAD-CTA-F\":\"A\",\"NUM-SEC-AC-F\":\"1556192415\",\"COD-CTA-F\":\"01\",\"COD-NUMRCO-MONEDA-F\":\"978\",\"NUM-SEC-F\":\"1751\",\"SGN-F\":\"D\",\"IMP-APNTE-F\":\"300.00\",\"FECHA-CTBLE-F\":\"2016-02-15\",\"FECHA-VALOR-F\":\"2016-02-13\",\"IND-1-F\":\"0\",\"IND-4-F\":\"A\",\"COD-ORGN-APNTE-F\":\"AC\",\"ID-ORGN-APNTE-F\":\"3059  í/k *4599810002830026\",\"CONCPT-APNTEL\":\"60\",\"CONCPT-APNTEC\":\"TJ-CAJERO RURAL\",\"COD-ORIGEN-F\":\"470034\",\"COD-INTERNO-UO-F\":\"0097\"} |\n</pre><p>and you can see, the NUM-PARTICION-F='046' exist, why the where doesnt work??</p><p>thanks</p>","tags":["drill"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-05-27 02:44:44.0","id":35993,"title":"Name node Manual failover","body":"<p>Hi All ,</p><p>I am familiar with Cloudera manager there is an option in it to fail over the Name node manually.</p><p> Could some body tell me how can do it in Ambari.</p><p>Thanks in Advance</p><p>Abdul</p>","tags":["fail-over","namenode-ha","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-28 20:04:17.0","id":36263,"title":"unable to log on to ambari","body":"<p>Hi All,</p><p>I am a newbie to Hadoop and need your help to configure HotonWork Sandbox. I have installed Sandbox and can connect to Welcome Page using localhost:8888. However , another URL localhost:8080 is not accessible. I tried to implement all the options mentioned on multiple threads but failed to get through. Also, as somewhere mentioned on the post that amabari log is put on the location /var/log/ambari-server/ambari-server.log and looking at the log i could get the clue . But , this path is not accessible to me as a root user. Attached is the screenshot<a href=\"/storage/attachments/4622-4568-1.png\">4568-1.png</a></p>","tags":["Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-30 08:26:39.0","id":36361,"title":"HDP2.4.2 Spark jobs log can't be access","body":"<p>I've enabled yarn ranger plugin under ambari.also configured yarn.admin.acl with \"dr.who\". but after I submit job, I found I can't access logs. It shows below:</p><h1>User [dr.who] is not authorized to view the logs for container_e18_1464594716018_0007_01_000001 in log file [host1_45454]</h1><h1>No logs available for container container_e18_1464594716018_0007_01_000001</h1><p>anyone has same problems?thanks in advance.</p>","tags":["Spark"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-05-30 14:36:54.0","id":36367,"title":"Ni Fi Installed in VMWare CentOs and URL access error","body":"<p>Hi All </p><p>Perhaps an apache mailing list question but nevertheless asking here from experts. I have a standalone hadoop environment in CentOs. I have now untared NiFi in the same VM. I am able to start and Stop NiFi from CentOs. I am trying to access the Ni Fi application from my laptop (not from VM) using http://192.168.191.128:9080/nifi </p><p>I have altered nifi.properties to change port to 9080. I am able to ping my server and other services on Vmware. Is there any setting I am missing. Any help appreciated!</p><p>On the other hand. If I have NiFi set up on windows and have my Hadoop cluster running on CentOs. How can I give the config path for core-site and hdfs-site to processors : Do I have to suffix IPaddress somewhere?</p><p>Regards</p><p>Srivi</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-30 01:14:57.0","id":36297,"title":"what is map reduce master port & local port in for hortonworks sandbox?","body":"<p>Hi All,</p><p>I'm trying to create DFS location in my Eclipse. </p><p>I'm running my sandbox in virtual box. with localhost:127.0.0.1:8080</p><p>While, creating the DFS location , its asking me to give the Map/Reduce Master Port & Map/Reduce Local port.</p><p>Can anyone help me how do i know those for creating the dfs location in eclipse.</p><p>Regards,</p><p>Naren.</p>","tags":["MapReduce"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-30 10:03:18.0","id":36357,"title":"create oozie workflow error","body":"<p>i get sometime this error when i try to create a oozie workflow ,</p>\n<pre>Error: E0501:Couldnot perform authorization operation,Failed on local exception: java.io.IOException:Couldn't set up IO streams; Host Details : local host is: \"xxxx\"; destination host is: xxxx</pre><p>Any help will be greatly appreciated!</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-01 03:15:02.0","id":36879,"title":"Error while executing sql in spark-shell","body":"<p>Hi,</p><p>In my local machine I have setup the hadoop, hive, spark and scala  this week and would want to execute the query from spark-shell. The following statements goes fine until the sql statement. </p><p>scala&gt; import org.apache.spark.sql.hive.HiveContext</p><p>scala&gt; import org.apache.spark.sql.SQLContext\nscala&gt; val hiveCtx = new HiveContext(sc)\nscala&gt; import hiveCtx._</p><p>scala&gt; var rec1 = hiveCtx.sql(\"\"\" SELECT * from bkfs.tbl1 LIMIT 5 \"\"\")</p><p>scala&gt; rec1=hiveCtx.sql(\"\"\"SELECT * FROM BKFS.ASMT_02013 LIMIT 1\"\"\")\nscala.reflect.internal.Types$TypeError: bad symbolic reference. A signature in HiveMetastoreCatalog.class refers to term cache\nin package com.google.common which is not available.\nIt may be completely missing from the current classpath, or the version on\nthe classpath might be incompatible with the version used when compiling HiveMetastoreCatalog.class.\nThat entry seems to have slain the compiler.  Shall I replay\nyour session? I can re-run each line except the last one.\n[y/n]Replaying: import org.apache.spark.sql.hive.HiveContext\nerror: \n  while compiling: &lt;console&gt;\n  during phase: jvm\n  library version: version 2.10.4</p><p>[y/n]Replaying: import org.apache.spark.sql.hive.HiveContext\nerror: \n  while compiling: &lt;console&gt;\n  during phase: jvm\n  library version: version 2.10.4\n  compiler version: version 2.10.4\n  reconstructed args: \n\n  last tree to typer: Apply(constructor $read)\n  symbol: constructor $read in class $read (flags: &lt;method&gt; &lt;triedcooking&gt;)\n  symbol definition: def &lt;init&gt;(): $line28.$read\n  tpe: $line28.$read\n  symbol owners: constructor $read -&gt; class $read -&gt; package $line28\n  context owners: class iwC -&gt; package $line28\n\n== Enclosing template or block ==\n\nTemplate( // val &lt;local $iwC&gt;: &lt;notype&gt;, tree.tpe=$line28.iwC\n  \"java.lang.Object\", \"scala.Serializable\" // parents\n  ValDef(\n  private\n  \"_\"\n  &lt;tpt&gt;\n  &lt;empty&gt;\n  )</p><p>== Expanded type of tree ==\n\nTypeRef(TypeSymbol(class $read extends Serializable))\n\nuncaught exception during compilation: java.lang.AssertionError\nStopping spark context.\n&lt;console&gt;:8: error: not found: value sc\n  sc.stop()\n  ^\nException in thread \"main\" java.lang.AssertionError: assertion failed: Tried to find '$line28' in '/tmp/spark-e0d2a57e-c03e-467a-92b2-c560baaa9c34' but it is not a directory</p><p>Could you suggest what might be the issue.\n\nThanks!!!</p>","tags":["spark-sql"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-06-01 10:31:06.0","id":36924,"title":"Hive line break issue","body":"<p>Hi all,</p><p>I have a hive table over an accumulo table (because we need cell level security):</p><p>CREATE TABLE testtable(rowid string, value string) \nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler' \nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,c:value') TBLPROPERTIES ('<a href=\"http://accumulo.table.name\">accumulo.table.name</a>' = 'testtable');</p><p>If i have a value which contains \"/n\" it conflicts with the default hive line break property which is also \"/n\".</p><p>for example:</p><p>accumulo insert: `insert 1 c value line\\x0Abreak`</p><p>hive select: `select rowid, value, row_number() over (order by null) as rank  from testtable;`</p><p>you will get back two rows instead of one.</p><p>  | rowid  | value  | rank  | </p><p> +---------+--------+-------+ </p><p> | 2  | line  | NULL  | </p><p> | break  | 1  | NULL  |</p><p>Is there any idea how can I avoid this? Thank you for all the help!<img src=\"https://ssl.gstatic.com/ui/v1/icons/mail/images/cleardot.gif\"></p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-02 15:25:27.0","id":37323,"title":"LDAP: error code 4 - Sizelimit Exceeded - KNOX","body":"<p>Hi,</p><p>I got an error that sizelimit of my search base in Knox exceeded. Is there any possibility to expand the limit? I use the same LDAP searchbase for Ranger and everything is ok, so it is a Knox problem, not AD/LDAP (I read that I need to expand it in LDAP Admin).</p><p>Thank you in advance! :)</p>","tags":["active-directory","ldap","Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-06 10:20:08.0","id":37839,"title":"Hi i am new to hadoop admin, i want to  remove write access to all databases and file systems in production cluster and we are using hdp 2.1.5. can any one help me how to remove write access,which directory i should modified","body":"","tags":["hadoop-maintenance"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-06 21:32:22.0","id":37993,"title":"Which database does Atlas use?","body":"<p>Hello guys,</p><p>I'd like to know which database Apache Atlas uses and how I can have access to it.\nBest,\nMisael Castro</p>","tags":["governance","Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-07 16:05:35.0","id":38225,"title":"Is there any Documentation on Migration Strategy/Plan/Best practices/Methodology for Migration from Pivotal HD to HDP?","body":"","tags":["migration","pivotal"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-05-27 06:10:50.0","id":36017,"title":"Raw Historical data for Hadoop POC","body":"<p>Hey Guys! Is there any website/way to get bulk of raw data (approx 50-100TB) for Hadoop POC? Can we get historical data from Twitter feed? No specific use-case as of now, just wanted to check if we can download this much amount of data.</p>","tags":["bulk","data"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-27 13:14:02.0","id":36065,"title":"How to clean and fresh install of HDP","body":"<p>Hi,</p><p>My installation got aborted half way through.  How to clean and do a fresh installation of HDP 2.3 cluster using Ambari 2.2.</p><p>I tried the steps given in the uninstall hdp (chapter 27) but I am unable to do the installation.  I am getting the attached error message.</p><p>Kindly share the steps to do it successfully.  Thanks in advance.</p><p>I am trying this on RHEL 7.2 (3 node cluster).</p><p>Regards,</p><p>Subrmanian S.</p>","tags":["installation"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-27 13:35:50.0","id":36081,"title":"failed to detect group by on hive query","body":"<p>while executing a simple hive query with group by getting following error:</p><p>FAILED: ParseException line 1:135 Failed to recognize predicate 'group'. Failed rule: 'identifier' in table or column identifier</p><p>I restarted my entire cluster by did'nt get success. can somebody help me identifying and resolving the issue.</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-09 10:32:18.0","id":38748,"title":"org.apache.commons.dbcp.SqlNestedException, cannot get a connection,Connection timeout exception????","body":"<p>Hello Everyone,</p><p>I'm trying to connect hive through Nifi, it was giving me as connection timeout exception. In nifi we have dbconnection service class is there , i'm using that calss to connect to hive. Thanks in advance</p><p><a href=\"/storage/attachments/4900-capture.png\">capture.png</a></p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-09 14:32:37.0","id":38806,"title":"Yarn ClassPath Value for Polybase Pushdown","body":"<p>I have Polybase setup and configured to work with my HDP Sandbox. I am able to Query my ORC table successfully but when I try to use Pushdown the query fails. I setup the Pushdown by copying the yarn.application.classpath from my yarn-site.xml in the path /etc/hadoop/conf on my HDP Sandbox to the yarn-site.xml file located in the path C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\Binn\\Polybase\\Hadoop\\conf. </p><p>&lt;name&gt;yarn.application.classpath&lt;/name&gt;\n  &lt;value&gt;$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*&lt;/value&gt;\n  &lt;/property&gt;</p><p>When I review the job log within Ambari I see the below error. Can anyone tell me what I am missing in my classpath?</p><pre>Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster\n</pre>","tags":["YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-09 19:32:43.0","id":38890,"title":"ambari new views","body":"<p>Hi:</p><p>How can i put more views like storm or apache zepelin into ambari???</p><p>Thanks</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-09 15:04:30.0","id":38814,"title":"ambari java.io.IOException: Cannot lock storage /nn/dfs/. The directory is already locked","body":"<p>Not able to start namenode on ha enabled cluster</p>","tags":["Ambari","ha"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-09 17:16:30.0","id":38861,"title":"New groups created in AD not showing in ranger while clicking on \"Groups\" tab","body":"<p>Hi Team,</p><p>I created a group in AD named bd_training but when i restarted ranger \nservice several times from ambari, it is still not showing in ranger \n\"groups\" tab. I want to create ranger policy for bd_training group but i cannot as it is not appearing in the group. Can anyone help me in this. I am using HDP-2.4.2 and ranger 0.5.0.2.4</p><p>Thanks</p><p>Rahul</p>","tags":["ranger-usersync"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-06-09 21:48:35.0","id":38897,"title":"Does Hortonworks officially support HUE anymore?","body":"<p>Does Hortonworks officially support HUE anymore?</p><p>If not, from what HDP release?</p><p>Is there a release note or some link to the notification on this that someone can send me?</p><p>Thank you.</p>","tags":["hue"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-06-10 02:54:44.0","id":38946,"title":"Can a SQL Server User ID be setup that's not dbo and work with Sqoop to SQL Server","body":"<p>Currently access to all the tables only works with the dbo account and it won't work with other accounts.</p><p>Are there special SQL Server permissions required?</p>","tags":["sql-server","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-10 17:53:46.0","id":39104,"title":"We are observing an intermittent issue while running a storm topology we are observing that supervisors are taking a lots of time downloading the topology jars.need community help to identify the bottleneck in my system.","body":"","tags":["Storm"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-13 09:34:24.0","id":39331,"title":"spark installation 1.4 on hdp 2.3.4","body":"<p>How to install spark 1.4 on  HDP 2.3.4. Because by default version on HDP 2.3.4 spark version is 1.5.</p>","tags":["Spark"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-12 20:53:18.0","id":39274,"title":"Aggregate multiple text files into one table in Hive","body":"<p>Hi experts,</p><p>I've 100 text files in HFDS and I want to aggregate all of them into one big table in Hive (Having the Date as Key). How can I load this multiple files to one table created in hive?\n\nThanks!</p>","tags":["HDFS","Hive","groups"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-06-13 11:11:19.0","id":39352,"title":"Wanted to understand how come both MongoDB's snappy & zlip compression consumes the same space for same inserts ? Any ideas !","body":"<p>http://dbversity.com/how-to-get-the-compression-type-of-a-mongodb-collection/</p><p>To specify compression for specific collections, you’ll need to override the defaults by passing the appropriate options in the db.createCollection() command. For example, to create a collection called “myNewCol” using the zlib compression library:</p><p>db.createCollection( “myNewCol”, { storageEngine: { wiredTiger: { configString: ‘block_compressor=zlib’ }}})</p><p>But I wanted to understand how come both snappy & zlip compression cosumes the same space for same inserts ? Any ideas !</p><p>More details : http://dbversity.com/how-to-get-the-compression-type-of-a-mongodb-collection/</p>","tags":["zlib","snappy","mongodb"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-13 18:59:17.0","id":39457,"title":"Kafka Custom alert with ambari 2.2.2","body":"<p>Currently ambari alerts for kafka is just to monitor the kafka broker process on a particular port. Is there a way we can monitor and alert any other parameters like under replicated counts or active controller count etc... ? After upgrading to Ambari 2.2.2 we are able to create and add new widgets for these fields. But there is nothing in the alerts section. Has anyone been successful in setting up the custom alerting for these parameters?</p>","tags":["ambari-alerts","ambarialerts","Kafka","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 20:10:54.0","id":316,"title":"Hive JDBC via Knox and configuring Hive in Ranger","body":"<p>I have HDP-2.2 cluster with FreeIPA configured.But when we are trying to access hive jdbc via knox. Following is the JDBC uri that we are using:</p><blockquote>\n<p>jdbc:hive2://xxxxxxxxxxx:8443/;ssl=true;sslTrustStore=/var/lib/knox/data/security/keystores/gateway.jks;trustStorePassword=xxxxxxxxxxxx?hive.server2.transport.mode=http;hive.server2.thrift.http.path=gateway/default/hive</p></blockquote><p>Below is the error I am getting: _</p><blockquote>\n<p>Keystore was tampered with, or password was incorrect (state=08S01,code=0)</p></blockquote><p>It seems that password of trustStore does not match as that of mentioned in JDBC URI.I tried changing the Knox Master password but ambari does not allow to change the it.Is their any way wherein I can change the trustStore password and create new knox master? Will it affect the other services if the master secret password is changed?</p><p>In addition to that if I use the same uri for creating hive repository in Ranger we get _\"Connection failed\" _error.Is the same JDBC uri to be used in ranger to create repository for hive?</p><p>Note: If I set hive transport mode to \"binary\" instead of \"http\" then we are able to create repository in ranger but in that case hive over knox will not work as it requires \"http\" mode</p>","tags":["Ranger","Knox","Hive"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 01:10:02.0","id":402,"title":"How to setup High Availability for Ambari server?","body":"","tags":["best-practices","ambari-2.1.0","high-availability","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-28 22:56:39.0","id":380,"title":"Solr crashing once the shards hit 10 GB.","body":"<p>Has anyone heard of Solr crashing once the shards get to 10Gigs?  Know the fix?</p>","tags":["shards","SOLR","sizing"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-29 15:20:00.0","id":494,"title":"What sort of throughput should I expect from a Region Server?","body":"<p>Can anyone point me at some good recent benchmarks for HBase throughput, ideally with read and write numbers on both a wide- and narrow- column schema. </p><p>Transaction counts and Mbits throughput vs theoretical disk throughput would be very useful as a measure of HBase overheads.</p>","tags":["performance","Hbase","benchmark"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-30 00:58:20.0","id":577,"title":"On Ambari 2.1.1 (Centos/RHEL 7): removed service keeps reappearing after server restart","body":"<p>While developing <a href=\"https://github.com/hortonworks-gallery/ambari-zeppelin-service\">Zeppelin Ambari service</a>, I was going through the cycle of updating the code, restarting ambari, installing service. Then once the install failed (e.g. due to error in my code), I would <a href=\"https://github.com/hortonworks-gallery/ambari-zeppelin-service#remove-zeppelin-service\">delete the failed install via REST api</a> and restart ambari and then start over. <em>Initially this worked fine</em> (first 3 or 4 times), but then it gets into this weird state where the failed install icon comes back after posting the DELETE request and running ambari restart. Usually with failed installs, one can go under Services tab of Ambari and see the failed install service listed and attempt to re-install, but in this weird state, the service does not appear under Services any more. So basically I'm stuck with a cluster I can't remove this service from.</p><p>I have been able to reproduce this on 2 envs: one on CentOS 7 and one on RHEL 7, but never seen the problem on CentOS 6. </p><p>Questions</p><p>1. Is this a bug? </p><p>2. How do I manually remove all traces of this service on my cluster before attempting to re-install the service?</p><p>Ambari log shows that its unable to delete the service due to FK constraint</p><pre>Caused by: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException\nInternal Exception: org.postgresql.util.PSQLException: ERROR: update or delete on table \"servicecomponentdesiredstate\" violates foreign key constraint \"hstcomponentstatecomponentname\" on table \"hostcomponentstate\"\n  Detail: Key (component_name, cluster_id, service_name)=(ZEPPELIN_MASTER, 2, ZEPPELIN) is still referenced from table \"hostcomponentstate\".\nError Code: 0\nCall: DELETE FROM servicecomponentdesiredstate WHERE (((cluster_id = ?) AND (component_name = ?)) AND (service_name = ?))\n        bind =&gt; [3 parameters bound]\nQuery: DeleteObjectQuery(org.apache.ambari.server.orm.entities.ClusterServiceEntity@3426596b)\n        at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)\n        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1611)\n        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:898)\n        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)\n        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)\n        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)\n        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)\n        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.appendCall(ParameterizedSQLBatchWritingMechanism.java:82)\n        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:603)\n        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:558)\n        at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:2002)\n        at org.eclipse.persistence.sessions.server.ClientSession.executeCall(ClientSession.java:298)\n</pre>","tags":["centos","rhel","ambari-2.1.1","ambari-service"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 16:08:51.0","id":509,"title":"Site-to-site protocol load balancing","body":"<p>Hi,</p><p>I'd like to understand how the receiving end of the site-to-site protocol works. The sending side drops a remote process group on the canvas and is mostly done. The receiving side - simple in case of a single NiFi node. In a cluster, though, we still need to specify a FQDN to connect to. What is the best practice there? If we put a load balancer in front, would it break batch affinity (when site2site batches sends for you)? </p>","tags":["high-availability","hdf","Nifi","dataflow","site2site"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-09-29 20:22:59.0","id":544,"title":"Where can I find a good example of a Storm topology that uses multiple Kafka spouts taking advantage of Kafka partitions?","body":"","tags":["Storm","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-01 21:04:45.0","id":711,"title":"Ranger implementation - hive security access recommendation","body":"<p><strong>Current scenario (no Ranger):</strong></p><p>Hive security is relying on HDFS file permissions (storage based auth). In Hive warehouse, each database/table folder is owned by the respective user who created the database/table. </p><p><strong>with Ranger how do we manage hive security:</strong></p><p>Hive impersonation turned on:</p><p>In this case, I am having to provide access to underlying HDFS folders to the user who is requesting to access the hive table. It means that I have to create a HDFS policy and also a Hive policy to enable access to a user on a given Hive table. </p><p>It is difficult to create hdfs policies for each and every hive db & table. (thousands of tables) Is there a recomanded approach?</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-30 17:52:01.0","id":639,"title":"How to migrate Ambari DB from postgres to Oracle?","body":"","tags":["ambari-2.1.1","Ambari","ambari-service","oracle"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-01 20:58:59.0","id":710,"title":"User can view entire hdfs dir and navigate further via WebHDFS. hadoop-policy (Access Control Lists) does not seem to be applicable to WebHDFS. how to incorporate ACLs when accessed via WebHDFS?","body":"<p>User can view entire hdfs dir and navigate more via WebHDFS. hadoop-policy (Access Control Lists) does not seem to be applicable to WebHDFS. how to incorporate ACLs when accessed via WebHDFS?</p>","tags":["ambari-2.1.1","HDFS","security","hdfs-permissions"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-02 15:45:18.0","id":743,"title":"mirror maker and security","body":"<p>As part of the release notes, we have got - Mirror Maker (not supported when Kafka security is active )</p><p>Do we know when it will be working with Security ?</p><p>Thanks,</p><p>Olivier</p>","tags":["mirroring","security","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-09-29 12:49:08.0","id":456,"title":"How to configure NiFi for a Kerberized Hadoop Cluster?","body":"<p>I have NiFi setup on my cluster but it only works in standalone mode. My cluster is Kerberized and NiFi fails to connect when I use cluster mode. Any suggestions on how to get NiFi to work on a Kerberos enabled cluster??</p>","tags":["configuration","Nifi","kerberos","security"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-02 19:37:44.0","id":793,"title":"Can we install Ambari Metrics on HDP2.1 cluster, we just upgraded Ambari to 2.1 from 1.7 ?","body":"<p>Can we install Ambari Metrics on HDP2.1 cluster, we just upgraded Ambari to 2.1 from 1.7 ? I am assuming the answer is NO , but just wanted to double check with the experts. We uninstalled the Nagios and Ganglia. And I assume we have to upgrade to HDP 2.3 to be able to install the Metrics service.  </p>","tags":["ambari-metrics","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-05 23:26:51.0","id":930,"title":"Failing Oozie Launcher threw exception, hive-site.xml (Permission denied)","body":"<p>Oozie HIve action failed</p><pre>Oozie Hive action configuration \n================================================================= \n\n\nUsing action configuration file /hadoop/data01/hadoop/yarn/local/usercache/hadoopdev/appcache/application_1443111597609_2691/container_1443111597609_2691_01_000002/action.xml \n------------------------ \nSetting env property for mapreduce.job.credentials.binary to: /hadoop/data01/hadoop/yarn/local/usercache/hadoopdev/appcache/application_1443111597609_2691/container_1443111597609_2691_01_000002/container_tokens \n------------------------ \n------------------------ \nSetting env property for tez.credentials.path to: /hadoop/data01/hadoop/yarn/local/usercache/hadoopdev/appcache/application_1443111597609_2691/container_1443111597609_2691_01_000002/container_tokens \n------------------------ \n\n\n&lt;&lt;&lt; Invocation of Main class completed &lt;&lt;&lt; \n\n\nFailing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.HiveMain], main() threw exception, hive-site.xml (Permission denied) \njava.io.FileNotFoundException: hive-site.xml (Permission denied) \nat java.io.FileOutputStream.open(Native Method) \nat java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:221) \nat java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:110) \nat org.apache.oozie.action.hadoop.HiveMain.setUpHiveSite(HiveMain.java:166) \nat org.apache.oozie.action.hadoop.HiveMain.run(HiveMain.java:196) \nat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:38) \nat org.apache.oozie.action.hadoop.HiveMain.main(HiveMain.java:66) \nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \nat java.lang.reflect.Method.invoke(Method.java:606) \nat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:225) \nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) \nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430) \nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) \nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:415) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594) \nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163) \n\n\nOozie Launcher failed, finishing Hadoop job gracefully \n</pre><p>Oozie action is configured like this:</p><pre>&lt;action name=\"HivePartitionAction\"&gt; \n&lt;hive xmlns=\"uri:oozie:hive-action:0.3\"&gt; \n&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt; \n&lt;name-node&gt;${nameNode}&lt;/name-node&gt; \n&lt;job-xml&gt;script/hive-site.xml&lt;/job-xml&gt; \n&lt;job-xml&gt;script/xasecure-audit.xml&lt;/job-xml&gt; \n&lt;job-xml&gt;script/xasecure-hive-security.xml&lt;/job-xml&gt; \n&lt;configuration&gt; \n&lt;property&gt; \n&lt;name&gt;mapred.job.queue.name&lt;/name&gt; \n&lt;value&gt;${queueName}&lt;/value&gt; \n&lt;/property&gt; \n&lt;/configuration&gt; \n&lt;script&gt;script/addPartition.sql&lt;/script&gt; \n&lt;file&gt;script/xasecure-audit.xml&lt;/file&gt; \n&lt;file&gt;script/xasecure-hive-security.xml&lt;/file&gt; \n&lt;file&gt;script/xasecure-policymgr-ssl.xml&lt;/file&gt; \n&lt;file&gt;script/hive-site.xml&lt;/file&gt; \n&lt;/hive&gt; \n&lt;ok to=\"end\"/&gt; \n&lt;error to=\"kill\"/&gt; \n&lt;/action&gt; \n</pre><p>-- and the files exist OK in the /script subdirectory of the workflow directory. The files have read/write permission for all users. </p><p>-- 'hive-site.xml' under /etc/oozie/conf/action-conf/hive is owned by oozie user and set permission 644</p><p>Can anyone advise? Thanks.</p>","tags":["Oozie","Hive"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-10-23 20:13:59.0","id":2034,"title":"when running sqoop through shell script in Oozie, getting FileNotFoundException job.splitmetainfo","body":"<p>can't find any solution to this error, sqoop in the shell script runs fine on the command line but not in Oozie</p><pre>org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.FileNotFoundException: File does not exist: hdfs://servername:8020/user/username/.staging/job_1444331888071_2109/job.splitmetainfo\nat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.createSplits(JobImpl.java:1568)\nat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1432)\nat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1390)\nat org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\nat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\nat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\nat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\nat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:996)\nat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:138)\nat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1312)\nat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1080)\nat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\nat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1519)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1515)\nat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1448)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://servername:8020/user/username/.staging/job_1444331888071_2109/job.splitmetainfo\nat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)\nat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\nat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\nat org.apache.hadoop.mapreduce.split.SplitMetaInfoReader.readSplitMetaInfo(SplitMetaInfoReader.java:51)\nat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.createSplits(JobImpl.java:1563)\n... 17 more\n2015-10-23 15:45:55,263 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized</pre>","tags":["HDFS","MapReduce","Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-02 20:49:12.0","id":2632,"title":"Loading HBase from Hive ORC Tables","body":"<p>Looking for approaches for loading HBase tables if all I have is the data in an ORC backed Hive table.</p><p>I would prefer a bulk load approach, given there are several hundred million rows in the ORC backed Hive table.</p><p>I found the following, anyone have experience with Hive's HBase bulk load feature? Would it be better to create a CSV table and CTAS from ORC into the CSV table, and then use ImportTsv on the HBase side?</p><p><a target=\"_blank\" href=\"https://cwiki.apache.org/confluence/display/Hive/HBaseBulkLoad\">HiveHBaseBulkLoad</a></p><p>Any experiences here would be appreciated.</p>","tags":["bulk-operations","Hbase","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-03 23:21:30.0","id":2737,"title":"Kafka spout not reading from beginning even if the fKakfaSpout.forceFromStart is set to true","body":"<p>Spout try to query zookeeper to get the state and set the offset to prior location. We have retention policy of three days in kafka. So I guess offset moves back to location which does not exists in kafka and its does not pull anymore data.</p><p>Already checked that in the spout config, the Zookeeper configuration is pointing to Kafka ZooKeeper.</p><p>Anything else that we're missing here? Thank you.</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-21 15:14:14.0","id":1802,"title":"Tez with Transaction with Bucketing","body":"<p>When a table is partitioned and bucketed and Transactions enabled on it , the number of map tasks launched by TEZ = 2 , while MR jobs still launches 72 Tasks (Table is about 17Gig). if transaction is not enabled , then the query is launching Correct number of Tez tasks, If there are any hints on why this may occur, please share. </p>","tags":["Hive","bucketing","Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-03 22:02:00.0","id":2728,"title":"What is recommended NLP solution on top of HDP stack for Text Analytics","body":"<p>What is recommended NLP solution on top of HDP stack for Text Analytics. I know they can use Tika/Stanbol etc for this but are these recommended tech? Anything better than this especially using spark etc?</p><p>Use case on-hand is to scan comments ( free text ) and generate insights in the form of recommendations . </p>","tags":["analytics","nlp","stanbol","tika"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-04 17:33:51.0","id":2784,"title":"Is there a way to get results of hive command instead of getting job id in WebHCat?","body":"<p>If I execute the following</p><p>curl -s -d execute=\"select+*+from+sample_08;\" \\</p><p>       -d statusdir=\"outputofthejob\" \\</p><p>       '<a href=\"http://localhost:50111/templeton/v1/hive?user.name=root\">http://localhost:50111/templeton/v1/hive?user.name=root</a>'</p><p>I get a job id of the job in JSON format, is there a way to get actual result of the query rather than job id?</p>","tags":["api","Hive","Pig","hcatalog","webhcat"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-04 20:45:49.0","id":2825,"title":"Status of FreeIPA support","body":"<p>Do we officially support using HDP with <a href=\"https://www.freeipa.org/page/Main_Page\">FreeIPA</a> and do any of our customers use FreeIPA to secure large clusters?</p><p>I found a workshop for FreeIPA+Kerberos integration with <a href=\"http://hortonworks.com/wp-content/uploads/2014/10/Security-workshop-HDP-2_1-script.txt\">Sandbox 2.1</a>.</p>","tags":["freeipa","security"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-05 18:06:40.0","id":2904,"title":"How do I remove LDAP accounts from Ambari","body":"<p>I loaded LDAP accounts into Ambari and now need to remove them and re-sync. How do I remove the accounts?</p>","tags":["ldap","Ambari"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-19 03:20:11.0","id":4298,"title":"Snapshotting Apps/Hive/Warehouse","body":"<p>Are there any concerns with turning on HDFS snapshots on the Apps/Hive/Warehouse directory? I want to make sure the directory is recoverable if someone were to accidentally rmr it (or if anything else happened). </p><p>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html</p>","tags":["backup","snapshot","Hive","HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-24 15:15:22.0","id":4645,"title":"Is it supported to use IBM Elastic storage server as a replacement for HDFS (similar to Isilon) with HDP?","body":"<p>I need to mount folders in a HDP cluster from windows. Isilon OneFS is an option but I wonder if IBM Elastic storage server is an alternative and if it's supported with HDP. </p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-11-24 17:41:17.0","id":4719,"title":"YARN logs seems to be truncated","body":"<p>I should have 2 DAGs in my yarn log, but I only see one. </p><p>The last line of the YARN log is: </p><p>2015-11-23 12:56:16,935 INFO [main] common.TezUtilsInternal: Redirecting log file based on addend:</p><p>&lt;blank&gt;</p><p>&lt;blank&gt;</p><p>Is there a property that controls the redirect? </p><p>Under what circumstances should the log get redirected? </p>","tags":["Tez","YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-24 19:16:04.0","id":4716,"title":"What does 'ambari-server setup -s' use to determine setup has already been executed?","body":"","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-01 07:09:57.0","id":4961,"title":"How to export all the output/error logs for a Bulk operation e.g. Start all Service","body":"<p>Suppose executed a bulk operation through Amabri UI e.g. \"Start All Services\" .</p><p><strong>The requirement is</strong> - </p><p>I want to export the complete log of this operation for further analysis</p><p>One way is to find all hosts impacted by this operation , get the sub tasks executed on each hosts and then loop through them and get the logs and merge them </p><p>Is there a more simple way of getting all logs for a bulk operation done through ambari</p>","tags":["bulk-operations","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-04 05:39:57.0","id":5156,"title":"Nifi executeSQL processor streaming capability","body":"<p>Is the executeSQL processor able to leverage streaming capability of a JDBC driver level 4 or 5. This is quite important for us as if not it would mean that Nifi is not able \nto get data in small batches that are processed progressively and that it needs to receive the whole dataset before going to processing.\ne.g: if we are extracting 1 Million records we need to wait for the whole dataset to arrive to do any processing instead of receiving small batch of 10 records or 100 records that we process and \nstream to the final location.</p>","tags":["streaming","jdbc","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-02 15:28:17.0","id":5068,"title":"SolrCloud Replication factor with index files in HDFS","body":"<p>Hortonworks has a tutorial that shows how to configure Solr to store index files in HDFS. Since HDFS is already a fault tolerant file system, does it mean that with this approach we can keep the replication factor of 1 for any collections (shards) that we create? It sounds like a lot of redundancy if we keep the default HDFS replication factor of 3 plus Solr replication on top of that.</p>","tags":["SOLR","solrcloud"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-02 22:28:55.0","id":5080,"title":"Pig job hanging when using FixedWidthStorer","body":"<p>Using our sandbox to load fixed-width file using pig: https://martin.atlassian.net/wiki/pages/viewpage.action?pageId=21299205</p><p>dump or store into works fine; however, when running with FixedWidthStorer, the job hanging--- no error:\nstore employees into '/user/root/emps-regenerated' USING org.apache.pig.piggybank.storage.FixedWidthStorer('-10, 11-30, 31-50, 51-70, 71-','SKIP_HEADER'); </p><p>2015-12-02 18:35:39,386 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://sandbox.hortonworks.com:8088/proxy/application_1449078859822_0001/\n2015-12-02 18:35:39,387 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_1449078859822_0001\n2015-12-02 18:35:39,387 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases employees\n2015-12-02 18:35:39,387 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: employees[1,12] C:  R:\n2015-12-02 18:35:39,400 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n2015-12-02 18:35:39,400 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1449078859822_0001]\n</p>","tags":["MapReduce","Pig","jobs"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-04 14:48:11.0","id":5210,"title":"Can we use Sqoop to retrieve data from MS SQL Server function using this syntax \"Select X,Y,Z from MyFunction()\" ?","body":"<p>They can't test right now so I am wondering if someone has already done that.</p><p>Thanks !</p>","tags":["mssql","Sqoop"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-04 18:45:48.0","id":5250,"title":"Spark HiveContext - Querying External Hive Table","body":"<p>Hi,</p><p>I am currently trying to query an external Hive Table that is pointed to a directory via SparkSQL. When I attempt to do a SELECT * FROM TABLE, I get the following error:</p><pre>15/11/30 15:25:01 INFO DefaultExecutionContext: Created broadcast 3 from broadcast at TableReader.scala:68\n\n15/11/30 15:25:01 INFO FileInputFormat: Total input paths to process : 2\n\njava.io.IOException: Not a file: hdfs://clster/data/raw/EDW/PROD/Prod_DB/test/20151124/2014\n\n        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:320)\n\n        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\n\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:220)\n\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:218)\n\n        at scala.Option.getOrElse(Option.scala:120)\n\n        at org.apache.spark.rdd.RDD.partitions(RDD.scala:218)\n\n        at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:220)\n\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:218)\n\n        at scala.Option.getOrElse(Option.scala:120)\n\n        at org.apache.spark.rdd.RDD.partitions(RDD.scala:218)\n\n        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)\n\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:220)\n\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:218)\n\n        at scala.Option.getOrElse(Option.scala:120)\n\n        at org.apache.spark.rdd.RDD.partitions(RDD.scala:218)\n\n        at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:220)\n\n</pre>","tags":["spark-sql","Hive","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-03 14:26:22.0","id":5115,"title":"Hive default location as google storage","body":"<p>Google storage and Hive works fine as per this setup <a target=\"_blank\" href=\"https://community.hortonworks.com/questions/553/hive-and-google-cloud-storage.html\">https://community.hortonworks.com/questions/553/hive-and-google-cloud-storage.html </a></p><p>Is it possible to change hive.metastore.warehouse.dir to google bucket?</p>","tags":["Hive","google","cloud"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-05 01:48:10.0","id":5273,"title":"How do I decrypt AES-256-CBC data in HDF if it was encrypted by OpenSSL?","body":"<p>I have data encrypted by OpenSSL / external applications using password-based encryption (PBE) and AES-256-CBC. The HDF EncryptContent processor cannot decrypt this data. </p>","tags":["encryption","ssl","hdf","security","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-06 00:03:33.0","id":5290,"title":"How will Erasure Coding affect the principle of data locality?","body":"<p>Hadoop has long stressed moving the code to the data, both because it's faster to move the code than to move the data, and more importantly because the network is a limited shared resource that can easily be swamped.  Erasure coding would seem to require that a large proportion of the data must move across the network because the contents of a single block will reside on multiple nodes. This would presumably apply not just the ToR switch, but the shared network as well, if the ability to tolerate the loss of a rack is preserved. Is this true and how are these principles reconciled?</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-07 11:44:03.0","id":5292,"title":"Namenode Best Pratices / Performance tunning checklist","body":"<p>Does anyone have a best practices guide / performance checklist for namenode? </p><p>I know when we increase namenode_heapsize, we should also adjust namenode_opt_newsize, namenode_opt_maxnewsize, namenode_opt_permsize, namenode_opt_maxpermsize, but what are the reference calculations for this adjustment?</p><p>Also there is: dfs.namenode.handler.count and dfs.namenode.service.handler.count, as discussed in question below, but same here, what are reference values for these properties? Is it based number of nodes only or should also be increased based on usage patterns?\n<a target=\"_blank\" href=\"https://community.hortonworks.com/questions/4799/is-there-any-suggested-ratio-for-dividing-the-numb.html#answer-5206\">https://community.hortonworks.com/questions/4799/is-there-any-suggested-ratio-for-dividing-the-numb.html#answer-5206</a></p>","tags":["performance","namenode","HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-08 05:16:35.0","id":5408,"title":"Spark vs Tez?","body":"<p>Whats the difference between the two?</p>","tags":["Tez","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-07 23:04:34.0","id":5373,"title":"How do you specify a highly-available HDFS namespace in an Apache Falcon cluster definition","body":"<p>for the following interfaces:</p><p>&lt;interface type=\"readonly\" endpoint=\"hftp://&lt;host&gt;:50070\"/&gt;</p><p>&lt;interface type=\"write\" endpoint=\"hdfs://&lt;host&gt;:8020\" /&gt;</p><p>If we are pointing to a cluster with HDFS H/A enabled?</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-08 05:29:29.0","id":5415,"title":"Spark on YARN vs Mesos?","body":"<p>What are the considerations for running Spark\non YARN vs Spark on Mesos?</p>","tags":["Spark","mesos"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-08 12:01:45.0","id":5429,"title":"What is the best-practice regarding filtering the existing users in several OU (specific attribute in the AD, specific Hadoop group) ?","body":"","tags":["active-directory"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-09 14:46:48.0","id":5552,"title":"PySpark in Zeppelin: Does not have all libraries","body":"<p>Iam able to import a library in pyspark shell without any problems, but when I try to import the same library in Zeppelin, I get an error </p><p>ImportError: No module named xxxxx</p>","tags":["zeppelin","pyspark"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-10 18:39:35.0","id":5878,"title":"Services failing to restart after kerberizing cluster  - HDP2.3","body":"<p>Hi</p><p>Getting the following error in the hdfs log files \"hadoop-hdfs-datanode-hashmap.domain.com.log\" </p><p>Error</p><pre>javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:558)\n        at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:373)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:727)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:723)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:722)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:373)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1493)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1397)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n        at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:272)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:173)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:219)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:821)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service crews)</pre><p>Need help  / pointers to fix the issue.</p>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-10 23:34:19.0","id":5971,"title":"how does service_check works in ambari custom stack","body":"<p>I was trying to develop a service for cassandra. my question is regarding the service check method. I wrote some smoke test in service_check.py but when I install the service and do service check it tries to do service check on client node where cassandra service won't be running, so it fails. One work around I thought of is that I will provide host IP address in the smoke test so that the client executes the check on that node but then it is possible that that node is down and the service check will always fail in that case. Is there any way that I can get the IP address of all the hosts in the cluster from ambari and do Execute(smoke test) on each node unless it is passed on any one of them before ambari declare that service check failed. I see the Execute method will run the commands specified in it and if they execute correctly it will update the service check ran correctly. Below is sample code of service check.py Any help is appreciated</p><pre>class ServiceCheck(Script):\n    def service_check(self, env):\n        import params\n        env.set_params(params)\n        cmdfile = format(\"/tmp/cmds\")\n        cmdfile=\n        File(cmdfile,\n               mode=0600,\n               content=InlineTemplate(\"CREATE KEYSPACE IF NOT EXISTS smokedemotest WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\\n\"\n                                  \"Use smokedemotest;\\n\"\n                                  \"CREATE TABLE IF NOT EXISTS smokeusers (firstname text,lastname text,age int,email text,city text,PRIMARY KEY (lastname));\\n\"\n                                  \"INSERT INTO smokeusers (firstname, lastname, age, email, city) VALUES ('John', 'Smith', 46, 'johnsmith@email.com', 'Sacramento');\\n\"\n                                  \"DROP TABLE smokedemotest.smokeusers;\\n\"\n                                  \"DROP KEYSPACE smokedemotest;\\n\\n\")\n        )\n        Execute(format(\"cqlsh -f {cmdfile}\"))\n\n\n</pre><p>Currently the execute command will run on the machine on which ambari executes service check. Can you please correct my understanding how service check executes the command, how does it decides where to execute the commands( which machine?</p>","tags":["Ambari","how-to-tutorial"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-10 06:59:09.0","id":5785,"title":"Why the number of reducer determined by Hadoop MapReduce and Tez has  a great differ?","body":"<p>Hive on tez,sometimes the reduce number of tez is very fewer,in hadoop mapreduce has 2000 reducers, but in tez only 10.This cause take a long time to complete the query task.</p><p>the hive.exec.reducers.bytes.per.reducer is same.Is there any mistake in judging the Map output in tez?</p><p>how can to solve this problem?</p>","tags":["Hive","Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-11 15:52:18.0","id":5938,"title":"incremental loads from oracle to hadoop","body":"<p>Hi Estimated,</p><p>Please, as you might consume an incremental loading of data from Oracle to Hadoop. </p><p>I will comment briefly as work currently I use Oracle materialized views to get only the data that changed last time and new records, so I avoid each time bringing the entire database. </p><p>But now that we are entering in the world of BigData, I want to know if we can replace Oracle materialized views and incremental loads with a tool Hadoop as flume, storm, Kafka nifi, what would be the most appropriate</p><p>\nIf you need additional information do not hesitate to ask </p><p>Thanks in advance </p><p>Angel</p>","tags":["data-ingestion","Nifi","oracle"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-11 21:36:37.0","id":6118,"title":"Could not submit falcon feed schedule with different user","body":"<p>Hi,</p><p>We are using falcon-0.6.</p><p>I am trying to submit Falcon feed schedule as user1 but it throws below error</p><p>Permission denied: user=user1, access=WRITE, inode=\"/apps/falcon/backupCluster/staging/falcon/workflows/feed\":falcon:hdfs:drwxr-xr-x</p><p>I gave 777 permissions to staging directory but I still see the same error.</p><p>I am able to submit the schedule as falcon user but could not as any other user.</p><p>Please help me with this</p><p>Thanks,</p><p>Venkat</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-12 11:47:01.0","id":6140,"title":"mapreduce.tar.gz does not exist","body":"<p>I am trying to run my jar with the command: </p><pre>hadoop jar my.jar /user/root/input /user/root/output</pre><p>And the following exception appears: </p><pre>Exception in thread \"main\" java.io.FileNotFoundException: File file:/hdp/apps/2.3.2.0-2950/mapreduce/mapreduce.tar.gz does not exist</pre><p>I've checked, that file exists. </p><p>What can cause this error?</p>","tags":["HDFS","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-12 20:40:26.0","id":6191,"title":"Ambari enable Kerberos does not create principals","body":"<p>Hi,</p><p>I am using Ambari 2.0.1 and MIT Kerberos.</p><p>After running through the enabling Kerberos wizard, the services are failing to start. After some search I found out that there are no principals being created in the KDC:</p><p>\"listprincs\" just shows the previously (manually) created admin/admin@REALM principal, but no further principals as expected from enabling Kerberos via the wizard?!?!?!</p><p>This is the first time I see this strange behaviour, several other kerberized clusters didn't have this problem.</p><p>Why doesn't the Ambari wizard create principals in the KDC, while showing no errors at running through the wizard ?</p><p>Thanks in advance...</p>","tags":["Ambari","principal","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-14 15:06:26.0","id":6380,"title":"Averaging RandomForest votes in Spark 1.3.1","body":"<p>I'm trying to calculate an averange of randomForest predictions in Spark 1.3.1, since the predicted probability of all trees is available only in 1.5.0.</p><p>The best I could do until now is using the function below:</p><pre>def calculaProbs(dados, modelRF):\n    trees = modelRF._java_model.trees()\n    nTrees = modelRF.numTrees()\n    nPontos = dados.count()\n    predictions = np.zeros(nPontos)\n    for i in range(nTrees):\n        dtm = DecisionTreeModel(trees[i])\n        predictions+= dtm.predict(dados.map(lambda x: x.features)).collect()\n    predictions = predictions/nTrees\n    return predictions</pre><p>This code is running very slow, as expected, since I'm collecting (collect()) predictions from each Tree and adding them up in Driver. I cannot put the dtm.predit() inside a Map operation in this version of Spark. Here is the Note from documentation: \"Note: In Python, predict cannot currently be used within an RDD transformation or action. Call predict directly on the RDD instead.\"</p><p>Any Idea to improve performance? How can I add values from 2 RDDs without collecting their values to a vector?</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-14 13:30:18.0","id":6359,"title":"Error on concatenating ORC Hive table (merge files)","body":"<p>I have a Spark job running frequently that populates a Hive table backed by ORC files. Spark generates many small files and even using coalesce can't help to efficiently fill the HDFS large blocks.\nThe best solution I found was to schedule a job to concatenate the hive table periodically.</p><p>The alter table actually works fine but it raises an exception as you see below:</p><p>CREATE TABLE my_test(id String) STORED AS ORC;</p><p>ALTER TABLE my_test CONCATENATE;</p><pre>Loading data to table default.my_test\nTable default.my_test stats: [numFiles=0, totalSize=0]\nFAILED: Hive Internal Error: java.lang.IllegalArgumentException(No enum constant org.apache.hadoop.hive.ql.plan.HiveOperation.ALTER_TABLE_MERGE)\njava.lang.IllegalArgumentException: No enum constant org.apache.hadoop.hive.ql.plan.HiveOperation.ALTER_TABLE_MERGE\n    at java.lang.Enum.valueOf(Enum.java:238)\n    at org.apache.hadoop.hive.ql.plan.HiveOperation.valueOf(HiveOperation.java:23)\n    at org.apache.atlas.hive.hook.HiveHook.run(HiveHook.java:151)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1522)\n    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</pre><p>I also don't understand 2 things:</p>\n<ol>\n<li>What is the relation between running simple Hive shell query and Atlas? <pre>at org.apache.atlas.hive.hook.HiveHook.run(HiveHook.java:151)</pre>\n</li><li>Why Hive is trying to use ALTER_TABLE_MERGE enum constant which actually implemented as ALTERTABLE_MERGEFILES? on <a href=\"https://github.com/hortonworks/hive-release/blob/HDP-2.3.2.0-tag/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java\">https://github.com/hortonworks/hive-release/blob/HDP-2.3.2.0-tag/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java</a></li></ol>","tags":["hdp-2.3.2","Hive","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 20:55:46.0","id":6429,"title":"ui.filter.params reset to default value when additional supervisors added to the cluster","body":"<p>Hey all,</p><p>We added additional storm supervisor nodes to our cluster and noticed our previously customized ui.filter.params value had been reset back to the stock/default value.  Has anyone else noticed this?</p><p>Thanks!\nBrendan</p>","tags":["Storm"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-15 21:04:48.0","id":6599,"title":"Does Tez run slower than hive on larger dataset (~2.5 TB)?","body":"<p>We have started to look into testing tez query engine. From initial results, we are getting 30% performance boost over Hive on smaller data set(1-10 GB) but Hive starts to perform better than Tez as data size increases. Like when we run a hive query with Tez on about 2.3 TB worth of data, it performs worse than hive alone.(~20% less performance) Details are in the post below. </p><p>On a cluster with 1.3 TB RAM, I set the following property :</p><p>  set tez.task.resource.memory.mb=10000;\n  set tez.am.resource.memory.mb=59205;\n  set tez.am.launch.cmd-opts =-Xmx47364m;\n  set hive.tez.container.size=59205;\n  set hive.tez.java.opts=-Xmx47364m;\n  set tez.am.grouping.max-size=36700160000;</p><p>Is it normal or I am missing some property / not configuring some property properly? Also, I am using an older version of Tez as of now. Could that be the issue too? I still to bootstrap latest version of Tez on EMR and test it and see if that could do any better    </p><p>http://www.jwplayer.com/blog/hive-with-tez-on-emr/</p>","tags":["help","MapReduce","Tez","Hive","hadoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-08 21:48:30.0","id":26728,"title":"NiFi - PutKafka Error \"failed while waiting for acks from kafka\"","body":"<p>Hello,</p><p> I am learning how to use the PutKafka processor to store my live trucking data as a message into kafka. However, when I start my dataflow, I receive the following error message:</p><p><a href=\"/storage/attachments/3312-putkafka-error.png\">putkafka-error.png</a></p><p>I also included the property values that I have set for my PutKafka processor:</p><p><a href=\"/storage/attachments/3313-error-putkafka-processor.png\">error-putkafka-processor.png</a></p><p>The <strong>Client Name</strong> value is <strong>Zookeeper</strong>.</p><p>I have the latest version of HDF 1.2.</p><p>Why am I receiving this error?</p><p>Here is my nifi template:</p><p><a href=\"/storage/attachments/3314-iot-stream-kafka-nifi.xml\">iot-stream-kafka-nifi.xml</a></p><p>Any help is highly appreciated! Thank you!</p>","tags":["Nifi","Kafka","Sandbox","nifi-processor","teradata"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-09 07:38:01.0","id":26744,"title":"ambiguous Zeppelin error message - how to get more details?","body":"<p>I am going through the tutorial <a href=\"http://hortonworks.com/hadoop-tutorial/interacting-with-data-on-hdp-using-scala-and-apache-spark/\">Interacting with Data on HDP using Apache Zeppelin and Apache Spark</a>.  </p><p>I am getting an error <strong><em>Process exited with an error: 4 (Exit value: 4) </em></strong>when trying to download the littlelog.csv file. using the following code </p><pre>%sh \n\nwget no-check-certificate 'https://docs.google.com/uc?export=download&id=0BzhlOywnOpq8OWFzQjJObUtlck0' -O /tmp/littlelog.csv</pre><p>What might be the cause of this error? How can I get more details on the error</p><p>PS: </p><ul>\n<li>I am using Hortonworks Sandbox </li><li>I tried the same command on my Ubuntu host machine and it's working </li></ul>","tags":["how-to-tutorial","error","Sandbox","zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-10 18:25:35.0","id":26840,"title":"Erreur au lancement de jupyter, Bonjour,","body":"<p>Bonjour,</p><p>J'ai suivi à la lettre le tutorial pour installer et lancer jupyter et au lancement du script .\"start_ipython_notebook.sh\"</p><p>j'ai une erreur de syntaxe dans le fichier de configuration, que j'ai pourtant copié depuis le guide :</p><p>script start_ipython_notebook.sh</p><p>*****************************************************************</p><p>#!/bin/bash\nsource </p><p>/opt/rh/python27/enable </p><p>IPYTHON_OPTS=\"notebook --port 8889 --notebook-dir='/usr/hdp/current/spark-client/' --ip='*' --no-browser\" pyspark</p><p>*****************************************************************</p><p>Erreur obtenue : invalid syntax, avec la flèche pointant sur 'pyspark'</p><p>[E 14:02:17.608 NotebookApp] Exception while loading config file /root/.jupyter/                                                                                                         jupyter_notebook_config.py </p><p>    Traceback (most recent call last): </p><p>      File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/traitlets/conf                                                                                                         ig/application.py\", line 534, </p><p>in _load_config_files\n        config = loader.load_config() </p><p>      File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/traitlets/conf                                                                                                         ig/loader.py\", line 458, </p><p>in load_config\n        self._read_file_as_dict() </p><p>File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/traitlets/conf                                                                                                         ig/loader.py\", line 490, </p><p>in _read_file_as_dict\n        py3compat.execfile(conf_filename, namespace) </p><p>      File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/ipython_genuti                                                                                                         ls/py3compat.py\", line 288, </p><p>in execfile\n        builtin_mod.execfile(filename, *where) </p><p>      File \"/root/.jupyter/jupyter_notebook_config.py\", line 566 </p><p>        IPYTHON_OPTS=\"notebook --port 8889 --notebook-dir='/usr/hdp/current/spar                                                                                                         k-client/' --ip='*' --no-browser\" pyspark </p><p>                                                                                                                                                                                        ^ </p><p>    SyntaxError: invalid syntax</p><p>Cordialement,</p><p>Pierre Bouillaguet</p><p>,</p>","tags":["jupyter","tutorial-380","hdp-2.4.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-11 09:32:01.0","id":26892,"title":"flume regex_extractor interceptor can not capture values","body":"<p>Dear community,</p><p>I am trying to start a trivial example. However I get a whole message in elasticsearch but not the parsed fields.</p><p>Example of flume.conf:</p><pre># Name the components on this agent\na1.sources = f1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.f1.type = exec\na1.sources.f1.command = tail -f /tmp/flumespool/log.txt\na1.sources.f1.restart = true\na1.sources.f1.channels = c1\na1.sources.f1.batchSize = 1\n\na1.sources.r1.interceptors = i1\na1.sources.r1.interceptors.i1.type = regex_extractor\na1.sources.r1.interceptors.i1.regex = ([0-9]):([0-9]):([0-9])\na1.sources.r1.interceptors.i1.serializers = s1 s2 s3\na1.sources.r1.interceptors.i1.serializers.s1.name = one\na1.sources.r1.interceptors.i1.serializers.s2.name = two\na1.sources.r1.interceptors.i1.serializers.s3.name = three\n\n# Describe the sink\na1.sinks.k1.type = elasticsearch\na1.sinks.k1.hostNames = 127.0.0.1:9300\na1.sinks.k1.clusterName = elasticsearch\na1.sinks.k1.batchSize = 1000\na1.sinks.k1.ttl = 5d\na1.sinks.k1.channel = c1\na1.sinks.k1.serializer = org.apache.flume.sink.elasticsearch.ElasticSearchLogSt\n#a1.sinks.k1.indexNameBuilder.dateFormat=yyyy-MM-dd'T'HH:mm:ss Z\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 10000\na1.channels.c1.batchSize = 15000\na1.channels.c1.transactionCapacity = 10000\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n</pre><p>The contents of file /tmp/flumespool/log.txt is:</p><pre>1:2:3\n1:2:3\n</pre><p>However when it comes to flume feed in elasticsearch I only get:</p><pre>{\"_index\": \"flume-2016-04-11\",\"_type\": \"log\",\"_id\": \"AVQEocZFNlYcm-dythHW\",\"_version\": 1,\"_score\": 1,\"_source\": {\"@message\": \"1:2:3\",\"@fields\": { }}}\n</pre><p>Though I expected to get a parsed message, something like this:</p><pre>{\"_index\": \"flume-2016-04-11\",\"_type\": \"log\",\"_id\": \"AVQEocZFNlYcm-dythHW\",\"_version\": 1,\"_score\": 1,\"_source\": {\"@message\": \"1:2:3\",\"@fields\": { \"one\": \"1\", \"two\" : \"2\", \"three\": \"3\" }}}</pre><p>As an addition, is it possible to make values parsed as integer, but not string?</p>","tags":["Flume","elasticsearch"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-27 16:29:53.0","id":24654,"title":"query hive tables with spark sql","body":"<p>Hi</p><p>I have a hadoop single node cluster and also hive installed. And I have one hive database with some hive tables stored in hdfs.</p><p>Now I want to do some sql queries in that hive tables using Spark SQL.</p><p>Someone already did this? What is the process to achieve this? We need to create again the tables now in spark? Or we can acess direct the hive tables with spark sql? Im trying to find some article about this but it seems always that we need to create again the tables with spark sql, and load data in that tables again, but Im not understanding why if we alredy have this in hive!</p>","tags":["spark-sql"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-04-08 11:39:33.0","id":26605,"title":"ambari alerts-","body":"<p>hI:</p><p>What its this alert:</p><p>There are 1 stale alerts from 1 host(s): lnxbig04.cajarural.gcr[App Timeline Web UI (41m)]</p><p><img src=\"/storage/attachments/3290-11111.png\"></p>","tags":["alerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-16 23:10:54.0","id":27894,"title":"cannot paste query into ambari hive sql workspace","body":"<p>So I'm following this tutorial.  At Lab 2, section 2.2.5 I cannot paste the second query</p><p>http://hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#section_4</p><p>It seems the first query from 2.2.1 always gets pasted.  I've opened up a plain text editor to try to confirm this was copied correctly.  I am able to paste the 2.2.5 longer query into the plain text editor no problem.  I try to recopy it from the plain text editor and paste to the worksheet - still the first query.  First attempt was Safari on Mac, then opened up Chrome and the issue was still there - it pasted the 2.2.1 query.  I can open another plain text editor window and hit Command V and I get the correct 2.2.5 query which makes me think it is not an issue with the clipboard.</p>","tags":["how-to-tutorial"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-25 20:20:00.0","id":8062,"title":"NiFi GetHTTP processor's uuid attribute","body":"<p>I'm using nifi-1.1.0.0-10 and trying to save the output of a URL to a unique filename using GetHTTP processor.  There is a 'Filename' property that supports expression language, but <em>${uuid}</em> does not seem to work like other variables.</p><p><em>./downloads/${uuid}</em> evaluates to ./downloads/</p><p>while</p><p><em>./downloads/${UUID()}</em> evaluates to ./downloads/40b08a2c-de4f-4504-ac86-1743b59a3810/</p><p>is <em>${uuid}</em> a special attribute that cannot be used in 'FIlename' inside of GetHTTP processor?</p>","tags":["nifi-processor","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-30 06:28:38.0","id":8337,"title":"Sqoop export hangs at 95% for oracle Database","body":"<p>Sqoop version: 1.4.4.2.1.7.0-784 </p><p>The following is the sqoop command that i used to export simple(comma separated) records .</p><pre>sqoop-export --connect jdbc:oracle:thin:@ipaddress:1521:orcl --username user --password password --table EMP --columns EMPNO,ENAME,JOB,MGR --export-dir /sqooptest/export -m 1 --batch</pre><p>The above command stucks at 95% and doesn't get completed.</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-11 19:18:04.0","id":9449,"title":"Sqoop/PolyBase/PDW","body":"<p>Using sqoop to create a flat file in HDFS.  Using '\\N '  as null-string and null-non-string. This file is then used to create an external file in Microsoft's Parallel Data Warehouse (PDW) using PolyBase.</p><p>HDFS interprets the '\\N' as a null, but PDW does not recognize the '\\N' as a valid value, so any query from PDW against the external table fails. </p><p>What value can I use as my null-string and null-non-string parameters, so PDW will interpret it as a NULL?</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-13 03:22:34.0","id":9614,"title":"How to change FQDN  in added hosts in HDP cluster ?","body":"<p>In my Ambari server I have added 40 slaves to the master, But the problem is I had changed the FQDN on slaves and master, therefore Master Node cannot identify the slaves now.</p><p>So can anyone help me to changed those FQDN which are already added?</p><p><em>Ambari server version is 2.1.2 in my cluster.</em></p>","tags":["hdp-2.3.4","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-14 13:41:02.0","id":9886,"title":"Merge two folders in HDFS ( like Windows move )","body":"<p>I have a scenario where data needs to be split up by date as it arrives. Which means that my pig script divides up data into timestamps using piggybank multistorage. ( we can't use hive partition loading ) </p><p>I.e. the output of the first script can be </p><pre>/output1/2015/file1-job1.gz/output1/2016/file1-job1.gz</pre><p>Now the second job comes and creates</p><pre>/output2/2016/file1-job2.gz/output2/2017/file1-job2.gz</pre><p>And I have to merge the files in the end into</p><pre>/target/2015/file1-job1.gz\n/target/2016/file1-job1.gz;file1-job2.gz\n/target/2017/file1-job2.gz</pre><p>Now In windows I could simply copy a folder and he would essentially merge the two folders I had hoped the hdfs mv operation would have a similar option but I didn't see it. </p><p>I could change the MultiStorage to write directly into a folder but in that case a failed pig job will leave broken Gzip files around. So i need to write to a temp dir and move the files on success in oozie</p><p>So at the moment I am thinking of writing a script that recursively reads all files ( hadoop fs -ls -R /output )  and then moves them one by one into the new directory. Either some python or shell script. Anybody done something similar before? </p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-15 17:08:37.0","id":10060,"title":"ClickStream Events Reporting in in Real Time with Storm","body":"<p><img src=\"/storage/attachments/1385-screen-shot-2016-01-15-at-82759-am.png\"></p>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-14 21:03:03.0","id":9985,"title":"HDPCD Certification practice test","body":"<p>I am practicing on AWS instance for HDPCD test.  I have taken m3.xlarge instance.  </p><p>Hadoop services are not started by default.  I just executed start-all-services.sh. It is taking very long time.  I waited for more  than 15 mins.  It is still going on.</p><p>Is the speed of m3.xlarge instance enough?  Or should I go for higher?</p><p>Response on the terminal is fine.</p>","tags":["hortonworks-university"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-30 05:03:56.0","id":12576,"title":"When runtime modifications are written to Edits log file in Name Node, is the Edits Log file getting updated on RAM or Local Disk?","body":"<p>When runtime modifications are written to Edits log file in Name Node, is the Edits Log file getting updated on RAM or Local Disk?</p>","tags":["namenode"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-01 01:25:07.0","id":12758,"title":"like to know if HDP can be installed without ambari, and if yes, what is the link for this install?","body":"<p>install HDP with ambari and without internet will require setup http server, for quick solution, we like to find if any way to install HDP without ambari, if yes, please provide the link. </p>","tags":["installation","links"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-02 07:37:13.0","id":13051,"title":"Is there any way to disable \"kill application\" button to Resource Manager's Web UI ?","body":"<p>Hello Friends,</p><p>Is there any way to disable \"kill application\" button to Resource Manager's Web UI ?</p><p>Thanks in advance. </p>","tags":["resource-manager"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-22 10:58:43.0","id":7651,"title":"Oozie: Throttling the number of forks.","body":"<p>I have a workflow.xml which is generated by one of our tool. The workflow contains a fork/join, to execute sqoop actions in parallel. What I want to archive is to limit the number of concurrent sqoop actions that can run in parallel from the fork. My questions are </p><p>\n1. Is there a configurations that can be used to limit/throttle maximum number of forks in parallel? </p><p>2. Is there a configuration to limit maximum number of concurrent execution of an action type (i.e. allow only 3 sqoop actions to be running at anytime in a workflow)</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-23 02:36:43.0","id":7763,"title":"VM desktop","body":"<p>Is it possible to get to the VM's desktop? I would like to upload some sample data files from my host windows machine to the VM? thanks</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-05 12:29:36.0","id":8745,"title":"Hive Error -  transport error 202: bind failed: Address already in use","body":"<p>Dear Team,</p><p>Attached an error found after used hive -debug command</p><p><strong><a href=\"/storage/attachments/1193-hive-error.png\">hive-error.png</a></strong></p><p>Regards,</p><p>Nilesh</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-06 14:20:21.0","id":8895,"title":"falcon  in Distributed-mode,    can not submit Entity  by {http://falcon-server:port}  web ui with error 404 not found。","body":"<p>falcon  0.8  version</p><p>Falcon  in Distributed-mode,    can not submit Entity  by {http://falcon-server:port}  web ui with error 404 not found。</p><p>And submit Entity  by falcon-cli  to  falcon-prism:port  sucessed. But the {falcon-prism:port}  web-ui  only has   three {cluster, dataset,process} types?   what's wrong with the falcon-server ui?</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-07 11:34:56.0","id":9088,"title":"Is there a Spark Notebook which is easily installable on HDP?","body":"<p>I've been trying to figure out what is the best/most appropriate Spark Notebook (web based IDE) for use on an HDP cluster. </p><p>Is there one available through Hortonworks packages?</p><p>I have found a number of training lessons from Hortonworks which talk about installing iPython Notebook (which can work for python+spark as well as scala+spark). However they seem to assume you are on a sandbox and are compiling and installing the software they demand, not checking what is already there and not using packages. </p><p>I have also seen mentions of Zeppelin - an Apache incubating project. This is good too - but I have the same problem - I have to build it myself and it doesn't for me.</p><p>There are also third party Notebooks such as the Cloud based one from Data Bricks.</p><p>Any recommendations? What has worked for you?</p>","tags":["spark-notebook"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-07 13:39:02.0","id":9098,"title":"piggybank compilation","body":"<p>Hi,</p><p>I need to change the code which is part of the Piggybank and i need to compile my own piggybank.jar.... is it possible inside sanbox?</p><p>I went step by step by <a href=\"https://cwiki.apache.org/confluence/display/PIG/PiggyBank\">https://cwiki.apache.org/confluence/display/PIG/P...</a> ... but it doesnt work at all.</p><pre>[root@sandbox trunk]# pwd\n/root/pig/trunk\n[root@sandbox trunk]# export CLASSPATH=$CLASSPATH:/usr/hdp/2.3.2.0-2950/hadoop/client/hadoop-mapreduce-client-core.jar:/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-ant.jar:/usr/hdp/2.3.2.0-2950/pig/pig-0.15.0.2.3.2.0-2950-core-h2.jar\n[root@sandbox trunk]# ant\n/usr/share/java-utils/java-functions: line 15: .: /etc/java/java.conf: cannot execute binary file\n/usr/share/java-utils/java-functions: line 15: .: /etc/java/java.conf: cannot execute binary file\n/usr/bin/build-classpath: error: JAVA_LIBDIR must be set\nError: Could not find or load main class org.apache.tools.ant.launch.Launcher</pre><p>thanks</p>","tags":["piggybank"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-05 15:53:02.0","id":8776,"title":"Customized HDP installation  on Different Default Directories, Not /usr","body":"<p>I have been using a sand box for a while now I need to take my test to another level. Ihave 5 dell PowerEdge 2850 with each having no less tha 8GB of RAM 2 and 4 cores. I intend to do a manual installation of the HDP and layout my file system eg \n/u01,/u02,/u03,/u04,/u05  each patition of about 200 GB.\nWhere can I download all the components of the HDP 2.3.2 .gz otherthan the RPM's that will allow me to do what I want because the HDP, HDP Utils all install the packages in /var etc or is there I way I could tell the install to install for example all under /u01/hadoop/pig,oozie,hdfs,yarn </p>","tags":["hdp-2.3.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-25 03:08:39.0","id":11491,"title":"java version to download for rhel 6","body":"<p>since we dont have internet access and not able to install Java from yum, so we have to download java jdk8 for rhel 6.5 installation. I pick this one -- jdk-8u71-linux-x64.gz and like to confirm weather this is a right one.</p><p>if not, please let me know the correct one along with URL, thanks</p>","tags":["java","rhel"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-01-25 18:58:11.0","id":11627,"title":"Ambari Setup Automation using MYSQL database","body":"<p>Hi All,</p><p>    I am working  on a task to automate the whole installation.. where I am struck with ambari-server installation and setup.  My script is issuing a command like..</p><p>      ambari-server setup -j /usr/lib/jvm/jre-1.7.0-openjdk.x86_64 -s --jdbc-driver /usr/share/java/mysql-connector-java.jar --jdbc-db mysql</p><p>    The installation is going through successfull, but he server is not starting.. The log is pasted below.</p><blockquote>\n<p><em>## installing ambari-agent</em></p><p><em>Verifying Python version compatibility...</em></p><p><em>Using python  /usr/bin/python2.6</em></p><p><em>Checking for previously running Ambari Agent...</em></p><p><em>Starting ambari-agent</em></p><p><em>Verifying ambari-agent process status...</em></p><p><em>Ambari Agent successfully started</em></p><p><em>Agent PID at: /var/run/ambari-agent/ambari-agent.pid</em></p><p><em>Agent out at: /var/log/ambari-agent/ambari-agent.out</em></p><p><em>Agent log at: /var/log/ambari-agent/ambari-agent.log</em></p><p><em>## install ambari-server</em></p><p><em>cp: cannot stat `/var/lib/ambari-server/resources/views/*.jar': No such file or directory</em></p><p><em>Using python  /usr/bin/python2.6</em></p><p><em>Setup ambari-server</em></p><p><em>Copying /usr/share/java/mysql-connector-java.jar to /var/lib/ambari-server/resources</em></p><p><em>JDBC driver was successfully initialized.</em></p><p><em>Ambari Server 'setup' completed successfully.</em></p><p><em>nohup: ignoring input and appending output to `nohup.out'</em></p><p><em>Ambari Server failed to start</em></p><p><em>## Success! All done.</em></p><p>root@ico-svr hdp_install]# ambari-server start </p><p>Using python  /usr/bin/python2.6\nStarting ambari-server </p><p>ERROR: Exiting with exit code -1.\nREASON: DB Name property not set in config file. </p><p>- If this is a new setup, then run the \"ambari-server setup\" command to create the user\n- If this is an upgrade of an existing setup, run the \"ambari-server upgrade\" command.\nRefer to the Ambari documentation for more information on setup and upgrade.</p></blockquote><p>   Can any faced this issue , any help would be appreciated.</p><p>Thanks</p><p>Siva</p>","tags":["ambari-1.7"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-14 00:15:51.0","id":9831,"title":"Hbase region servers crash with Out of Memory","body":"<p>not sure what parameter i have to change</p><pre>Caused by: java.lang.reflect.InvocationTargetException\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\nat org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2484)\n... 5 more\nCaused by: java.lang.OutOfMemoryError: Direct buffer memory\nat java.nio.Bits.reserveMemory(Bits.java:658)\nat java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123)\nat java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306)</pre>","tags":["Ambari","Hbase"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-27 07:34:46.0","id":11912,"title":"Oozie shell action: exec and file tags","body":"<p>I'm a newbie in Oozie and I've read some Oozie shell action examples but this got me confused about certain things.\nThere are examples I've seen where there is no &lt;file&gt; tag. </p><p>Some example, like in <a href=\"http://blog.cloudera.com/blog/2013/03/how-to-use-oozie-shell-and-java-actions/\">Cloudera here</a>, repeats the shell script in file tag:</p><pre>&lt;shell xmlns=\"uri:oozie:shell-action:0.2\"&gt;\n\t&lt;exec&gt;check-hour.sh&lt;/exec&gt;\n        &lt;argument&gt;${earthquakeMinThreshold}&lt;/argument&gt;\n        &lt;file&gt;check-hour.sh&lt;/file&gt;\n&lt;/shell&gt; </pre><p>While in <a href=\"https://oozie.apache.org/docs/4.1.0/DG_ShellActionExtension.html#Shell_Action\">Oozie's website</a>, writes the shell script (the reference ${EXEC} from job.properties, which points to script.sh file) twice, separated by #. </p><pre>&lt;shell xmlns=\"uri:oozie:shell-action:0.1\"&gt;\n        ...\n        &lt;exec&gt;${EXEC}&lt;/exec&gt;\n        &lt;argument&gt;A&lt;/argument&gt;\n        &lt;argument&gt;B&lt;/argument&gt;\n        &lt;file&gt;${EXEC}#${EXEC}&lt;/file&gt;\n&lt;/shell&gt; </pre><p>There are also examples I've seen where the path (HDFS or local?) is prepended  before the `script.sh#script.sh` within the &lt;file&gt; tag. </p><pre>&lt;shell xmlns=\"uri:oozie:shell-action:0.1\"&gt;\n        ...\n        &lt;exec&gt;script.sh&lt;/exec&gt;\n        &lt;argument&gt;A&lt;/argument&gt;\n        &lt;argument&gt;B&lt;/argument&gt;\n        &lt;file&gt;/path/script.sh#script.sh&lt;/file&gt;\n&lt;/shell&gt; </pre><p>As I understand, any shell script file can be included in the workflow HDFS path (same path where workflow.xml resides).\nCan someone explain the differences in these examples and how `&lt;exec&gt;`, `&lt;file&gt;`, `script.sh#script.sh`, and the `/path/script.sh#script.sh` are used?</p>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-20 05:57:59.0","id":10742,"title":"Why does a user need CREATE permission for \"list\" command on hbase shell?","body":"<p>Is READ permission not suitable?</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-18 06:46:01.0","id":10300,"title":"Sqoop: Importing from SQL Server throwing “The TCP/IP connection to the host x.x.x.x, port 1433 has failed”","body":"<p>On HDP 2.3.2 with Sqoop 1.4.6, I'm trying to import tables from SQL Server 2008.</p><p>I'm able to successfully connect to the SQL Server because I can list databases and tables etc.</p><p>However, every single time during imports I run into the following error:</p><blockquote>Error: java.lang.RuntimeException: java.lang.RuntimeException:\n  com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection\n  to the host x.x.x.x, port 1433 has failed. Error: \"connect timed\n  out. Verify the connection properties. Make sure that an instance of\n  SQL Server is running on the host and accepting TCP/IP connections at\n  the port. Make sure that TCP connections to the port are not blocked\n  by a firewall.\".</blockquote><p>Again, I am actually able to successfully import from SQL Server, but\n only after a couple of retries. However, regardless of whether the \nimport succeeded or failed, I <em>always</em> get the error mentioned above and I\n was wondering what could be causing the problem? It's rather cumbersome\n to have to keep repeating the imports whenever they fail.</p><p>I've already turned off the connection time-out on the SQL Server, \nand though the connection from the Hadoop cluster and the SQL Server \npasses through our corporate firewall, our admins tell me that the \ntimeout on the firewall is 3600 seconds. The imports fail before getting\n anywhere near that mark.</p><p>Just an example of one of the sqoop commands I use:</p><pre>sqoop import \\\n  -D mapred.task.timeout=0 \\\n  --connect \"jdbc:sqlserver://x.x.x.x:1433;database=CEMHistorical\" \\\n  --table MsgCallRefusal \\\n  --username hadoop \\\n  --password-file hdfs:///user/sqoop/.adg.password \\\n  --hive-import \\\n  --hive-overwrite \\\n  --create-hive-table \\\n  --split-by TimeStamp \\\n  --hive-table develop.oozie \\\n  --map-column-hive Call_ID=STRING,Stream_ID=STRING,AgentGlobal_ID=STRING</pre><p><strong>Update:</strong></p><p>After getting in touch with our network team, it seems this is most definitely a network issue. To add context, the Hadoop cluster is on a different VLAN as the SQL Server and it goes through a number of firewalls. To test, I tried importing from a different SQL Server within the same VLAN as the Hadoop cluster and I didn't encounter this exception at all.\n<strong></strong></p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-22 11:43:10.0","id":11238,"title":"–hive-drop-import-delims OR --hive-delims-replacement not really take effect","body":"<p>I used the following command </p><pre><em>sqoop import --connect \"jdbc:sqlserver://xxxx.xxxx.xxxx.xxxx;username=xxx; password=xxx;database=xxxx\" --query \"select GoodsCode,GoodsNo,GoodsName,ProdArea from  hq.[000].tbgoods where  \\$CONDITIONS\" -m 1 --target-dir /user/hive/warehouse/tmp/tbgoods --as-textfile --input-null-string '\\\\N' --input-null-non-string '\\\\N' --fields-terminated-by '\\t' --lines-terminated-by '\\n' --hive-drop-import-delims --delete-target-dir</em></pre><p>Some of Columns like <em>GoodsNo includes '\\t' , </em>but use the command can't really delete the '\\t' from the Column. </p><p>How to solve this problem?</p>","tags":["Sqoop"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-22 20:20:08.0","id":11329,"title":"how can i get flume 1.6 configuration file set up according to kafka sink?please provide me step by step instructions","body":"","tags":["flume-1.6"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-25 04:42:08.0","id":11500,"title":"Error virtual box call from sandbox.hortonworks.com/10.0.2.15 to sandbox.hortonworks.com:8020 failed on connection exception:connection refused","body":"","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-25 22:33:09.0","id":11657,"title":"How to substractduration  of 1min from a ISO time in Pig ?","body":"","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-10 01:21:00.0","id":38,"title":"Cause of DBVisulizer client error after turning on LZO/snappy compression?","body":"<p>Experts,</p><p>Has anyone encountered the exception below?</p><pre>... Physical database connection acquired for: Hive QA1\n15:03:16  [SELECT - 0 row(s), 0.000 secs]  [Error Code: 35, SQL State: S1000]  [Hortonworks][HiveODBC] (35) Error from Hive: error code: '0' error message: 'java.io.IOException: java.io.IOException: Cannot create an instance of InputFormat class org.apache.hadoop.mapred.TextInputFormat as specified in mapredWork!'.\n... 1 statement(s) executed, 0 row(s) affected, exec/fetch time: 0.000/0.000 sec  [0 successful, 0 warnings, 1 errors]\n </pre><p>I suspect it is caused by knox gateway server is missing lzo/snappy.<br><br>1. Executing same query on hive cli works fine<br>2. Checked Knox Gateway machine, /etc/hadoop/conf/core-site.xml does not have io.compression properties while the cluster(and all other nodes) has them.<br>3. knox gateway machine missing lzo/snappy packages/jars<br> <br>Appreciate your quick input.</p>\n","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-17 17:34:18.0","id":119,"title":"Hipchat test 10003","body":"<p>More hipchat tests</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-23 21:16:42.0","id":160,"title":"Storm and Kafka Development Cycle - Best Practices","body":"<p>Wanted to ask what process our Storm and Kafka developers & testers follow when they’re developing new topologies? The process of building a Storm/Kafka topology on laptop, copying files to cluster or sandbox, and testing can be time consuming and difficult to iterate quickly. Do you have any tips you could pass on for efficient development process:</p><ol><li>Do you develop on the same machine (linux or mac) which is also running your Storm/Kafka cluster?</li><li>Do you instead rely on JUnit tests to instantiate and test Storm bolts on their own and manually trigger Tuples through the execute() method (<a href=\"https://github.com/apache/storm/blob/master/external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java#L106\">example here</a>)?</li></ol><p>Thanks for any tips you can pass along to help speed up our development.</p>","tags":["development","Storm","Kafka"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-24 23:07:37.0","id":213,"title":"Support for Container Executor?","body":"<p>Do we actually support Docker Container Executor in HDP 2.3 release?</p>","tags":["docker","YARN","hdp-2.3.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-25 14:05:31.0","id":266,"title":"How to resolve open SSL error upon host registration in Ambari?","body":"<p>I'm trying to register hostnames in Ambari but getting the error below. We tried to run yum update openssl but its got the latest version. We tried to run yum - update it didn’t help. I also tried removing the other openssl packages openssl-devel-1.0.1e-42.el6.x86_64, openssl098e-0.9.8e-17.el6.centos.2.x86_64 and restarting the ambari server/agents and same error. Tried running yum -y install opens* as well but didn’t help. Any ideas?</p><pre>ERROR 2015-09-23 09:47:07,402 NetUtil.py:77 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol\nERROR 2015-09-23 09:47:07,402 NetUtil.py:78 - SSLError: Failed to connect. Please check openssl library versions.\nRefer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:17,403 NetUtil.py:59 - Connecting to https://test.org:8440/ca\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:82 - Failed to connect to https://test.org:8440/ca due to [Errno 111] Connection refused\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\n</pre>","tags":["ssl","Ambari","ambari-2.1.1"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 13:59:00.0","id":265,"title":"How to resolve SSL error upon registration in Ambari? - Testing AnswerHub","body":"<p>I'm trying to register hostnames in Ambari but getting the error below. We tried to run yum update openssl but its got the latest version. We tried to run yum - update it didn’t help. I also tried removing the other openssl packages openssl-devel-1.0.1e-42.el6.x86_64, openssl098e-0.9.8e-17.el6.centos.2.x86_64 and restarting the ambari server/agents and same error. Tried running yum -y install opens* as well but didn’t help. Any ideas?</p><pre>ERROR 2015-09-23 09:47:07,402 NetUtil.py:77 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol\nERROR 2015-09-23 09:47:07,402 NetUtil.py:78 - SSLError: Failed to connect. Please check openssl library versions.\nRefer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:17,403 NetUtil.py:59 - Connecting to https://test.org:8440/ca\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:82 - Failed to connect to https://test.org:8440/ca due to [Errno 111] Connection refused\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\n</pre>","tags":["ssl","Ambari","ambari-2.1.1"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 17:37:17.0","id":285,"title":"How to resolve SSL error upon host registration with Ambari?","body":"<p>I'm trying to register hostnames in Ambari but getting the error below. We tried to run yum update openssl but its got the latest version. We tried to run yum - update it didn’t help. I also tried removing the other openssl packages openssl-devel-1.0.1e-42.el6.x86_64, openssl098e-0.9.8e-17.el6.centos.2.x86_64 and restarting the ambari server/agents and same error. Tried running yum -y install opens* as well but didn’t help. Any ideas?</p><pre>ERROR 2015-09-23 09:47:07,402 NetUtil.py:77 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol\nERROR 2015-09-23 09:47:07,402 NetUtil.py:78 - SSLError: Failed to connect. Please check openssl library versions.\nRefer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:17,403 NetUtil.py:59 - Connecting to https://test.org:8440/ca\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:82 - Failed to connect to https://test.org:8440/ca due to [Errno 111] Connection refused\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\n\n\n</pre>","tags":["ssl","Ambari","ambari-2.1.1"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 14:40:46.0","id":271,"title":"Has anyone tried using a Apache NiFi on Raspberry Pi?","body":"<p>Raspberry Pi is a very inexpensive system often hooked up to variety of sensors. It will be useful to use Raspberry Pi as the host for Apache NiFi to ingest and coordinate the data from sensors before transporting the stream to alerting system or persistent storage</p>","tags":["Nifi","dataflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-09-29 13:37:23.0","id":466,"title":"Vectorization Causing a Select COUNT(1) Query to Fail","body":"<p>When I run the SQL statement below with Tez and Vectorization on, it fails. However the minute I disable Vectorization it returns without any issues. Has anyone ever seen this type of behavior before? If so any suggestions on how to resolve it?</p><p>Code:</p><pre>SELECT ROW1, COUNT(1) FROM TABLE GROUP BY ROW1 </pre><p>Error:</p><pre>Vertex failed, vertexName=Map 1, vertexId=vertex_1440535871082_0590_1_00, diagnostics=[Task failed, taskId=task_1440535871082_0590_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\nat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:294)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)\n... 16 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 973\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateLong(ConstantVectorExpression.java:102)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(ConstantVectorExpression.java:150)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.aggregateInput(VectorUDAFCount.java:170)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processAggregators(VectorGroupByOperator.java:143)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate.processBatch(VectorGroupByOperator.java:321)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.processOp(VectorGroupByOperator.java:859)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:138)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\nat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)\n... 17 more\n], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\nat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:294)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)\n... 16 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 973\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateLong(ConstantVectorExpression.java:102)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(ConstantVectorExpression.java:150)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.aggregateInput(VectorUDAFCount.java:170)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processAggregators(VectorGroupByOperator.java:143)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate.processBatch(VectorGroupByOperator.java:321)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.processOp(VectorGroupByOperator.java:859)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:138)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\nat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)\n... 17 more\n], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\nat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:294)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)\n... 16 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 973\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateLong(ConstantVectorExpression.java:102)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(ConstantVectorExpression.java:150)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.aggregateInput(VectorUDAFCount.java:170)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processAggregators(VectorGroupByOperator.java:143)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate.processBatch(VectorGroupByOperator.java:321)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.processOp(VectorGroupByOperator.java:859)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:138)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\nat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)\n... 17 more\n], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\nat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\nat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:294)\nat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row \nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)\nat org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)\n... 16 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 973\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateLong(ConstantVectorExpression.java:102)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(ConstantVectorExpression.java:150)\nat org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.aggregateInput(VectorUDAFCount.java:170)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processAggregators(VectorGroupByOperator.java:143)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate.processBatch(VectorGroupByOperator.java:321)\nat org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.processOp(VectorGroupByOperator.java:859)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:138)\nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\nat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\nat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\nat org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)\n... 17 more\n]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1440535871082_0590_1_00 [Map 1] killed/failed due to:null]\nVertex killed, vertexName=Reducer 2, vertexId=vertex_1440535871082_0590_1_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1440535871082_0590_1_01 [Reducer 2] killed/failed due to:null]\nDAG failed due to vertex failure. failedVertices:1 killedVertices:1\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask</pre>","tags":["Tez","Hive","sql"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-29 14:13:58.0","id":475,"title":"Kerberos client for Windows workstations?","body":"<p>I am working with a Kerberized HDP 2.3 cluster that uses AD with Kerberos for strong authentication via Knox. Does the end user need to have a local Kerberos client installed on their Windows workstations to request the ticket? Or is it possible to enable SSO such that Kerberos ticket is issues as part of Windows domain login?</p>","tags":["Knox","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 14:30:39.0","id":477,"title":"Do we need to rebuild the VM's on RHEL6 to use ranger 0.5.0 in HDP2.3 ambari 2.1 managed cluster?","body":"<p>HDP2.3 documentation has RHEL7 as a supported OS. After installing an Ambari 2.1 managed cluster proceed to Ranger installation where the documentation states Ranger is not yet supported on the OS version. Don't recommend managing/installing some hosts/components via Ambari and others manually. In order to use Ranger with this environment do we need to rebuild the VM's on RHEL6? Is there a better workaround?</p>","tags":["ranger-0.5.0","hdp-2.3.0","configuration"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 14:47:05.0","id":480,"title":"Spark 1.4.1 issue with thrift server java.lang.NoSuchFieldError: SASL_PROPS","body":"<p>I am trying to run the spark thrift server 1.4.1 with user “hive”. Environment: HDP 2.3 kerberized cluster. The user has a kerberos ticket. The same command with the same user works with spark 1.3.1 installed by default in HDP 2.3</p><p><strong>Command to start spark thrift server :</strong> ./sbin/start-thriftserver.sh –master yarn –hiveconf hive.server2.thrift.bind.host &lt;hostname&gt; –hiveconf hive.server2.thrift.port 10002</p><p><strong>Error:</strong></p><p>15/09/29 10:22:08 INFO yarn.YarnSparkHadoopUtil: getting token for namenode: hdfs://&lt;hostname&gt;:8020/user/hive/.sparkStaging/application_1442865532157_0589&lt;/span&gt;</p><p>15/09/29 10:22:08 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 1804 for hive on 10.65.128.101:8020</p><p>15/09/29 10:22:09 INFO hive.metastore: Trying to connect to metastore with URI thrift://&lt;hostname&gt;:9083</p><p>15/09/29 10:22:09 ERROR metadata.Hive: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient</p><p>at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)</p><p>at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:62)</p><p>at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)</p><p>at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)</p><p>at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)</p><p>at org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(Hive.java:2572)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</p><p>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>at java.lang.reflect.Method.invoke(Method.java:606)</p><p>at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$obtainTokenForHiveMetastore(Client.scala:1142)</p><p>at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:263)</p><p>at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:561)</p><p>at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:115)</p><p>at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)</p><p>at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)</p><p>at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:497)</p><p>at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:51)</p><p>at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:73)</p><p>at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</p><p>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</p><p>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</p><p>at java.lang.reflect.Method.invoke(Method.java:606)</p><p>at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)</p><p>at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)</p><p>at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)</p><p>at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)</p><p>at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</p><p>Caused by: java.lang.reflect.InvocationTargetException</p><p>at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</p><p>at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</p><p>at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</p><p>at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</p><p>at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)</p><p>… 28 more</p><p>Caused by: java.lang.NoSuchFieldError: SASL_PROPS</p><p>at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.getHadoopSaslProperties(HadoopThriftAuthBridge20S.java:126)</p><p>at org.apache.hadoop.hive.metastore.MetaStoreUtils.getMetaStoreSaslProperties(MetaStoreUtils.java:1475)</p><p>at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:322)</p><p>at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:214)</p><p>… 33 more</p><p>15/09/29 10:22:09 ERROR yarn.Client: Unexpected Exception java.lang.reflect.InvocationTargetException</p><p>15/09/29 10:22:09 ERROR spark.SparkContext: Error initializing SparkContext.</p>","tags":["Spark","Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-09-29 16:31:45.0","id":510,"title":"Script to get files from HDFS to local OS \"Automated process\"","body":"<p>End user needs to get files from HDFS. The process is :</p><p>End user --&gt; Gateway box ( Look for file locally. If not there then talk to HDFS) --&gt; HDFS --&gt; copy file in gateway box </p>","tags":["HDFS"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-08 13:09:41.0","id":15193,"title":"Hi, is it possible to install HUE in HDP2.3.4.0 using Ambari? If so can you please provide the steps?","body":"<p>Hi, is it possible to install HUE in HDP2.3.4.0 using Ambari? If so can you please provide the steps?</p>","tags":["hue"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-08 16:51:23.0","id":15294,"title":"How to load the excel data into hive using python script?","body":"<p>I created a python script using excel data and want to load the excel data into hive using python script</p>","tags":["python"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-04-20 08:43:14.0","id":28438,"title":"Can i get twitter data to HDP, process it and show the result in Android Application using web services?","body":"<p>Hello anyone,</p><p>First, i want say sorry because maybe my question is out of the topic. But, i need to ask it because i dont find another forum that discuss about Big Data and how to use it with your own (Using Tutorial that provided by Hortonworks).</p><p>My question is</p><p>1. Can i get data from twitter to HDP ? </p><p>    Is it true that i can use Apache Nifi or Can i use another Software like Apache Kafka ? </p><p>    But How ?</p><p>2. When i get the data form Twitter, how to process it ? </p><p>    Can i use Spark or Mahout ? </p><p>    In this case, i want to find the most people personality from twitter, which is the best option ?</p><p>3. Then, if i can calculate the data, how i can access it with android aplication ?</p><p>    In my mind, i think i can use web service but i dont know how to make it with HDP ?</p><p>    I will make a clear explanation about this,</p><p>    a. User want to use my app. But, user must login to Twitter before use this app.</p><p>    b. After this, i want to collect some twitter data from user.</p><p>    c. Twitter data passes to HDP using Web services.</p><p>    d. HDP will calculate the data with certain algorithm.</p><p>    e. After that, HDP will send the result the data to android app.</p><p>    f. Voila, use can see his/her personality using twitter data.</p><p>   Can i implement this ? Or maybe i think too much how use HDP ?</p><p>4. Do you some another idea for my final project in the college ?</p><p>   Actually, my older lecturer use HDP but only single cluster (Sandbox). And i want to make it multi node, but i dont know          how to maximize the potential how to use HDP and real Bid Data.</p><p>I hope my question with not confuse you guys, why i ask to many question because my lack knowledege how to use HDP and Big Data. I hope i can get the answer and have a nice day :).</p>","tags":["Hive","oracle","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-03 22:09:49.0","id":31232,"title":"Upgrade All components in Ambari other than Kafka","body":"<p>Is it possible to upgrade all components like hive, mr,tez, spark and just leave out   Kafka. The reason is kafka is running in 0.8.1 version and upgrading to 0.9 -   consumer jobs will get impacted. After 0.8.2 onwards the moved away from ZK dependencies to broker and we are not sure how much time it will take to re-write the kafka consumer jobs. </p>","tags":["upgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-11 14:24:50.0","id":32742,"title":"Newbie question about how Hortonworks works with transaction log data, HVR software, and Database Agent version","body":"<p>Hello, \nI am new to Hortonworks.  My company is in the process of setting up a HW environment, and we are looking at how it integrates with our existing HVR Transaction Log solution. http://www.hvr-software.com/\nMy manager has tasked me with finding out about how Hortonworks handles transaction-type data, and asked me to find out about the Database Agent version. \nNeither he or I have much knowledge of Hortonworks, and so any guidance/information would be greatly appreciated!  \nThanks! </p>","tags":["Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-03 04:06:01.0","id":13759,"title":"Did any one upload HDP-2.3.4.0-centos6.rmp file to server? any tool that I can use to upload this 5gb file?","body":"<p>never make this 5gb file upload to server by use winscp and pscp(putty). hope someone had experience to upload this file and let me know which tool used for this transaction.</p><p>thanks,</p>","tags":["help","files"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-09 03:28:31.0","id":15454,"title":"How to set up SQL Server Settings?","body":"<p>Hi, </p><p>I manage to set up SQL Server 2012 for HDP2.3.4 on Windows Server 2012.</p><p>I set this in the same way as the manual \"Hortonworks Data Platform -Installing HDP on Windows\".</p><p>\"1.2.4.(Optional)Microsoft SQL Server for Hive and Oozie Database Instances\" says below.</p><p>1st.Make hive and oozie DB.</p><p>2nd.Make accounts for hive and oozie DB with sysadmin.</p><p>3rd.Set policy to user SQL and Windows authentication for SQL.</p><p>But installation fails.</p><p>Error logs say \"Can't access \"oozie\" database from this login. Login fails\".</p><p>How do I set MS SQL Server 2012 for HDP?</p>","tags":["sql"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-13 12:55:29.0","id":27345,"title":"Can we change location of staging data dir in hive ?","body":"<p>Team:</p><p>Can we change location of temp staging file .hive-staging_hive_2016-04-13_08-1* which is getting create inside /apps/hive/warehouse whenever we are running some create table as select statement.</p><p>CREATE TABLE test_survey_answers_2016_01_04_oldrecords_temp AS SELECT AM.question_id AS question_id_old, AM.answer_id AS answer_id_old, AM.value AS value_old, AM.text AS text_old, AM.start_date AS start_date_old, AM.end_date as end_date_old, AM.load_date_time as load_date_time_old, AMD.question_id AS question_id_new, AMD.answer_id AS answer_id_new, AMD.value AS value_new, AMD.text AS text_new FROM test_survey_answers AM LEFT OUTER JOIN test_survey_answers_2016_01_04_daily_temp AMD ON (AM.question_id= AMD.question_id AND AM.answer_id=AMD.answer_id;</p><p>I tried to change hive.metastore.warehouse.dir to some other dir but problem is new database also getting create in that location. </p><p>So I want to have to diffrent dirs, one is for databases(i.e /apps/hive/warehouse) and another one is for temp staging data(like /tmp/ or something else). </p><p>Can someone please help on this issue. </p>","tags":["Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-10 23:35:48.0","id":49,"title":"This is another test question","body":"<p>This is another test</p>\n","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-23 21:18:29.0","id":161,"title":"​Storm and Kafka Development Cycle - Best Practices","body":"<p>Wanted to ask what process our Storm and Kafka developers & testers follow when they’re developing new topologies? The process of building a Storm/Kafka topology on laptop, copying files to cluster or sandbox, and testing can be time consuming and difficult to iterate quickly. Do you have any tips you could pass on for efficient development process:</p><ol><li>Do you develop on the same machine (linux or mac) which is also running your Storm/Kafka cluster?</li><li>Do you instead rely on JUnit tests to instantiate and test Storm bolts on their own and manually trigger Tuples through the execute() method (<a href=\"https://github.com/apache/storm/blob/master/external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java#L106\">example here</a>)?</li></ol><p>Thanks for any tips you can pass along to help speed up our development.</p>","tags":["development","Kafka","Storm"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-30 21:22:46.0","id":654,"title":"How to get job status notifications from oozie?","body":"<p>In an application, oozie jobs get submitted to be executed on the cluster. </p><p>We want to track when the job starts, runs and completes so that the application is updated. </p><p>Oozie provides a way to send the job status via a property called ‘oozie.wf.workflow.notification.url’ which can be configured in the workflow.xml. </p><p>Oozie sends a GET request to whatever the url they configure it to.\nHowever, it doesn’t seem to be generating these notifications. </p><p>Does anything else need to be configured on the Oozie side in addition to the above modification to the workflow.xml?   Does anything need to be set elsewhere? </p><p>Here is what the workflow.xml looks like: </p><pre>&lt;workflow-app xmlns=\"uri:oozie:workflow:0.4\" name=\"ingest-tblrig-wf\"&gt;\n   &lt;start to=\"run-ingest-script\" /&gt;\n   &lt;action name=\"run-ingest-script\"&gt;\n      &lt;shell xmlns=\"uri:oozie:shell-action:0.2\"&gt;\n         &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;\n         &lt;name-node&gt;${nameNode}&lt;/name-node&gt;\n         &lt;configuration&gt;\n            &lt;property&gt;\n               &lt;name&gt;mapred.job.queue.name&lt;/name&gt;\n               &lt;value&gt;${queueName}&lt;/value&gt;\n            &lt;/property&gt;\n            &lt;property&gt;\n               &lt;name&gt;mapred.job.user.name&lt;/name&gt;\n               &lt;value&gt;${jobUserName}&lt;/value&gt;\n            &lt;/property&gt;\n            &lt;property&gt;\n               &lt;name&gt;oozie.wf.workflow.notification.url&lt;/name&gt;\n               &lt;value&gt;http://server_name:8080/oozieNotification/jobUpdate?jobId=$jobId%26status=$status&lt;/value&gt;\n            &lt;/property&gt;\n         &lt;/configuration&gt;\n         &lt;exec&gt;ingest_script_invoker.sh&lt;/exec&gt;\n         &lt;env-var&gt;HADOOP_USER_NAME=DKari&lt;/env-var&gt;\n         &lt;file&gt;ingest_script_invoker.sh&lt;/file&gt;\n         &lt;file&gt;ingest_tblRig.sh&lt;/file&gt;\n         &lt;file&gt;test_expect.sh&lt;/file&gt;\n         &lt;capture-output /&gt;\n      &lt;/shell&gt;\n      &lt;ok to=\"end\" /&gt;\n      &lt;error to=\"fail\" /&gt;\n   &lt;/action&gt;\n   &lt;kill name=\"fail\"&gt;\n      &lt;message&gt;Workflow failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;\n   &lt;/kill&gt;\n   &lt;end name=\"end\" /&gt;\n&lt;/workflow-app&gt;\n</pre>","tags":["Oozie"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-08 22:29:07.0","id":5474,"title":"Why are there 21 separate service accounts?","body":"<p>The admins want to know why every service has its own account ID, and is there any harm is using the same account for all? The cluster will be tightly secured. What is the best practice?</p>","tags":["service-account"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-09 15:57:10.0","id":5579,"title":"insert overwrite of 2 GB data","body":"<p>I am running\ninsert overwrite table emp_rc partition(year) select *,year_num from table1;</p><p>table1 size is 1.8 GB and this table is textfile, while emp_rc is RCFile.\nWhen i run this sql, it takes 1 hour, I have set mapreduce.job.reduces = 30 and now it takes 15 mins.</p><p>Can you advise, what else I can do to improve performance.</p><p>Can you explain how to analyse explain plan for such type of HQL.</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-11 14:27:32.0","id":6055,"title":"Having problem in accessing Hive","body":"<p>Ambari console is showing that hive and hive2 have failed to start on the hiverserver node, but it is responsive to queries. </p>","tags":["Ambari","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-19 15:28:00.0","id":7327,"title":"I am not able to download sandbox as I am stuck at register your sandbox.Please help.","body":"","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-25 15:31:13.0","id":8055,"title":"Getting eror while loading a HIVE table.","body":"<p>I have started to work with HDP Sandbox in Azure. I created a table and when I attempted to load data into the table I am getting the below error.</p><p>Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</p><p>I think this is associated with the user access. Can someone tell me the remedy for this issue?</p>","tags":["Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-31 12:43:35.0","id":8479,"title":"Hive Acid: How to kill  Locks & transaction ?","body":"<p>Hello,</p><p>I have many open transactions on table that i cant' delete.</p><p>Even if i delete the table, transactions are still open.</p><p>There is a way to rollback these transactions?</p><p>There is a way to kill pending locks?</p><p>Hive version : 1.2.1</p><p>Thanks </p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-04 20:10:40.0","id":8663,"title":"Unable to build Spark application due to missing jetty-util:jar:6.1.26.hwx dependency","body":"<p>I'm trying to build a simple Spark Java application that pulls its dependencies from the HDP Releases repository.  My project only depends on:</p><pre>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n    &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;\n    &lt;version&gt;1.5.2.2.3.4.0-3485&lt;/version&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n&lt;/dependency&gt;</pre><p>Through a complex web of dependencies jetty-util version 6.1.26.hwx is required, but it is not found in any publicly visible repository.  Where can I find this dependency so that I can build a Spark application that uses Hortonworks packaged jars?  Is it best to exclude the jetty-util dependency from spark-core_2.10 and then explicitly add it to the project with a non-HWX version?</p><p>(This question is similar to <a href=\"https://community.hortonworks.com/questions/6256/jetty-util-6126hwxjar.html\">https://community.hortonworks.com/questions/6256/j...</a> but the solution posted there does not work for my scenario.)</p>","tags":["hdp-2.3.0","hadoop","java","Spark"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-05-22 07:58:53.0","id":34727,"title":"Too many levels of symbolic links when running basic installation","body":"<p>Latest Ambari 2.2 download 5/22/16. Installing HDP 2.3 on a ten-node cluster.</p><p>HDFS-Client and HBase-Client installs fail with: Applying File['/usr/hdp/current/hadoop-client/conf/hdfs-site.xml'] failed, parent directory /usr/hdp/current/hadoop-client/conf doesn't exist (similar for the HBase Client.)\n\nSymbolic link appears circular:\n\n\n</p><pre>$ cd /usr/hdp/current/hadoop-client\n\n$ ls -al conf lrwxrwxrwx 1 root root 16 May 22 05:44 conf -&gt; /etc/hadoop/conf </pre><pre>$ ls -al /etc/hadoop/conf lrwxrwxrwx 1 root root 35 May 22 05:44 /etc/hadoop/conf -&gt; /usr/hdp/current/hadoop-client/conf</pre><p>Is this a known issue? Is there a way out of it?</p>","tags":["installation","Hbase"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-23 08:48:28.0","id":34859,"title":"How to ingest the changed data only in hive data base","body":"<p>I have a table in Hcatelog (hadoop DB). The content for this table is coming from two sources I.e one is RDBMS and other is through web channel. Can someone suggest how can I take only the changed records to be updated in my table from these two sources. I can take the data using sqoop/flume etc. but does they support to fetch only changed values to be inserted. Please suggest me if i have to use Kafka or any other tool. Also let me know if i could not classify the question well. Thanks.</p>","tags":["data-ingestion","help"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-05-24 05:36:52.0","id":35102,"title":"Hive metastore installation fails with constraints issues","body":"<p>I'm trying to install HDP 2.3 on an AWS cluster. HDP 2.3.4.7-4.</p><p>Starting the Hive Metastore yields:\n\nresource_management.core.exceptions.Fail: Execution of 'export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-metastore/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED]' returned 1. WARNING: Use \"yarn jar\" to launch YARN applications.</p><pre>Metastore connection URL:\t jdbc:mysql://ip-172-31-53-134.ec2.internal/hive?createDatabaseIfNotExist=true\nMetastore Connection Driver :\t com.mysql.jdbc.Driver\nMetastore connection User:\t hive\nStarting metastore schema initialization to 1.2.0\nInitialization script hive-schema-1.2.0.mysql.sql\nError: Can't create table 'hive.BUCKETING_COLS' (errno: 121) (state=HY000,code=1005)\norg.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!\n*** schemaTool failed ***\n\nVersion information:\n\n\nHive 1.2.1.2.3.4.7-4\nSubversion git://c66-slave-20176e25-3/grid/0/jenkins/workspace/HDP-patch-centos6/bigtop/build/hive/rpm/BUILD/hive-1.2.1.2.3.4.7 -r eb78776f8bfd65b27ded6c1736ecb42d5c06c137\nCompiled by jenkins on Thu Feb 11 08:27:22 UTC 2016\nFrom source with checksum 79234aeebb6f0580c584c61257828e61\n\nSo, I decided to try to source in the schema DDL from the mysql shell. A LOT of error 121; that is, constraint issues.\n\nMy first attempt was to source in /usr/hdp/2.3.4.7-4/hive/scripts/metastore/upgrade/mysql/hive-schema-1.2.0.mysql.sql, which yielded many errors. So, it must be the DDL itself. Partial output:\n\nmysql&gt; source hive-schema-1.2.0.mysql.sql;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nERROR 1005 (HY000): Can't create table 'hive.BUCKETING_COLS' (errno: 121)\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nERROR 1005 (HY000): Can't create table 'hive.CDS' (errno: 121)\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nERROR 1005 (HY000): Can't create table 'hive.COLUMNS_V2' (errno: 121)\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nERROR 1005 (HY000): Can't create table 'hive.DATABASE_PARAMS' (errno: 121)\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nERROR 1005 (HY000): Can't create table 'hive.DBS' (errno: 121)\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nERROR 1005 (HY000): Can't create table 'hive.DB_PRIVS' (errno: 121)\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\nERROR 1005 (HY000): Can't create table 'hive.GLOBAL_PRIVS' (errno: 121)\nQuery OK, 0 rows affected (0.00 sec)\n\nSo, I tried with a single CREATE TABLE example:\n\n\n\nmysql&gt; /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40101 SET NAMES utf8 */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40103 SET TIME_ZONE='+00:00' */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; \nmysql&gt; --\nmysql&gt; -- Table structure for table `BUCKETING_COLS`\nmysql&gt; --\nmysql&gt; \nmysql&gt; /*!40101 SET @saved_cs_client     = @@character_set_client */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; /*!40101 SET character_set_client = utf8 */;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nmysql&gt; CREATE TABLE IF NOT EXISTS `BUCKETING_COLS` (\n    -&gt;   `SD_ID` bigint(20) NOT NULL,\n    -&gt;   `BUCKET_COL_NAME` varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,\n    -&gt;   `INTEGER_IDX` int(11) NOT NULL,\n    -&gt;   PRIMARY KEY (`SD_ID`,`INTEGER_IDX`),\n    -&gt;   KEY `BUCKETING_COLS_N49` (`SD_ID`),\n    -&gt;   CONSTRAINT `BUCKETING_COLS_FK1` FOREIGN KEY (`SD_ID`) REFERENCES `SDS` (`SD_ID`)\n    -&gt; ) ENGINE=InnoDB DEFAULT CHARSET=latin1;\nERROR 1005 (HY000): Can't create table 'hive.BUCKETING_COLS' (errno: 121)\nmysql&gt; \n\n\nI've also added a new metastore on a different node and deleted the old one. No difference.\n\n</pre>","tags":["installation","Hive","hdp-2.3.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-07 08:09:29.0","id":38103,"title":"HDP Repository on Windows IIS","body":"<p>Hello</p><p>I am installing HDP on a Centos 7 VM. I have setup a local HDP repository in a Windows Machine (IIS), however i am getting the following error when installing HDP. I am able to ping the windows machine from my ambari linux server. What could the problem be?</p><pre>failure: repodata/repomd.xml from HDP-2.4: [Errno 256] No more mirrors to try.\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Service Unavailable\nhttp://be-15-061/hdp/HDP/centos7/2.x/updates/2.3.4.0/repodata/repomd.xml: [Errno 14] HTTP Error 503 - Ser\nvice Unavailable</pre>","tags":["installation"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-17 16:38:51.0","id":117,"title":"Hipchat test 10001","body":"<p>Ambari test 10001</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-23 20:04:16.0","id":146,"title":"Where to change AMS account name in Ambari 2.1","body":"<p>We have upgraded to Ambari 2.1.1  and we cannot find where to override the new AMS account name. All of our Service Accounts are defined in AD and we must follow an internal naming convention.</p><p>Can someone point us where and how to change the AMS account name? </p><p>Thanks,</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-23 21:19:47.0","id":162,"title":"Storm and Kafka Development Cycle - Best Practices","body":"<p>Wanted to ask what process our Storm and Kafka developers & testers follow when they’re developing new topologies? The process of building a Storm/Kafka topology on laptop, copying files to cluster or sandbox, and testing can be time consuming and difficult to iterate quickly. Do you have any tips you could pass on for efficient development process:</p><ol><li>Do you develop on the same machine (linux or mac) which is also running your Storm/Kafka cluster?</li><li>Do you instead rely on JUnit tests to instantiate and test Storm bolts on their own and manually trigger Tuples through the execute() method (<a href=\"https://github.com/apache/storm/blob/master/external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java#L106\">example here</a>)?</li></ol><p>Thanks for any tips you can pass along to help speed up our development.</p>","tags":["development","Storm"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-24 04:52:42.0","id":172,"title":"test somethign that is flagged","body":"<p>test something that is flagged</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-09-24 15:09:53.0","id":178,"title":"Is there a way to tune the performance of the Falcon UI search ?","body":"<p>When searching for Name:* in the UI, it can take up to 30 seconds. </p><p>Any advice on how to tune this search? </p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-09-23 20:02:13.0","id":145,"title":"Openssl error upon host registration","body":"<p>I'm trying to register hostnames in Ambari but getting the error below. We tried to run yum update openssl but its got the latest version. We tried to run yum - update it didn’t help. I also tried removing the other openssl packages openssl-devel-1.0.1e-42.el6.x86_64, openssl098e-0.9.8e-17.el6.centos.2.x86_64 and restarting the ambari server/agents and same error. Tried running yum -y install opens* as well but didn’t help. Any ideas?</p><pre>ERROR 2015-09-23 09:47:07,402 NetUtil.py:77 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol\nERROR 2015-09-23 09:47:07,402 NetUtil.py:78 - SSLError: Failed to connect. Please check openssl library versions.\nRefer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:17,403 NetUtil.py:59 - Connecting to https://test.org:8440/ca\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:82 - Failed to connect to https://test.org:8440/ca due to [Errno 111] Connection refused\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\n</pre>","tags":["ssl","Ambari","hdp-2.3.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-24 04:06:32.0","id":168,"title":"Problem installing Ranger","body":"<p>We ran into an error during Ranger installation on Mysql. Error 1071 the unique key is longer than 767 bytes. We could extend the innodb prefix to 3072 but this still means that no unique column can be longer than 767 and one of the unique key columns is 1024</p><p>anybody ran into this before?</p><p>Its not in the problem solving guide</p><p>Attaching the screen shot.</p><p>It’s Ambari-2.1.1 and HDP-2.3.0</p><p><img src=\"/storage/attachments/41-exception.png\"></p>","tags":["hdp-2.3.0","ambari-2.1.1"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-24 17:58:18.0","id":192,"title":"How do I install DataFlow or Apache NiFi on HDP?","body":"<p>I'm IoT Java developer, and I want a quick get started tutorial. </p>","tags":["Nifi","dataflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-09-24 15:52:57.0","id":179,"title":"Unable to view hive_db in Atlas [Sandbox]","body":"<p>I'm trying to get a demo up and running with Atlas. </p><p>I am running into issues with just simply trying to view the hive db and tables.</p><p>Can someone please</p><ol>\n<li>provide the resolution / pointers?</li><li>provide a way to clean the Atlas demo and start again?</li><li>share how the engineering team is visualizing the data in the graph database in Atlas?</li></ol><p>Please see below - </p><p>2 - Output graph page errors out</p><p>3 - Exception causing the error is below</p><p>4- I tried importing hive metadata but it errors out </p><p><img src=\"/storage/attachments/44-temp3.jpg\"></p>","tags":["Sandbox","Atlas"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-09-24 19:22:27.0","id":195,"title":"Is there an Ala carte or Sandbox lite catered towards developers?","body":"","tags":["development","Spark","Hbase"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-09-24 21:12:48.0","id":201,"title":"I'm a DevOps like and love CLI. I know Ambari shell abstracts the REST API via calls. I downloaded and compile fails.","body":"","tags":["shell","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-24 23:32:34.0","id":225,"title":"Test Question","body":"<p>Thisi s a test question to check email template change</p>","tags":["Oozie"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-24 23:48:43.0","id":233,"title":"another test qestion disregard - test","body":"<p>testing testin test</p>","tags":["test"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 00:15:39.0","id":236,"title":"I have a test qeustion","body":"<p>test question test question</p>","tags":["test"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-25 00:41:01.0","id":253,"title":"Are there any good places to find Ambari Views?","body":"<p>I need to create some automation views for creating accounts in Ambari. Are there any views that are already created that do this?</p>","tags":["ambari-views","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 01:42:15.0","id":255,"title":"How to resolve OpenSSL error upon host registration in Ambari","body":"<p>I'm trying to register hostnames in Ambari but getting the error below. We tried to run yum update openssl but its got the latest version. We tried to run yum - update it didn’t help. I also tried removing the other openssl packages openssl-devel-1.0.1e-42.el6.x86_64, openssl098e-0.9.8e-17.el6.centos.2.x86_64 and restarting the ambari server/agents and same error. Tried running yum -y install opens* as well but didn’t help. Any ideas?</p><pre>ERROR 2015-09-23 09:47:07,402 NetUtil.py:77 - [Errno 8] _ssl.c:492: EOF occurred in violation of protocol\nERROR 2015-09-23 09:47:07,402 NetUtil.py:78 - SSLError: Failed to connect. Please check openssl library versions.\nRefer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:07,402 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:14,746 main.py:74 - loglevel=logging.INFO\nINFO 2015-09-23 09:47:17,403 NetUtil.py:59 - Connecting to https://test.org:8440/ca\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:82 - Failed to connect to https://test.org:8440/ca due to [Errno 111] Connection refused\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nWARNING 2015-09-23 09:47:17,404 NetUtil.py:105 - Server at https://test.org:8440is not reachable, sleeping for 10 seconds...\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\nERROR 2015-09-23 09:47:19,780 main.py:315 - Fatal exception occurred:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\n</pre>","tags":["ssl","Ambari","ambari-2.1.1"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 00:38:04.0","id":251,"title":"HDP Sandbox on AWS?","body":"<p>Is the Hortonworks Sandbox available on Amazon Web Services as well?</p>","tags":["hdp-2.3.4","Sandbox","aws"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-28 22:29:17.0","id":376,"title":"testin new hipchat mapping","body":"<p>mapping tag security to hipchat room sme-security</p>","tags":["security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-29 01:29:41.0","id":409,"title":"Any recommendation on using G1GC over CMS GC for HBase?","body":"<p>What are advantages and benefits of G1GC? Any benchmarks?</p>","tags":["garbage-collector","Hbase","best-practices"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-30 13:22:38.0","id":611,"title":"When can I expect Spark 1.5 to be available on HDP?","body":"<p>Spark 1.5 was released in early September and I would like to start using it. When can I expect Hortonworks to provide support for it? </p>","tags":["security","Spark"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-05 17:11:57.0","id":904,"title":"Why is ambari showing incorrect number of data nodes available?","body":"<p>Ambari shows 6 of 5 datanodes live in dashboard.  The correct count is 6 nodes, which is reflected in the NameNode UI. I have tried restarting Ambari-server and all the agents and issue will not resolve. </p><p>What other options should I explore to update the correct node count?</p><p><img src=\"/storage/attachments/204-screen-shot-2015-10-05-at-120527-pm.png\" style=\"background-color: initial;\"></p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-06 20:00:43.0","id":1014,"title":"hipchat test","body":"<p>test test test</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-06 20:05:39.0","id":1016,"title":"hipchat test","body":"<p>data flow</p><p>data flow</p><p>data flow</p>","tags":["dataflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-13 23:10:09.0","id":1391,"title":"Is container_e70_144523545423_0003_01_000010 a valid containerId ?","body":"<p>I am not sure why e70 is in the part of containerId, I do see it in a HDP 2.3 environment. Is this expected or a known issue ?</p>","tags":["YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-13 20:07:04.0","id":1383,"title":"IntelliJ and Maven HDP public Repos to index","body":"<p>Trying to add the HDP repo's to InteliJ Maven repo list, so i would get all the versions in my autocomplete not just my local repo's but getting an error when i try to run the update. Please see screenshot below: </p><p><img src=\"/storage/attachments/240-intelij.png\"></p>","tags":["hdp-2.3.4","intellij","maven"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-16 02:49:10.0","id":1558,"title":"Best/Optimized Way to move data from Phoenix to Hive/Orc Table","body":"<p>Can you please provide some recommended way to move data from Phoenix based on certain query to Hive/orc without dumping into text Format</p>","tags":["Hive","Phoenix","Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-19 19:01:48.0","id":1680,"title":"When a service is removed from a kerborized cluster authenticating with Active Directory, is the associated service principal removed from the corresponding Active Directory OU?","body":"<p>When a cluster is kerborized using Active Directory as the KDC, the service principals are created in the AD OU for the Hadoop realm.  If a service, such as Storm, is removed from the cluster will this cause the entry in Active Directory to be removed as well?</p>","tags":["kerberos","keytab","active-directory"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-21 16:22:18.0","id":1813,"title":"Kafka: Can the number of partitions (per topic) be changed after creation?","body":"<ul>\n</ul><p>The Kafka documentation states we can’t reduce the\nnumber of partitions per topic once created. Can we then increase them or is it\na documentation mistake? If we are allowed to add new partitions and use keys\nto get our messages wouldn’t the old messages be unreachable?</p>","tags":["Kafka","partitioning"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-22 07:26:41.0","id":1870,"title":"One dead big job blocks all jobs","body":"<p>On relatively busy cluster, ran a huge job which consumed almost 100% resources, then during shuffle phase, it died with OOM on a NodeManager, after that, all jobs including this job are not progressing. </p><p>To recover from this state, needed to kill this job and also other jobs.</p><p>This can't reproduce at will but occasionally happens.</p><p>Have you come across any similar symptom?\nIs there any smarter way to recover from this state? Killing jobs manually wouldn't be ideal.\nMaybe need to check/modify some yarn config?</p>","tags":["resource-manager","YARN","Tez"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-27 03:24:05.0","id":2176,"title":"How to Modify the Verbosity of Yarn Logs","body":"<p>Hi,</p><p>How do you reduce the level of logging for YARN?</p>","tags":["logs","YARN"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-27 18:23:14.0","id":2210,"title":"LIBSASL version fro HIVE OBDC & RHEL 7","body":"<p>[root@M2MOCHDPK001 tmp]# rpm -ivh hive-odbc-native-2.0.5.1005-1.el6.x86_64.rpm</p><p>error: Failed dependencies:</p><p>libsasl2.so.2()(64bit) is needed by hive-odbc-native-2.0.5.1005-1.x86_64</p><p>Currently the OS has the following library file</p><p>[root@M2MOCHDPK001 tmp]# ls -lrta /usr/lib64/libsasl*</p><p>-rwxr-xr-x. 1 root root 121288 Jan 24  2014 /usr/lib64/libsasl2.so.3.0.0</p><p>lrwxrwxrwx. 1 root root     17 Aug 13 09:09 /usr/lib64/libsasl2.so.3 -&gt; libsasl2.so.3.0.0</p>","tags":["rhel","Hive","odbc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-09 21:22:50.0","id":1248,"title":"Installing ElasticSearch on HDP","body":"<p>I see the following blog/tutorial from a year ago on how to install ES on HDP. Has anything changed or is this blog still accurate? - http://hortonworks.com/blog/configure-elastic-search-hadoop-hdp-2-0/</p><p>Thanks,</p><p>Andrew</p>","tags":["installation","elasticsearch","search"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-09-29 16:05:05.0","id":505,"title":"How to disable IPv6 in Docker containers deployed by Cloudbreak on Azure","body":"<p>When trying to troubleshoot an Ambari issue on an HDP deployment using Cloudbreak on Azure, I noticed that IPv6 was enabled and that Ambari was only listening on IPv6. The container had not been started with the --ipv6 flag and I was under the impression that IPv6 was disabled by default in the images. Is this not the case? Presumably '--ipv6=false' needs to be passed when starting the containers to turn it off?</p>","tags":["azure","Cloudbreak"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-14 19:44:46.0","id":1466,"title":"Oozie SSH Action - StrictHostKeyChecking=no","body":"<pre>I see it in the code but can someone confirm that 'StrictHostKeyChecking=no' is indeed on be default, for the SSH action, in Oozie 4.1?</pre>","tags":["ssh","Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-16 18:50:41.0","id":1605,"title":"Is there an event fired when Backpressure is engaged?","body":"<p>Thinking of a use case when one would pro-actively notify an admin of a potential problem (or just let them now something is happening, but the system is handling it). Is there such an event in NiFi today for backpressure?</p>","tags":["help","Nifi","hdf"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-16 19:02:56.0","id":1609,"title":"Capture the Download event in provenance?","body":"<p>NiFi has a a special role requirement to allow/disallow downloading the content from the provenance view. However, is it possible to record the fact that someone downloaded the contents of an event?</p>","tags":["provenance","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-18 14:47:05.0","id":1641,"title":"When principals are created in Active Directory during Kerberos installation (no local KDC), what password is used for each principal?","body":"<p>A client asks this question \"I see objects in AD.  These objects are AD user objects with the password set to next expire.  Do you know what default password is used on these accounts on the AD side?\"</p>","tags":["active-directory","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-20 15:08:30.0","id":1722,"title":"Does the latest HDP installation depends on the RHEL EPEL repo?","body":"","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-20 16:41:37.0","id":1732,"title":"How to avoid variables in hadoop-env being called more than once?","body":"<p>In some situations, for example when using a profiling tool, the fact that hadoop-env is called more than once is a problem, because it starts multiple instances of the profiler (in the way of HADOOP-9873, HADOOP-9902 & HADOOP-11010).</p><p>Is there a way to avoid the env variables in hadoop-env being called more than once?</p>","tags":["java","hadoop"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-27 09:55:31.0","id":2170,"title":"Controlling size of the kafka.out log file.","body":"<p>Kafka error logs are getting filled bringing the kafka down.Looking for options to purge the old kafka errors logs.</p><p>Logs that are getting fillled are server.log.**. kafka.out.</p>","tags":["logs","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-27 19:05:31.0","id":2228,"title":"java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Invalid status -128","body":"<p>Ambari 2.1.2</p><p>HDP 2.3.2</p><p>Kerberos in place</p><pre><strong>015-10-27 14:45:44,515 ERROR [HiveServer2-Handler-Pool: Thread-47]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.\n</strong><strong>java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Invalid status -128\n</strong><strong>        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)\n</strong><strong>        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory$1.run(HadoopThriftAuthBridge.java:739)\n</strong><strong>        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory$1.run(HadoopThriftAuthBridge.java:736)\n</strong><strong>        at java.security.AccessController.doPrivileged(Native Method)\n</strong><strong>        at javax.security.auth.Subject.doAs(Subject.java:356)\n</strong><strong>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)\n</strong><strong>        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory.getTransport(HadoopThriftAuthBridge.java:736)\n</strong><strong>        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)\n</strong><strong>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n</strong><strong>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n</strong><strong>        at java.lang.Thread.run(Thread.java:745)\n</strong><strong>Caused by: org.apache.thrift.transport.TTransportException: Invalid status -128\n</strong><strong>        at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)\n</strong><strong>        at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:184)\n</strong><strong>        at </strong></pre>","tags":["Hive","hiveserver2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-28 16:39:45.0","id":2302,"title":"Is there a way to change compression level in Gzip in GzipCodec ?","body":"<p>Looking at the code, it looks like GZipCodec uses Deflater.DEFAULT_COMPRESSION. Is there a way to tweak compression levels of Gzip for mapreduce output? </p>","tags":["compression"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-29 08:25:24.0","id":2353,"title":"Is there any risk to delete old HDP directories?","body":"<p>Upgraded a cluster many times and current version is HDP 2.3.2</p><p>Would like to delete the following directories.\nWould it be safe to delete with rm command?</p><p>$ sudo du -hx --max-depth=1 /usr/hdp | grep G </p><p>2.8G /usr/hdp/2.3.2.0-2950 </p><p>\n1.7G /usr/hdp/2.2.0.0-2041 </p><p>2.0G /usr/hdp/2.2.6.0-2800 </p><p>2.0G /usr/hdp/2.2.4.2-2 </p><p>2.6G /usr/hdp/2.3.0.0-2557</p><p>...</p>","tags":["help","disk","delete"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-06 13:21:39.0","id":976,"title":"Is there an updated document on setting up wire-encryption for HDP 2.2 and 2.3?","body":"<p>The latest documentation I can find on docs.hortonworks.com for setting up wire-encryption is this:</p><p>http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.15/bk_Security_Guide/content/security-wire-encryption.html</p><p>Is there anything newer, or is that information still good for HDP 2.2 and 2.3?</p>","tags":["hdp-2.3.0","wire-encryption","documentation"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-03 19:55:40.0","id":5135,"title":"Can I have an oozie coordinator that runs once per hour trigger a particular action only once per day?","body":"<p>For example, I have a coordinator with 2 actions, A and B.  The coordinator runs hourly. Once per day I want to run action A followed by action B.  The other 23 times of the day I just want to run action B.</p><p>Is there an elegant way to orchestrate that with a single oozie coordinator?</p>","tags":["Oozie"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-07 18:38:35.0","id":5338,"title":"Recommended data quality test suite for Hive / Pig / Oozie","body":"<p>Hi</p><p>I am looking for Best practices around data quality Testing for hive / pig/ oozie based ETL. </p><p>Client is looking at tools like Data flux data quality for Hadoop . </p><p>If there are any alternate recommendations , please update this question. </p>","tags":["data-quality"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-16 06:08:14.0","id":6698,"title":"Getting error \"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.2.0/repodata/repomd.xml: [Errno 14] PYCURL ERROR 6 - \"Couldn't resolve host 'public-repo-1.hortonworks.com'\"\"","body":"<p>I am getting error while trying to install hadoop packages.</p><p>Reference :  <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_installing_manually_book/content/install_the_hadoop_packages.html\">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-...</a></p><p>-----------------------------------------------------------------------------------------------------------</p><p>[root@sdp2 ~]# yum install hadoop hadoop-hdfs hadoop-libhdfs hadoop-yarn hadoop-mapreduce hadoop-client openssl\nLoaded plugins: aliases, changelog, downloadonly, kabi, presto, product-id, refresh-packagekit, security, subscription-manager, tmprepo, verify, versionlock\nThis system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\nLoading support for Red Hat kernel ABI\nSetting up Install Process\nRepository HDP-2.3.2.0 is listed more than once in the configuration\nRepository HDP-UTILS-1.1.0.20 is listed more than once in the configuration\nRepository HDP-2.3.2.0 is listed more than once in the configuration\nRepository HDP-UTILS-1.1.0.20 is listed more than once in the configuration\nRepository HDP-2.3.2.0 is listed more than once in the configuration\nRepository HDP-UTILS-1.1.0.20 is listed more than once in the configuration\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.2.0/repodata/repomd.xml: [Errno 14] PYCURL ERROR 6 - \"Couldn't resolve host 'public-repo-1.hortonworks.com'\"\nTrying other mirror.\nError: Cannot retrieve repository metadata (repomd.xml) for repository: HDP. Please verify its path and try again\n</p>","tags":["hdp-2.3.4","hadoop","repository"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-16 15:48:21.0","id":6813,"title":"Hello Wondering if anyone knows the timeframe to support kafka 0.9 in HDP ?","body":"<p>We would like to use kafka 0.9 for some of the  new java functionality along with other products in HDP. Current version HDP 2.3.2 which supports kafka .0.8.2.3 does not have that functionality. So, wondering when would we expect kafka 0.9 in HDP !</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-16 18:17:05.0","id":6855,"title":"Does Blueprint supports add hostgroups post cluster create?","body":"<p>Blueprint supports SLAVE node expansion through \"add host\" API. We could provide blueprint and hostgroup name to add host API and the API takes care of replicating hostgroup components (from corresponding Blueprint) in the new set of hosts. In scenario where if only DATANODE\" needs to be expanded, it is not possible unless the BP has a hostgroup that has only \"DATANODE\" component.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-17 06:28:29.0","id":7002,"title":"How to get file list of \" blocks with corrupt replicas\"","body":"<p>fsck & -list-corruptfileblocks not showing any corrupt blocks in their output while dfsadmin report shows corrupt blocks</p><p>DFSAdmin report -</p><p>Under replicated blocks: 35016\nBlocks with corrupt replicas: 113\nMissing blocks: 0</p><p>fsck</p><p>Corrupt blocks:  0\n Number of data-nodes:  3\n Number of racks:  1</p><p>Also the number of under replicated blocks are increasing</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-18 09:52:47.0","id":7215,"title":"Don't know where Sqoop creates Hive table","body":"<p>Ran following sqoop (version 1.4.6.2.3.2.0-2950) command.</p><p>sqoop import --verbose --connect 'jdbc:sqlserver://10.97.80.169:53441;database=hsfadev6007_archive;username=xxxx;password=xxxx' --table user_log --hive-overwrite --create-hive-table --hive-table test.user_log_sqoop --warehouse-dir /apps/hive/warehouse/test.db</p><p>However, it does NOT create hive table user_log_sqoop in test or DEFAULT Database.</p><p>Instead, I see following HDFS files and do not know which hive table in which database they are associated with.</p><p>dfs -ls /apps/hive/warehouse/test.db/user_log; </p><p>-rw-r--r--   3 vagarwal-a hdfs          0 2015-12-18 04:43 /apps/hive/warehouse/test.db/user_log/_SUCCESS </p><p>-rw-r--r--   3 vagarwal-a hdfs  194706017 2015-12-18 04:43 /apps/hive/warehouse/test.db/user_log/part-m-00000 </p><p>-rw-r--r--   3 vagarwal-a hdfs  198890196 2015-12-18 04:43 /apps/hive/warehouse/test.db/user_log/part-m-00001 </p><p>-rw-r--r--   3 vagarwal-a hdfs  199013375 2015-12-18 04:43 /apps/hive/warehouse/test.db/user_log/part-m-00002 </p><p>-rw-r--r--   3 vagarwal-a hdfs  199013147 2015-12-18 04:43 /apps/hive/warehouse/test.db/user_log/part-m-00003</p><p>Any ideas? </p>","tags":["Sqoop","Hive"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-22 01:01:47.0","id":7620,"title":"Force delete or move services from terminated node?","body":"<p>I had a root directory issue on one of my ambari nodes during setup and now I am trying to re-move it from the cluster. When I try to remove the node in the web ui I get a message saying I need to remove the following services: Atlas Metadata Server, DRPC Server, NameNode, Spark History Server, Storm UI Server. My steps were:</p><p>1) Decommission node</p><p>2) Move the NameNode with the UI</p><p>3) Try restart service but received a similar error (without the namenode warning)</p><p>4) I tried deleting the node and the services with the API, but that yeilded errors also</p><p>Many of these master services do not have a move feature in the UI and I have not seen move operations in the api either. For these remaining services, how can I either force Ambari to delete this terminated node, and then manually install services on new nodes, or move the services to a new node?</p>","tags":["hdp-2.3.2","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-05 10:08:01.0","id":8731,"title":"how to use path globs to copy files from local to hdfs???","body":"<p>I have some number of files having format like </p><p>1)filename+date.filefomat</p><p> 2)filename.fileformat</p><p>now i need to copy only files which have some number before .(dot).     </p>","tags":["HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-29 21:26:25.0","id":2443,"title":"How to add a line of text to a flowfile","body":"<p>Is it possible to add a line of text to a flowfile?  Or perhaps convert an attribute to a line in a flowfile?</p>","tags":["hdf","Nifi","dataflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-30 22:12:13.0","id":2549,"title":"Customer recently implemented ldap for hive authentication. When they connect to hive with beeline, or via ODBC, user is not prompted for the password. and the user can connect to any database, irrespective of ldap group","body":"","tags":["security"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-31 01:08:53.0","id":2555,"title":"What are the best practices to secure spark on HDP 2.3","body":"","tags":["Spark","security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-31 02:28:54.0","id":2556,"title":"How can I configure different machines separately in Ambari?","body":"","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-16 20:18:14.0","id":4064,"title":"test question test question","body":"<p>test qyestuib test qyestuib</p><p>test qyestuib</p>","tags":["test"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-17 20:00:18.0","id":4149,"title":"Zookeeper connection requests to, and from, a Zookeeper node are failing with end of stream exception.","body":"<p>A status command from the node will work with no issues and no errors in the log  e.g.</p><pre>echo status | nc zkhostname 2181  </pre>","tags":["zookeeper"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-17 20:30:24.0","id":4163,"title":"QlikView connect to Kerberos-enabled HiveServer2","body":"<p>Abbott has a third party tool, QlikView, which needs to run Hive queries.</p><p>Suppose our cluster is kerberized via existing AD.</p><p>Once HDP cluster is kerberized, QlikView needs to be kerberized as well----it can get through Knox, but let's take Knox out of the picture for this discussion.</p><p>My questions are: </p><p>1. since both QlikView and HDP cluster are kerberized via the same AD,\nQlikView ticket should be recognized by HDP cluster. But, is there any\nextra setting needed? do we need to put QlikView service Account to\nOU=HadoopService?</p><p>2. QlikView is a server, user can log in to QlikView through AD. I\nbelieve Hive ACLs applies to each individual user no matter they are via\nQlikView or not --- this question is more like to QlikView guy, not us.</p>","tags":["security"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-17 17:36:15.0","id":4135,"title":"What to backup? and how? (only metadata, not data)","body":"<p>What components and folders to backup (only metadata, not data)? What would be the commands?</p><p>- Namenode\n- Ambari database\n- Hive Metastore database\n- Oozie database\n- Hue database\n- Ranger database</p>","tags":["backup"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-02 02:17:21.0","id":5022,"title":"How can I setting the rack ID in Ambari API？","body":"<p>I have to build a large cluster, for which I’ll need to set rack ID many times.\nI know two methods in Ambari Web for setting the Rack ID. \n<a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.0/bk_Ambari_Users_Guide/content/ch03s11.html\">http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.0/bk_Ambari_Users_Guide/content/ch03s11.html</a> </p><p>So I want to know if I can write a script which sets the rack ID in Ambari API</p><p>Ambari: 2.1.2.1\nHadoop: HDP2.3.0</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-14 01:55:04.0","id":6272,"title":"Spark-csv support in HDP 2.3.2","body":"<p>\nIs spark-csv packages is not supported by HDP2.3.2?\nI am getting below error when I try to run spark-shell that spark-csv package is not supported.\n</p>\n<pre>\n[hdfs@sandbox root]$ spark-shell   --packages com.databricks:spark-csv_2.10:1.1.0  --master yarn-client --driver-memory 512m --executor-memory 512m\nIvy Default Cache set to: /home/hdfs/.ivy2/cache\nThe jars for the packages stored in: /home/hdfs/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.3.2.0-2950/spark/lib/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n        confs: [default]\n:: resolution report :: resolve 332ms :: artifacts dl 0ms\n        :: modules in use:\n        ---------------------------------------------------------------------\n        |                  |            modules            ||   artifacts   |\n        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n        ---------------------------------------------------------------------\n        |      default     |   1   |   0   |   0   |   0   ||   0   |   0   |\n        ---------------------------------------------------------------------\n:: problems summary ::\n:::: WARNINGS\n                module not found: com.databricks#spark-csv_2.10;1.1.0\n        ==== local-m2-cache: tried\n          file:/home/hdfs/.m2/repository/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.pom\n          -- artifact com.databricks#spark-csv_2.10;1.1.0!spark-csv_2.10.jar:\n          file:/home/hdfs/.m2/repository/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.jar\n        ==== local-ivy-cache: tried\n          /home/hdfs/.ivy2/local/com.databricks/spark-csv_2.10/1.1.0/ivys/ivy.xml\n        ==== central: tried\n          https://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.pom\n          -- artifact com.databricks#spark-csv_2.10;1.1.0!spark-csv_2.10.jar:\n          https://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.jar\n        ==== spark-packages: tried\n          http://dl.bintray.com/spark-packages/maven/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.pom\n          -- artifact com.databricks#spark-csv_2.10;1.1.0!spark-csv_2.10.jar:\n          http://dl.bintray.com/spark-packages/maven/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.jar\n                ::::::::::::::::::::::::::::::::::::::::::::::\n                ::          UNRESOLVED DEPENDENCIES         ::\n                ::::::::::::::::::::::::::::::::::::::::::::::\n                :: com.databricks#spark-csv_2.10;1.1.0: not found\n                ::::::::::::::::::::::::::::::::::::::::::::::\n:::: ERRORS\n        Server access error at url https://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.pom (java.net.ConnectException: Connection refused)\n        Server access error at url https://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.1.0/spark-csv_2.10-1.1.0.jar (java.net.ConnectException: Connection refused)\n:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\nException in thread \"main\" java.lang.RuntimeException: [unresolved dependency: com.databricks#spark-csv_2.10;1.1.0: not found]\n        at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:995)\n        at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:263)\n        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:145)\n        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)\n        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n15/12/14 01:49:39 INFO Utils: Shutdown hook called\n[hdfs@sandbox root]$\n</pre>\n\n<p>\nWould really appreciate your help.\n</p>\n","tags":["hdp-2.3.2","Spark","spark-csv"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-14 23:25:04.0","id":6446,"title":"When kerbros is enabled not able to connect to remote Filesystem with out keytab, always getting error saying \"failure to login using ticket cache file\"","body":"","tags":["kerberos","hdfs-permissions","java"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-10 18:29:27.0","id":43,"title":"Hipchat test","body":"","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-24 07:48:50.0","id":19125,"title":"/tmp partition of operating system | noexec flag","body":"<p>Hi Team,</p><p>/tmp partition of operating system having \"noexec\" flag. Does it impact on hadoop cluster? </p>","tags":["os"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-02-24 16:20:31.0","id":19289,"title":"Ranger Usersync didn't fetching users and groups from LDAPS/AD","body":"<p>HDP 2.3, Ambari 2.2</p><p>Please see the Ranger usersync log below. <strong><u>The users and groups are not fetching from LDAPS/AD in Ranger usersync</u></strong>.</p><pre>INFO UserGroupSync [UnixUserSyncThread] - Begin: update user/group from source==&gt;sink \nINFO LdapUserGroupBuilder [UnixUserSyncThread] - LDAPUserGroupBuilder updateSink started \nINFO LdapUserGroupBuilder [UnixUserSyncThread] - LdapUserGroupBuilder initialization started \nINFO LdapUserGroupBuilder [UnixUserSyncThread] - LdapUserGroupBuilder initialization completed with --  ldapUrl: ldaps://xxx-pro-ods-ed.infra.xxxcorp.net:636,  ldapBindDn: cn=ranger_ldap,ou=Applications,o=zz.com,  ldapBindPassword: ***** ,  ldapAuthenticationMechanism: simple,  searchBase: o=zz.com,  userSearchBase: o=zz.com,  userSearchScope: 2,  userObjectClass: person,  userSearchFilter: (|(memberof=uid={0},ou=Peoples,o=zz.com)(memberof=cn=ranger_ldap,ou=Applications,o=zz.com)),  extendedUserSearchFilter: (&(objectclass=person)(|(memberof=uid={0},ou=Peoples,o=zz.com)(memberof=cn=ranger_ldap,ou=Applications,o=zz.com))),  userNameAttribute: uid,cn,  userSearchAttributes: [uid,cn, memberof, ismemberof],  userGroupNameAttributeSet: [memberof, ismemberof],  pagedResultsEnabled: true,  pagedResultsSize: 500,  groupSearchEnabled: true,  groupSearchBase: ou=Groups,o=zz.com,  groupSearchScope: 2,  groupObjectClass: groupofnames,  groupSearchFilter: (|(memberof=cn=TEAM_EDL_Dev,ou=Groups,o=zz.com)(memberof=cn=edl_*,ou=Groups,o=zz.com)),  extendedGroupSearchFilter: (&(objectclass=groupofnames)(|(memberof=cn=TEAM_EDL_Dev,ou=Groups,o=zz.com)(memberof=cn=edl_*,ou=Groups,o=zz.com))(member={0})),  extendedAllGroupsSearchFilter: (&(objectclass=groupofnames)(|(memberof=cn=TEAM_EDL_Dev,ou=Groups,o=zz.com)(memberof=cn=edl_*,ou=Groups,o=zz.com))),  groupMemberAttributeName: member,  groupNameAttribute: cn,  groupUserMapSyncEnabled: true,  ldapReferral: ignore \nINFO LdapUserGroupBuilder [UnixUserSyncThread] - LDAPUserGroupBuilder.updateSink() completed with user count: 0 \nINFO LdapUserGroupBuilder [UnixUserSyncThread] - groupSearch is enabled, would search for groups and compute memberships \nINFO LdapUserGroupBuilder [UnixUserSyncThread] - LdapUserGroupBuilder initialization started \nINFO LdapUserGroupBuilder [UnixUserSyncThread] - LdapUserGroupBuilder initialization completed with --  ldapUrl: ldaps://xxx-pro-ods-ed.infra.xxxcorp.net:636,  ldapBindDn: cn=ranger_ldap,ou=Applications,o=zz.com,  ldapBindPassword: ***** ,  ldapAuthenticationMechanism: simple,  searchBase: o=zz.com,  userSearchBase: o=zz.com,  userSearchScope: 2,  userObjectClass: person,  userSearchFilter: (|(memberof=uid={0},ou=Peoples,o=zz.com)(memberof=cn=ranger_ldap,ou=Applications,o=zz.com)),  extendedUserSearchFilter: (&(objectclass=person)(|(memberof=uid={0},ou=Peoples,o=zz.com)(memberof=cn=ranger_ldap,ou=Applications,o=zz.com))),  userNameAttribute: uid,cn,  userSearchAttributes: [uid,cn, memberof, ismemberof],  userGroupNameAttributeSet: [memberof, ismemberof],  pagedResultsEnabled: true,  pagedResultsSize: 500,  groupSearchEnabled: true,  groupSearchBase: ou=Groups,o=zz.com,  groupSearchScope: 2,  groupObjectClass: groupofnames,  groupSearchFilter: (|(memberof=cn=TEAM_EDL_Dev,ou=Groups,o=zz.com)(memberof=cn=edl_*,ou=Groups,o=zz.com)),  extendedGroupSearchFilter: (&(objectclass=groupofnames)(|(memberof=cn=TEAM_EDL_Dev,ou=Groups,o=zz.com)(memberof=cn=edl_*,ou=Groups,o=zz.com))(member={0})),  extendedAllGroupsSearchFilter: (&(objectclass=groupofnames)(|(memberof=cn=TEAM_EDL_Dev,ou=Groups,o=zz.com)(memberof=cn=edl_*,ou=Groups,o=zz.com))),  groupMemberAttributeName: member,  groupNameAttribute: cn,  groupUserMapSyncEnabled: true,  ldapReferral: ignore \nINFO UserGroupSync [UnixUserSyncThread] - End: update user/group from source==&gt;sink</pre>","tags":["Ranger","ldap"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-25 19:14:47.0","id":19587,"title":"How to limit the size of ranger log?","body":"","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-27 22:25:55.0","id":19926,"title":"Ranger configuration for Hive Metastore","body":"<p><a rel=\"user\" href=\"/users/140/nsabharwal.html\" nodeid=\"140\">@Neeraj Sabharwal</a></p><p>Trying to configure Hive service to restrict access to Database and Tables. Not sure what parameter is required for a successful connection. The username and password I am using is for Ranger admin. Is that correct. I also tried Ambari Admin. but the test connection is failing. </p><p><img src=\"/storage/attachments/2445-hive-database.jpg\"></p>","tags":["Ranger"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-28 06:27:18.0","id":19976,"title":"query issue....it is in hang state","body":"<pre>&lt;code&gt; insert overwrite table yearly_aggregates \n  select a.symbol, year(a.trade_date), max(a.high), min(a.low),  avg(a.close), sum(b.dividend)from price_data a \n  left outer join dividends_data b \n  on (a.symbol = b.symbol and a.trade_date = b.trade_date)groupby a.symbol, year(a.trade_date);</pre>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-27 22:56:49.0","id":19958,"title":"Is there any info or examples in HDP document regarding httpd.conf to setup http server on the master server?","body":"<p>we need info to modify this file in order to set http service on local master server when install HDP/ambari without internet access. Please help. </p>","tags":["installation","httpd.conf"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-29 20:04:00.0","id":20239,"title":"Unable to log in to Ambari UI - NPE\\500 Server Error","body":"<p>I have installed ambari-server 2.2.0 on CentOS 7 using all default responses to ambari-server setup.\nI have not added new users, nor have I changed the password of 'admin' from the default.\nI was able to login to the UI initially, and use the REST API to submit blueprints and cluster. The cluster was created correctly and all installed hadoop services are running fine. The stack is HDP 2.3.\nHowever, after a few days, I am no longer able to log in to the Ambari UI. When I land on the login page and try to login as 'admin', I see the following message: </p><p>    Unable to connect to Ambari Server. Confirm Ambari Server is running and you can reach Ambari Server from this machine. </p><p>When I check /var/log/ambari-server/ambari-server.log I see the following stack trace: </p><pre>29 Feb 2016 11:18:17,234  WARN [qtp-ambari-client-23] ServletHandler:563 - /api/v1/users/admin\njava.lang.NullPointerException\n        at org.apache.ambari.server.controller.internal.ActiveWidgetLayoutResourceProvider.getResources(ActiveWidgetLayoutResourceProvider.java:153)\n        at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:945)\n        at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:132)\n        at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:508)\n        at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:463)\n        at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:436)\n        at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:216)\n        at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:68)\n        at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)\n        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:106)\n        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:75)\n        at org.apache.ambari.server.api.services.UserService.getUser(UserService.java:68)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:196)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:201)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n        at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:216)\n        at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:205)\n        at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        at org.eclipse.jetty.server.Server.handle(Server.java:370)\n        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n        at java.lang.Thread.run(Thread.java:745) </pre><p>I really need to know how to recover from this. The hadoop services are all running fine for now, but I need the Ambari UI for management\\alerts, etc. I would like to not have to recreate cluster as it is already in use.\n**EDIT**\nI should add that I am still able to make REST API requests directly using an app like POSTMAN, or curl. for instance: /api/v1/clusters/{name}  returns the correct information when using admin\\admin as the auth.</p>","tags":["ambari-server"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-01 15:09:55.0","id":20485,"title":"HDFS space allocation question","body":"<p>I added 200 GB to one of the data node, I could see the new space on Ambari Dashboard but HDFS size remains the same. How could that be available to HDFS ?</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-01 17:46:31.0","id":20549,"title":"STRSPLIT pig","body":"<p>HI:</p><p>its posible to split in pig www.amazon.es?? iam doing this but doest work:</p><pre>orders4 = FOREACH orders3 GENERATE $0 as freq, STRSPLIT($1,'.') as word;\n</pre><p>its just print thr $0. any suggestions??</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-01 19:39:08.0","id":20591,"title":"Can i get the hotonworks sandbox  1.2 download and install file ? not the latest 2.4","body":"<p>System has 4gb RAM and 600GB memory on it</p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-01 15:52:03.0","id":20499,"title":"Capacity Scheduler Job Hung","body":"<p>At my client location they have turned on capacity scheduler , oozie job was initiated, first part it sqoops the data then a pig job is run. Sqoop job was successful, then pig job is initiated but without 0% progress, any pointers ?</p>","tags":["scheduler"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-02 11:30:53.0","id":20750,"title":"HDPCA - Preparation","body":"<p>Wondering if there is a forum for folks who are taking the HDPCA exam. I am interested in it and wondering if there are any in the same state. This would be to colloborate and share ideas for the preparation.</p>","tags":["hadoop-ecosystem"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-02 16:13:06.0","id":20791,"title":"sentiment analysis with HDP","body":"<p>Hi all </p><p>I am a master student I want work my thesis in sentiment analysis with hadoop in arabic language, so my question is here support this language ? </p><p>Thanks </p>","tags":["api"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-03 14:25:50.0","id":21013,"title":"Sqoop - Hive-import using --columns option giving error","body":"<p>If I try sqoop query with --columns option, then hive-import is giving error</p><pre>sqoop import  --options-file a.config  --table abc --columns \"colA,colB, colC\"  --fields-terminated-by '|'  --hive-overwrite  --hive-import  --hive-table abc</pre><p>After successfully copying data to HDFS, I get an error </p><pre>util.SqlTypeMap: It seems like you are looking up a column that does not exist in the table. Please ensure that you've specified correct column names in Sqoop options.\nImported Failed: column not found: colA</pre><p>If I try same Sqoop statement without --columns switch, it is working correctly. Is this how Sqoop is supposed to work ? </p><pre>sqoop import --options-file a.config --table abc --fields-terminated-by '|' --hive-overwrite --hive-import --hive-table abc</pre>","tags":["Hive","Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-03 17:31:34.0","id":21059,"title":"How can i recommission Node managers and Data nodes through REST API?","body":"<pre>curl -u admin:admin -i -H 'X-Requested-By: ambari' -X POST -d '{\n\"RequestInfo\":{\n\"context\":\"Recommission DataNodes\",\n\"command\":\"RECOMMISSION\",\n\"parameters\":{\n\"slave_type\":\"DATANODE\",\n\"included_hosts\":\"c6401.ambari.apache.org,c6402.ambari.apache.org,c6403.ambari.apache.org\"\n},\n\"operation_level\":{\n\"level\":\"HOST_COMPONENT\",\n\"cluster_name\":\"c1\"\n}\n},\n\"Requests/resource_filters\":[\n{\n\"service_name\":\"HDFS\",\n\"component_name\":\"NAMENODE\"\n}\n]\n}' http://localhost:8080/api/v1/clusters/c1/requests</pre><p><code>\n</code></p><pre>curl -u admin:admin -i -H 'X-Requested-By: ambari' -X POST -d '{\n\"RequestInfo\":{\n\"context\":\"Recommission NodeManagers\",\n\"command\":\"RECOMMISSION\",\n\"parameters\":{\n\"slave_type\":\"NODEMANAGER\",\n\"included_hosts\":\"c6401.ambari.apache.org,c6402.ambari.apache.org,c6403.ambari.apache.org\"\n},\n\"operation_level\":{\n\"level\":\"HOST_COMPONENT\",\n\"cluster_name\":\"c1\"\n}\n},\n\"Requests/resource_filters\":[\n{\n\"service_name\":\"YARN\",\n\"component_name\":\"RESOURCEMANAGER\"\n}\n]\n}' http://localhost:8080/api/v1/clusters/c1/requests</pre><p>@Artem Ervits .Can i use like this to recommission the data node and nodemanagers? Whcih one has to be excuted first? Datanodes ?</p><p><code>\n</code></p><p><code>\n</code></p>","tags":["api","Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-03 21:08:43.0","id":21109,"title":"Is there a \"Data Lake BluePrint\" ?","body":"<p>Hi Guys, this is my first post here.</p><p>I'm talking with some customer, everybody is talking about Data Lakes.</p><p>But, is there any kind of blueprint ? ou designed architecture for this ?</p><p>Like in DataWarehouses, we have Kimball and Inmon</p><p>Regards</p>","tags":["design","data-model"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-03 23:37:38.0","id":21116,"title":"Best Practices for Solr","body":"<p>What would be best practices in terms solr placement, replication factor, coping shards using curl to place indexes.</p>","tags":["SOLR"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-03 12:11:45.0","id":20984,"title":"How to test the local groups created in Sentry policy file","body":"<p>For the sake of creating POC, i have create local groups in the policy file sentry-provider.ini</p><pre>[databases]\n# Defines the location of the per DB policy file for the customers DB/schema \ndw_arcticblue_staging = hdfs://quickstart.cloudera:8020/home/cloudera/Desktop/dw_arcticblue_staging_policy.ini \n\n[groups]\n# Assigns each Hadoop group to its set of roles \ngroup1 = analyst_role\ngroup2 = admin_role \ngroup3 = analyst_role[users]\nuser1 = group1, group2, group3\nuser2 = group2, group3\n\n[roles]\n# The uris below define a define a landing skid which\n# the user can use to import or export data from the system.\n# Since the server runs as the user \"hive\" files in that directory\n# must either have the group hive and read/write set or\n# be world read/write.\nanalyst_role = server=server1-&gt;db=dw_arcticblue_staging -&gt;table=*-&gt;action=select \nadmin_role = server=server1-&gt;db=dw_arcticblue_staging -&gt;table=*-&gt;action=select, \\ \nserver=server1-&gt;db=dw_arcticblue_staging -&gt;table=*-&gt;action=Insert\n\n# Implies everything on server1 -&gt; customers. Privileges for\n# customers can be defined in the global policy file even though \n# customers has its only policy file. Note that the Privileges from\n# both the global policy file and the per-DB policy file\n# are merged. There is no overriding.\narcticeblue_admin_role = server=server1-&gt;db=dw_arcticblue_staging \n\n# Implies everything on server1.\nadmin_role = server=server1\n\nDW_ArcticBlue_staging_policy.ini\n\n[groups]\ngroup1 = dw_all_access\n\n[roles]\ndw_all_acess = server = NirvanaServer -&gt; db = dw_arcticblue_staging -&gt; table = * -&gt; action = Insert,\\ \nserver = NirvanaServer -&gt; db = dw_arcticblue_staging -&gt; table = * -&gt; action = Select</pre><p>I have updated all configurations in hive and yarn as recommended. Few things which i dont understand</p><ol><li>How to test the user1,user2 that whether they are authorized or not ? I want to test from hive / impala cli .User1, User2 etc created in the policy file are not real users to login with. </li><li>We need to create a HDFS sentry-provider.ini file. How we could create ini file in linux? i did not find any relevant document</li></ol><p>Any help how to test the authorization using sentry on hive??? </p>","tags":["Hive","cloudera"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-03 01:11:49.0","id":20873,"title":"Hiverserver2 - Error validating the login","body":"<p>In Hiveserver2.log, I am getting this error. I don't know why? Using beeline, I am able to connect to Hiveserver2 with any user account on UNIX box using PAM authentication.</p><pre>2016-03-02 23:59:02,691 ERROR [HiveServer2-Handler-Pool: Thread-46]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.\njava.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Error validating the login\n        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.thrift.transport.TTransportException: Error validating the login\n        at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)\n        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:316)\n        at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)\n        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)\n        ... 4 more</pre>","tags":["hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-03-06 03:11:17.0","id":21442,"title":"after tar -zxvf ambari-2.1.2.1-centos6.tar.gz, I dont have ambari.repo file, why?","body":"<p>I am trying to install HDP ambari without internet access, according to </p><p>https://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.1/bk_Installing_HDP_AMB/content/_ambari_repositories.html</p><p>I downloaded the tarball file to /var/www/http from following URL</p><ul>\n<li>http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.1.2.1/ambari-2.1.2.1-centos6.tar.gz</li></ul><p>tar -zxvf ambari-2.1.2.1-centos6.tar.gz, -- this step create some subdirs and files, but I dont see the ambari.repo file on server at all. </p><p>according to HDP doc, after untar, ambari.repo needs to move to /etc/yum/repo.d directory. so I stuck here.</p><p>Please help.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-05 15:58:26.0","id":21395,"title":"unable to Access Ambari sandbox 2.4","body":"<p>I am trying to access Hortonworks sandbox hdp2.4 and i am using userid \"admin\" and password \"admin\" . I would appreciate if anyone can help....thanks</p>","tags":["ambari-alerts"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-07 21:23:39.0","id":21819,"title":"Spark UI thinks job is incomplete","body":"<p>For the past month or so, all Spark jobs are either not appearing in the Spark History UI or showing as incomplete.  YARN is correctly reporting all jobs, but Spark claims there are more steps yet to be run.</p><p>A little background: At one point the logs started filling with errors from the Spark history service about a non-existent file.  I ended up stopping the Spark history server and deleting everything in the directory it was yelling about, then restarting.  I suspect I damaged something in the process and could use some advice on reinitializing the service.</p>","tags":["spark-history-server"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-08 02:45:07.0","id":21840,"title":"spark-sql on hdp2.3.4 ( hive on spark) authentication with LDAP","body":"<p>Hi , </p><p>We want spark to go through authentication. </p><p>1. While launching spark-sql , is there an option that authentication can be done with LDAP</p><p>2. While using spark via spark thrift server jdbc connection, is there a option to enable authentication just like beeline.</p>","tags":["spark-sql","authentication"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-03-09 14:44:47.0","id":22131,"title":"Where {{hadoop_java_io_tmpdir}} can be configured?","body":"<p>In mapred-env template the jvm property java.io.tmpdir is set to {{hadoop_java_io_tmpdir}}, which resolves to /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir. Can we change that? If yes, where?</p><p>Thanks in advance</p><p>export HADOOP_OPTS=\"-Djava.io.tmpdir={{hadoop_java_io_tmpdir}} $HADOOP_OPTS\"</p>","tags":["configuration","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-09 15:31:27.0","id":22140,"title":"Question about HBase and region Servers","body":"<ol><li>What's the purpose of Region Server</li><li>Where is should be located ? Every datanode ?</li><li>What's the purpose of HBase Master</li><li>HBase is NoSql database. What does it store ?</li></ol>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-12 20:42:57.0","id":22647,"title":"Download Link Sandbox HDP 2.3.2","body":"<p>Is it possible to download the sandbox hdp 2.3.2? I only find the link with the actual version (HDP 2.4).</p>","tags":["Sandbox","hdp-2.3.2"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-14 19:07:37.0","id":22923,"title":"What steps need to be taken care when all of sudden services like hive and Webhdfs are running slower than earlier after adding the services(Kafka/storm/Ranger)? what place you need to start with investigation?","body":"","tags":["Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-15 00:09:56.0","id":22982,"title":"Is there a list of available metrics in Ambari Metrics ?","body":"<p>Is there a list of available metrics in Ambari Metrics ?</p>","tags":["Ambari","ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-15 07:27:06.0","id":23029,"title":"hive query issue with orc file format.","body":"<p>Hi Team,</p><p>I am having a hive table with orc files. When I am querying on this table, I am getting below error but when I do query on same table which is having AVRO file are giving proper result.</p><p>jdbc:hive2://test-a:8443/default&gt; select * from clickstream.clickstream_mobile limit 100; </p><p>Error: <a href=\"http://java.io/\">java.io</a>.IOException: java.lang.IndexOutOfBoundsException: toIndex = 993 (state=,code=0) </p><p>Can you suggest possible solution for this.</p>","tags":["query"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-15 16:17:05.0","id":23107,"title":"Lost HeartBeats Ambari","body":"<p>Hello,</p><p>I don't really know why but I lost all heartbeats on the main node of my cluster.</p><p><img src=\"/storage/attachments/2813-heartbeats.png\"></p><p>Do you know how I can solve this problem ? I already try to reboot manually the node.</p>","tags":["heartbeat","Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-15 17:54:06.0","id":23114,"title":"Unable to complete Hands-on Tour of Apache Spark in 5 Minutes","body":"<p>%sh </p><p>wget http://en.wikipedia.org/wiki/Hortonworks </p><p>Process exited with an error: 4 (Exit value: 4)</p>","tags":["zeppelin"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-16 10:43:31.0","id":23260,"title":"how to run hbase MapReduce jar using WinScp on hortonwroks sandbox","body":"","tags":["MapReduce","hortonwork"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-03-16 14:25:38.0","id":23308,"title":"HDP Migration Lab - first instruction 'ssh node1' fails","body":"<p>I've begun the HDP Migration Lab, and the very first instruction (after logging into the VM as root, is to ssh into node1) fails and returns this error message: ssh:connect to host node1 port 22: No route to host. The documented instructions (in Installing HDP) seem to be missing one or more critical preceeding instructions.</p><p>In the /etc/hosts file, node1 has the IP address 172.17.0.5. After installing nmap, and running 'nmap -sS -F 172.17.0.5' the remitted message states that 'Host seems down'. And 'nmap -Pn 172.17.0.5' remits the message that '0 hosts up'. These commands appear to indicate that node1 (docker container) is not running.</p><p>In the root home directory, there is an install_course.sh script. Thinking that running this script is the missing initial instruction, I ran that script, but it failed with the following error message: Usage: install_course.sh &lt;course_id&gt;. What value must be assigned to course_id in order for that script to execute successfully?</p>","tags":["Sandbox","learning"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-03-16 15:18:05.0","id":23319,"title":"connection failed for  Ambari  setup automation with exsiting mysql database","body":"<p>env:Ambari 1.6.1,hdp2.1,</p><p> Ambari  setup automation with exsiting mysql database,and when i press 'test connection' button with the following error message:</p><p>DB connection check started.\nThere was an unknown error while checking database connectivity: Configuration parameter 'jdk_name' was not found in configurations dictionary!</p><p>i have tried the following command,but it didn't work:</p><p>ambari-server setup -s -v -j /usr/java/jdk1.7.0_51 --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-29 08:26:19.0","id":12354,"title":"Scheduling a Python script in OOZIE","body":"<p>Hi,</p><p>I have created a Python script. The script pulls RSS feed and writes the output to a text file. I would like to execute the Python job once a day. Can this be done using OOZIE? Please feel free to suggest a better solution.</p><p>Thanks for your help in advance!</p>","tags":["yarn-scheduler","scheduling","Oozie"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-29 04:08:47.0","id":12336,"title":"ambari server unable to connect to collector for hbase metrics - Read SocketTimeoutException (both on same host)","body":"<p>When i login to the ui, metrics summary for hbase is not shown. for the rest like hdfs/storm the metrics show up.</p><p>collector and ambari server are on the same host.</p><p>The exception i get in server log is:</p><p>--------------------------------</p><p>DEBUG [qtp-ambari-client-26 - /api/v1/clusters/cluster_name/services/HBASE/components/HBASE_REGIONSERVER?_=1454039202858] MetricsRequestHelper:116 - Error getting timeline metrics : Read timed out</p><p>java.net.SocketTimeoutException: Read timed out</p><p>        at java.net.SocketInputStream.socketRead0(Native Method)</p><p>--------------------------------</p><p>any help would be appreciated.</p>","tags":["ambari-metrics"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-29 13:18:18.0","id":12401,"title":"Hive Alter column type","body":"<p>hello,</p><p>I am using hive 1.2.1.</p><p>I executed the following command to change the type of a column from timestamp to date</p><p>ALTER TABLE table_name CHANGE column_name column_name date CASCADE;</p><p>the command was executed successfully.</p><p>when i want to retrieve the data from the table using: select * from table_name</p><p>I got the following error:</p><p>Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.TimestampWritable cannot be cast to org.apache.hadoop.hive.serde2.io.DateWritable</p><p>thanks for help.</p>","tags":["Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-29 10:13:28.0","id":12368,"title":"storage strategy of OCR / Parquet file","body":"<p>let's assume that my HDFS block size is equal to 256Mb and that i need to store 20Gb of data on OCR/Parquet file(s), is it better to store all the data on one OCR/Parquet File, or is it better to store it on many ORC/Parquet files of 256Mb (HDFS Block Size) ?</p>","tags":["hadoop","orc","parquet"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-01-29 16:32:38.0","id":12470,"title":"Cannot get Cloudbreak to work on Google Cloud: Cloubreak deployer \"Cannot connect to Swarm manager\"","body":"<p>I'm trying to deploy an HDP cluster using Cloudbreak 1.1.0 on Google cloud.</p><p>VM are well created, but Hadoop is never started, I get stuck on the following message which is repeated over and over upon failure:</p><p>cloudbreak_1  | 2016-01-29 15:51:08,574 [reactorDispatcher-42] areAllNodesAvailable:264 INFO  c.s.c.o.s.SwarmContainerOrchestrator - [owner:29041148-757f-4bc4-a973-6938f97eebb9] [type:STACK] [id:1] [name:cloudbreak-hadoop-0119-03] Checking if Swarm manager is available and if the agents are registered. </p><p>cloudbreak_1  | 2016-01-29 15:51:08,574 [reactorDispatcher-42] getAvailableNodes:278 INFO  c.s.c.o.s.SwarmContainerOrchestrator - [owner:29041148-757f-4bc4-a973-6938f97eebb9] [type:STACK] [id:1] [name:cloudbreak-hadoop-0119-03] Checking if Swarm manager is available and if the agents are registered. </p><p>cloudbreak_1  | 2016-01-29 15:51:08,631 [reactorDispatcher-42] getAvailableNodes:303 WARN  c.s.c.o.s.SwarmContainerOrchestrator - [owner:29041148-757f-4bc4-a973-6938f97eebb9] [type:STACK] [id:1] [name:cloudbreak-hadoop-0119-03] Cannot connect to Swarm manager, maybe it hasn't started yet: 502 Bad Gateway</p><p>Does anyone has any idea ?</p><p>Swarm since to be started on my gateway:</p><p>$docker ps </p><p>CONTAINER ID  IMAGE  COMMAND  CREATED  STATUS  PORTS  NAMES </p><p>de3f932598fb  swarm:0.4.0  \"/swarm --debug manag\"  23 minutes ago  Up 23 minutes  2375/tcp, 3376/tcp  tmp-swarm-manager </p><p>e1937453f6cf  sequenceiq/cb-gateway-nginx:0.3  \"nginx -g 'daemon off\"  23 minutes ago  Up 23 minutes  gateway</p>","tags":["Cloudbreak","google"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-30 11:31:53.0","id":12589,"title":"Missing /user/admin directory on sandbox","body":"<p>Just started using a Hortonworks sandbox on Azure, no previous experience. Started going through the 'How to process data with Apache Hive' tutorial...one of the first steps is to upload files to /user/admin. It doesn't exist. I am not able to create the admin directory under /user because the add directory option is greyed out. Instead I create a new directory under /tmp and successfully uploaded the files. However I get to loading the data into the table:</p><p>LOAD DATA INPATH '/tmp/admin/Batting.csv' OVERWRITE INTO TABLE temp_batting;</p><p>and get :</p><p>INFO : Loading data to table default.temp_batting from hdfs://sandbox.hortonworks.com:8020/tmp/admin/Batting.csv\nERROR : Failed with exception Unable to move source hdfs://sandbox.hortonworks.com:8020/tmp/admin/Batting.csv to destination hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/temp_batting/Batting.csv\norg.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://sandbox.hortonworks.com:8020/tmp/admin/Batting.csv to destination</p><p>.</p><p>Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=hive, access=WRITE, inode=\"/tmp/admin/Batting.csv\":admin:hdfs:drwxr-xr-x</p><p>Obviously a permissions error, but how do I change them, and how can I create /user/admin for future use?</p><p>Thanks. </p>","tags":["how-to-tutorial","Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-01 16:52:17.0","id":12884,"title":"Error Handling during Pig LOAD Function","body":"<p>Does anyone have experience using how Pig can handle error Tuples during the LOAD function?</p><p>E.g. if we LOAD 10 lines which are comma delimited using PigStorage(',') yet the 9th line of the input data is Pipe delimited. What controls do we have on how these tuples are parsed and which Variable (relation) they are assigned to?</p><p>Ideally, I'd like to have one Relation/Variable loaded with the successful rows and some other relation holding the rows which were not parsed properly.</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-04 23:05:47.0","id":8682,"title":"Spark history server startup error","body":"<p>HDP-2.3.8.0-74</p><p>Spark 1.5.2.2.3</p><p>The start of \"Spark history server\" is successful in Ambari but very quickly it stops/fails. In the logs I see the following </p><pre>16/01/01 16:42:36 INFO HistoryServer: History provider class: org.apache.spark.deploy.history.yarn.server.YarnHistoryProvider</pre><pre>Exception in thread \"main\" java.lang.ClassNotFoundException: org.apache.spark.deploy.history.yarn.server.YarnHistoryProvider</pre><pre>        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)</pre><pre>        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)</pre><pre>        at java.security.AccessController.doPrivileged(Native Method)</pre><pre>        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)</pre><pre>        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)</pre><pre>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)</pre><pre>        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)</pre><pre>        at java.lang.Class.forName0(Native Method)</pre><pre>        at java.lang.Class.forName(Class.java:191)</pre><pre>        at org.apache.spark.deploy.history.HistoryServer$.main(HistoryServer.scala:224)</pre><pre>        at org.apache.spark.deploy.history.HistoryServer.main(HistoryServer.scala)</pre><pre>16/01/01 16:42:36 INFO Utils: Shutdown hook called</pre><p>The spark-defaults.conf is as follows</p><pre># Generated by Apache Ambari. Fri Jan  1 16:42:31 2016\n\nspark.history.kerberos.keytab none\nspark.history.kerberos.principal none\nspark.history.provider org.apache.spark.deploy.history.yarn.server.YarnHistoryProvider\nspark.history.ui.port 18080\nspark.yarn.containerLauncherMaxThreads 25\nspark.yarn.driver.memoryOverhead 384\nspark.yarn.executor.memoryOverhead 384\nspark.yarn.historyServer.address host10.demo.com:18080  \nspark.yarn.max.executor.failures 3\nspark.yarn.preserve.staging.files false\nspark.yarn.queue default\nspark.yarn.scheduler.heartbeat.interval-ms 5000\nspark.yarn.services org.apache.spark.deploy.yarn.history.YarnHistoryServicespark.yarn.submit.file.replication 3</pre>","tags":["Spark","spark-history-server"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-20 09:42:39.0","id":10760,"title":"How to disable hive shell for all users (Hive CLI)","body":"<p>I have configured ranger authorization for hive and want to force all the users to use beeline and want to block access to hive shell to all the users. </p><p>I know one workaround - we can revoke execute access for below file on all hive-clients.</p><pre>/usr/hdp/current/hive-client/bin/hive </pre><p>By doing this it could cause an issue to jobs scheduled via workflow engines like oozie or azkabaan etc.</p><p>Is there any other effective way to do this ?</p>","tags":["Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-02 20:22:30.0","id":13530,"title":"ERROR grunt.Grunt: ERROR 1200: null while reading a parquet file in pig","body":"<p>sample data i am trying to read </p><pre>\"stTime\": \"2014-05-31T22:00:00-07:00\",\n  \"tzone\": -7,\n  \"Time\": \"2015-12-04T05:01:47-07:00\",\n  \"user\": \"4\",\n  \"body\": {\n  \"prev\": null, -------&gt; i think this i where its failing not sure how to read it ?\n  \"status\": \"pairing\"\n  },\n  \"type\": \"Status\",\n  \"time\": \"2014-05-31T22:00:00-07:00\",\n  \"device\": \"1:1\"</pre>","tags":["Pig"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-03 00:28:38.0","id":13641,"title":"I need to connect HiveServer2 to MySQL metastore over SSL","body":"<p>I have MySQL, which only accepting SSL connections fro user hive\nI have HiveServer2, which connect to MySQL on start, so I have to pass these parameters to jdbc:myssql://  connect string:</p><p>-Djavax.net.ssl.keyStore=/etc/hive/conf/keystore -Djavax.net.ssl.keyStorePassword=P@ssw0rd -Djavax.net.ssl.trustStore=/etc/hive/conf/truststore -Djavax.net.ssl.trustStorePassword=P@ssw0rd</p><p>Can not find where to put it in the Hive configs.</p><p>javax.jdo.option.ConnectionURL do not accepting adding it to the string.</p><p>Same parameters works perfectly fine when passed to Java application in command line or by setting System.setProperty</p><p>Is this even possible?</p>","tags":["mysql","hiveserver2"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-02-03 01:55:51.0","id":13690,"title":"How to recover from a failed NameNode move?","body":"<p>Ambari has a handy wizard for helping move a NameNode from one machine to another.  In the event something were to go wrong with the move in, say, a NameNode HA enviornment are there any recommendations on how to recover and restore the NameNode setup back to its original server?  Would the move NameNode wizard be the best approach to putting the server back or would the Ambari API be a better approach? </p>","tags":["namenode-ha","HDFS","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-03 07:02:46.0","id":13782,"title":"Downgrade of Ambari 2.2 to 2.1","body":"<p>\n\tHi,</p><p>\n\tI need a advice how to correctly downgrade Ambari from 2.2 to 2.1, I haven't seen any docs about it.</p><p>\n\tMy issue is described <a target=\"_blank\" href=\"https://community.hortonworks.com/questions/11216/ambari-change-permission-of-mr-history-folder-whic.html#comment-13618\">here</a>, I already upgraded Ambari from 2.1 to 2.2, but I have HDP 2.0.6 (HDFS/MapReduce2/YARN on version 2.1.0.2.0), so my Ambari 2.2 doesn't work correctly with HDP 2.0. I have to downgrade Ambari or upgrade HDP, but when I want to upgrade HDP:</p><blockquote><strong>Ambari 2.2 does not support managing an HDP 2.0 cluster. If you are running HDP 2.0, in order to use Ambari 2.2 you must first upgrade to HDP 2.1 or higher using either Ambari 1.7 or 2.0 prior to upgrading to Ambari 2.2. Once completed, upgrade your current Ambari to Ambari 2.2.</strong></blockquote><p>I know how to change repo for downgrade, but Ambari server DB schema 2.2 is not working with Ambari 2.1, is there any script howto change back DB schema?</p><p>Or is there a any chance to upgrade my HDP 2.0.6 to version of HDP which works with Ambari 2.2 without using Ambari? I see there is a Non-Ambari Cluster Upgrade Guide.<a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4/bk_upgrading_hdp_manually/content/ch_upgrade_2_2.html\"></a></p>","tags":["schema","downgrade","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-04 11:14:51.0","id":14383,"title":"DFSInputStream has been closed already","body":"<p>Hi:</p><p>After run the job I am receiving this warning , The result its fine but the yarn doesnt execute anything, is posible that the result is in memory?</p><pre>16/02/04 12:07:37 WARN hdfs.DFSClient: DFSInputStream has been closed already</pre>","tags":["YARN","r"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-04 16:11:38.0","id":14442,"title":"Permanently add data type mappings to Hive.","body":"<p>\n\tI have a fairly regular use case where I have an initial data set with types that I want to cast differently into Hive:</p>\n<pre>\tSQLSERVER =&gt; HIVE\n</pre><pre>\tMoney            =&gt; Decimal(18,2)\n</pre><pre>\tNTEXT           =&gt; String\n</pre><p>\n\tInstead of adding the \n\t<code>--map-column-hive some_money_column=Decimal(18,2)</code>, is there a way that I can have Hive default to Decimal(18,2) any time it received a Money datatype upon <code>--hive-import</code> and/or <code>--create-hive-table</code>.</p><p>\n\tRight now I'm manually creating the DB in Hive with the data types I want, then using Sqoop to <code>--hive-import</code>. It's working, but it requires that I manually create the entire Database/Table/Column structure at the outset. Is there a better way to import data and automatically have Hive do the castings? <strong>It's onerous to write mappings for 50+ columns per table.</strong></p>","tags":["hdp-2.3.0","Sqoop","Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-06 21:27:03.0","id":14942,"title":"adding sandbox.hortonworks.com to /etc/hosts does not seem to make a difference and that seems to be a problem for Banana","body":"<p>Hello. I have been going through the Analyzing Twitter Data With Apache NiFi and HDP Search tutorial using Sandbox VM I downloaded a couple of weeks ago.</p><p>I did run 'echo 127.0.0.1     sandbox.hortonworks.com &gt;&gt; /etc/hosts' (I am assuming the partial quotation marks in 'echo \"127.0.0.1     sandbox.hortonworks.com &gt;&gt; /etc/hosts' in the instructions are a typo), and my /etc/hosts file now looks like this:</p><p>127.0.0.1                          \t\tlocalhost.localdomain localhost\n10.0.2.15             \tsandbox.hortonworks.com sandbox ambari.hortonworks.com\n127.0.0.1 sandbox.hortonworks.com</p><p>However, the following links never work:</p><p><a href=\"http://sandbox.hortonworks.com:4200\">http://sandbox.hortonworks.com:4200</a>\n<a href=\"http://sandbox.hortonworks.com:9090/nifi\">http://sandbox.hortonworks.com:9090/nifi</a>\n<a href=\"http://sandbox.hortonworks.com:8983/solr/\">http://sandbox.hortonworks.com:8983/solr/</a></p><p>So, I replace 'http://sandbox.hortonworks.com' with '127.0.0.1' and NiFi and Solr seem to work fine, based on my limited understanding. However, when I get to starting Banana, and, go to '127.0.0.1:8983/solr/banana/index.html' (I am assuming that leaving out 'index.html' from '<a href=\"http://sandbox.hortonworks.com:8983/solr/banana/index.html\">http://sandbox.hortonworks.com:8983/solr/banana/</a>' in the instructions is just an oversight, since without it we just see contents of the directory), I do get the basic Banana dashboard layout with black/gray background and buttons and rows and headers, but no data. I get the following two errors:</p><p>'Error Could not contact Solr at <a href=\"http://sandbox.hortonworks.com:8983/solr/\">http://sandbox.hortonworks.com:8983/solr/</a>. Please ensure that Solr is reachable from your system.'\n'Error Could not retrieve collections from Solr (error status = 0)'</p><p>I thought, perhaps, if I replaced 'http://sandbox.hortonworks.com' with '127.0.0.1' in the 'default.json' file I downloaded from github earlier in the tutorial that might fix the problem, but Banana still saw no data, even though the error messages have changed slightly:</p><p>'Error Collection not found at 127.0.0.1:8983/solr/tweets. Please check your configuration or create the collection. If you are using a proxy ensure it is configured correctly.'\n'Error Could not retrieve collections from Solr (error status = 404)'</p><p>The 'tweets' collection DOES exist, from what I can tell. If I enter the command we are given to create it, I am told it already exists. I can delete it and create it again, but that does not seem to make a difference for Banana.</p><p>Why does adding 'sandbox.hortonworks.com' to /etc/hosts not seem to work for me, does that mess anything else up besides Banana that I perhaps do not realize, and is there a workaround? </p><p>Many thanks!</p>","tags":["Sandbox","banana","error"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-08 19:15:22.0","id":15368,"title":"Ambari metrics widget (custom) for service cannot be added to service dashboard","body":"<p>I'm running a NIFI cluster (1 NCM and 3 nodes) with ambari. I have added metrics and widget definition to it . Data are collected (present in DB) but the widget are not added to the service dashboard. When trying to add them (they are visible in the \"browse widgets\") nothing happens . On the service dashboard ,I cannot create any widget either.  </p>","tags":["ambari-service","Ambari","ambari-metrics"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-09 06:07:01.0","id":15506,"title":"Error: \"Cannot retrieve repository metadata (repomd.xml) for repository: HDP-2.3. Please verify its path and try again\"","body":"<p>when I try to use yum to update or install some packages, I get this error. .</p><p><strong>\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.2.0/repodata/repomd.xml: [Errno 14] PYCURL ERROR 7 - \"couldn't connect to host\"\nTrying other mirror.\nError: Cannot retrieve repository metadata (repomd.xml) for repository: HDP-2.3. Please verify its path and try again\"</strong></p><p>I am working on HDP 2.3.4 Sandbox deployed on a CentOS 6 server which is behind the proxy. The server has internet access. I can ping google.com , for example.</p><p>the contents of /etc/yum.repos.d looks like this:</p><p><img src=\"/storage/attachments/1930-yumreposd.png\"></p><p>Any workaround regarding this issue would be highly appreciated!</p>","tags":["hdp-2.3.4"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-11 21:34:47.0","id":68,"title":"Who would cross the Bridge of Death must answer me these questions three, ere the other side he see","body":"<ol>\n<li>What . . . is your name?</li><li>What . . . is your quest?</li><li>What . . . is your favorite color?</li></ol><p><strong>Attribution</strong>: <em>Monty Python and the Holy Gra</em>il</p>\n","tags":["Knox"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-15 19:06:21.0","id":6575,"title":"I am using, JDBC to connect with hive and execute a query for bulk insert. I want to capture row wise, insert statement log. How can it be done??","body":"<p>@colorsoflife@gmail.com</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-18 08:40:15.0","id":7184,"title":"RHEL 7.2 Support","body":"<p>Does HDP support running on RHEL 7.2?</p><p>On Hortonworks documentation site, RHEL 7.x is documented as supported OS. Does the new released (Nov. 19) RHEL 7.2 included?</p>","tags":["rhel"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-22 15:44:26.0","id":7684,"title":"I am not able to load the data in elastic search","body":"<p>i have created an index in elastic search </p><p>below is the snippet :</p><p>\"created_date\":{  \n               \"type\":\"date\",\n               \"format\": \"yyy-MM-dd HH:mm:ss\",\n               \"index\":\"not_analyzed\"\n            },\n\"data_source\":{  \n               \"type\":\"string\",\n               \"index\":\"not_analyzed\"\n            }</p><p>then i have created a table. but when i am trying to load the data from main table in the created date column i am getting null values</p><p>can anyone please suggest me where i am wrong</p>","tags":["elasticsearch"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-29 08:52:28.0","id":8236,"title":"How to assign unique IP address to each of our sandbox HDP2.2?","body":"<p>We are a group of 7 people each having our own individual laptops and we have installed sandbox HDP2.2 on our machines and we all are working under one wifi network in our office. My question is how does each one of us get unique ip address for each of our sandbox so that we can ssh to each of our HDP from our local machines and work individually. If we do ifconfig in sandbox, all 7 are getting inet addr as 192.168.0.166 in bridged network and 10.0.2.15 in NAT network and 192.168.56.101 in Host-Only network.</p>","tags":["network"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-10-27 21:16:45.0","id":2246,"title":"Is there a set way to change jdk from 1.8.0_40 to 1.8.0_60","body":"<p>Currently default JDK is jdk1.8.0_40 which seems to have issues with Kerberos. Currently at HDP 2.3.2, and Ambari 2.1.2.\nCustomer is planning on setting this up with jdk1.8.0_60 and needs conformation that these below changes are enough for jdk1.8.0_60 to be accepted as new default in file serverSetup.py: \nJDKRelease(\"jdk1.8\", \"Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8\", \n\"http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-8u40-linux-x64.tar.gz\", \"jdk-8u40-linux-x64.tar.gz\", \n\"http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-8.zip\", \"jce_policy-8.zip\", \n\"/usr/jdk64/jdk1.8.0_40\", \n\"(jdk.*)/jre\") \nHe will change names from jdk1.8.0_40 to jdk1.8.0_60 and use my own repository to host the file. Not touching jce-policy. \nCould you please confirm if these changes are enough? </p>","tags":["hdp-2.3.2"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-11-02 13:50:54.0","id":2604,"title":"What is a good resource for best practices around Hadoop log management?","body":"","tags":["logs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-28 22:37:29.0","id":2330,"title":"In which format are yarn container logs stored in HDFS?","body":"<p>I went into /app-logs/&lt;username&gt;/ to get the logs. But I don't see how these files are stored. I tried getting the file and find format using 'file' but it just says 'data'. hdfs dfs -text also just yields garbled text. We are looking to run some pig jobs of container logs to gain some insights. </p>","tags":["logs"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-28 19:54:58.0","id":2310,"title":"Hadoop Log Monitoring","body":"<p>Do we have a list of things that can be monitoring from hadoop logs (datanode/nodemanager/namenode/resourcemanager)? Right now, I am working on ingesting logs into Kibana and they monitor Errors, Exceptions and application statistics. Are there any other things that we can get from these logs or has someone already worked on these logs to gain some intelligence on working of the cluster? </p>","tags":["logs"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-04 17:02:28.0","id":5204,"title":"resource manager reIP - Is there anything special needed when you reIP a resource manager host?","body":"<p>We are in the process of moving our Hadoop cluster to a different network and ran into issue with our NN failing over after one of them were reIP'ed.  Turns out we needed to reformat the ZooKepper znode and all is well. Becuase of that we are now questioning whether we might run into a simliar issue with our resource manager.  </p><p>Does anyone have any experience with reIP'ing a RM and have any issues?</p><p>Thanks,\nJason</p>","tags":["resource-manager"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-09 20:57:23.0","id":5674,"title":"yarn.application.classpath in yarn-site.xml","body":"<p>Hortonworks Data Platform doc suggest </p><p> Copy yarn-site.xml from the companion files and modify:\n&lt;property&gt;\n&lt;name&gt;yarn.application.classpath&lt;/name&gt;\n&lt;value&gt;$HADOOP_CONF_DIR,/usr/hdp/${hdp.version}/hadoop-client/*,\n/usr/hdp/${hdp.version}/hadoop-client/lib/*,\n/usr/hdp/${hdp.version}/hadoop-hdfs-client/*,\n/usr/hdp/${hdp.version}/hadoop-hdfs-client/lib/*,\n/usr/hdp/${hdp.version}/hadoop-yarn-client/*,\n/usr/hdp/${hdp.version}/hadoop-yarn-client/lib/*&lt;/value&gt;\n&lt;/property&gt;</p><p>but there is no such path like /usr/hdp/${hdp.version}/hadoop-client on the server. It is 2.3.2.0-2950 version</p><p>job submitted kept failing with errors:</p><p>Diagnostics: Exception from container-launch.\nContainer id: container_1449690108073_0001_02_000001\nExit code: 1\nStack trace: ExitCodeException exitCode=1:\nat org.apache.hadoop.util.Shell.runCommand(Shell.java:576)\nat org.apache.hadoop.util.Shell.run(Shell.java:487)\nat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753)\nat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nContainer exited with a non-zero exit code 1\nFailing this attempt. Failing the application.</p><p>Is this failing related to the classpath?</p><p>Please help. Thank you!</p>","tags":["jobs"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 04:48:57.0","id":6263,"title":"Looking for directions to pull the data from twitter through a ‘search’ widget.","body":"<p>I tried generating a URL via\ngoogle RSS feed but the URL is https and when I tried the GetHTTP processor it\nis expecting a SSL Context to be set up.</p><p>So the questions are a) is there\na direct way to pull the data (XML) from twitter through a widget?b) if google RSS is the only way then do\nyou know how to set up GetHTTP’s SSL Context property.</p>","tags":["nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-12-14 09:00:02.0","id":6311,"title":"what is the best way to process word documents?","body":"<p>I have large number of word documents in one folder. what is the best way to run analytics on it? shall i add all document to Hadoop supported zip format and push to HDFS? this folder will keep on updating by adding new documents. so there will be requirement to update HDFS with new data. Any possiblity to use SPARK on it?</p>","tags":["help"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 14:50:09.0","id":6378,"title":"Access denied in issue in Hive - HDP 2.2","body":"<p>I recently downloaded HDP 2.2 and facing issues with hive configuration. Every time I login to Ambari/Hue after first use, i get permission denied error. I tried up changing hdfs permissions,few basics authorizations setting in hive conf, grant access through CLI, but nothing works. Please assist</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-28 04:42:02.0","id":8117,"title":"Can any one send me proper link  for configuring kerberos through ambari 1,7,1 ?","body":"","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-24 17:31:18.0","id":8018,"title":"Secondary Namenode: Connection refused","body":"<p>am getting the below mentioned critical alerts in HDFS, please could you let me know whether secondary node process is mandatory for carrying out the activities or can it be ignored? if its mandatory let me know how to resolve this issue?</p><p>Connection failed to <a href=\"http://sandbox.hortonworks.com:50090\">http://sandbox.hortonworks.com:50090</a> (&lt;urlopen error [Errno 111] Connection refused&gt;)</p><p><img src=\"https://community.hortonworks.com/storage/attachments/1084-secondarynamenode.jpg\"></p>","tags":["namenode"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-30 22:39:17.0","id":8421,"title":"How to start mySQL in HDPCDeveloper_2.2 PracticeExam instance?","body":"<p>Is mySQL already installed on  HDPCD  Practice Exam instance? if yes, How to start mySQL in HDPCD  Practice Exam instance?</p>","tags":["hdpcd"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-10-05 14:02:47.0","id":884,"title":"Include md5sums for Ambari, HDP, and HDP-Utils tar.gz packages","body":"<p>It is helpful to have the corresponding md5sums for Ambari, HDP, and HDP-Utils tar.gz packages for individuals/organizations that want a private repository of HDP packages. Especially, as these packages continue to grow. I'd like to see a link of the HDP documentation but there could be a better place to have these.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-15 05:30:39.0","id":27705,"title":"hortonworks Sandbox dont have the Action option in Service Tab","body":"<p>hortonworks Sandbox dont have the Action option in Service Tab . The service action option is available in sandbox 2.3 but unable to find in latest version.</p><p><img src=\"/storage/attachments/3468-ambari.jpg\"></p>","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-04-17 05:45:57.0","id":27903,"title":"Encountered warnings and errors while install,start,test phase of ambari cluster setup","body":"<p>I'm trying to setting up a 4 node cluster using HDP 2.3.x ambari2.1.x.At final stage of installation i.e., Install,start,test phase.</p><p><img src=\"/storage/attachments/3485-installstarttest.png\"></p><p>I've received the following errors and warnings.</p><p>1.Failure on the  4th node:</p><pre>stderr: \nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py\", line 153, in &lt;module&gt;\n    DataNode().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 216, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py\", line 34, in install\n    self.install_packages(env, params.exclude_packages)\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 392, in install_packages\n    Package(name)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 152, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 118, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py\", line 45, in action_install\n    self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py\", line 49, in install_package\n    shell.checked_call(cmd, sudo=True, logoutput=self.get_logoutput())\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of '/usr/bin/yum -d 0 -e 0 -y install snappy-devel' returned 1. Error: Package: snappy-devel-1.0.5-1.el6.x86_64 (HDP-UTILS-1.1.0.20)\n           Requires: snappy(x86-64) = 1.0.5-1.el6\n           Installed: snappy-1.1.0-1.el6.x86_64 (@anaconda-RedHatEnterpriseLinux-201507020259.x86_64/6.7)\n               snappy(x86-64) = 1.1.0-1.el6\n           Available: snappy-1.0.5-1.el6.x86_64 (HDP-UTILS-1.1.0.20)\n               snappy(x86-64) = 1.0.5-1.el6\n You could try using --skip-broken to work around the problem\n You could try running: rpm -Va --nofiles --nodigest\n stdout:\n2016-04-17 06:05:11,465 - Group['hadoop'] {}\n2016-04-17 06:05:11,466 - Group['users'] {}\n2016-04-17 06:05:11,466 - Group['knox'] {}\n2016-04-17 06:05:11,466 - Group['spark'] {}\n2016-04-17 06:05:11,467 - User['oozie'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,468 - User['hive'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,468 - User['ambari-qa'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,469 - User['flume'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,470 - User['hdfs'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,471 - User['knox'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,472 - User['storm'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,473 - User['spark'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,473 - User['mapred'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,474 - User['hbase'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,475 - User['tez'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,475 - User['zookeeper'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,476 - User['kafka'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,477 - User['falcon'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,478 - User['sqoop'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,478 - User['yarn'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,479 - User['hcat'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,480 - User['ams'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,481 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-04-17 06:05:11,483 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2016-04-17 06:05:11,492 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2016-04-17 06:05:11,492 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'recursive': True, 'mode': 0775, 'cd_access': 'a'}\n2016-04-17 06:05:11,499 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-04-17 06:05:11,501 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}\n2016-04-17 06:05:11,509 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if\n2016-04-17 06:05:11,510 - Group['hdfs'] {'ignore_failures': False}\n2016-04-17 06:05:11,510 - User['hdfs'] {'ignore_failures': False, 'groups': ['hadoop', 'hdfs']}\n2016-04-17 06:05:11,511 - Directory['/etc/hadoop'] {'mode': 0755}\n2016-04-17 06:05:11,533 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-04-17 06:05:11,534 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}\n2016-04-17 06:05:11,552 - Repository['HDP-2.2'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.2.8.0', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP', 'mirror_list': None}\n2016-04-17 06:05:11,561 - File['/etc/yum.repos.d/HDP.repo'] {'content': InlineTemplate(...)}\n2016-04-17 06:05:11,562 - Repository['HDP-UTILS-1.1.0.20'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP-UTILS', 'mirror_list': None}\n2016-04-17 06:05:11,565 - File['/etc/yum.repos.d/HDP-UTILS.repo'] {'content': InlineTemplate(...)}\n2016-04-17 06:05:11,565 - Package['unzip'] {}\n2016-04-17 06:05:11,737 - Skipping installation of existing package unzip\n2016-04-17 06:05:11,737 - Package['curl'] {}\n2016-04-17 06:05:11,756 - Skipping installation of existing package curl\n2016-04-17 06:05:11,756 - Package['hdp-select'] {}\n2016-04-17 06:05:11,773 - Skipping installation of existing package hdp-select\n2016-04-17 06:05:11,954 - Package['hadoop_2_2_*'] {}\n2016-04-17 06:05:12,119 - Skipping installation of existing package hadoop_2_2_*\n2016-04-17 06:05:12,120 - Package['snappy'] {}\n2016-04-17 06:05:12,138 - Skipping installation of existing package snappy\n2016-04-17 06:05:12,139 - Package['snappy-devel'] {}\n2016-04-17 06:05:12,157 - Installing package snappy-devel ('/usr/bin/yum -d 0 -e 0 -y install snappy-devel')</pre><p>And it gave a warning on the top \" ! Data Node Install \".</p><p>2.Warnings on the 2nd node:</p><pre>stderr: \nNone\n stdout:\n2016-04-17 06:05:11,468 - Group['hadoop'] {}\n2016-04-17 06:05:11,469 - Group['users'] {}\n2016-04-17 06:05:11,469 - Group['knox'] {}\n2016-04-17 06:05:11,469 - Group['spark'] {}\n2016-04-17 06:05:11,470 - User['oozie'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,470 - User['hive'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,471 - User['ambari-qa'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,472 - User['flume'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,472 - User['hdfs'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,473 - User['knox'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,474 - User['storm'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,475 - User['spark'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,475 - User['mapred'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,476 - User['hbase'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,477 - User['tez'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,477 - User['zookeeper'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,478 - User['kafka'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,479 - User['falcon'] {'gid': 'hadoop', 'groups': ['users']}\n2016-04-17 06:05:11,479 - User['sqoop'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,480 - User['yarn'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,481 - User['hcat'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,481 - User['ams'] {'gid': 'hadoop', 'groups': ['hadoop']}\n2016-04-17 06:05:11,482 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-04-17 06:05:11,484 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2016-04-17 06:05:11,489 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2016-04-17 06:05:11,490 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'recursive': True, 'mode': 0775, 'cd_access': 'a'}\n2016-04-17 06:05:11,491 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-04-17 06:05:11,493 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}\n2016-04-17 06:05:11,499 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if\n2016-04-17 06:05:11,500 - Group['hdfs'] {'ignore_failures': False}\n2016-04-17 06:05:11,500 - User['hdfs'] {'ignore_failures': False, 'groups': ['hadoop', 'hdfs']}\n2016-04-17 06:05:11,501 - Directory['/etc/hadoop'] {'mode': 0755}\n2016-04-17 06:05:11,525 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-04-17 06:05:11,525 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}\n2016-04-17 06:05:11,541 - Repository['HDP-2.2'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.2.8.0', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP', 'mirror_list': None}\n2016-04-17 06:05:11,552 - File['/etc/yum.repos.d/HDP.repo'] {'content': InlineTemplate(...)}\n2016-04-17 06:05:11,553 - Repository['HDP-UTILS-1.1.0.20'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP-UTILS', 'mirror_list': None}\n2016-04-17 06:05:11,557 - File['/etc/yum.repos.d/HDP-UTILS.repo'] {'content': InlineTemplate(...)}\n2016-04-17 06:05:11,557 - Package['unzip'] {}\n2016-04-17 06:05:11,707 - Skipping installation of existing package unzip\n2016-04-17 06:05:11,708 - Package['curl'] {}\n2016-04-17 06:05:11,725 - Skipping installation of existing package curl\n2016-04-17 06:05:11,725 - Package['hdp-select'] {}\n2016-04-17 06:05:11,743 - Skipping installation of existing package hdp-select\n2016-04-17 06:05:11,931 - Package['falcon_2_2_*'] {}\n2016-04-17 06:05:12,076 - Installing package falcon_2_2_* ('/usr/bin/yum -d 0 -e 0 -y install 'falcon_2_2_*'')\n2016-04-17 06:08:11,638 - Execute['ambari-sudo.sh  -H -E touch /var/lib/ambari-agent/data/hdp-select-set-all.performed ; ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.2 | tail -1`'] {'not_if': 'test -f /var/lib/ambari-agent/data/hdp-select-set-all.performed', 'only_if': 'ls -d /usr/hdp/2.2*'}\n2016-04-17 06:08:11,641 - Skipping Execute['ambari-sudo.sh  -H -E touch /var/lib/ambari-agent/data/hdp-select-set-all.performed ; ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.2 | tail -1`'] due to not_if\n2016-04-17 06:08:11,641 - XmlConfig['core-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'configuration_attributes': {}, 'owner': 'hdfs', 'only_if': 'ls /usr/hdp/current/hadoop-client/conf', 'configurations': ...}\n2016-04-17 06:08:11,668 - Generating config: /usr/hdp/current/hadoop-client/conf/core-site.xml\n2016-04-17 06:08:11,668 - File['/usr/hdp/current/hadoop-client/conf/core-site.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}\n2016-04-17 06:08:11,687 - Can only link configs for HDP-2.3 and higher.</pre><p>And the title of the warning is \"Falcon server install \".</p><p>In the earlier step of \"Configurations\":</p><p>I faced this warning which i didn't attend as i was said that DB can be changed after the deployment of the cluster.</p><p><img src=\"/storage/attachments/3488-configurations4.png\"></p><p>Is this one of the reasons behind the failures?.</p><p>Anybody please share your thoughts to resolve this issue.And post me any useful links if available.</p><p>Thanks in advance,</p><p>Karthik.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-19 18:26:20.0","id":1672,"title":"Add host to Ambari Alerts group that is not managed by Ambari","body":"<p>Are there API calls or other capability to add a non-Ambari managed instance to an Alert group?  For example, I would like to add a Knox instance to the existing Knox Alerts group.  I thought I could add by hostname but realized that I cannot.  </p>","tags":["alerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-31 07:43:37.0","id":36583,"title":"How to save data in HDFS using R","body":"<p>How do I save data in HDFS using R?</p><p>Let's say I have all my data that I need to save in HDFS in a variable in R and I would like to save it in csv format. </p><p>R has a function write.csv() to save it in CSV format but how do I save it in HDFS?</p><p>I guess rhdfs is the way to go?But how exactly?</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-06-09 15:20:25.0","id":38824,"title":"Ambari  reports 956 under replicated blocks but fsck reports 0","body":"<p>Ambari reports 956 under replicated blocks but when I run an fsck on / ( referencing: https://community.hortonworks.com/articles/4427/fix-under-replicated-blocks-in-hdfs-manually.html ) it reports only three under replicated blocks. I repaired the three files from the command line and the count did decrease by three in Ambari, but there is still a discrepancy. I tend to trust the command line a bit more, but I'd sure like Ambari to agree. Any tips? </p>","tags":["datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-15 13:02:38.0","id":1497,"title":"Resource Manager API's","body":"<p>Looking at hitting allt he right API's for charge back scenario.  In the following call to RM API's </p><pre>http://192.168.245.131:8088/ws/v1/cluster/apps/application_1442459299785_0002</pre><ol><li>What is memory Seconds, how to measure the memory used by an application using this number</li><li>What is vcoreSEconds ?  Is it directly proportional to the CPU seconds ?</li><li>Is running containers supposed to show us all the containers launched for that app ?</li><li>Allocated MB, allocated VCores are also showing invalid values or zeros.</li><li>How frequently can I call this API. I don’t want to miss any records, but at the same time we don’t want duplicate records.</li></ol><p>Output of call below</p><pre>&lt;app&gt;\n&lt;id&gt;application_1442459299785_0002&lt;/id&gt;\n&lt;user&gt;hive&lt;/user&gt;\n&lt;name&gt;HIVE-521c0020-4786-4e55-8822-abe11a517ba1&lt;/name&gt;\n&lt;queue&gt;default&lt;/queue&gt;\n&lt;state&gt;FINISHED&lt;/state&gt;\n&lt;finalStatus&gt;SUCCEEDED&lt;/finalStatus&gt;\n&lt;progress&gt;100.0&lt;/progress&gt;\n&lt;trackingUI&gt;History&lt;/trackingUI&gt;&lt;trackingUrl&gt;\nhttp://sandbox.hortonworks.com:8088/proxy/application_1442459299785_0002/\n&lt;/trackingUrl&gt;\n&lt;diagnostics&gt;null&lt;/diagnostics&gt;\n&lt;clusterId&gt;1444871487358&lt;/clusterId&gt;\n&lt;applicationType&gt;TEZ&lt;/applicationType&gt;\n&lt;applicationTags/&gt;\n&lt;startedTime&gt;1442460228059&lt;/startedTime&gt;\n&lt;finishedTime&gt;1442502311402&lt;/finishedTime&gt;\n&lt;elapsedTime&gt;42083343&lt;/elapsedTime&gt;\n&lt;amContainerLogs&gt;\nhttp://sandbox.hortonworks.com:8042/node/containerlogs/container_e17_1442459299785_0002_01_000001/hive\n&lt;/amContainerLogs&gt;\n&lt;amHostHttpAddress&gt;sandbox.hortonworks.com:8042&lt;/amHostHttpAddress&gt;\n&lt;allocatedMB&gt;-1&lt;/allocatedMB&gt;\n&lt;allocatedVCores&gt;-1&lt;/allocatedVCores&gt;\n&lt;runningContainers&gt;-1&lt;/runningContainers&gt;\n&lt;memorySeconds&gt;10540022&lt;/memorySeconds&gt;\n&lt;vcoreSeconds&gt;42158&lt;/vcoreSeconds&gt;\n&lt;preemptedResourceMB&gt;0&lt;/preemptedResourceMB&gt;\n&lt;preemptedResourceVCores&gt;0&lt;/preemptedResourceVCores&gt;\n&lt;numNonAMContainerPreempted&gt;0&lt;/numNonAMContainerPreempted&gt;\n&lt;numAMContainerPreempted&gt;0&lt;/numAMContainerPreempted&gt;\n&lt;logAggregationStatus&gt;NOT_START&lt;/logAggregationStatus&gt;\n&lt;/app&gt;</pre>","tags":["resource-manager"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-25 03:32:56.0","id":2070,"title":"OOzie smoke tests failed in Ambari UI","body":"<pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/OOZIE/4.0.0.2.0/package/scripts/service_check.py\", line 138, in &lt;module&gt;\n    OozieServiceCheck().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/OOZIE/4.0.0.2.0/package/scripts/service_check.py\", line 61, in service_check\n    OozieServiceCheckDefault.oozie_smoke_shell_file(smoke_test_file_name, prepare_hdfs_file_name)\n  File \"/var/lib/ambari-agent/cache/common-services/OOZIE/4.0.0.2.0/package/scripts/service_check.py\", line 123, in oozie_smoke_shell_file\n    logoutput=True\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 154, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 152, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 118, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 260, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 70, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 92, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 140, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 291, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of '/var/lib/ambari-agent/tmp/oozieSmoke2.sh redhat /usr/hdp/current/oozie-client /usr/hdp/current/oozie-client/conf /usr/hdp/current/oozie-client/bin http://hdpmaster3.bigdataprod1.wh.xxxxxcorp.com:11000/oozie /usr/hdp/current/oozie-client/doc /usr/hdp/current/hadoop-client/conf /usr/hdp/current/hadoop-client/bin ambari-qa False' returned 1. source /usr/hdp/current/oozie-client/conf/oozie-env.sh ; /usr/hdp/current/oozie-client/bin/oozie -Doozie.auth.token.cache=false job -oozie http://hdpmaster3.bigdataprod1.wh.xxxxcorp.com:11000/oozie -config /usr/hdp/current/oozie-client/doc/examples/apps/map-reduce/job.properties -run\nJob ID : 0000005-151024200514443-oozie-oozi-W\n------------------------------------------------------------------------------------------------------------------------------------\nWorkflow Name : map-reduce-wf\nApp Path      : hdfs://WHPROD1NN/user/ambari-qa/examples/apps/map-reduce/workflow.xml\nStatus        : FAILED\nRun           : 0\nUser          : ambari-qa\nGroup         : -\nCreated       : 2015-10-25 03:15 GMT\nStarted       : 2015-10-25 03:15 GMT\nLast Modified : 2015-10-25 03:15 GMT\nEnded         : 2015-10-25 03:15 GMT\nCoordAction ID: -\n\nActions\n------------------------------------------------------------------------------------------------------------------------------------\nID                                                                            Status    Ext ID                 Ext Status Err Code  \n------------------------------------------------------------------------------------------------------------------------------------\n0000005-151024200514443-oozie-oozi-W@:start:                                  OK        -                      OK         -         \n------------------------------------------------------------------------------------------------------------------------------------\n0000005-151024200514443-oozie-oozi-W@mr-node                                  FAILED    -                      -          EJ001      </pre><p>-------------</p>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-19 04:44:03.0","id":4310,"title":"SmartSense HST Agents dying","body":"<p>Hi,</p><p>In one of our clients cluster the HST agents are dying repeatedly and throwing below error in the logs.</p><p>Internal Exception: java.sql.BatchUpdateException: A truncation error was encountered trying to shrink VARCHAR '====================[ WARNING! WARNING! WARNING! WARNING! ]=&' to length 255.\nError Code: 20000</p><p> Any help is appreciated.</p><p>Thanks</p>","tags":["smartsense"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-16 03:15:40.0","id":6682,"title":"slots/containers limitatation?","body":"<p>If I have only one job running on a host (either mapreduce or yarn), will it be able to use all resources available on the host or only one slot/container for map and one slot/container for reduce?</p><p>Appreciate the insights.</p>","tags":["YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-22 20:13:46.0","id":7717,"title":"HADOOP_PREFIX","body":"<p>The HDP 2.3.2 VM is built on Centos and to study the same we need to find out where the HADOOP_PREFIX  is located  Can somebody point to me where this is located in the hdp2.3.2?</p>","tags":["hdp-2.3.2"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-19 21:03:49.0","id":10657,"title":"MapReduce with HBase fails","body":"<p>When trying to run an mapreduce with HBase TableOutputFormat I get the following exception:</p><pre>16/01/19 20:58:48 INFO mapreduce.Job: Job job_1453235757406_0002 failed with state FAILED due to: Application application_1453235757406_0002 failed 2 times due to AM Container for appattempt_1453235757406_0002_000002 exited with  exitCode: 1\nFor more detailed output, check application tracking page:http://sandbox.hortonworks.com:8088/proxy/application_1453235757406_0002/Then, click on links to logs of each attempt.\nDiagnostics: Exception from container-launch.\nContainer id: container_e02_1453235757406_0002_02_000001\nExit code: 1\nStack trace: ExitCodeException exitCode=1:\n  at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)\n  at org.apache.hadoop.util.Shell.run(Shell.java:455)\n  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\n  at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)\n  at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)\n  at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:745)</pre><p>Tried to run it with hbase command and with hadoop jar (after updating the classpath) </p><p>The same error</p>","tags":["MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-20 10:26:43.0","id":10764,"title":"effective way to store image files, pdf files in hdfs as sequence format using nifi","body":"<p>Currently working on a POC to effectively store image files or pdf files in hdfs as sequence format may be. In hdfs as there is a block size of 64mb lets say if i want to store couple of images whose size is 2mb each then i ll be wasting 60mb block size. So iam trying to come up with a way to effectively store small image files or pdf files in hdfs without wasting block size. Also please let me know whether we can ingest these files into hdfs using apache nifi and if so which processors would be best to use. thanks</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-25 12:42:52.0","id":11574,"title":"Can we delete data in a Hive table from spark-scala shell.","body":"<p>Hi,</p><p>In Hive DB I got a table named \"employee\" with employee id as one field, </p><p>can I set HiveContext and delete data from Hive table() like below (if not what is the best way to do) </p><p>val sqlContext = new HiveContext(sc) </p><p>sqlContext.sql(\"DELETE FROM employee WHERE employee_id=6\") </p><p>Thanks</p>","tags":["scala","Spark","hivecontext"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-26 01:24:52.0","id":11671,"title":"install ambari 2.2 without internet","body":"<p>hi,</p><p>I following the instruction to install ambari 2.2.0.0 without internet</p><p><a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_Installing_HDP_AMB/content/_using_a_local_repository.html\">http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_Installing_HDP_AMB/content/_using_a_local_repository.html</a></p><p>so i have finished all download/upload following 4 files. they are ambari.rep, ambari.tarball, hdp tarball and hdp-utils tarball.</p><p>1. download/upload ambari 2.2 repositories to /var/www/html, untar</p><p><a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari.repo\">http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari.repo</a> </p><p>2.download/upload ambari tarball to /var/www/html, untar</p><p><a href=\"http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari-2.2.0.0-centos6.tar.gz\">http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.0.0/ambari-2.2.0.0-centos6.tar.gz</a></p><p>3. downloaded and uploaded HDP to namenode and mv to /var/www/html/hdp then untar </p><p><a href=\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0/HDP-2.3.4.0-centos6-rpm.tar.gz\">http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.3.4.0/HDP-2.3.4.0-centos6-rpm.tar.gz</a></p><p>4. downloaded anduploaded HDP-UTILS to namenode and mv to /var/www/html/hdp  untar</p><p><a href=\"http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6/HDP-UTILS-1.1.0.20-centos6.tar.gz\">http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6/HDP-UTILS-1.1.0.20-centos6.tar.gz</a></p><p>I like to double check to see if I miss any files I need and if any file is unnecessary.</p><p>then I can untar and start to ambari-server setup. </p>","tags":["installation"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-02-03 21:35:34.0","id":14230,"title":"How to enable name node HA without cluster downtime?","body":"<p>We have a process which pulls messages from MQ and puts it in HBase. Since the messages have a 10 sec expiry we cannot afford to have the cluster down. What do people do in such situations?</p><p>We need enable namenode HA on the hortonworks cluster without taking the cluster offline.</p>","tags":["namenode-ha"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-21 22:01:38.0","id":7603,"title":"Minimum HA Components Required for Rolling Restart","body":"<p>Which components must have HA in order to perform Rolling Upgrade? The documentation for Rolling Restart is a bit vague in regards to the bare minimum required to perform Rolling Upgrade:</p><ul><li>HDFS HA is clearly required</li><li>YARN RM HA says \"should be enabled to prevent a disruption in service during the upgrade\"  - Can Rolling Upgrade be performed without it?</li><li>Hive Metastore HA is \"recommended\". Is it <strong>required</strong>?</li><li>HiveServer2 and Oozie HA - are these <strong>required</strong>?</li></ul><p>Thank you</p><p>Reference: <a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.2.1/bk_upgrading_Ambari/content/_prerequisites_rmiu.html\">http://docs.hortonworks.com/HDPDocuments/Ambari-2....</a></p>","tags":["upgrade"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-30 10:45:08.0","id":8373,"title":"Sqoop - dynamically import from SQL server","body":"<p>I have a log table on SQL server, it tells me which tables need to be loaded into HDFS. Can I use sqoop to loop through the log table and load only the tables flagged out? </p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-13 19:45:33.0","id":9790,"title":"org.apache.hadoop.ipc.StandbyException","body":"<p>I have succesfully setup a hadoop cluster with HDFS HA enabled. I did a manual failover of Active NN (nn1) . Now the Standby NN (nn2) has become Active as expected. nn1 is now the standby. All HDFS write and read operations work fine (hdfs command line ..). However the below error pops up while using Hive (create table ) . I know turning the nn1 back to ACTIVE solves the issue. Looking for workarounds which doesn't require this manual operation. Thanks in advance. </p><p>FAILED: SemanticException MetaException(message:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby\n  at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\n  at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1915)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1407)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4501)\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:961)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:835)\n  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2141)\n  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2137)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.security.auth.Subject.doAs(Subject.java:415)\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2135)</p>","tags":["high-availability","namenode-ha"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-14 11:10:29.0","id":9891,"title":"Can I install hadoop client on standby node?","body":"<p>Hi,</p><p>I've install hadoop on 4 nodes. Node 1 and Node 4 are Namenode and standbynode with high availability. I've installed hadoop client on all 4 nodes. When I try to execute command on StandbyNode, \"hadoop fs -text myfile.txt\", I'm getting error \"text: java.lang.NullPointerException\". I can execute the same command on other 3 nodes.</p><p>I'm aware Namenode and Standbynode are master nodes and we don't install hadoop client on master nodes in production environment. However, I'm doing this in my test environment (for learning purpose) and I would like to know if there is any specific reason why I cannot execute \"hadoop fs -text myfile.txt\" on standby node.</p><p>PS: I can execute -ls, -mkdir, -chmod commands successfully on standby node.</p><p>Many thanks in advance.</p>","tags":["hadoop","namenode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-01-14 18:53:59.0","id":9978,"title":"How to implement standalone Ambari views and configuring them ?","body":"","tags":["ambari-views"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-22 17:26:56.0","id":11267,"title":"​hadoop fs -put ./Batting.csv /user/guest/","body":"<p>hadoop fs -put ./Batting.csv /user/guest/ </p><p>put: Permission denied : user=root, access=WRITE, inode=\"/user/guest/Battings.csv_COPYING_\":guest:guest:drwxr-xr-x</p><p>login as a root with password hadoop </p>","tags":["Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-03 18:09:55.0","id":14073,"title":"org.apache.ambari.server.AmbariException: Cannot add foreign key constraint","body":"<p>ambari upgrade </p><p>upgrading ambari-server from 1.7 to 2.2 with mysql 5.6</p><p>while running ambari-server upgrade  it failed with org.apache.ambari.server.AmbariException: Cannot add foreign key constraint</p><p>any work around, i couldn't even create the FK manually either.. is it a known issue?</p>","tags":["mysql","ambari-2.2.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-04 20:58:51.0","id":14533,"title":"Registration with the server failed.","body":"<p>after fix root is no in sudoers file, this problem come up, anything I need to check on?</p>","tags":["cluster","logging"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-02-05 06:32:03.0","id":14631,"title":"How to update Falcon cluster entities that have feeds and processes dependencies?","body":"<p>Hello friends !</p><p>How to update Falcon cluster entities that have feeds and processes dependencies?</p>","tags":["cluster","Falcon"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-04-05 06:39:42.0","id":25934,"title":"Not able to drop database in hive","body":"<p>Hi Team,</p><p>I am not able to drop database though we do not have any table in that database ?</p><p>hive&gt; use prashant_db;</p><p>OK</p><p>Time taken: 0.221 seconds</p><p>hive&gt; drop database prashant_db;</p><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database prashant_db is not empty. One or more functions exist.)</p><p>hive&gt; show tables;</p><p>OK</p><p>Time taken: 0.226 seconds</p><p>hive&gt; </p>","tags":["error"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-04 22:43:52.0","id":868,"title":"CentOS 7 support","body":"<p>When will HDP 2.x will start giving support on CentOS 7 . I don’t understand the python26 dependency fully but I presume that is a major roadblock in moving to the new OS ?</p>","tags":["hdp-2.3.0","centos"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-15 16:10:20.0","id":1520,"title":"How do you increase the heapsize for Hue?","body":"","tags":["configuration"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-27 20:58:35.0","id":2243,"title":"Ambari view tuning guide?","body":"<p>Where can I find the ambari views tuning guide?</p>","tags":["ambari-views","performance"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-03 01:39:33.0","id":2649,"title":"Do we have any internal Hortonworks initiatives around Oozie visualization and flow editing?","body":"<p>I am working on a project and I'm wondering if we have any initiatives going around around Oozie?  It seems that it would be a good fit instead of using Talend.</p>","tags":["etl"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-26 05:04:43.0","id":19658,"title":"where to find complete set of RHEL 6.5 packages?  like to check on my OS if any missing package and like to know the list of required RHEL package for ambari(without internet) installation","body":"<p>HDP doc said: The installer pulls many packages from the base OS repositories. If you do not\nhave a complete set of base OS repositories available to all your machines at\nthe time of installation you may run into issues.</p><p>1. when we have issues install ambari without internet, I think I need to compare with the current OS packages to the complete set of RHEL 6.5 packages. any one can give me a clue on that?</p><p>2. or any one can show me the required packages for install HDP/ambari without internet access. </p><p>3. It will be great if HDP doc would provide list of required package for its installation, not just overall ask for complete set of package for OS.  </p>","tags":["installation"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-11-24 02:09:00.0","id":4673,"title":"How to configure standalone Ambari Files View to another cluster with NameNode HA","body":"<p>Someone recently asked me how to configure a standalone Ambari Server with the Files View to point correctly to a server running NameNode HA.</p><p>Whenever active/standby NameNodes switch roles, the Files View can correctly handle the transition.</p>","tags":["ambari-views","namenode-ha"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-04 22:41:06.0","id":867,"title":"Building rpm from HDP tar.gz","body":"<p>Is there a source which makes rpm from “HDP tar.gz” like “Bigtop”?</p><p>I am using ambari and HDP.\nI want to change some part of HDP and make rpm.\nDo I have to customize Bigtop or rpmbuild to make rpm?</p>","tags":["best-practices","hdp-2.3.0"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-31 08:48:56.0","id":8468,"title":"Not able to download companion files in Sandbox ?","body":"","tags":["Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-06 07:26:19.0","id":8908,"title":"I get a pop-up often saying \"500 status code received on GET method for API: /api/v1/clusters/HDP_ctrls/alerts?format=groupedSummary \".​","body":"","tags":["hdp-2.3.0","error"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-10 19:48:44.0","id":9346,"title":"DataNode Block Report","body":"<p>What is the control flow for data node block reporting to the name node?</p>","tags":["datanode"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-09 07:12:36.0","id":5513,"title":"Ambari Hive view cannot connect when using http transport mode","body":"<p>I’m using Sandbox 2.3.2\n\nI have configured Hive with hive.server2.transport.mode=http (because of\n Knox integration). However, the Hive view in Ambari does not seem to \nlike that.\n\nWhen I leave it at the default out of the box settings (port 10000), I \nget a connection refused, as expected. But when I change the port to \n10001, which is the value of hive.server2.thrift.http.port, I get the \nfollowing:</p><pre>ERROR [qtp-client-1901] ServiceFormattedException:96 - org.apache.ambari.view.hive.client.HiveClientException: H020 Could not establish connecton to sandbox.hortonworks.com:10001: org.apache.thrift.transport.TTransportException: org.apache.thrift.transport.TTransportException</pre><p>The UI doesn’t give me a way to define the JDBC URL myself, so I can’t set the proper transport mode. So I’m stuck, at least until Hive can support both modes simultaneously (<a target=\"_blank\" href=\"https://issues.apache.org/jira/browse/HIVE-5312\">HIVE-5312</a>). If I can, I'd like to avoid setting up a second Hive instance just so that I can get the view working.</p><p>Any help would be appreciated.</p>","tags":["ambari-views"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-15 22:34:42.0","id":6623,"title":"No Services Running on Practice Exam","body":"<p>When I set up the Amazon Instance I followed the directions on the Java Developer Practice Exam except for Network type (which I assume to be the problem), because Classic-EC2 was not an option for me, I just went forward with the only option that I had.  </p><p>When I'm on the instance Ambari shows lost heartbeat to all of the services and I cannot access the hdfs file system which gives me an Exception that the connection was lost to the namenode.</p>","tags":["hortonworks-university"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-18 17:11:54.0","id":7282,"title":"Why did portreserve:  [Failed] while starting?","body":"<p>While loading the latest version of the Oracle VM VirtualBox 5.0.10 \nusing Hortonworks Sandbox with HDP 2.3.2, portreserve: [Failed] and then\nlater safemode: received a Connection Refused Exception.</p><p>I am running on Dell using Windows 8.1 and I was able to do this previously.</p>","tags":["port"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-31 16:19:13.0","id":8491,"title":"sandbox VM crash with kernel:bug: soft lockup","body":"<p>I downloaded the sandbox VM for virtualbox and started it on ubuntu host with virtualbox 5 and after a couple of hours it crash.</p><p>On the screen I get :</p><p>Message from syslog@sandbox at dec ....</p><p>kernel:Bug: soft lockup - CPU2 stuck for 67s! [java:30253]</p><p>and this message repeat itself on and on. </p><p>The vm is stuck and non responsive. I need to poweroff and restart it!</p><p>Here are the log vboxlog relevant lignes:</p><p>00:01:05.829053 VMMDev: Guest Log: 00:00:00.011489 main  4.3.22 r98236 started. Verbose level = 0\n00:08:50.366623 TM: Giving up catch-up attempt at a 60 002 234 869 ns lag; new total: 60 002 234 869 ns\n14:11:48.845736 PIIX3 ATA: Ctl#0: RESET, DevSel=0 AIOIf=0 CmdIf0=0xca (30357026 usec ago) CmdIf1=0x00 (-1 usec ago)\n14:12:05.363192 PIIX3 ATA: execution time for ATA command 0xca was 46 seconds\n14:12:05.366460 PIIX3 ATA: Ctl#0: finished processing RESET</p><p>The host is a dell server with 2 quad xeon 32 gig ecc ram.</p><p>I have two other VM running on the host. A debian 64 and a ubuntu lts 14.04 that run just fine.</p><p>Anybody had a luck with virtualbox 5?</p><p>Peter</p>","tags":["help","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-14 15:42:21.0","id":9928,"title":"How to restore data from dfs local directory","body":"<p>Hi,</p><p>I got a zip that contains dfs directory.  this directory contains following directory</p><p>data\nname \nnamesecondary</p><p>Now if i open name or namesecondary they contain current directory with files like  \"edits_0000000000000000001-0000000000000000004\"    \"edits_inprogress_0000000000000000005\"     \"fsimage_0000000000000000000\"    \"VERSION\"   etc etc</p><p>Data directory contains current directory and then more files and directories in that. </p><p>Look like this is dump of some hadoop directories       </p><p>Now question is how to retrieve files from this.    i guess this is some sort of hadoop archive and i need to restore it </p><p>Please help me out </p><p>Thanks</p><p>Shahzad </p>","tags":["archive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-01-28 07:18:16.0","id":12149,"title":"Port 8042 in HDP 2.3.4 configuration files","body":"<p>Hi,</p><p>Where can I find port 8042 in HDP 2.3.4 configuration files ?</p><p>My Hadoop Hortonworks cluster si set up in Amazon EC2. When I visit\n<code>http://ec2-xx-xx-xxx-xxx.ap-southeast-1.compute.amazonaws.com:8088/cluster</code> to view the application logs, the page gets displayed with all the the applications When I click of any of the application also page get displayed(Attached screenshot) Now when I click on the logs,it shows the internal IP of one of the data node and</p><pre>&lt;code&gt;http://ip-xx-xx-xx-xxx.ap-southeast-1.compute.internal:8042/node/containerlogs/container_e06_1453856711181_0026_02_000001/hdfs\n</pre>\n<p>shows : page not displayed .. (Attached screenshot)</p><p>I could not find the port 8042 in configuration file,yarn--default.xml</p><pre>&lt;code&gt;yarn.nodemanager.webapp.address ${yarn.nodemanager.hostname}:8042</pre>\n<p><img src=\"/storage/attachments/1613-application-webui-screenshotsmerge.png\"></p>","tags":["hdp-2.3.4","logs"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-24 20:32:47.0","id":19321,"title":"when start sqoop, error: [sqoop@ip-172-31-31-73 ~]$ sqoop Warning: /usr/hdp/2.3.4.0-3485/hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Try 'sqoop help' for usage.","body":"<p>so I am wondering if I need install Hbase in order to use sqoop.</p><p>I will use sqoop to extract data from teradata and dump to a hive table, do I need Hbase? </p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-03-16 21:29:13.0","id":23378,"title":"Namenode switch monitoring","body":"<p>Hi All,</p><p>I am trying to write a script that would monitor the namenodes (i have HA enabled) and notify me when a failover happens between the namenodes. I am thinking of using Ambari rest api to achieve this, but I am not sure which field/parameter i should use in the namenode component withing HDFS service that would provide me with the Active namenode.</p><p>I am open to other ideas :-)</p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-04-01 15:07:36.0","id":25543,"title":"Is it possible to restore predefined alert that has been deleted over API?","body":"<p>Hello everyone!\nI tried to disable the predefined \"Metrics Collector - HBase Master CPU Utilization\" alert with \"id 8\" using the following API request:</p><pre>curl -H \"X-Requested-By: ambari\" -X DELETE \"http://ambari-server:8080/api/v1/clusters/newcluster/alert_definitions/8\" -u admin:admin</pre><p>But it turned out I completely removed it. I cannot find alert in a list of alerts that are disabled.</p><p>Is there any way to recover it? And how to properly disable and not delete predefined alert over API?</p><p>Thanks!</p>","tags":["delete","alerts","ambarialerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-13 06:30:22.0","id":27259,"title":"I am trying to connect to AWS machine with AMI ID Hortonworks HDPCDeveloper_2.2 PracticeExam_v7 (ami-617b6700) using VNC Viwer and getting Timed Out Connection. Please Help.","body":"<p>I am trying to connect to AWS first time and have followed all the instructions from HDPCD-Java-PracticeExamGuide.pdf.</p>","tags":["timeout"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-04-22 12:12:29.0","id":28929,"title":"HDP 2.4 installation on windows server 2008 R2","body":"<p>Hi,</p><p>I am trying to install HDP 2.4 on windows server 2008 R2 64 bit. my environment details are as follows:</p><p>SQL Server 2008 R2, .Net frameworl 4.5, VC++ 2010 , Python 2.10.</p><p>In the beginning all is fine, but later it stuck with \"Creating oozie war file\" in a powershell window. it takes here for ever.</p><p>Please suggest in this regard.</p><p>Thanks</p><p>Rishi</p>","tags":["windows"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-05-19 07:47:24.0","id":34240,"title":"datanode stopped working","body":"<p>Everything in my cluster was working good and there were no warnings in ambari but today outofnowhere datanode stopped working. </p><p>ambari showed 0/1 live datanode ,I could successfully start it each time but ambari could never see it as live and soon it stopped.</p><p>I checked the logs and it showed this:</p><pre>java.io.EOFException\nat java.io.DataInputStream.readShort(DataInputStream.java:315)\nat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:58)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)\nat java.lang.Thread.run(Thread.java:745)\n2016-05-05 16:46:58,282 ERROR datanode.DataNode (DataXceiver.java:run(278)) - warehouse.swtched.com:50010:DataXceiver error processing unknown operation  src: /10.10.10.9:60107 dst: /10.10.10.9:50010\njava.io.EOFException\nat java.io.DataInputStream.readShort(DataInputStream.java:315)\nat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:58)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)\nat java.lang.Thread.run(Thread.java:745)</pre><p>How do I debug this?</p><p>I tried everything but had to reinstall stuff from scratch. It just does not look good or acceptable. What could be the way to debug such errors. where did it come from when there were no changes made anywhere in HDFS?</p>","tags":["help"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-08-23 13:11:04.0","id":52958,"title":"HDP 2.5 components and release date","body":"<p>Hi</p><p>Does anybody have information about the HDP 2.5 (component version and release date) ?\nSomeone told me that it should occur this summer, but I can't find any confirmation.</p><p>Thanks in advance</p>","tags":["hdp-2.5.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-09-10 14:58:24.0","id":51,"title":"Test Question","body":"","tags":["Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-10 22:57:50.0","id":56,"title":"Test Question from AnswerHub with Ambari Topic","body":"","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-14 20:22:00.0","id":73,"title":"test hipchating integration","body":"<p>testing hipchat integration</p>\n","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-17 15:10:42.0","id":116,"title":"testing hipchat take 1000","body":"<p>testing hipchat take 1000</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-17 16:43:11.0","id":118,"title":"Hipchat Test 10002","body":"<p>Testhis is a testing</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-21 18:51:52.0","id":139,"title":"Yarn vs Mesos","body":"<p>Why is it more difficult to write Mesos applications than slider or yarn native applications? What was the genesis of each of the technologies</p>","tags":["YARN"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-25 00:11:48.0","id":235,"title":"test test for email template","body":"<p>test test for email template</p>","tags":["test"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-15 10:05:05.0","id":23046,"title":"Question on hdfs automatic failover","body":"<p>Hey Guys,</p><p>Consider below scenario:</p><p>1. I have NN HA configured on my cluster</p><p>2. I have configured ssh fencing</p><p>3. My active NN went down and automated failover did not work</p><p>4. I had to failover manually using -forcemanual flag.</p><p>What fencing method we can use so that in case of power failure/physical server crash/OS reboot there would be automated failover? is it possible ?</p>","tags":["HDFS","fail-over"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-03-13 03:54:43.0","id":22709,"title":"Geolocation zip can not be downloaded","body":"<p>An introduction to Hive and Pig</p><h2>Lab 1 - Loading Data into HDFS</h2><h2>Outline</h2><ul><li><a href=\"http://zh.hortonworks.com/hadoop-tutorial/hello-world-an-introduction-to-hadoop-hcatalog-hive-and-pig/#step1.1\">Step 1.1: Download data</a> – <a href=\"https://app.box.com/HadoopCrashCourseData\"><strong>Geolocation.zip</strong></a></li></ul><p>the sample data <strong><a href=\"https://app.box.com/HadoopCrashCourseData\">Geolocation.zip</a> can't be downloaded!!!, here is it's link </strong>https://app.box.com/HadoopCrashCourseData</p>","tags":["Hive"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-05-06 04:58:41.0","id":31828,"title":"Where can I find HDP 2.2 version sandbox for development purpose.","body":"<p>Where can I find HDP 2.2 version sandbox for development purpose.</p>","tags":["hdp-2.2.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-06-14 17:52:26.0","id":39716,"title":"Sqoop Import Network Adaptor Error","body":"<p>Hi ,</p><p>I am doing sqoop import from oracle using below command</p><p>sqoop import --connect jdbc:oracle:thin:@192.168.146.1:1521/orcl \\\n--username scott --password orcl --table emp --target-dir /tmp/test/ --m 1</p><p>But getting below error like</p><p>Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n16/06/14 17:41:29 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5.2.2.0.0-2041\n16/06/14 17:41:29 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n16/06/14 17:41:30 INFO oracle.OraOopManagerFactory: Data Connector for Oracle and Hadoop is disabled.\n16/06/14 17:41:30 INFO manager.SqlManager: Using default fetchSize of 1000\n16/06/14 17:41:30 INFO tool.CodeGenTool: Beginning code generation\n16/06/14 17:42:31 ERROR manager.SqlManager: Error executing statement: java.sql.SQLRecoverableException: IO Error: The Network Adapter could not establish the connection\njava.sql.SQLRecoverableException: IO Error: The Network Adapter could not establish the connection\n        at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:548)\n        at oracle.jdbc.driver.PhysicalConnection.&lt;init&gt;(PhysicalConnection.java:564)\n        at oracle.jdbc.driver.T4CConnection.&lt;init&gt;(T4CConnection.java:251)\n        at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:29)\n        at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:563)\n        at java.sql.DriverManager.getConnection(DriverManager.java:571)\n        at java.sql.DriverManager.getConnection(DriverManager.java:215)\n        at org.apache.sqoop.manager.OracleManager.makeConnection(OracleManager.java:327)\n        at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:52)\n        at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:736)\n        at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:759)\n        at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:269)\n        at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:240)\n        at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:226)\n        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:295)\n        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1773)\n        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1578)\n        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)\n        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)\n        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)\n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)\n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)\n        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)\nCaused by: oracle.net.ns.NetException: The Network Adapter could not establish the connection\n        at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:436)\n        at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:451)\n        at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:897)\n        at oracle.net.ns.NSProtocol.connect(NSProtocol.java:271)\n        at oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:1663)\n        at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:385)\n        ... 25 more\nCaused by: java.net.SocketTimeoutException: connect timed out\n        at java.net.PlainSocketImpl.socketConnect(Native Method)\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n        at java.net.Socket.connect(Socket.java:579)\n        at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:146)\n        at oracle.net.nt.ConnOption.connect(ConnOption.java:130)\n        at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:402)\n        ... 30 more\n16/06/14 17:42:31 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter\n        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1584)\n        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:96)\n        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)\n        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)\n        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)\n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)\n        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)\n        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)</p><p>Please help me on this.</p><p>Regards</p><p>Abhishek</p>","tags":["Sqoop"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-06-13 22:56:08.0","id":39512,"title":"upgrade mysql 5.6.x to mysql 5.6.30/5.6.31","body":"<p>We are using HDP2.2. stack and by default mysql 5.6.x has been bundled. The security team assessment report shows that there are vulnerabilities in mysql 5.6 and the remedy of the issue could be upgrade mysql 5.6.30 or above. I am not sure how to upgrade mysql alone in HDP 2.2 and would be good if any one have such experience of upgrading mysql 5.6.x to 5.6.30 or above. Please reply. </p>","tags":["mysql"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-17 08:48:47.0","id":40384,"title":"Configure pig job logs in Tez view","body":"<p>I am not able to see tez view on my ambari. Additional to that I have configured Pig view which i can see and run jobs perfectly fine. How can I see the pig job logs in Tez View when I set my execution engine as Tez. How to configure Tez view so that it is visible on the tez view instance. Currently I cannot see anything on tez view.</p><p>Attached screenshot<a href=\"/storage/attachments/5097-tez-view.png\">tez-view.png</a></p>","tags":["Pig","ambari-views"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-09-17 17:35:09.0","id":121,"title":"Hipchat test on 2nd node","body":"<p>Just to be sure</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-24 14:38:26.0","id":173,"title":"How to change Knox log directory?","body":"<p>How do I change the directory where Knox  logs are located ?   default is /var/log/knox?</p><p>Customers wants on /var/log/hdp/knox</p>","tags":["Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-09-24 16:24:21.0","id":182,"title":"What are Oozie Production Recommendations?","body":"<p>What our deployment/production recommendations for Oozie?</p><p>Such as:</p><p>- Database recommendations<em> (i.e. don’t use Derby)</em></p><p>- HA considerations</p><p>- Component placement</p><p>- Scaling, required resources, ...</p>","tags":["deployment best practice","Oozie"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-09-24 21:28:21.0","id":203,"title":"Anyone successfully compiled the ambari shell?","body":"<p>I'm a @DevOps guy and like using CLI for quick queries. While I can use the REST API from the command line, I prefer the Ambari shell syntax and subcommands.It abstracts and hides the REST syntax.</p><p>I cloned the git clone <a target=\"_blank\" href=\"https://github.com/sequenceiq/ambari-shell.git\">https://github.com/sequenceiq/ambari-shell.git</a>, but having compilation error due to a java method signature incompatibility. Has anyone seen this error? </p><p>I did a git pull to sync with the repository but to no avail. </p><p>[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 01:18 min\n[INFO] Finished at: 2015-09-24T21:18:37+00:00\n[INFO] Final Memory: 33M/342M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.0:compile (default-compile) on project ambari-shell: Compilation failure\n[ERROR] /root/ambari-shell/src/main/java/com/sequenceiq/ambari/shell/commands/UsersCommands.java:[76,15] cannot find symbol\n[ERROR] symbol:   method changePassword(java.lang.String,java.lang.String,java.lang.String,boolean)\n[ERROR] location: variable client of type com.sequenceiq.ambari.client.AmbariClient</p><p>Any pointers appreciated.</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-24 23:28:43.0","id":224,"title":"Inherit group ownership from user (not directory)?","body":"<p>It it possible for new files/dirs to <strong>inherit group ownership from the user</strong> instead of the parent directory?</p><p>For reference this is how default group permissions are handled:</p><p><em>\"When a file or directory is created, its owner is the user identity of the client process, </em><strong><em>and its group is the group of the parent directory (the BSD rule).”</em></strong></p><p><a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html\">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.h</a></p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-24 23:44:51.0","id":231,"title":"test question","body":"<p>test question for email template</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-24 23:36:32.0","id":226,"title":"test question","body":"<p></p><p>test question to check email tempalte</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 00:30:25.0","id":249,"title":"Spark on HDP?","body":"<p>Can I run Spark on HDP?</p>","tags":["hdp-2.3.4","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-09-25 00:27:17.0","id":245,"title":"HDP vs HDInsights? Pros and cons?","body":"<p>I'm looking to start a Hadoop pilot at my company. I'm not sure if I should use HDP or HDInsights. We are an Azure shop, so we will host in the cloud. Is there a good document that helps me make the right choice?</p>","tags":["hdp-2.3.4","azure","hdinsight"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-09-25 14:35:56.0","id":270,"title":"test test test","body":"<p>test test sdflkjsdlfkj lksdjflksdf jlksdlkfj lkjsdlfkjlksdf</p>","tags":["test"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-25 18:35:37.0","id":307,"title":"Better way the change Logging directories for Ambari-Server and Ambari-Agent?","body":"<p>For Ambari-Server, I currently make adjustments to:</p><p>/etc/ambari-server/conf/log4j.properties </p><p>And for the Agent, I have to create a symlink to redirect /var/log/ambari-agent.</p><p>Is there a better method?</p>","tags":["logs","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 14:08:32.0","id":474,"title":"How do I change the JVM memory settings for Knox","body":"","tags":["Knox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-21 14:51:12.0","id":7489,"title":"Cluster entity is giving error while submitting the falcon entry","body":"<p>I have the following error while submitting the falcon entity cluster.  I have stmp is in HDFS directory already.</p><p>\"</p><p>[hdfs@sandbox ~]$ falcon entity -type cluster -submit -file primary-cluster.xml </p><p>ERROR: Bad Request;Unable to validate the location with path: /stmp for cluster:primary due to transient failures</p><p>\"</p><p>The following is my primary-cluster.xml</p><p>&lt;cluster xmlns='uri:falcon:cluster:0.1' name='primary-cluster' description='primary cluster' colo='toronto'&gt;\n  &lt;tags&gt;class=product,site=pacific-west&lt;/tags&gt;\n  &lt;interfaces&gt;\n    &lt;interface type='readonly' endpoint='hdfs://sandbox.hortonworks.com:8020' version='2.2.0'/&gt;    \n    &lt;interface type='write' endpoint='hdfs://sandbox.hortonworks.com:8020' version='2.2.0'/&gt;    \n    &lt;interface type='execute' endpoint='sandbox.hortonworks.com:8050' version='2.2.0'/&gt;    \n    &lt;interface type='workflow' endpoint='http://sandbox.hortonworks.com:11000/oozie/' version='4.0.0'/&gt;    \n    &lt;interface type='messaging' endpoint='tcp://sandbox.hortonworks.com:61616?daemon=true' version='5.1.6'/&gt;    \n  &lt;/interfaces&gt;\n  &lt;locations&gt;\n    &lt;location name='staging' path='/stmp'/&gt;    \n    &lt;location name='temp' path='/tmp'/&gt;    \n    &lt;location name='working' path='/mtmp'/&gt;    \n  &lt;/locations&gt;\n  &lt;ACL owner='hdfs' group='hdfs' permission='0x755'/&gt;\n&lt;/cluster&gt;</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-01-20 14:09:58.0","id":10802,"title":"Why do we need SQL server installed on the box on which we are setting up HDP for windows?","body":"<p>While installing HDP on Windows Server2012 R2, it gives me the following error:</p><p><em>java.lang.Exception: Could not connect to the database: com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host MDNJAPP02, port 1433 has failed. Error: \"Connection refused: connect. Verify the connection properties, check that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port, and that no firewall is blocking TCP connections to the port.\".</em></p>","tags":["windows"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-20 16:24:31.0","id":10850,"title":"can I configured flume 1.6 version to ambari?","body":"<p>we have flume new version 1.6 version now but Hdp 2.3 doesn't have it and have it 1.5.2 version. can you please suggest me how to configured flume latest version in ambari.</p>","tags":["Flume"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-02-03 02:06:56.0","id":13700,"title":"upload HDP-2.3.4.0-centos6.rmp failed","body":"<p>I know this file is very big one, but I have to upload to HDP server for hdp/ambari installation.</p><p>tried use winscp and ptscp(putty) to upload this file, failed many time, got error like 'timeout', 'network error occured'.</p><p>someone there mush uploaded this file to server, how you guys did it?</p><p>Please share your exp.</p><p>thanks,</p><p>Robin </p>","tags":["error","help","installation"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-02-09 13:59:24.0","id":15624,"title":"Ambari Pig View throws below issue","body":"<pre>Stack Trace\n\njava.net.UnknownHostException: hdpqa\n\njava.net.UnknownHostException: hdpqa\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:589)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\n\tat sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:308)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:326)\n\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1168)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1104)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:998)\n\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:932)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$Runner.connect(WebHdfsFileSystem.java:513)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$Runner.connect(WebHdfsFileSystem.java:506)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$Runner.getResponse(WebHdfsFileSystem.java:595)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$Runner.run(WebHdfsFileSystem.java:530)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlOpener.connect(WebHdfsFileSystem.java:839)\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:119)\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:103)\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.read(ByteRangeInputStream.java:187)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.ambari.view.pig.utils.FilePaginator.readPage(FilePaginator.java:88)\n\tat org.apache.ambari.view.pig.resources.files.FileService.getFile(FileService.java:90)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:715)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)\n\tat org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:182)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)\n\tat org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:198)\n\tat org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:145)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SslConnection.handle(SslConnection.java:196)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n\n</pre>","tags":["ambari-views","hdp-2.3.2","ambari-2.1.2.1"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-14 00:21:39.0","id":6256,"title":"jetty-util-6.1.26.hwx.jar","body":"<p>How does one build Hadoop Auth 2.7.1-SNAPSHOT? As near as I can tell, jetty-util-6.1.26.hwx is unavailable.</p><p>I notice that somebody else had a similar problem, but it hasn't helped me: </p><p><a href=\"http://mail-archives.apache.org/mod_mbox/incubator-flink-issues/201509.mbox/%3Cgit-pr-1113-flink@git.apache.org%3E\">http://mail-archives.apache.org/mod_mbox/incubator-flink-issues/201509.mbox/%3Cgit-pr-1113-flink@git.apache.org%3E</a></p><pre>Following are the build errors:\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Apache Hadoop Auth 2.7.1-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\nDownloading: <a href=\"https://repository.apache.org/content/repositories/snapshots/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.pom\">https://repository.apache.org/content/repositories/snapshots/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.pom\n</a>Downloading: <a href=\"http://repository.jboss.org/nexus/content/groups/public/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.pom\">http://repository.jboss.org/nexus/content/groups/public/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.pom\n</a>Downloading: <a href=\"https://repo.maven.apache.org/maven2/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.pom\">https://repo.maven.apache.org/maven2/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.pom\n</a>[WARNING] The POM for org.mortbay.jetty:jetty-util:jar:6.1.26.hwx is missing, no dependency information available\n[WARNING] The POM for org.mortbay.jetty:jetty:jar:6.1.26.hwx is missing, no dependency information available\nDownloading: <a href=\"https://repository.apache.org/content/repositories/snapshots/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.jar\">https://repository.apache.org/content/repositories/snapshots/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.jar\n</a>Downloading: <a href=\"http://repository.jboss.org/nexus/content/groups/public/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.jar\">http://repository.jboss.org/nexus/content/groups/public/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.jar\n</a>Downloading: <a href=\"https://repo.maven.apache.org/maven2/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.jar\">https://repo.maven.apache.org/maven2/org/mortbay/jetty/jetty-util/6.1.26.hwx/jetty-util-6.1.26.hwx.jar\n</a>[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Apache Hadoop Main ................................. SUCCESS [  0.941 s]\n[INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.677 s]\n[INFO] Apache Hadoop Annotations .......................... SUCCESS [  1.440 s]\n[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.094 s]\n[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  1.603 s]\n[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  1.539 s]\n[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  1.173 s]\n[INFO] Apache Hadoop Auth ................................. FAILURE [ 21.470 s]\n[INFO] Apache Hadoop Auth Examples ........................ SKIPPED\n[INFO] Apache Hadoop Common ............................... SKIPPED\n[INFO] Apache Hadoop NFS .................................. SKIPPED\n[INFO] Apache Hadoop KMS .................................. SKIPPED</pre>","tags":["development"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-14 15:25:42.0","id":6383,"title":"Use of Spark Streaming for interactive Reporting/Visualization","body":"<p>Hi,</p><p>Is SPARK Streaming (may be along with SPARKSQL) suited for interactive querying – to generate reporting dashboards using Tableau?</p><p>We are building a data lake with all our organization’s data in the data lake (as AVRO formatted files). We need to create dashboards & reports using Tableau with the data available in the data lake. The challenge is that some of these reports have to process millions of records and have strict timelines for loading (sometimes as strict as &lt;10 seconds load time for reports). We are right now forced to create an Oracle datamart (populated with the data from the data lake) – from where Tableau pull data to generate reports.</p><p>We want to avoid creating a separate data mart and hence are looking at connecting Tableau directly to the Hadoop Datalake. While pure SPARK is ruled out as the Reports need to be interactive, go to know (from yesterday’s Hortonworks webinar on SPARK) that SPARK Streaming can be used here.\nIs SPARK Streaming (may be along with SPARKSQL) suited for interactive querying – to generate reporting dashboards using Tableau?</p><p>Are there any similar example use cases that you can point me to please?</p><p>thanks,\nRaga</p>","tags":["spark-streaming"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-12-28 18:09:05.0","id":8175,"title":"Registration of one of my server failed because some of the index files failed to download","body":"<p>I enter the latest URL for Ubuntu 14, passed the URL validation but when registering my master node, failed because some of the index files failed to download and old ones are used instead.  So my Slave node register correctly but my master node fail. Can any body help me with this issue?.</p><p>Here is the entire registration log for my master.dev.local node.</p><pre>==========================\nCreating target directory...\n==========================\n\nCommand start time 2015-12-28 18:56:32\n\nConnection to master.dev.local closed.\nSSH command execution finished\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:32\n\n==========================\nCopying common functions script...\n==========================\n\nCommand start time 2015-12-28 18:56:32\n\nscp /usr/lib/python2.6/site-packages/ambari_commons\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:33\n\n==========================\nCopying OS type check script...\n==========================\n\nCommand start time 2015-12-28 18:56:33\n\nscp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.py\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:33\n\n==========================\nRunning OS type check...\n==========================\n\nCommand start time 2015-12-28 18:56:33\nCluster primary/cluster OS family is ubuntu14 and local/current OS family is ubuntu14\n\nConnection to master.dev.local closed.\nSSH command execution finished\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:33\n\n==========================\nChecking 'sudo' package on remote host...\n==========================\n\nCommand start time 2015-12-28 18:56:33\nsudo\t\t\t\t\t\tinstall\n\nConnection to master.dev.local closed.\nSSH command execution finished\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:33\n\n==========================\nCopying repo file to 'tmp' folder...\n==========================\n\nCommand start time 2015-12-28 18:56:33\n\nscp /etc/apt/sources.list.d/ambari.list\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:33\n\n==========================\nMoving file to repo dir...\n==========================\n\nCommand start time 2015-12-28 18:56:33\n\nConnection to master.dev.local closed.\nSSH command execution finished\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:34\n\n==========================\nChanging permissions for ambari.repo...\n==========================\n\nCommand start time 2015-12-28 18:56:34\n\nConnection to master.dev.local closed.\nSSH command execution finished\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:34\n\n==========================\nUpdate apt cache of repository...\n==========================\n\nCommand start time 2015-12-28 18:56:34\n0% [Working]\n            \nHit http://ppa.launchpad.net trusty InRelease\n\n            \n0% [Waiting for headers]\n                        \n0% [InRelease gpgv 15,5 kB] [Waiting for headers]\n                                                 \n12% [Waiting for headers]\n                         \nHit http://ppa.launchpad.net trusty/main amd64 Packages\n\n12% [Waiting for headers]\n                         \n12% [Packages 19,5 kB] [Waiting for headers] [Waiting for headers]\n                                                                  \n100% [Waiting for headers] [Waiting for headers]\n                                                \nHit http://public-repo-1.hortonworks.com Ambari InRelease\n\n                                                \nHit http://ppa.launchpad.net trusty/main i386 Packages\n\n100% [Waiting for headers] [Waiting for headers]\n                                                \n100% [InRelease gpgv 3.187 B] [Waiting for headers] [Waiting for headers]\n                                                                         \n100% [Packages 19,5 kB] [InRelease gpgv 3.187 B] [Waiting for headers] [Waiting\n                                                                               \n100% [InRelease gpgv 3.187 B] [Waiting for headers] [Waiting for headers]\n                                                                         \n100% [Waiting for headers] [Waiting for headers]\n                                                \nHit http://ppa.launchpad.net trusty/main Translation-en\n\n                                                \n100% [Waiting for headers]\n                          \n100% [Translation-en 6.192 B] [Waiting for headers]\n                                                   \n100% [Waiting for headers]\n                          \nHit http://public-repo-1.hortonworks.com HDP-UTILS InRelease\n\n100% [Waiting for headers]\n                          \n100% [InRelease gpgv 7.439 B] [Waiting for headers]\n                                                   \n100% [Waiting for headers]\n                          \nIgn http://public-repo-1.hortonworks.com HDP InRelease\n\n                          \n100% [Working]\n              \nHit http://public-repo-1.hortonworks.com Ambari/main amd64 Packages\n\n              \n100% [Waiting for headers]\n100% [Waiting for headers]\n                          \nHit http://public-repo-1.hortonworks.com Ambari/main i386 Packages\n\n                          \n100% [Working]\n              \n100% [Waiting for headers]\n100% [Waiting for headers]\n100% [Waiting for headers]\n100% [Waiting for headers]\n                          \nHit http://public-repo-1.hortonworks.com HDP-UTILS/main amd64 Packages\n\n100% [Waiting for headers]\n100% [Waiting for headers]\n                          \nHit http://public-repo-1.hortonworks.com HDP-UTILS/main i386 Packages\n\n100% [Waiting for headers]\n100% [Waiting for headers]\n100% [Waiting for headers]\n                          \nIgn http://public-repo-1.hortonworks.com HDP Release.gpg\n\n                          \n100% [Working]\n              \n100% [Waiting for headers]\n                          \nIgn http://public-repo-1.hortonworks.com HDP Release\n\n                          \n100% [Working]\n              \n100% [Waiting for headers]\n100% [Waiting for headers]\n100% [Waiting for headers]\n                          \n100% [Waiting for headers]                                         14,5 kB/s 0s\n100% [Waiting for headers]                                         14,5 kB/s 0s\n100% [Waiting for headers]                                         14,5 kB/s 0s\n100% [Waiting for headers]                                         14,5 kB/s 0s\n100% [Waiting for headers]                                         14,5 kB/s 0s\n                                                                               \nIgn http://public-repo-1.hortonworks.com Ambari/main Translation-en_US\n\n100% [Waiting for headers]                                         14,5 kB/s 0s\n                                                                               \nIgn http://public-repo-1.hortonworks.com Ambari/main Translation-en\n\n100% [Working]                                                     14,5 kB/s 0s\n                                                                               \nIgn http://public-repo-1.hortonworks.com HDP-UTILS/main Translation-en_US\n\n100% [Working]                                                     14,5 kB/s 0s\n                                                                               \nIgn http://public-repo-1.hortonworks.com HDP-UTILS/main Translation-en\n\n100% [Working]                                                     14,5 kB/s 0s\n100% [Waiting for headers]                                         14,5 kB/s 0s\n100% [Waiting for headers]                                         14,5 kB/s 0s\n100% [Waiting for headers]                                         14,5 kB/s 0s\n                                                                               \nErr http://public-repo-1.hortonworks.com HDP/main amd64 Packages\n  404  Not Found [IP: 54.230.218.83 80]\n\n100% [Working]                                                     14,5 kB/s 0s\n                                                                               \nErr http://public-repo-1.hortonworks.com HDP/main i386 Packages\n  404  Not Found [IP: 54.230.218.83 80]\n\n100% [Working]                                                     14,5 kB/s 0s\n                                                                               \nIgn http://public-repo-1.hortonworks.com HDP/main Translation-en_US\n\n100% [Working]                                                     14,5 kB/s 0s\n                                                                               \nIgn http://public-repo-1.hortonworks.com HDP/main Translation-en\n\n100% [Working]                                                     14,5 kB/s 0s\n                                                                               \nW: Failed to fetch http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.0.0/dists/HDP/main/binary-amd64/Packages  404  Not Found [IP: 54.230.218.83 80]\n\nW: Failed to fetch http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.0.0/dists/HDP/main/binary-i386/Packages  404  Not Found [IP: 54.230.218.83 80]\n\nE: Some index files failed to download. They have been ignored, or old ones used instead.\n\nConnection to master.dev.local closed.\nSSH command execution finished\nhost=master.dev.local, exitcode=100\nCommand end time 2015-12-28 18:56:46\n\n==========================\nCopying setup script file...\n==========================\n\nCommand start time 2015-12-28 18:56:46\n\nscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py\nhost=master.dev.local, exitcode=0\nCommand end time 2015-12-28 18:56:46\n\nERROR: Bootstrap of host master.dev.local fails because previous action finished with non-zero exit code (100)\nERROR MESSAGE: Execute of '&lt;bound method BootstrapDefault.copyNeededFiles of &lt;BootstrapDefault(Thread-1, started daemon 140058271373056)&gt;&gt;' failed\nSTDOUT: Try to execute '&lt;bound method BootstrapDefault.copyNeededFiles of &lt;BootstrapDefault(Thread-1, started daemon 140058271373056)&gt;&gt;'</pre>","tags":["cluster","logging"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-27 20:01:48.0","id":8111,"title":"ubuntu14 repository not available","body":"<p>Ubunrtu14 repository does not pass url validation. When will be available?</p>","tags":["help","hdp-2.3.0","ubuntu"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-02-19 01:44:10.0","id":18070,"title":"we are going to extract data from teradata to Hortonworks for an initial load. I was wondering what would be the fast and efficient way to do this load?","body":"<p>I need hundreds tables from teradata, so I need to create all these in hive tables first and all data can be directly import to hive tables, shall we? if troubleshooting I need aware? </p>","tags":["Hive","teradata"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-25 10:44:10.0","id":19489,"title":"hive.exec.post.hooks Class not found:org.apache.atlas.hive.hook.HiveHook FAILED: Hive Internal Error: java.lang.ClassNotFoundException(org.apache.atlas.hive.hook.HiveHook)","body":"<p>Hi Team,</p><p>My hive action with oozie is failing with below error.</p><p>hive.exec.post.hooks Class not found:org.apache.atlas.hive.hook.HiveHook FAILED: Hive Internal Error: java.lang.ClassNotFoundException(org.apache.atlas.hive.hook.HiveHook)</p><p>Please note that this error started after upgrade only from 2.2 to 2.3.</p><p>I have implemented below solution as well but still getting error, can anyone please help me to get it resolve. </p><p>http://stackoverflow.com/questions/32759933/hive-internal-error-java-lang-classnotfoundexceptionorg-apache-atlas-hive-hook</p>","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-02-25 13:48:36.0","id":19517,"title":"we didnt make hdp/ambari without internet at work. so I am wondering if I can install hdp/ambari without internet with AWS/EC2, any one done this before?","body":"<p>1. Even though, I installed ambari on AWS/EC2, it is working for testing purpose. but our company need to install hdp/ambari without internet access.</p><p>2. I am not familiar with http so I think this would be my question before I install ambari without internet with AWS/EC2.</p><p>Basically , with AWS/EC2,  ambari need to be accessed by http://aws_masternode_id:8080, so this is public ip, if need install without internet,  URL should be private ip here, right? then how can I access http://aws_masternode_private_id:8080 here?</p><p>Please let me know if any confusion here.</p><p>please help me here.</p>","tags":["installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-03-09 01:34:29.0","id":22042,"title":"How to add custom stack in Ambari","body":"<p>How can i add custom stack in Ambari?I'm failed to add my custom hdfs in my custom stack</p>","tags":["installation","Ambari"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-03-09 15:10:39.0","id":22126,"title":"Possible to contribute by building spark-core for new HDP versions?","body":"<p>So we have upgraded our Hadoop HDP through Ambari and I was looking to use same version in our scala project, but noticed its not been built in http://repo.hortonworks.com/content/repositories/releases/org/apache/spark/</p><p>is it possible to contribute this or whats the procedure?</p>","tags":["development","hadoop"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-01-19 09:56:26.0","id":10525,"title":"Sandbox - web ui problem","body":"<p>Hi,</p><p>when i setup NAT or bridged connection inside the oracle Virtualbox the local :8080 /:8000 /:8888 web ui doesnt work!</p><p>Any idea? Im really tired of that sanbox, how can one works with it if its fully of bugs.</p>","tags":["ui","help"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-18 20:03:28.0","id":10429,"title":"Many Ambari \"stale alerts\" messages","body":"<p>Hi all,</p><p>Last night I got many of the following Ambari critical alerts:</p><p><em>There are {x} stale\nalerts from {n} host(s): {components list}</em></p><p>where {x}, {n} and {components list} were not always the same. For example:</p><p><em>There are 20 stale\nalerts from 1 host(s): NameNode Web UI, Metrics Monitor Status, WebHCat Server\nStatus, NameNode High Availability Health, HST Server Process, NameNode Last\nCheckpoint, Flume Agent Status, Oozie Server Status, ZooKeeper Failover Controller\nProcess, HBase Master Process, ResourceManager Web UI, HDFS Upgrade Finalized\nState, Ambari Agent Disk Usage, NameNode Directory Status, DataNode Health\nSummary, Oozie Server Web UI, DRPC Server Process, NodeManager Health Summary,\nRegionServers Health Summary, HiveServer2 Process </em></p><p>After 6 minutes, Ambari sent an OK alerts:</p><p><em>All alerts have run\nwithin their time intervals.</em></p><p>These messages repeated over and over again (13 critical, then 13 OK in 5 hours). This is the first time I see so many alerts from our cluster in one single night and all the services are fine from Ambari this morning. No more alerts either.</p><p>Does anybody have any insight what might cause this?</p><p>Thank you very much in advance!</p><p>Xi Sanderson</p>","tags":["ambari-alerts"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-06-07 10:13:43.0","id":38116,"title":"Hive query failing with org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.RuntimeException: serious problem.","body":"<p>Team,</p><p>My simple query is failing with following error when I am running on external portioned table though beeline. </p><p>0: jdbc:hive2://server1:8443/default&gt; select * from testtable where lct_nbr=2413 and ivo_nbr in (17469,18630);</p><p>Error: java.io.IOException: java.lang.RuntimeException: serious problem (state=,code=0)</p><p>I found following in hiveserver2 log. </p><p>Caused by: java.io.IOException: java.lang.RuntimeException: serious problem\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)\nat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)\nat org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:347)\n... 43 more\nCaused by: java.lang.RuntimeException: serious problem\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1059)\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1086)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:363)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:295)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:446)\n... 47 more</p>","tags":["Hive","help"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-29 21:24:41.0","id":562,"title":"what the difference between src and source rpm in the HDP stack binary repo?","body":"","tags":["help"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-09-30 12:51:45.0","id":602,"title":"ORC support for Spark on HDP 2.3","body":"<p>The release notes for HDP 2.3 are suggesting that Spark support for ORC is not available. I was under the impression that it was supported as part of HDP 2.1 onwards. </p><p>Is it supported to read it but not write it or is it not at all supported ? </p><p>http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_HDP_RelNotes/content/ch01s02s01.html</p>","tags":["hdp-2.3.0","Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-09-30 14:01:29.0","id":617,"title":"Is it possible to add another Kafka broker to my cluster through Ambari?","body":"<p>The cluster was initially installed with the broker only on one node. Is there a way to install and register additional brokers in Ambari after the fact?</p>","tags":["Kafka","ambari-2.1.0"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-04-11 15:35:19.0","id":26978,"title":"Can we copy entire table or database with data and metadata through falcon ?","body":"<p>Team:</p><p>I have requirement where I have to copy data between cluster from one table to another table. So is it possible through falcon ?</p><p>I was reading following articles but I am lil bit confused about them. </p><p>https://falcon.apache.org/HiveIntegration.html</p><p>http://hortonworks.com/blog/introduction-apache-falcon-hadoop/ here example 3. </p>","tags":["Falcon"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-03 07:17:31.0","id":2660,"title":"Issue with Spark on Hive.","body":"<p>I can run Queries using hive with the map reduce engine, but when running hive on spark it fails giving an expection:</p><p>Failed to execute spark task, with exception: 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'</p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-11-20 16:46:11.0","id":4421,"title":"Error: Unsupported distributable during SmartSense Ambari install","body":"<p>When running the command \" hst add-to-ambari \" , following the prompt we added the path to rpm and receive the error. We are using ambari 2.1.2 and HDP 2.3.2.0-2950, and the rpm is: smartsense-hst-1.1.0-119.x86_64.rpm. Our O/S is RHEL 7.</p>","tags":["smartsense"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-24 15:17:06.0","id":4646,"title":"Is there a RACI Matrix for hadoop operations available?","body":"<p>I need a RACI matrix for Hadoop operations which I can copy with pride (). Anything available? https://en.wikipedia.org/wiki/Responsibility_assignment_matrix</p>","tags":["operations"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-11-24 23:10:29.0","id":4759,"title":"Hive Explain says \"Plan not optimized by CBO due to missing statistics\" but stats are present. What gives?","body":"<p>I'm running an explain on a query and the result includes \"Plan not optimized by CBO due to missing statistics. Please check log for more details\"</p><p>1. All the tables in the query have had compute statistics run on them and the describe formatted output shows that stats are present and up to date. What is missing?</p><p>2. Which log file is the message referring to? I looked in Hive the hiveserver2.log and can see the log entries for the explain command here but there's no explanation on what stats it thinks are missing.</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-02 12:41:46.0","id":5047,"title":"Falcon fails to create feed with HDFS permission error","body":"<p>Hi,</p><p>our Falcon installation abruptly ceased to work and no feed could be created. It complained about file permission</p><pre>Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=kefi, access=WRITE, inode=\"/apps/falcon-MiddleGate/staging/falcon/workflows/feed\":middlegate_test1:falcon:drwxr-xr-x\n   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)\n   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)\n   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:238)\n   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:179)\n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6515)\n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6497)\n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6449)\n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4251)\n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4221)\n   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4194)\n   at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:813)\n   at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:600)\n   at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n   at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n   at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n   at java.security.AccessController.doPrivileged(Native Method)\n   at javax.security.auth.Subject.doAs(Subject.java:415)\n   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n   at org.apache.hadoop.ipc.Server$Handler.run(Server.ja</pre><p>where 'kefi' is the user trying to create the feed and 'middlegate_test1' is another user that created some feed before.</p><p>the folders on hdfs looked like this</p><pre>bash-4.1$ hadoop fs -ls /apps/falcon-MiddleGate/staging/falcon/workflows/\nFound 2 items\ndrwxr-xr-x   - middlegate_test1 falcon          0 2015-12-02 09:13 /apps/falcon-MiddleGate/staging/falcon/workflows/feed\ndrwxrwxrwx   - middlegate_test1 falcon          0 2015-12-02 09:13 /apps/falcon-MiddleGate/staging/falcon/workflows/process\n</pre><p>I can think of two questions related to this:</p><ul><li>why the permission for the 'feed' folder is now 'drwxr-xr-x' whereas the 'process' folder has permissions  'drwxrwxrwx'? The feed creation has been working before, so I guess someone or something had to change it. It is not very probable that some user did it manually, is it possible that it was falcon itself who did it?</li><li>it does not seem correct that such internal falcon system folders are owned by some normal user; quite probably the first one who ever tried to create some entity in falcon. Is it as expected, or rather some our misconfiguration of falcon?</li></ul><p>Thanks for any input,</p><p>Regards,</p><p>Pavel</p>","tags":["Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-10 20:26:43.0","id":5906,"title":"How can we create \"configuration groups\" using API?","body":"<p>I would like to create a host specific configuration group to limit some changes to core-site configuration on a specif host. How can I achive this using API?</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-16 12:30:32.0","id":6752,"title":"Where and how can I find the peer.adr value in HBase?","body":"<p>Hi, </p><p>I need to copy an exiting table from one cluster to another. I will be using the </p><pre>hbase org.apache.hadoop.hbase.mapreduce.CopyTable </pre><p>command to achieve this operation however i need to know a parameter which I should pass along with it i.e peer.adr. Do any body has got any clue on how to get this information from Hbase?</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-18 18:30:02.0","id":7287,"title":"ParseException line 1:162 cannot recognize input near '' '' '' in statement","body":"<p>I am getting error when running the below query ;</p><p>Error occurred executing hive query: Error while compiling statement: FAILED: ParseException line 1:162 cannot recognize input near '&lt;EOF&gt;' '&lt;EOF&gt;' '&lt;EOF&gt;' in statement</p><p>With Q as (select CLAIMNUMBER,EXP_ID,EXP_COVERAGE,VEHICLE_STYLE From orc_claiminfo Where VEHICLE_STYLE != \"\" AND EXP_COVERAGE =\"Property Damage - Vehicle Damage\");</p><p>\nselect * from Q;</p><p>Select statement works all well, if I execute separate.</p><p>Anyone knows whats the issue??</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-10-06 19:47:08.0","id":1008,"title":"sme-nif hipchat test","body":"<p>test</p><p>test</p><p>test</p>","tags":["dataflow"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-19 18:55:38.0","id":1677,"title":"Bringing ranger under ambari","body":"<p>What is the best way to upgrade and bring Ranger under Ambari? </p><p>Updating manually from 2.2 (where ranger was not managed under ambari) to 2.1.2 and 2.3.2. Then run install via ambari. Want to ensure this will not affect the LDAP configurations already in place, or do we still run the script ranger-migration-util.tar.gz?</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-21 18:47:41.0","id":1835,"title":"adding premium storage to existing Azure VM","body":"<p>I'm trying to add premium storage to an existing cluster vm. I created a storage account with premium storage but I don't see an option where I can add these disks to an existing vm.</p>","tags":["azure"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2015-10-26 17:24:56.0","id":2129,"title":"How do I configure HDFS storage types and policies?","body":"<p>According to the Apache and our own documentation, I would use the hdfs dfsadmin -setStoragePolicy and -getStoragePolicy commands to configure and use HDFS storage types and policies. However, on my HDP 2.3.0 cluster, installed using Ambari 2.1.1, the hdfs dfsadmin command does not have the -getStoragePolicy and -setStoragPolicy commands. So I do I configure storage types and policies?</p>","tags":["hdfs-policies"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-04 23:04:29.0","id":871,"title":"Upgrading MetaStore schema from 0.12.0 to 0.13.0","body":"<pre>While upgrading HDP 2.0 to HDP 2.1 and Metastore Schema from 0.12 to 0.13 I got the Error: Duplicate column name ‘OWNER_NAME’ (state=42S21,code=1060). The Metastore version is 0.12 in the VERSION Table however the ‘OWNER_NAME’ column in the ‘DBS’ table already exists.</pre><p>Here is the detailed error:</p><pre>+———————————————+\n| &lt; HIVE-6386: Add owner filed to database &gt; |\n+———————————————+\n1 row selected (0.001 seconds)\n0: jdbc:mysql://hadoop.domain&gt; ALTER TABLE &lt;code&gt;DBS</pre>\n ADD <code>OWNER_NAME</code> varchar(128)\nError: Duplicate column name ‘OWNER_NAME’ (state=42S21,code=1060)\nClosing: 0: jdbc:mysql://hadoop.domain/hive?createDatabaseIfNotExist=true\norg.apache.hadoop.hive.metastore.HiveMetaException: Upgrade FAILED! Metastore state would be inconsistent !!\norg.apache.hadoop.hive.metastore.HiveMetaException: Upgrade FAILED! Metastore state would be inconsistent !!\nat org.apache.hive.beeline.HiveSchemaTool.doUpgrade(HiveSchemaTool.java:242)\nat org.apache.hive.beeline.HiveSchemaTool.doUpgrade(HiveSchemaTool.java:211)\nat org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:489)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.io.IOException: Schema script failed, errorcode 2\nat org.apache.hive.beeline.HiveSchemaTool.runBeeLine(HiveSchemaTool.java:377)\nat org.apache.hive.beeline.HiveSchemaTool.runBeeLine(HiveSchemaTool.java:350)\nat org.apache.hive.beeline.HiveSchemaTool.doUpgrade(HiveSchemaTool.java:237)\n… 7 more\n*** schemaTool failed ***<p>Has anyone run into the same issue? Any idea what is the source of this problem?</p>","tags":["Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-27 15:50:25.0","id":2202,"title":"Ranger Audit Options - Is DB Audit still supported in HDP 2.3+?","body":"<p>Our customer is planning to take advantage of the new Apache Solr auditing capability in HDP 2.3. </p><p>They would also like to keep their exisitng MySQL DB auditing in place.\nIn HDP 2.3+ is the DB Auditing still supported (deprecated)? Or will we be dropping support for this going forward? </p><p>Thank you! </p><p>References: </p><p>http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Ranger_Install_Guide/content/ranger_database_settings.html\nhttp://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_Ranger_Install_Guide/content/ch_install_solr.html</p>","tags":["hdp-2.3.0","Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-11-18 13:47:19.0","id":4215,"title":"Hive GUI !","body":"<p>Hello,</p><p>I have a customer wondering what can they use as Hive GUI (but not based on web technology, so not Ambari view or Hue)</p><p>I was thinking of / found http://squirrel-sql.sourceforge.net/ and https://www.toadworld.com/products/toad-for-hadoop and probably eclipse with JDBC ? Any feedback is welcome.</p><p>Thanks and kind regards.</p>","tags":["ui","Hive"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-01 13:52:19.0","id":4978,"title":"HA Wizard: can't initialize JournalNodes","body":"<p>On Ambari 2.1.2 & HDP 2.3.2</p><ol><li>Open enable HA wizard</li><li>Go through steps</li><li>On the step \"Manual Steps Required: Initialize JournalNodes\"</li>\n<li>When performing command<pre>sudo su -l hdfs -c 'hdfs namenode -initializeSharedEdits'\n</pre>\n<p>\"Next\" button is not enabled.</p></li></ol>","tags":["namenode-ha","ambari-2.1.2"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-12-09 23:46:28.0","id":5719,"title":"Unable to start Haddop services on Sandbox","body":"<p>I am running a sandbox with HDP 2.2.4.2. on VMware. after starting when I start all hadoop services, the services do not start. The error reported is:</p><pre>Fail: Execution of 'ls /var/run/hadoop-yarn/yarn/yarn-yarn-timelineserver.pid &gt;/dev/null 2&gt;&1 && ps -p `cat /var/run/hadoop-yarn/yarn/yarn-yarn-timelineserver.pid` &gt;/dev/null 2&gt;&1' returned 1. \n2015-12-09 23:13:07,046 - Command: /usr/bin/hdp-select status hadoop-yarn-timelineserver &gt; /tmp/tmp6_S5Dp\nOutput: hadoop-yarn-timelineserver - 2.2.4.2-2\nWhen single hdfs service is started it shows success but within few minutes the service is stopped again. but no error is reported. the log file in /var/log/hadoop/hdfs/hadoop-hdfs-namenode-sandbox.hortonworks.com.log shows following entries:\n2015-12-09 23:43:13,572 INFO  namenode.FSNamesystem (FSNamesystem.java:listCorruptFileBlocks(7509)) - there are no corrupt file blocks.\n2015-12-09 23:43:14,014 INFO  namenode.FSNamesystem (FSNamesystem.java:listCorruptFileBlocks(7509)) - there are no corrupt file blocks.\n2015-12-09 23:43:17,678 INFO  provider.AsyncAuditProvider (AsyncAuditProvider.java:logSummaryIfRequired(215)) - AsyncAuditProvider-stats:HdfsAuditProvider: past 01:00.008 minutes: inLogs=36, outLogs=36, dropped=0, currentQueueSize=0\n2015-12-09 23:43:17,678 INFO  provider.AsyncAuditProvider (AsyncAuditProvider.java:logSummaryIfRequired(221)) - AsyncAuditProvider-stats:HdfsAuditProvider: process lifetime: inLogs=630, outLogs=630, dropped=0\n2015-12-09 23:43:17,742 INFO  provider.AsyncAuditProvider (AsyncAuditProvider.java:logSummaryIfRequired(215)) - AsyncAuditProvider-stats:DbAuditProvider: past 01:00.015 minutes: inLogs=36, outLogs=36, dropped=0, currentQueueSize=0\n2015-12-09 23:43:17,742 INFO  provider.AsyncAuditProvider (AsyncAuditProvider.java:logSummaryIfRequired(221)) - AsyncAuditProvider-stats:DbAuditProvider: process lifetime: inLogs=631, outLogs=631, dropped=0\n2015-12-09 23:43:19,145 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(178)) - Rescanning after 30001 milliseconds\n2015-12-09 23:43:19,145 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(201)) - Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).\n2015-12-09 23:43:20,079 INFO  namenode.FSNamesystem (FSNamesystem.java:listCorruptFileBlocks(7509)) - there are no corrupt file blocks.\n2015-12-09 23:43:20,524 INFO  namenode.FSNamesystem (FSNamesystem.java:listCorruptFileBlocks(7509)) - there are no corrupt file blocks.\n2015-12-09 23:43:21,384 INFO  httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(439)) - I/O exception (java.net.ConnectException) caught when processing request: Connection refused\n2015-12-09 23:43:21,385 INFO  httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(445)) - Retrying request\n2015-12-09 23:43:21,385 INFO  httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(439)) - I/O exception (java.net.ConnectException) caught when processing request: Connection refused\n2015-12-09 23:43:21,385 INFO  httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(445)) - Retrying request\n2015-12-09 23:43:21,386 INFO  httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(439)) - I/O exception (java.net.ConnectException) caught when processing request: Connection refused\n2015-12-09 23:43:21,386 INFO  httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(445)) - Retrying request\n2015-12-09 23:43:21,386 WARN  timeline.HadoopTimelineMetricsSink (HadoopTimelineMetricsSink.java:putMetrics(206)) - Unable to send metrics to collector by address:http://sandbox.hortonworks.com:6188/ws/v1/timeline/metrics\n[root@sandbox hdfs]#</pre>","tags":["hadoop","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-10 17:29:44.0","id":5854,"title":"Getting a Pig Error \"Only sequential read is supported\" from an OrcStorage table","body":"<p>Getting a pig Error \"Only sequential read is supported\"  It involved using data from an OrcStorage table. </p><p>When I loaded the orc table, stored it using PigStorage, and reran the job loading from the PigStorage folder, the job worked.</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-15 14:08:43.0","id":6519,"title":"ResourceManager cannot start","body":"<p>After building out a HDP 2.2 cluster (single node) using blueprint I'm getting the following error around the ResourceManager. </p><pre>$ less /var/log/hadoop-yarn/yarn/yarn-yarn-resourcemanager-gsc01-ost-tesla-h-hb01.td.local.log\nSTARTUP_MSG: Starting ResourceManager\nSTARTUP_MSG:   host = gsc01-ost-tesla-h-hb01.td.local/192.168.106.26\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 2.6.0.2.2.9.0-3393\n...\n2015-12-15 01:01:47,671 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service RMActiveServices failed in state INITED; cause: java.lang.IllegalArgumentException: Illegal capacity of -1.0 for node-label=default in queue=root, valid capacity should in range of [0, 100].\njava.lang.IllegalArgumentException: Illegal capacity of -1.0 for node-label=default in queue=root, valid capacity should in range of [0, 100].\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.internalGetLabeledQueueCapacity(CapacitySchedulerConfiguration.java:465)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getLabeledQueueCapacity(CapacitySchedulerConfiguration.java:477)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils.loadCapacitiesByLabelsFromConf(CSQueueUtils.java:143)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils.loadUpdateAndCheckCapacities(CSQueueUtils.java:122)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue.setupConfigurableCapacities(AbstractCSQueue.java:99)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue.setupQueueConfigs(AbstractCSQueue.java:242)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setupQueueConfigs(ParentQueue.java:109)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.&lt;init&gt;(ParentQueue.java:100)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:589)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:465)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:297)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:326)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:576)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1016)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:269)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1237)\n2015-12-15 01:01:47,672 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping ResourceManager metrics system...\n</pre><p>My blueprint file is intentionally sparse so I'm only calling out components without setting any configurations unless needed.</p><pre>{\n  \"host_groups\" : [\n    {\n      \"name\" : \"host_group_1\",\n      \"configurations\" : [ ],\n      \"components\" : [\n        { \"name\" : \"ZOOKEEPER_SERVER\" },\n        { \"name\" : \"ZOOKEEPER_CLIENT\" },\n\n...\n\n  ],\n  \"Blueprints\" : {\n    \"stack_name\" : \"HDP\",\n    \"stack_version\" : \"2.2\"\n  }</pre><p>I suspect this message a bit up in the logs might be related:</p><pre>2015-12-15 01:01:47,598 INFO  conf.Configuration (Configuration.java:getConfResourceAsInputStream(2236)) - found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\n2015-12-15 01:01:47,663 WARN  capacity.CapacitySchedulerConfiguration (CapacitySchedulerConfiguration.java:getAccessibleNodeLabels(433)) - Accessible node labels for root queue will be ignored, it will be automatically set to \"*\".\n2015-12-15 01:01:47,668 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler failed in state INITED; cause: java.lang.IllegalArgumentException: Illegal capacity of -1.0 for node-label=default in queue=root, valid capacity should in range of [0, 100].</pre><p>Looking in the mentioned .xml file:</p><pre>    &lt;property&gt;\n      &lt;name&gt;yarn.scheduler.capacity.root.accessible-node-labels.default.capacity&lt;/name&gt;\n      &lt;value&gt;-1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n      &lt;name&gt;yarn.scheduler.capacity.root.accessible-node-labels.default.maximum-capacity&lt;/name&gt;\n      &lt;value&gt;-1&lt;/value&gt;\n    &lt;/property&gt;</pre><p>Do I just need to set these in my blueprint file?</p><p><strong>NOTE:</strong> Here's the full .xml file: <a href=\"/storage/attachments/836-capacity-schedulerxml.txt\">capacity-schedulerxml.txt</a></p><p><strong>EDIT #1</strong></p><p>I took these 2 properties out of the above .xml file and attempted to restart ResourceManager, but it's still throwing the same exception:</p><pre>2015-12-15 10:40:51,231 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(1241)) - Error starting ResourceManager\njava.lang.IllegalArgumentException: Illegal capacity of -1.0 for node-label=default in queue=root, valid capacity should in range of [0, 100].\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.internalGetLabeledQueueCapacity(CapacitySchedulerConfiguration.java:465)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getLabeledQueueCapacity(CapacitySchedulerConfiguration.java:477)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils.loadCapacitiesByLabelsFromConf(CSQueueUtils.java:143)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils.loadUpdateAndCheckCapacities(CSQueueUtils.java:122)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue.setupConfigurableCapacities(AbstractCSQueue.java:99)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue.setupQueueConfigs(AbstractCSQueue.java:242)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setupQueueConfigs(ParentQueue.java:109)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.&lt;init&gt;(ParentQueue.java:100)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:589)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:465)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:297)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:326)\nat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\nat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:576)\nat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1016)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:269)\nat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1237)\n2015-12-15 10:40:51,233 INFO  resourcemanager.ResourceManager (LogAdapter.java:info(45)) - SHUTDOWN_MSG:</pre>","tags":["resource-manager","Ambari"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2015-12-16 09:44:42.0","id":6707,"title":"Ambari Server Recovery/Reconfigure","body":"<p>Hi Team,</p><p>How to recover / reconfigure Ambari server after crashed without backup?</p><p>Regards,</p><p>NileshP</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-29 23:28:04.0","id":2450,"title":"Is is possible to run Storm topology which runs in one cluster to a remote cluster? If so how?","body":"","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2015-10-31 21:54:44.0","id":2572,"title":"Kerberos local KDC - Error 401 Authentication required","body":"<p>This is the Ambari error we receive when we attempt to restart the services.</p><pre>raise Fail(err_msg) \nresource_management.core.exceptions.Fail: Execution of 'curl -sS -L -w '%{http_code}' -X GET --negotiate -u : '<a href=\"http://devehdp001.unix.gsm1900.org:50070/webhdfs/v1/user/hcat?op=GETFILESTATUS&user.name=hdfs\">http://devehdp001.unix.xxxx1900.org:50070/webhdfs/v1/user/hcat?op=GETFILESTATUS&user.name=hdfs</a>'' returned status_code=401. \n&lt;html&gt; \n&lt;head&gt; \n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\"/&gt; \n&lt;title&gt;Error 401 Authentication required&lt;/title&gt; \n&lt;/head&gt; \n&lt;body&gt;&lt;h2&gt;HTTP ERROR 401&lt;/h2&gt; \n&lt;p&gt;Problem accessing /webhdfs/v1/user/hcat. Reason: \n&lt;pre&gt; Authentication required&lt;/pre&gt;&lt;/p&gt;&lt;hr /&gt;&lt;i&gt;&lt;small&gt;Powered by Jetty://&lt;/small&gt;&lt;/i&gt;&lt;br/&gt; </pre>","tags":["kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-12-10 21:36:00.0","id":5931,"title":"Datanode failure","body":"<p>Under what circumstances we could notice  map-reduce job getting failed/terminated when one of datanode goes down ?</p>","tags":["datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-11 14:09:14.0","id":6044,"title":"Limiting Sqoop imports to some time intervals","body":"<p>Hi,</p><p>we are trying  to setup ingest of data from databases into Hive using Sqoop. Problem is that databases are used in production and we cannot use them too heavily during some working hours. There are many tables and some of them are quite huge ( &gt; 2 GRows)  so it is probable that we cannot ingest them all during the time window available. It is difficult to create some general delta queries that would run some given amount of time and no longer.</p><p>I am thinking about possibility to implement such feature directly into Sqoop. I am not very familiar with Sqoop implementation, but I guess there is some loop where a row gets loaded from JDBC resultset, converted and stored into Hive table. </p><p>All that would be required is to place a check in this loop and wait/sleep for some time if this is happening during the database working hours. This way the ingest will run at the full speed outside of the working hours and will be significantly reduced (but still running) when database is not supposed to be overloaded.</p><p>What do you think? Does this sound like a feature that could be useful to someone else as well?</p><p>Thanks,</p><p>Pavel</p>","tags":["Sqoop"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2015-12-12 13:28:21.0","id":6159,"title":"Is there any Input format available in framework to write  MapReduce Program such that 'n' lines of Input file(text file) goes to one mapper and next 'n+1' lines goes to another mapper 'n+2' goes to another...and so on .","body":"","tags":["MapReduce"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-16 14:50:39.0","id":6808,"title":"Hive support for Line Terminator other than '\\n' when creating table","body":"<p>I have XML data in one of DB fields which has '\\n' in it. So, I need to use an alternate Line Terminator. </p><p>I get following error when I try using a different one.</p><p>FAILED: SemanticException 13:20 LINES TERMINATED BY only supports newline '\\n' right now. Error encountered near token '';''</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 07:25:26.0","id":7020,"title":"How to create auto increment key for a table in hive?","body":"","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2015-12-17 10:05:17.0","id":7038,"title":"Can we upload a blueprint with Ha enabled?","body":"<p>I'm implemented a way to spawn a cluster in an automate way (launching VMS, install all the required RPMs and configure the cluster, using Ansible). The final part of configuring the cluster is done via blueprint upload through the API. I tried to apply a blueprint with NameNode and RM HA enabled, but after all the services are started, I have some alerts and not all of the processes come up (e.g. NameNode, RM). So my question is if blueprints with HA enabled can't be uploaded? Or, if is possible, do we need to do some extra steps prior to upload the blueprint?</p><p>Thanks</p><p>Paula</p>","tags":["ambari-blueprint","Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-17 16:26:32.0","id":7088,"title":"Why when install hdp 2.2, Hbase work with /hbase-unsecure ?","body":"","tags":["Hbase"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-12-19 14:10:26.0","id":7334,"title":"bucket map join","body":"<p>I have to join two large tables, so i am trying to use sort merge bucket map join. Both the tables are having partition on client id. There are 50 clients and I have bucketed both the tables in 20 buckets in another id field, which will be used in join.</p><p>When I execute the SMB join, I see 50*20= 1000 mappers. Please advise, how to correct that.</p>","tags":["Hive"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2015-10-22 20:10:28.0","id":1924,"title":"Is there a way we can set our oozie jobs so run on local time instead of GMT so they do not shift an hour between daylight savings and standard time?","body":"","tags":["Oozie"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-23 16:11:47.0","id":2013,"title":"Is there a minimum length for a user name in Ranger?","body":"<p>Creating an internal user name of 5 digits is not working correctly.</p>","tags":["Ranger"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-12 17:14:41.0","id":1295,"title":"How to schedule an Oozie job to run at 8PM EDT?","body":"","tags":["Oozie"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-13 23:06:54.0","id":1389,"title":"Is container_e70_144523545423_0003_01_000010 a valid containerId ?","body":"<p>I am not sure why e70 is in the part of containerId, I do see it in a HDP 2.3 environment. Is this expected or a known issue ?</p>","tags":["hdp-2.3.0","YARN"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2015-10-14 21:37:56.0","id":1470,"title":"Pagination is not supported in Phoenix 4.2.  Is there an alternate solution for pagination?","body":"","tags":["Phoenix"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-15 12:33:04.0","id":1493,"title":"Configuration file for SmartSense","body":"<p>In terms of security around smartsense file transfer, it is mentioned that we can use regex to replace some of the pieces within the bundle. Is there a configuration file for smartsense where this option is controlled that clients can change.</p><p>Also can you please let me know where i can find the instructions on how client can upload the smartsense bundle to hortonworks support. </p>","tags":["smartsense"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2015-10-16 18:53:23.0","id":1606,"title":"Cloudbreak install Mac OS","body":"<p>I've installed Cloudbreak locally on my Mac.  cbd doctor looks good, the install went without a hitch, however the UI never loads.  The only error i'm seeing in the logs is:</p><p>periscope_1     | 2015-10-16 18:11:30,020 [getThreadPoolExecutorFactoryBean-14] logExceptions:144 WARN  o.h.e.j.s.SqlExceptionHelper - SQL Error: 0, SQLState: 08001</p><p>ambassador_1    | 2015/10/16 18:11:30 dns: read udp 127.0.0.1:53: connection refused</p><p>periscope_1     | 2015-10-16 18:11:30,020 [getThreadPoolExecutorFactoryBean-14]</p><p>logExceptions:146 ERROR o.h.e.j.s.SqlExceptionHelper - The connection attempt failed.</p><p>The only other thing I noticed is that the cbreak_ambassadorips_1 container is not running.  It's state is Exit 0.  Could this be an ip issue?  I'm using my loopback (127.0.0.1) for the public and private ips in my Profile.  Should it be a different setting perhaps?</p><p>I've also tried starting the container dbs too but that didn't change anything (cbd startdb).</p><p>Thanks in advance!</p>","tags":["Cloudbreak"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2015-10-18 06:04:46.0","id":1640,"title":"AD users with Kerberos for multiple clusters (Prod and DR)","body":"<p>I am working on Kerberos with AD (no local KDC) integration for multiple Hadoop clusters (Prod and DR). The goal is to have all users and service principals reside in the corporate AD. Would I need to create two separate groups of users and service ids for each cluster? The idea is to have a single userid to be able to login into Prod or DR cluster depending which one is active.</p><p>When setting up the Prod cluster with Kerberos via Ambari it will generate all necessary principals and keytabs. What happens when the second cluster (DR) needs to be configured for Kerberos? Does Ambari know that all principals already exist? or will it try to regenerate? </p>","tags":["active-directory","disaster-recovery","kerberos"],"track_id":62,"track_name":"Security"}
{"creation_date":"2015-10-20 16:35:33.0","id":1730,"title":"What is the timeline for SSO integration for the Ambari REST API?","body":"<p>The following command was tried on a Kerborized cluster with no success.  </p><p>curl --negotiate -k -u:  -H \"X-Requested-By:ambari\" -i -X GET http://sandbox.hortonworks.com:8080/api/v1/users</p>","tags":["Ambari","kerberos"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-01-11 21:19:22.0","id":9470,"title":"Reading from Kafka","body":"<p>Is there a current pattern to read data from a Kafka topic for a range of time?  Looking at APIs, I only see options around log.retention in time and bytes.</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-18 01:18:55.0","id":10255,"title":"Issue with executing simple PigScript","body":"<p><a href=\"/storage/attachments/1422-capture1.jpg\">capture1.jpg</a><a href=\"/storage/attachments/1423-capture2.jpg\">capture2.jpg</a>STOCK_A = Load ' /user/admin/NYSE_daily_prices_A.csv' using PigStorage(',');</p><p>\nDESCRIBE STOCK_A;</p><p>I followed a simple tutorial \" http://hortonworks.com/hadoop-tutorial/how-to-use-basic-pig-commands/\"</p>","tags":["Pig"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-13 19:24:58.0","id":9772,"title":"How to add more disks to HDFS?","body":"","tags":["HDFS"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-01-15 11:22:03.0","id":10040,"title":"How to get realtime information from runing storm topology","body":"<p>Hi,</p><p>I have a topology that receives website clicks events from a stream. it keep track of events in last 20, 30 and 40 minutes of different topics.   ok now how to consume this information in my website      i want to show numbers of clicks on top of page of specific topic.    that means somehow i need that real time information out of running topology</p><p>my best guess is that i call a update service after say 30 seconds or produce a json file and save it to a location.     but that is not real time exact information.      is there a way that i call a topology service and get this realtime upto exact that time information </p><p>Regards</p><p>Shahzad Aslam </p>","tags":["Storm"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-01-15 06:57:13.0","id":10020,"title":"Services in ambari won't turn on","body":"<p><img src=\"/storage/attachments/1360-xr7pf.png\" alt=\"\"></p><p>I don't why all services stopped but when i try to restart all services the indicator would turn green for a few seconds(only for a few services) and then turn red again, how do i resolve this issue</p><p>thank you</p>","tags":["ambari-service"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-24 06:23:18.0","id":68234,"title":"Fail to install kerberos in HDP sandbox 2.5","body":"<p>I use the following command to install kerborse in HDP sandbox 2.5, but fails with the following error.</p><pre>yum install krb5-server krb5-libs krb5-workstation -y</pre><pre>Determining fastest mirrors\nepel/metalink                                                                                  | 5.2 kB     00:00\n * base: mirrors.btte.net\n * epel: mirror01.idc.hinet.net\n * extras: ftp.tc.edu.tw\n * updates: ftp.tc.edu.tw\nAMBARI.2.4.0.0-2.x                                                                             | 2.9 kB     00:00\nHDP-2.5                                                                                        | 2.9 kB     00:00\nHDP-SOLR-2.5-100                                                                               | 2.5 kB     00:00\nHDP-SOLR-2.5-100/primary_db                                                                    | 2.0 kB     00:00\nHDP-UTILS-1.1.0.21                                                                             | 2.9 kB     00:00\nbase                                                                                           | 3.7 kB     00:00\nepel                                                                                           | 4.3 kB     00:00\nTraceback (most recent call last):\n  File \"/usr/bin/yum\", line 29, in &lt;module&gt;\n    yummain.user_main(sys.argv[1:], exit_code=True)\n  File \"/usr/share/yum-cli/yummain.py\", line 298, in user_main\n    errcode = main(args)\n  File \"/usr/share/yum-cli/yummain.py\", line 146, in main\n    result, resultmsgs = base.doCommands()\n  File \"/usr/share/yum-cli/cli.py\", line 440, in doCommands\n    return self.yum_cli_commands[self.basecmd].doCommand(self, self.basecmd, self.extcmds)\n  File \"/usr/share/yum-cli/yumcommands.py\", line 211, in doCommand\n    return base.installPkgs(extcmds)\n  File \"/usr/share/yum-cli/cli.py\", line 702, in installPkgs\n    self.install(pattern=arg)\n  File \"/usr/lib/python2.6/site-packages/yum/__init__.py\", line 3551, in install\n    mypkgs = self.pkgSack.returnPackages(patterns=pats,\n  File \"/usr/lib/python2.6/site-packages/yum/__init__.py\", line 907, in &lt;lambda&gt;\n    pkgSack = property(fget=lambda self: self._getSacks(),\n  File \"/usr/lib/python2.6/site-packages/yum/__init__.py\", line 687, in _getSacks\n    self.repos.populateSack(which=repos)\n  File \"/usr/lib/python2.6/site-packages/yum/repos.py\", line 324, in populateSack\n    sack.populate(repo, mdtype, callback, cacheonly)\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 165, in populate\n    if self._check_db_version(repo, mydbtype):\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 223, in _check_db_version\n    return repo._check_db_version(mdtype)\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 1261, in _check_db_version\n    repoXML = self.repoXML\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 1460, in &lt;lambda&gt;\n    repoXML = property(fget=lambda self: self._getRepoXML(),\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 1452, in _getRepoXML\n    self._loadRepoXML(text=self)\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 1442, in _loadRepoXML\n    return self._groupLoadRepoXML(text, self._mdpolicy2mdtypes())\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 1418, in _groupLoadRepoXML\n    self._commonRetrieveDataMD(mdtypes)\n  File \"/usr/lib/python2.6/site-packages/yum/yumRepo.py\", line 1349, in _commonRetrieveDataMD\n    os.rename(local, local + '.old.tmp')\nOSError: [Errno 22] Invalid argument</pre>","tags":["kerberos"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-24 16:46:53.0","id":68295,"title":"ReplaceText Regex to replace double quotes in a string","body":"<p>I am working with the ReplaceText processor to replace only some instances of a double quote character (\") in a FlowFile and I am having difficulty with my Regex syntax. </p><p><strong>Background</strong>:</p><p>I am pulling an XML column from our database using ExecuteSQL which converts the results to Avro format. I run this through an AvroToJson processor but the JSON produced does not correctly escape double quotes found in my DB columns. I am converting to JSON because my end goal is to have the XML values in a FlowFile, line by line. </p><p><strong>Example</strong>:</p><pre>[ { \"XML\": \"&lt;MyXML&gt; This is a \"test\" XML &lt;/MyXML&gt;\" } ]</pre><p>As you can see the quotes surrounding \"test\" are invalid and need to be escaped to be:</p><pre>[ { \"XML\": \"&lt;MyXML&gt; This is a \\\"test\\\" XML &lt;/MyXML&gt;\" } ]</pre><p>I am trying to achieve this with the ReplaceText Processor. Using Regex I can correctly retrieve all the text between the &lt;MyXML&gt; tags but I am unable to single out the double quotes for replacement. </p><p>I have attempted to use back-references to replace the value in the middle capturing group, but that does not appear to work. Am I able to achieve this or do I need to be looking at an ExecuteScript processor and attempting it with Python/Groovy?</p><p><strong>Sample processor config:</strong></p><p><img src=\"/storage/attachments/9760-screen-shot-2016-11-24-at-44503-pm.png\"></p>","tags":["sql"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-24 18:03:11.0","id":68324,"title":"Nifi - Barrier/Latch","body":"<p>I am new to NIFI so pardon me if I asked something very obvious, although I did extensive search on google.</p><p>I have a workflow which does following things:</p><p>1. Read a json file</p><p>2. Split it into multiple jsons</p><p>3. Store each split json into ElasticSearch</p><p>Now I want to add a step 4 processor which will only get triggered once all the split files have been stored (ONLY ONCE).</p><p>If I just add a processor to success of PutES, it gets invoked for every split json. Can I define a barrier or latch which can wait for all files to be saved  or any another alternate approach ?</p>","tags":["nifi-processor","Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-24 19:00:25.0","id":68338,"title":"Ubuntu14 Ambari 2.4.1 install steps(from local repo)","body":"<p>\n\tCould any one explain the steps shown <a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.1.0/bk_ambari-installation/content/preparing_the_ambari_repository_configuration_file.html\">here</a>(http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.1.0/bk_ambari-installation/content/preparing_the_ambari_repository_configuration_file.html) for Ubuntu14 OS?</p><pre>Should I download ambari.repo?\nWhere should I place it?\nShould I edit ambari.list?\n\n</pre><p>I am not able to follow the steps to create an ambari local repo on Ubuntu. Please explain the steps.</p>","tags":["ubuntu"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-25 10:33:45.0","id":68399,"title":"Hide \"Connecting to namenode via...\" message from stdout when using hdfs fsck in a for loop","body":"<p>Hello ,</p><p>I usually use for loops to get info of some folders.</p><p>For example: I had to find which folder inside /user/bigdata was consuming high number of blocks due to small files.</p><p>So i used this:</p><pre>for i in $(hadoop fs -ls /user/bigdata/ | grep drwx | awk '{print $8}'); do echo \"$i $(hdfs fsck $i -blocks -files -locations | grep BP- | wc -l)\" ; done</pre><p>Getting a lot of  \"Connecting to namenode via http://&lt;hostname&gt;:50070/fsck?ugi=hdfs&blocks=1&files=1&locations=1&path=%2Fuser%2Fbigdata%2F.&lt;directory&gt;\" messages.</p><p>Does exist any way of hide this message? I currently have to redirect the output to a file and then use cat to read a clear infomation.</p><p>Thank you in advance.</p>","tags":["HDFS"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-24 12:34:32.0","id":68289,"title":"Hive job fail on TEZ due to out of memory..","body":"<p>Hi one of my job in hive on Tez gets fail at 98% due tojava.lang.OutOfMemoryError:Java heap space,</p><p>i am using HDP 2.1.2</p><p>i have below values </p><p>mapreduce.map.memory.mb=3584;</p><p>mapreduce.map.java.opts=-Xmx2560m;</p><p>mapreduce.reduce.memory.mb=3584; </p><p> mapreduce.reduce.java.opts=-Xmx2560m</p><p>Can any one help me to out this error is great appreciated..</p><p>Regards</p><p>sankar </p>","tags":["yarn-container"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-24 15:06:06.0","id":68310,"title":"Data Visualization for HDP and HDF scenarios","body":"<p>Hello. I would like to validate alternatives for *open source* and preferrably free visualization and reporting tools. I am aware of commercial alternatives, but I would like to double check for the best alternatives in the open source universe, both for HDP and HDF. These are the broad scenarios I need to address:</p><p>1) Traditional reporting; some popular choices could be Pentaho or SpagoBI.</p><p>2) Multi dimensional analysis; in the RDBMS world this could be mapped to Mondrian (ROLAP), and different visualizaiton interfaces like jpivot. </p><p>3) Real time analytics. Our first choice was Banana Dashboard, but we needed to tweak it a lot in order to provide the expected results. </p><p>Thanks in advance!</p>","tags":["visualization"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-24 23:27:29.0","id":68359,"title":"How to convert RDD[List[String]] to Dataframe in Scala","body":"<p style=\"margin-left: 20px;\">\n\tHello,\n\n\t\n\n\tHow do I convert the below RDD[List[String]] to Dataframe in scala?</p><p style=\"margin-left: 20px;\"></p><pre>List(Div, Date, HomeTeam, AwayTeam, FTHG, FTAG, FTR, HTHG, HTAG, HTR, HS, AS, HST, AST, HF, AF, HC, AC, HY, AY, HR, AR)\nList(D1, 09/08/13, Bayern Munich, M'gladbach, 3, 1, H, 2, 1, H, 26, 11, 9, 4, 12, 11, 6, 4, 1, 3, 0, 0)\nList(D1, 10/08/13, Augsburg, Dortmund, 0, 4, A, 0, 1, A, 11, 13, 4, 9, 18, 12, 5, 6, 2, 0, 0, 0)\nList(D1, 10/08/13, Braunschweig, Werder Bremen, 0, 1, A, 0, 0, D, 13, 12, 3, 4, 10, 18, 2, 7, 1, 0, 0, 0)\nList(D1, 10/08/13, Hannover, Wolfsburg, 2, 0, H, 1, 0, H, 20, 15, 8, 4, 28, 11, 7, 2, 4, 0, 0, 2)\nList(D1, 10/08/13, Hertha, Ein Frankfurt, 6, 1, H, 2, 1, H, 16, 10, 9, 4, 19, 18, 5, 4, 0, 2, 0, 0)\n\n</pre><p style=\"margin-left: 20px;\"></p>","tags":["Spark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-25 00:13:52.0","id":68370,"title":"Solr Installation Failure - Please Help!","body":"<pre>Hi\nmy Solr installation via ambari sandbox 2.4 keeps failing.\nAll required services are running.\n\nI followed the hortonworks guide but no avail. please can someone advise?\n\nThanks!\n\nstderr:   /var/lib/ambari-agent/data/errors-721.txt\n\nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/SOLR/package/scripts/master.py\", line 171, in &lt;module&gt;\n    Master().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 219, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/SOLR/package/scripts/master.py\", line 33, in install\n    create_parents=True\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 113, in __new__\n    cls(name.pop(0), env, provider, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 146, in __init__\n    raise Fail(\"%s received unsupported argument %s\" % (self, key)) \nstdout:   /var/lib/ambari-agent/data/output-721.txt\n2016-11-25 00:07:03,369 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.4.0.0-169\n2016-11-25 00:07:03,369 - Checking if need to create versioned conf dir /etc/hadoop/2.4.0.0-169/0\n2016-11-25 00:07:03,369 - call['conf-select create-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-11-25 00:07:03,429 - call returned (1, '/etc/hadoop/2.4.0.0-169/0 exist already', '')\n2016-11-25 00:07:03,429 - checked_call['conf-select set-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-11-25 00:07:03,496 - checked_call returned (0, '/usr/hdp/2.4.0.0-169/hadoop/conf -&gt; /etc/hadoop/2.4.0.0-169/0')\n2016-11-25 00:07:03,496 - Ensuring that hadoop has the correct symlink structure\n2016-11-25 00:07:03,497 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-11-25 00:07:03,504 - Group['hadoop'] {}\n2016-11-25 00:07:03,510 - Group['users'] {}\n2016-11-25 00:07:03,512 - Group['zeppelin'] {}\n2016-11-25 00:07:03,512 - Group['nifi'] {}\n2016-11-25 00:07:03,512 - Group['solr'] {}\n2016-11-25 00:07:03,512 - Group['knox'] {}\n2016-11-25 00:07:03,515 - Group['ranger'] {}\n2016-11-25 00:07:03,516 - Group['spark'] {}\n2016-11-25 00:07:03,517 - User['oozie'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-11-25 00:07:03,518 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,520 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,522 - User['nifi'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,526 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-11-25 00:07:03,532 - User['flume'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,536 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,537 - User['solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,538 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,540 - User['ranger'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['ranger']}\n2016-11-25 00:07:03,541 - User['storm'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,543 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,547 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,550 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,556 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-11-25 00:07:03,557 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,557 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,563 - User['falcon'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-11-25 00:07:03,565 - User['sqoop'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,567 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,569 - User['hcat'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,571 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,572 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-25 00:07:03,578 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-11-25 00:07:03,592 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2016-11-25 00:07:03,618 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2016-11-25 00:07:03,619 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'recursive': True, 'mode': 0775, 'cd_access': 'a'}\n2016-11-25 00:07:03,622 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-11-25 00:07:03,624 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}\n2016-11-25 00:07:03,635 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if\n2016-11-25 00:07:03,636 - Group['hdfs'] {}\n2016-11-25 00:07:03,636 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'hdfs']}\n2016-11-25 00:07:03,637 - Directory['/etc/hadoop'] {'mode': 0755}\n2016-11-25 00:07:03,667 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-11-25 00:07:03,670 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}\n2016-11-25 00:07:03,706 - Repository['HDP-2.4'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.0.0/', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP', 'mirror_list': None}\n2016-11-25 00:07:03,726 - File['/etc/yum.repos.d/HDP.repo'] {'content': '[HDP-2.4]\\nname=HDP-2.4\\nbaseurl=http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.0.0/\\n\\npath=/\\nenabled=1\\ngpgcheck=0'}\n2016-11-25 00:07:03,727 - Repository['HDP-UTILS-1.1.0.20'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\\nname={{repo_id}}\\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\\n\\npath=/\\nenabled=1\\ngpgcheck=0', 'repo_file_name': 'HDP-UTILS', 'mirror_list': None}\n2016-11-25 00:07:03,731 - File['/etc/yum.repos.d/HDP-UTILS.repo'] {'content': '[HDP-UTILS-1.1.0.20]\\nname=HDP-UTILS-1.1.0.20\\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos6\\n\\npath=/\\nenabled=1\\ngpgcheck=0'}\n2016-11-25 00:07:03,733 - Package['unzip'] {}\n2016-11-25 00:07:08,927 - Skipping installation of existing package unzip\n2016-11-25 00:07:08,927 - Package['curl'] {}\n2016-11-25 00:07:08,950 - Skipping installation of existing package curl\n2016-11-25 00:07:08,951 - Package['hdp-select'] {}\n2016-11-25 00:07:08,974 - Skipping installation of existing package hdp-select\n2016-11-25 00:07:09,481 - Package['wget'] {}\n2016-11-25 00:07:09,844 - Skipping installation of existing package wget\n2016-11-25 00:07:09,844 - Package['tar'] {}\n2016-11-25 00:07:09,873 - Skipping installation of existing package tar\n2016-11-25 00:07:09,874 - Package['lsof'] {}\n2016-11-25 00:07:09,898 - Skipping installation of existing package lsof\n2016-11-25 00:07:09,898 - Execute['find /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/SOLR/package -iname \"*.sh\" | xargs chmod +x'] {}\n\n\n</pre><p>resource_management.core.exceptions.Fail: Directory['/var/log/solr'] received unsupported argument create_parents</p>","tags":["installer","SOLR","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-25 12:39:12.0","id":68406,"title":"What is the difference between Apache atlas and Apache falcon?","body":"<p>Hi Everyone,</p><p>I have read so many blogs and document over internet regarding Apache atlas and Apache falcon and have done some POC also using these tools.but here,I don't understand what is the actual difference between these tool?</p><p>As per my understanding both the tools are committing to provide data management life cycle  and data governance featuresalso.so I am little bit confused here and feeling that both are providing similar features.</p><p>I don't understand which tool I should use in my use case for data governance as both are giving lineage?.</p><p>Here i am confused that where these above tool will fit in my use case(general questionj)?.</p><p>Thanks in advance.</p>","tags":["Atlas","Falcon"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-23 17:18:54.0","id":68119,"title":"HDCloud datanodes unmount after restart of servers","body":"<p>After shutdown of servers the data nodes unmount and I get this error </p><p>stderr: \n2016-11-23 14:37:57,967 - \n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\nDirectory /hadoopfs/fs1/hdfs/datanode became unmounted from /hadoopfs/fs1 . Current mount point: / . Directory /hadoopfs/fs2/hdfs/datanode became unmounted from /hadoopfs/fs2 . Current mount point: / . Please ensure that mounts are healthy. If the mount change was intentional, you can update the contents of /var/lib/ambari-agent/data/datanode/dfs_data_dir_mount.hist.\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\nTraceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py\", line 174, in &lt;module&gt;\n    DataNode().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 280, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py\", line 61, in start\n    datanode(action=\"start\")\n  File \"/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py\", line 89, in thunk\n    return fn(*args, **kwargs)\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_datanode.py\", line 68, in datanode\n    create_log_dir=True\n  File \"/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/utils.py\", line 269, in service\n    Execute(daemon_cmd, not_if=process_id_exists_command, environment=hadoop_env_exports)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 155, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 160, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 124, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 273, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 71, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 93, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 141, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 294, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh --config /usr/hdp/current/hadoop-client/conf start datanode'' returned 1. starting datanode, logging to /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.out\n stdout:\n2016-11-23 14:37:57,213 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.5.0.0-1245\n2016-11-23 14:37:57,214 - Checking if need to create versioned conf dir /etc/hadoop/2.5.0.0-1245/0\n2016-11-23 14:37:57,214 - call[('ambari-python-wrap', '/usr/bin/conf-select', 'create-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-11-23 14:37:57,235 - call returned (1, '/etc/hadoop/2.5.0.0-1245/0 exist already', '')\n2016-11-23 14:37:57,235 - checked_call[('ambari-python-wrap', '/usr/bin/conf-select', 'set-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-11-23 14:37:57,255 - checked_call returned (0, '')\n2016-11-23 14:37:57,255 - Ensuring that hadoop has the correct symlink structure\n2016-11-23 14:37:57,255 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-11-23 14:37:57,379 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.5.0.0-1245\n2016-11-23 14:37:57,379 - Checking if need to create versioned conf dir /etc/hadoop/2.5.0.0-1245/0\n2016-11-23 14:37:57,380 - call[('ambari-python-wrap', '/usr/bin/conf-select', 'create-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-11-23 14:37:57,400 - call returned (1, '/etc/hadoop/2.5.0.0-1245/0 exist already', '')\n2016-11-23 14:37:57,400 - checked_call[('ambari-python-wrap', '/usr/bin/conf-select', 'set-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-11-23 14:37:57,420 - checked_call returned (0, '')\n2016-11-23 14:37:57,421 - Ensuring that hadoop has the correct symlink structure\n2016-11-23 14:37:57,421 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-11-23 14:37:57,422 - Group['hadoop'] {}\n2016-11-23 14:37:57,423 - Group['users'] {}\n2016-11-23 14:37:57,424 - Group['spark'] {}\n2016-11-23 14:37:57,424 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,424 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,425 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,425 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-11-23 14:37:57,426 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,427 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['users']}\n2016-11-23 14:37:57,427 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,428 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,428 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,429 - User['hcat'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,429 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop']}\n2016-11-23 14:37:57,430 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-11-23 14:37:57,431 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}\n2016-11-23 14:37:57,435 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if\n2016-11-23 14:37:57,435 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}\n2016-11-23 14:37:57,436 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}\n2016-11-23 14:37:57,437 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}\n2016-11-23 14:37:57,440 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if\n2016-11-23 14:37:57,440 - Group['hdfs'] {}\n2016-11-23 14:37:57,440 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'hdfs']}\n2016-11-23 14:37:57,441 - FS Type: \n2016-11-23 14:37:57,441 - Directory['/etc/hadoop'] {'mode': 0755}\n2016-11-23 14:37:57,456 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-11-23 14:37:57,456 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}\n2016-11-23 14:37:57,468 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}\n2016-11-23 14:37:57,472 - Skipping Execute[('setenforce', '0')] due to not_if\n2016-11-23 14:37:57,473 - Directory['/var/log/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}\n2016-11-23 14:37:57,474 - Directory['/var/run/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'root', 'cd_access': 'a'}\n2016-11-23 14:37:57,475 - Changing owner for /var/run/hadoop from 508 to root\n2016-11-23 14:37:57,475 - Changing group for /var/run/hadoop from 503 to root\n2016-11-23 14:37:57,475 - Directory['/tmp/hadoop-hdfs'] {'owner': 'hdfs', 'create_parents': True, 'cd_access': 'a'}\n2016-11-23 14:37:57,479 - File['/usr/hdp/current/hadoop-client/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'hdfs'}\n2016-11-23 14:37:57,481 - File['/usr/hdp/current/hadoop-client/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'hdfs'}\n2016-11-23 14:37:57,482 - File['/usr/hdp/current/hadoop-client/conf/log4j.properties'] {'content': ..., 'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}\n2016-11-23 14:37:57,495 - File['/usr/hdp/current/hadoop-client/conf/hadoop-metrics2.properties'] {'content': Template('hadoop-metrics2.properties.j2'), 'owner': 'hdfs', 'group': 'hadoop'}\n2016-11-23 14:37:57,495 - File['/usr/hdp/current/hadoop-client/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}\n2016-11-23 14:37:57,497 - File['/usr/hdp/current/hadoop-client/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}\n2016-11-23 14:37:57,501 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop'}\n2016-11-23 14:37:57,504 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}\n2016-11-23 14:37:57,685 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.5.0.0-1245\n2016-11-23 14:37:57,685 - Checking if need to create versioned conf dir /etc/hadoop/2.5.0.0-1245/0\n2016-11-23 14:37:57,686 - call[('ambari-python-wrap', '/usr/bin/conf-select', 'create-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-11-23 14:37:57,706 - call returned (1, '/etc/hadoop/2.5.0.0-1245/0 exist already', '')\n2016-11-23 14:37:57,706 - checked_call[('ambari-python-wrap', '/usr/bin/conf-select', 'set-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-11-23 14:37:57,727 - checked_call returned (0, '')\n2016-11-23 14:37:57,727 - Ensuring that hadoop has the correct symlink structure\n2016-11-23 14:37:57,728 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-11-23 14:37:57,729 - Stack Feature Version Info: stack_version=2.5, version=2.5.0.0-1245, current_cluster_version=2.5.0.0-1245 -&gt; 2.5.0.0-1245\n2016-11-23 14:37:57,731 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.5.0.0-1245\n2016-11-23 14:37:57,731 - Checking if need to create versioned conf dir /etc/hadoop/2.5.0.0-1245/0\n2016-11-23 14:37:57,731 - call[('ambari-python-wrap', '/usr/bin/conf-select', 'create-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}\n2016-11-23 14:37:57,751 - call returned (1, '/etc/hadoop/2.5.0.0-1245/0 exist already', '')\n2016-11-23 14:37:57,751 - checked_call[('ambari-python-wrap', '/usr/bin/conf-select', 'set-conf-dir', '--package', 'hadoop', '--stack-version', '2.5.0.0-1245', '--conf-version', '0')] {'logoutput': False, 'sudo': True, 'quiet': False}\n2016-11-23 14:37:57,771 - checked_call returned (0, '')\n2016-11-23 14:37:57,772 - Ensuring that hadoop has the correct symlink structure\n2016-11-23 14:37:57,772 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf\n2016-11-23 14:37:57,778 - checked_call['rpm -q --queryformat '%{version}-%{release}' hdp-select | sed -e 's/\\.el[0-9]//g''] {'stderr': -1}\n2016-11-23 14:37:57,801 - checked_call returned (0, '2.5.0.0-1245', '')\n2016-11-23 14:37:57,805 - Directory['/etc/security/limits.d'] {'owner': 'root', 'create_parents': True, 'group': 'root'}\n2016-11-23 14:37:57,811 - File['/etc/security/limits.d/hdfs.conf'] {'content': Template('hdfs.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}\n2016-11-23 14:37:57,812 - XmlConfig['hadoop-policy.xml'] {'owner': 'hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'configuration_attributes': {}, 'configurations': ...}\n2016-11-23 14:37:57,822 - Generating config: /usr/hdp/current/hadoop-client/conf/hadoop-policy.xml\n2016-11-23 14:37:57,823 - File['/usr/hdp/current/hadoop-client/conf/hadoop-policy.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}\n2016-11-23 14:37:57,831 - XmlConfig['ssl-client.xml'] {'owner': 'hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'configuration_attributes': {}, 'configurations': ...}\n2016-11-23 14:37:57,840 - Generating config: /usr/hdp/current/hadoop-client/conf/ssl-client.xml\n2016-11-23 14:37:57,840 - File['/usr/hdp/current/hadoop-client/conf/ssl-client.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}\n2016-11-23 14:37:57,846 - Directory['/usr/hdp/current/hadoop-client/conf/secure'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'cd_access': 'a'}\n2016-11-23 14:37:57,846 - XmlConfig['ssl-client.xml'] {'owner': 'hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hadoop-client/conf/secure', 'configuration_attributes': {}, 'configurations': ...}\n2016-11-23 14:37:57,855 - Generating config: /usr/hdp/current/hadoop-client/conf/secure/ssl-client.xml\n2016-11-23 14:37:57,855 - File['/usr/hdp/current/hadoop-client/conf/secure/ssl-client.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}\n2016-11-23 14:37:57,861 - XmlConfig['ssl-server.xml'] {'owner': 'hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'configuration_attributes': {}, 'configurations': ...}\n2016-11-23 14:37:57,869 - Generating config: /usr/hdp/current/hadoop-client/conf/ssl-server.xml\n2016-11-23 14:37:57,869 - File['/usr/hdp/current/hadoop-client/conf/ssl-server.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}\n2016-11-23 14:37:57,875 - XmlConfig['hdfs-site.xml'] {'owner': 'hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'configuration_attributes': {'final': {'dfs.datanode.failed.volumes.tolerated': 'true', 'dfs.namenode.http-address': 'true', 'dfs.namenode.name.dir': 'true', 'dfs.support.append': 'true', 'dfs.webhdfs.enabled': 'true'}}, 'configurations': ...}\n2016-11-23 14:37:57,884 - Generating config: /usr/hdp/current/hadoop-client/conf/hdfs-site.xml\n2016-11-23 14:37:57,884 - File['/usr/hdp/current/hadoop-client/conf/hdfs-site.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}\n2016-11-23 14:37:57,927 - XmlConfig['core-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/current/hadoop-client/conf', 'mode': 0644, 'configuration_attributes': {'final': {'fs.defaultFS': 'true'}}, 'owner': 'hdfs', 'configurations': ...}\n2016-11-23 14:37:57,935 - Generating config: /usr/hdp/current/hadoop-client/conf/core-site.xml\n2016-11-23 14:37:57,935 - File['/usr/hdp/current/hadoop-client/conf/core-site.xml'] {'owner': 'hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0644, 'encoding': 'UTF-8'}\n2016-11-23 14:37:57,959 - File['/usr/hdp/current/hadoop-client/conf/slaves'] {'content': Template('slaves.j2'), 'owner': 'hdfs'}\n2016-11-23 14:37:57,961 - Directory['/var/lib/hadoop-hdfs'] {'owner': 'hdfs', 'create_parents': True, 'group': 'hadoop', 'mode': 0751}\n2016-11-23 14:37:57,961 - Directory['/var/lib/ambari-agent/data/datanode'] {'create_parents': True, 'mode': 0755}\n2016-11-23 14:37:57,964 - Host contains mounts: ['/proc', '/sys', '/', '/dev', '/dev/pts', '/dev/shm', '/proc/sys/fs/binfmt_misc'].\n2016-11-23 14:37:57,964 - Mount point for directory /hadoopfs/fs1/hdfs/datanode is /\n2016-11-23 14:37:57,964 - Directory /hadoopfs/fs1/hdfs/datanode became unmounted from /hadoopfs/fs1 . Current mount point: / .\n2016-11-23 14:37:57,965 - Mount point for directory /hadoopfs/fs2/hdfs/datanode is /\n2016-11-23 14:37:57,965 - Directory /hadoopfs/fs2/hdfs/datanode became unmounted from /hadoopfs/fs2 . Current mount point: / .\n2016-11-23 14:37:57,967 - Host contains mounts: ['/proc', '/sys', '/', '/dev', '/dev/pts', '/dev/shm', '/proc/sys/fs/binfmt_misc'].\n2016-11-23 14:37:57,967 - \n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\nDirectory /hadoopfs/fs1/hdfs/datanode became unmounted from /hadoopfs/fs1 . Current mount point: / . Directory /hadoopfs/fs2/hdfs/datanode became unmounted from /hadoopfs/fs2 . Current mount point: / . Please ensure that mounts are healthy. If the mount change was intentional, you can update the contents of /var/lib/ambari-agent/data/datanode/dfs_data_dir_mount.hist.\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n***** WARNING ***** WARNING ***** WARNING ***** WARNING ***** WARNING *****\n2016-11-23 14:37:57,968 - File['/var/lib/ambari-agent/data/datanode/dfs_data_dir_mount.hist'] {'content': '\\n# This file keeps track of the last known mount-point for each dir.\\n# It is safe to delete, since it will get regenerated the next time that the component of the service starts.\\n# However, it is not advised to delete this file since Ambari may\\n# re-create a dir that used to be mounted on a drive but is now mounted on the root.\\n# Comments begin with a hash (#) symbol\\n# dir,mount_point\\n/hadoopfs/fs1/hdfs/datanode,/hadoopfs/fs1\\n/hadoopfs/fs2/hdfs/datanode,/hadoopfs/fs2\\n', 'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}\n2016-11-23 14:37:57,969 - Directory['/var/run/hadoop'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0755}\n2016-11-23 14:37:57,969 - Changing owner for /var/run/hadoop from 0 to hdfs\n2016-11-23 14:37:57,969 - Changing group for /var/run/hadoop from 0 to hadoop\n2016-11-23 14:37:57,970 - Directory['/var/run/hadoop/hdfs'] {'owner': 'hdfs', 'group': 'hadoop', 'create_parents': True}\n2016-11-23 14:37:57,970 - Directory['/var/log/hadoop/hdfs'] {'owner': 'hdfs', 'group': 'hadoop', 'create_parents': True}\n2016-11-23 14:37:57,970 - File['/var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid'] {'action': ['delete'], 'not_if': 'ambari-sudo.sh  -H -E test -f /var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid && ambari-sudo.sh  -H -E pgrep -F /var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid'}\n2016-11-23 14:37:57,977 - Deleting File['/var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid']\n2016-11-23 14:37:57,977 - Execute['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh --config /usr/hdp/current/hadoop-client/conf start datanode''] {'environment': {'HADOOP_LIBEXEC_DIR': '/usr/hdp/current/hadoop-client/libexec'}, 'not_if': 'ambari-sudo.sh  -H -E test -f /var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid && ambari-sudo.sh  -H -E pgrep -F /var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid'}\n2016-11-23 14:38:02,037 - Execute['find /var/log/hadoop/hdfs -maxdepth 1 -type f -name '*' -exec echo '==&gt; {} &lt;==' \\; -exec tail -n 40 {} \\;'] {'logoutput': True, 'ignore_failures': True, 'user': 'hdfs'}\n==&gt; /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.out.5 &lt;==\nulimit -a for user hdfs\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 60086\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 128000\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n==&gt; /var/log/hadoop/hdfs/gc.log-201611211702 &lt;==\n2016-11-21T21:04:52.157+0000: 14542.730: [GC2016-11-21T21:04:52.157+0000: 14542.730: [ParNew: 166166K-&gt;3490K(184320K), 0.0061870 secs] 175603K-&gt;12970K(1028096K), 0.0062860 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n2016-11-21T22:09:57.667+0000: 18448.240: [GC2016-11-21T22:09:57.668+0000: 18448.240: [ParNew: 167330K-&gt;1563K(184320K), 0.0055350 secs] 176810K-&gt;11047K(1028096K), 0.0056220 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-21T23:07:31.170+0000: 21901.743: [GC2016-11-21T23:07:31.170+0000: 21901.743: [ParNew: 165403K-&gt;2098K(184320K), 0.0054260 secs] 174887K-&gt;12135K(1028096K), 0.0055120 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \n2016-11-22T00:11:31.862+0000: 25742.434: [GC2016-11-22T00:11:31.862+0000: 25742.434: [ParNew: 165938K-&gt;2272K(184320K), 0.0043610 secs] 175975K-&gt;12337K(1028096K), 0.0044360 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-22T01:04:12.388+0000: 28902.960: [GC2016-11-22T01:04:12.388+0000: 28902.960: [ParNew: 166112K-&gt;3356K(184320K), 0.0047310 secs] 176177K-&gt;13502K(1028096K), 0.0048500 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-22T02:04:01.698+0000: 32492.271: [GC2016-11-22T02:04:01.698+0000: 32492.271: [ParNew: 167196K-&gt;3023K(184320K), 0.0042870 secs] 177342K-&gt;13179K(1028096K), 0.0043850 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n2016-11-22T03:13:01.719+0000: 36632.291: [GC2016-11-22T03:13:01.719+0000: 36632.291: [ParNew: 166863K-&gt;1551K(184320K), 0.0041630 secs] 177019K-&gt;11740K(1028096K), 0.0042590 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T04:16:11.165+0000: 40421.738: [GC2016-11-22T04:16:11.165+0000: 40421.738: [ParNew: 165391K-&gt;3437K(184320K), 0.0049510 secs] 175580K-&gt;13647K(1028096K), 0.0050430 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T05:10:22.619+0000: 43673.192: [GC2016-11-22T05:10:22.619+0000: 43673.192: [ParNew: 167277K-&gt;3095K(184320K), 0.0043240 secs] 177487K-&gt;13315K(1028096K), 0.0044160 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T06:14:21.734+0000: 47512.307: [GC2016-11-22T06:14:21.734+0000: 47512.307: [ParNew: 166935K-&gt;1890K(184320K), 0.0040030 secs] 177155K-&gt;12122K(1028096K), 0.0040920 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n2016-11-22T07:09:31.787+0000: 50822.360: [GC2016-11-22T07:09:31.787+0000: 50822.360: [ParNew: 165730K-&gt;3477K(184320K), 0.0043660 secs] 175962K-&gt;13732K(1028096K), 0.0044580 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n2016-11-22T08:19:11.991+0000: 55002.564: [GC2016-11-22T08:19:11.991+0000: 55002.564: [ParNew: 167317K-&gt;801K(184320K), 0.0037820 secs] 177572K-&gt;11068K(1028096K), 0.0038690 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-22T09:28:12.673+0000: 59143.245: [GC2016-11-22T09:28:12.673+0000: 59143.245: [ParNew: 164641K-&gt;744K(184320K), 0.0036110 secs] 174908K-&gt;11013K(1028096K), 0.0036950 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n2016-11-22T10:36:11.161+0000: 63221.734: [GC2016-11-22T10:36:11.161+0000: 63221.734: [ParNew: 164584K-&gt;808K(184320K), 0.0038280 secs] 174853K-&gt;11086K(1028096K), 0.0039210 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \n2016-11-22T11:44:01.722+0000: 67292.294: [GC2016-11-22T11:44:01.722+0000: 67292.294: [ParNew: 164648K-&gt;798K(184320K), 0.0038170 secs] 174926K-&gt;11093K(1028096K), 0.0039030 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \n2016-11-22T12:49:49.635+0000: 71240.208: [GC2016-11-22T12:49:49.635+0000: 71240.208: [ParNew: 164638K-&gt;704K(184320K), 0.0035240 secs] 174933K-&gt;11015K(1028096K), 0.0036170 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T13:57:23.106+0000: 75293.679: [GC2016-11-22T13:57:23.107+0000: 75293.679: [ParNew: 164544K-&gt;644K(184320K), 0.0035160 secs] 174855K-&gt;11034K(1028096K), 0.0035990 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T15:03:01.728+0000: 79232.300: [GC2016-11-22T15:03:01.728+0000: 79232.300: [ParNew: 164484K-&gt;1268K(184320K), 0.0037900 secs] 174874K-&gt;11659K(1028096K), 0.0038790 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T15:59:01.931+0000: 82592.504: [GC2016-11-22T15:59:01.931+0000: 82592.504: [ParNew: 165108K-&gt;2462K(184320K), 0.0039180 secs] 175499K-&gt;12853K(1028096K), 0.0039920 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-22T17:00:41.326+0000: 86291.899: [GC2016-11-22T17:00:41.326+0000: 86291.899: [ParNew: 166302K-&gt;2193K(184320K), 0.0097560 secs] 176693K-&gt;12585K(1028096K), 0.0098640 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \n2016-11-22T18:05:41.340+0000: 90191.913: [GC2016-11-22T18:05:41.340+0000: 90191.913: [ParNew: 166033K-&gt;1246K(184320K), 0.0057410 secs] 176425K-&gt;11638K(1028096K), 0.0058260 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-22T19:03:37.952+0000: 93668.524: [GC2016-11-22T19:03:37.952+0000: 93668.524: [ParNew: 165086K-&gt;2299K(184320K), 0.0039490 secs] 175478K-&gt;12691K(1028096K), 0.0040400 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \n2016-11-22T19:57:31.998+0000: 96902.571: [GC2016-11-22T19:57:31.999+0000: 96902.571: [ParNew: 166139K-&gt;2581K(184320K), 0.0046020 secs] 176531K-&gt;12974K(1028096K), 0.0047090 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T21:02:41.329+0000: 100811.902: [GC2016-11-22T21:02:41.329+0000: 100811.902: [ParNew: 166403K-&gt;1224K(184320K), 0.0053900 secs] 176796K-&gt;11618K(1028096K), 0.0054850 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \n2016-11-22T22:11:02.035+0000: 104912.607: [GC2016-11-22T22:11:02.035+0000: 104912.608: [ParNew: 165064K-&gt;762K(184320K), 0.0047510 secs] 175458K-&gt;11160K(1028096K), 0.0048310 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \n2016-11-22T23:13:53.628+0000: 108684.200: [GC2016-11-22T23:13:53.628+0000: 108684.200: [ParNew: 164602K-&gt;1939K(184320K), 0.0039460 secs] 175000K-&gt;12343K(1028096K), 0.0040390 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n2016-11-22T23:53:09.707+0000: 111040.279: [GC2016-11-22T23:53:09.707+0000: 111040.279: [ParNew: 165779K-&gt;6475K(184320K), 0.0049720 secs] 176183K-&gt;16881K(1028096K), 0.0050680 secs] [Times: user=0.01 sys=0.01, real=0.00 secs] \n2016-11-23T00:39:37.378+0000: 113827.951: [GC2016-11-23T00:39:37.378+0000: 113827.951: [ParNew: 170315K-&gt;6100K(184320K), 0.0049640 secs] 180721K-&gt;16519K(1028096K), 0.0050740 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \n2016-11-23T01:32:51.171+0000: 117021.744: [GC2016-11-23T01:32:51.171+0000: 117021.744: [ParNew: 169940K-&gt;3884K(184320K), 0.0042920 secs] 180359K-&gt;14317K(1028096K), 0.0043760 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-23T02:19:31.162+0000: 119821.734: [GC2016-11-23T02:19:31.162+0000: 119821.734: [ParNew: 167724K-&gt;6511K(184320K), 0.0047890 secs] 178157K-&gt;16945K(1028096K), 0.0048810 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-23T03:07:12.248+0000: 122682.820: [GC2016-11-23T03:07:12.248+0000: 122682.820: [ParNew: 170351K-&gt;5368K(184320K), 0.0053940 secs] 180785K-&gt;15805K(1028096K), 0.0054970 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \n2016-11-23T04:18:41.329+0000: 126971.902: [GC2016-11-23T04:18:41.329+0000: 126971.902: [ParNew: 169208K-&gt;977K(184320K), 0.0064180 secs] 179645K-&gt;11416K(1028096K), 0.0064970 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \n2016-11-23T05:28:47.965+0000: 131178.537: [GC2016-11-23T05:28:47.965+0000: 131178.537: [ParNew: 164817K-&gt;967K(184320K), 0.0041370 secs] 175256K-&gt;11414K(1028096K), 0.0042250 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \nHeap\n par new generation   total 184320K, used 67749K [0x00000000b0000000, 0x00000000bc800000, 0x00000000bc800000)\n  eden space 163840K,  40% used [0x00000000b0000000, 0x00000000b41375f0, 0x00000000ba000000)\n  from space 20480K,   4% used [0x00000000ba000000, 0x00000000ba0f1fe0, 0x00000000bb400000)\n  to   space 20480K,   0% used [0x00000000bb400000, 0x00000000bb400000, 0x00000000bc800000)\n concurrent mark-sweep generation total 843776K, used 10446K [0x00000000bc800000, 0x00000000f0000000, 0x00000000f0000000)\n concurrent-mark-sweep perm gen total 131072K, used 40387K [0x00000000f0000000, 0x00000000f8000000, 0x0000000100000000)\n==&gt; /var/log/hadoop/hdfs/gc.log-201611231430 &lt;==\nOpenJDK 64-Bit Server VM (24.121-b00) for linux-amd64 JRE (1.7.0_121-b00), built on Nov 18 2016 00:22:36 by \"mockbuild\" with gcc 4.8.3 20140911 (Red Hat 4.8.3-9)\nMemory: 4k page, physical 15403944k(15060448k free), swap 0k(0k free)\nCommandLine flags: -XX:ErrorFile=/var/log/hadoop/hdfs/hs_err_pid%p.log -XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=209715200 -XX:MaxPermSize=268435456 -XX:MaxTenuringThreshold=6 -XX:NewSize=209715200 -XX:OldPLABSize=16 -XX:ParallelGCThreads=4 -XX:PermSize=134217728 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC \nHeap\n par new generation   total 184320K, used 108311K [0x00000000b0000000, 0x00000000bc800000, 0x00000000bc800000)\n  eden space 163840K,  66% used [0x00000000b0000000, 0x00000000b69c5c00, 0x00000000ba000000)\n  from space 20480K,   0% used [0x00000000ba000000, 0x00000000ba000000, 0x00000000bb400000)\n  to   space 20480K,   0% used [0x00000000bb400000, 0x00000000bb400000, 0x00000000bc800000)\n concurrent mark-sweep generation total 843776K, used 0K [0x00000000bc800000, 0x00000000f0000000, 0x00000000f0000000)\n concurrent-mark-sweep perm gen total 131072K, used 14021K [0x00000000f0000000, 0x00000000f8000000, 0x0000000100000000)\n==&gt; /var/log/hadoop/hdfs/gc.log-201611231429 &lt;==\nOpenJDK 64-Bit Server VM (24.121-b00) for linux-amd64 JRE (1.7.0_121-b00), built on Nov 18 2016 00:22:36 by \"mockbuild\" with gcc 4.8.3 20140911 (Red Hat 4.8.3-9)\nMemory: 4k page, physical 15403944k(15118720k free), swap 0k(0k free)\nCommandLine flags: -XX:ErrorFile=/var/log/hadoop/hdfs/hs_err_pid%p.log -XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=209715200 -XX:MaxPermSize=268435456 -XX:MaxTenuringThreshold=6 -XX:NewSize=209715200 -XX:OldPLABSize=16 -XX:ParallelGCThreads=4 -XX:PermSize=134217728 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC \nHeap\n par new generation   total 184320K, used 108295K [0x00000000b0000000, 0x00000000bc800000, 0x00000000bc800000)\n  eden space 163840K,  66% used [0x00000000b0000000, 0x00000000b69c1e90, 0x00000000ba000000)\n  from space 20480K,   0% used [0x00000000ba000000, 0x00000000ba000000, 0x00000000bb400000)\n  to   space 20480K,   0% used [0x00000000bb400000, 0x00000000bb400000, 0x00000000bc800000)\n concurrent mark-sweep generation total 843776K, used 0K [0x00000000bc800000, 0x00000000f0000000, 0x00000000f0000000)\n concurrent-mark-sweep perm gen total 131072K, used 14012K [0x00000000f0000000, 0x00000000f8000000, 0x0000000100000000)\n==&gt; /var/log/hadoop/hdfs/gc.log-201611231434 &lt;==\nOpenJDK 64-Bit Server VM (24.121-b00) for linux-amd64 JRE (1.7.0_121-b00), built on Nov 18 2016 00:22:36 by \"mockbuild\" with gcc 4.8.3 20140911 (Red Hat 4.8.3-9)\nMemory: 4k page, physical 15403944k(14744240k free), swap 0k(0k free)\nCommandLine flags: -XX:ErrorFile=/var/log/hadoop/hdfs/hs_err_pid%p.log -XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=209715200 -XX:MaxPermSize=268435456 -XX:MaxTenuringThreshold=6 -XX:NewSize=209715200 -XX:OldPLABSize=16 -XX:ParallelGCThreads=4 -XX:PermSize=134217728 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC \nHeap\n par new generation   total 184320K, used 108310K [0x00000000b0000000, 0x00000000bc800000, 0x00000000bc800000)\n  eden space 163840K,  66% used [0x00000000b0000000, 0x00000000b69c58c8, 0x00000000ba000000)\n  from space 20480K,   0% used [0x00000000ba000000, 0x00000000ba000000, 0x00000000bb400000)\n  to   space 20480K,   0% used [0x00000000bb400000, 0x00000000bb400000, 0x00000000bc800000)\n concurrent mark-sweep generation total 843776K, used 0K [0x00000000bc800000, 0x00000000f0000000, 0x00000000f0000000)\n concurrent-mark-sweep perm gen total 131072K, used 14019K [0x00000000f0000000, 0x00000000f8000000, 0x0000000100000000)\n==&gt; /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.out.4 &lt;==\nulimit -a for user hdfs\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 60086\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 128000\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n==&gt; /var/log/hadoop/hdfs/gc.log-201611231437 &lt;==\nOpenJDK 64-Bit Server VM (24.121-b00) for linux-amd64 JRE (1.7.0_121-b00), built on Nov 18 2016 00:22:36 by \"mockbuild\" with gcc 4.8.3 20140911 (Red Hat 4.8.3-9)\nMemory: 4k page, physical 15403944k(14739724k free), swap 0k(0k free)\nCommandLine flags: -XX:ErrorFile=/var/log/hadoop/hdfs/hs_err_pid%p.log -XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=209715200 -XX:MaxPermSize=268435456 -XX:MaxTenuringThreshold=6 -XX:NewSize=209715200 -XX:OldPLABSize=16 -XX:ParallelGCThreads=4 -XX:PermSize=134217728 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC \nHeap\n par new generation   total 184320K, used 108295K [0x00000000b0000000, 0x00000000bc800000, 0x00000000bc800000)\n  eden space 163840K,  66% used [0x00000000b0000000, 0x00000000b69c1ca8, 0x00000000ba000000)\n  from space 20480K,   0% used [0x00000000ba000000, 0x00000000ba000000, 0x00000000bb400000)\n  to   space 20480K,   0% used [0x00000000bb400000, 0x00000000bb400000, 0x00000000bc800000)\n concurrent mark-sweep generation total 843776K, used 0K [0x00000000bc800000, 0x00000000f0000000, 0x00000000f0000000)\n concurrent-mark-sweep perm gen total 131072K, used 14021K [0x00000000f0000000, 0x00000000f8000000, 0x0000000100000000)\n==&gt; /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.out &lt;==\nulimit -a for user hdfs\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 60086\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 128000\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n==&gt; /var/log/hadoop/hdfs/SecurityAuth.audit &lt;==\n2016-11-23 02:10:46,450 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:10:46,457 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:11:46,454 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:11:46,458 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:12:46,443 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:16:46,443 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:18:46,459 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:19:46,438 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:19:46,456 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:19:46,456 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:20:46,443 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:20:46,444 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:20:46,445 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:22:46,439 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:23:46,455 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:24:46,437 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:24:46,447 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:25:46,449 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:26:46,437 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:27:46,459 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:28:46,438 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:28:46,449 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:29:46,437 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:31:46,440 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:32:46,456 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:33:46,438 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:34:46,445 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:35:46,435 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:36:46,437 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:39:46,435 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:41:46,447 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:42:46,435 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:43:46,438 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:43:46,447 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:44:46,437 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:45:46,436 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:46:46,438 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 02:46:46,451 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 06:33:25,344 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n2016-11-23 06:34:25,322 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for yarn (auth:SIMPLE)\n==&gt; /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.out.3 &lt;==\nulimit -a for user hdfs\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 60086\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 128000\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n==&gt; /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.out.1 &lt;==\nulimit -a for user hdfs\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 60086\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 128000\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n==&gt; /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.out.2 &lt;==\nulimit -a for user hdfs\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 60086\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 128000\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n==&gt; /var/log/hadoop/hdfs/hadoop-hdfs-datanode-ip-10-0-1-104.log &lt;==\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)\nat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)\nat org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)\nat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)\nat org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2479)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2521)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2503)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2395)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2442)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2623)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2647)\n2016-11-23 14:37:59,749 WARN  datanode.DataNode (DataNode.java:checkStorageLocations(2524)) - Invalid dfs.datanode.data.dir /hadoopfs/fs2/hdfs/datanode : \njava.io.FileNotFoundException: File file:/hadoopfs/fs2/hdfs/datanode does not exist\nat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:624)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:850)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:614)\nat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:422)\nat org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)\nat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)\nat org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2479)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2521)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2503)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2395)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2442)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2623)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2647)\n2016-11-23 14:37:59,749 ERROR datanode.DataNode (DataNode.java:secureMain(2630)) - Exception in secureMain\njava.io.IOException: All directories in dfs.datanode.data.dir are invalid: \"/hadoopfs/fs1/hdfs/datanode\" \"/hadoopfs/fs2/hdfs/datanode\" \nat org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2530)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2503)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2395)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2442)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2623)\nat org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2647)\n2016-11-23 14:37:59,751 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1\n2016-11-23 14:37:59,753 INFO  datanode.DataNode (LogAdapter.java:info(47)) - SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at ip-10-0-1-104.us-west-2.compute.internal/10.0.1.104\n************************************************************/\n==&gt; /var/log/hadoop/hdfs/hdfs-audit.log &lt;==\n==&gt; /var/log/hadoop/hdfs/gc.log-201611230628 &lt;==\nOpenJDK 64-Bit Server VM (24.121-b00) for linux-amd64 JRE (1.7.0_121-b00), built on Nov 18 2016 00:22:36 by \"mockbuild\" with gcc 4.8.3 20140911 (Red Hat 4.8.3-9)\nMemory: 4k page, physical 15403944k(10532916k free), swap 0k(0k free)\nCommandLine flags: -XX:ErrorFile=/var/log/hadoop/hdfs/hs_err_pid%p.log -XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=209715200 -XX:MaxPermSize=268435456 -XX:MaxTenuringThreshold=6 -XX:NewSize=209715200 -XX:OldPLABSize=16 -XX:ParallelGCThreads=4 -XX:PermSize=134217728 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC \n2016-11-23T06:28:20.553+0000: 2.298: [GC2016-11-23T06:28:20.553+0000: 2.299: [ParNew: 163840K-&gt;13323K(184320K), 0.0193080 secs] 163840K-&gt;13323K(1028096K), 0.0194190 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] \n2016-11-23T06:29:40.990+0000: 82.735: [GC2016-11-23T06:29:40.990+0000: 82.735: [ParNew: 177163K-&gt;14141K(184320K), 0.0336240 secs] 177163K-&gt;18391K(1028096K), 0.0337210 secs] [Times: user=0.10 sys=0.01, real=0.03 secs] \nHeap\n par new generation   total 184320K, used 86126K [0x00000000b0000000, 0x00000000bc800000, 0x00000000bc800000)\n  eden space 163840K,  43% used [0x00000000b0000000, 0x00000000b464c658, 0x00000000ba000000)\n  from space 20480K,  69% used [0x00000000ba000000, 0x00000000badcf498, 0x00000000bb400000)\n  to   space 20480K,   0% used [0x00000000bb400000, 0x00000000bb400000, 0x00000000bc800000)\n concurrent mark-sweep generation total 843776K, used 4250K [0x00000000bc800000, 0x00000000f0000000, 0x00000000f0000000)\n concurrent-mark-sweep perm gen total 131072K, used 34364K [0x00000000f0000000, 0x00000000f8000000, 0x0000000100000000)\n==&gt; /var/log/hadoop/hdfs/gc.log-201611230552 &lt;==\nOpenJDK 64-Bit Server VM (24.121-b00) for linux-amd64 JRE (1.7.0_121-b00), built on Nov 18 2016 00:22:36 by \"mockbuild\" with gcc 4.8.3 20140911 (Red Hat 4.8.3-9)\nMemory: 4k page, physical 15403944k(9388860k free), swap 0k(0k free)\nCommandLine flags: -XX:ErrorFile=/var/log/hadoop/hdfs/hs_err_pid%p.log -XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=209715200 -XX:MaxPermSize=268435456 -XX:MaxTenuringThreshold=6 -XX:NewSize=209715200 -XX:OldPLABSize=16 -XX:ParallelGCThreads=4 -XX:PermSize=134217728 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC \n2016-11-23T05:52:31.904+0000: 2.287: [GC2016-11-23T05:52:31.904+0000: 2.287: [ParNew: 163840K-&gt;13341K(184320K), 0.0192840 secs] 163840K-&gt;13341K(1028096K), 0.0193830 secs] [Times: user=0.04 sys=0.02, real=0.02 secs] \n2016-11-23T05:54:12.618+0000: 103.001: [GC2016-11-23T05:54:12.618+0000: 103.001: [ParNew: 177181K-&gt;13401K(184320K), 0.0376900 secs] 177181K-&gt;17652K(1028096K), 0.0378070 secs] [Times: user=0.10 sys=0.01, real=0.04 secs] \nHeap\n par new generation   total 184320K, used 137569K [0x00000000b0000000, 0x00000000bc800000, 0x00000000bc800000)\n  eden space 163840K,  75% used [0x00000000b0000000, 0x00000000b7941db8, 0x00000000ba000000)\n  from space 20480K,  65% used [0x00000000ba000000, 0x00000000bad16658, 0x00000000bb400000)\n  to   space 20480K,   0% used [0x00000000bb400000, 0x00000000bb400000, 0x00000000bc800000)\n concurrent mark-sweep generation total 843776K, used 4250K [0x00000000bc800000, 0x00000000f0000000, 0x00000000f0000000)\n concurrent-mark-sweep perm gen total 131072K, used 32915K [0x00000000f0000000, 0x00000000f8000000, 0x0000000100000000)\nCommand failed after 1 tries</p>","tags":["mount","datanode"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-24 13:23:34.0","id":68304,"title":"Is there a way to add a custom function from HBase Shell?","body":"<p>I have a custom function written in Ruby to count the number of columns. But I don't know how to add this function in HBase shell? Is there a way to add any custom functions of UDFs in HBase shell?</p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-24 05:31:46.0","id":68238,"title":"How to crawl dynamic urls","body":"","tags":["How-To/Tutorial","nutch-2.3"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-24 19:50:42.0","id":68334,"title":"-bash: ambari-admin-password-reset: command not found","body":"<p>I am using HDP 2.5 Sandbox (Virtualbox).   I log on as root but I can't execute the following command.</p><p>-bash: ambari-admin-password-reset: command not found</p><p>I also tried ambari-agent restart or mysql.   I always get \"-bash: ... command not found\"</p><p>Any help is appreciated.</p><p>Marco</p>","tags":["bash"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-25 13:41:40.0","id":68416,"title":"Ambari server failed to start after SSL setup","body":"<p>I am trying SSL setup with a ambari server (ambari 2.4 version). Here are steps I followed :</p><p>[root@hdpsecurity ssl]# ambari-server  setup-security</p><p>Using python \n/usr/bin/python</p><p>Security setup options...</p><p>===========================================================================</p><p>Choose one of the following options:</p><p>  [1] Enable HTTPS for\nAmbari server.</p><p>  [2] Encrypt\npasswords stored in ambari.properties file.</p><p>  [3] Setup Ambari\nkerberos JAAS configuration.</p><p>  [4] Setup\ntruststore.</p><p>  [5] Import\ncertificate to truststore.</p><p>===========================================================================</p><p>Enter choice, (1-5): 1</p><p>Do you want to configure HTTPS [y/n] (y)?</p><p>SSL port [8443] ?</p><p>Enter path to Certificate: /etc/httpd/ssl/wserver.crt</p><p>Enter path to Private Key: /etc/httpd/ssl/wserver.key</p><p>Please enter password for Private Key:</p><p>Generating random password for HTTPS keystore...done.</p><p>Importing and saving Certificate...done.</p><p>Ambari server URL changed. To make use of the Tez View in\nAmbari please update the property tez.tez-ui.history-url.base in tez-site</p><p>Adjusting ambari-server permissions and ownership...</p><p>[root@hdpsecurity ssl]# netstat -tulpn|grep 8443</p><p>[root@hdpsecurity ssl]# service ambari-server restart</p><p>Using python \n/usr/bin/python</p><p>Restarting ambari-server</p><p>Using python \n/usr/bin/python</p><p>Stopping ambari-server</p><p>Ambari Server is not running</p><p>Using python \n/usr/bin/python</p><p>Starting ambari-server</p><p>Ambari Server running with administrator privileges.</p><p>Organizing resource files at\n/var/lib/ambari-server/resources...</p><p>Ambari database consistency check started...</p><p>No errors were found.</p><p>Ambari database consistency check finished</p><p>Server PID at: /var/run/ambari-server/ambari-server.pid</p><p>Server out at: /var/log/ambari-server/ambari-server.out</p><p>Server log at: /var/log/ambari-server/ambari-server.log</p><p>Waiting for server start....................</p><p>ERROR: Exiting with exit code -1.</p><p>REASON: Ambari Server java process died with exitcode 255.\nCheck /var/log/ambari-server/ambari-server.out for more information.</p><p>Here is the Ambari-server.log:</p><p>25 Nov 2016 17:35:07,916  INFO [main] ViewRegistry:1811 - Auto creating instance of view CAPACITY-SCHEDULER for cluster hdpsecurity.\n25 Nov 2016 17:35:07,917 ERROR [main] AmbariServer:927 - Failed to run the Ambari Server\njava.lang.NoSuchMethodError: org.apache.ambari.view.ViewInstanceDefinition.getClusterHandle()Ljava/lang/String;\n        at org.apache.ambari.view.capacityscheduler.PropertyValidator.validateProperty(PropertyValidator.java:40)\n        at org.apache.ambari.server.orm.entities.ViewInstanceEntity.getValidationResult(ViewInstanceEntity.java:927)\n        at org.apache.ambari.server.orm.entities.ViewInstanceEntity.validate(ViewInstanceEntity.java:871)\n        at org.apache.ambari.server.view.ViewRegistry.installViewInstance(ViewRegistry.java:572)\n        at org.apache.ambari.server.view.ViewRegistry.addAutoInstanceDefinition(ViewRegistry.java:1814)\n        at org.apache.ambari.server.view.ViewRegistry.readViewArchive(ViewRegistry.java:1684)\n        at org.apache.ambari.server.view.ViewRegistry.readViewArchives(ViewRegistry.java:1599)\n        at org.apache.ambari.server.view.ViewRegistry.readViewArchives(ViewRegistry.java:520)\n        at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:494)\n        at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:925)</p><p>I am using self signed SSL and followed steps mentioned in HDP document.</p>","tags":["security","ambari-views","ambari-server","ambari-ssl"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-23 21:05:41.0","id":68173,"title":"Ambari fails when registering node (Amazon EC2)","body":"<p>Hi, I am trying to use Apache Ambari to configure a Hadoop cluster on EC2.</p><p>During the registration phase I get this error:</p><pre>Command start time 2016-11-23 20:25:12\n('Traceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\nINFO 2016-11-23 20:25:18,716 ExitHelper.py:53 - Performing cleanup before exiting...\nINFO 2016-11-23 20:25:18,907 main.py:74 - loglevel=logging.INFO\nINFO 2016-11-23 20:25:18,907 DataCleaner.py:39 - Data cleanup thread started\nINFO 2016-11-23 20:25:18,908 DataCleaner.py:120 - Data cleanup started\nINFO 2016-11-23 20:25:18,909 DataCleaner.py:122 - Data cleanup finished\nINFO 2016-11-23 20:25:18,930 PingPortListener.py:50 - Ping port listener started on port: 8670\nINFO 2016-11-23 20:25:18,931 main.py:289 - Connecting to Ambari server at https://IPADDRESS.us-west-2.compute.internal:8440 (172.31.37.172)\nINFO 2016-11-23 20:25:18,931 NetUtil.py:59 - Connecting to https://IPADDRESS.us-west-2.compute.internal:8440/ca\nERROR 2016-11-23 20:25:18,983 NetUtil.py:77 - [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)\nERROR 2016-11-23 20:25:18,983 NetUtil.py:78 - SSLError: Failed to connect. Please check openssl library versions. \nRefer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.\nWARNING 2016-11-23 20:25:18,983 NetUtil.py:105 - Server at https://IPADDRESS.us-west-2.compute.internal:8440 is not reachable, sleeping for 10 seconds...\n', None)\n('Traceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 312, in &lt;module&gt;\n    main(heartbeat_stop_callback)\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 248, in main\n    stop_agent()\n  File \"/usr/lib/python2.6/site-packages/ambari_agent/main.py\", line 198, in stop_agent\n    sys.exit(1)\nSystemExit: 1\nINFO 2016-11-23 20:25:18,716 ExitHelper.py:53 - Performing cleanup before exiting...\nINFO 2016-11-23 20:25:18,907 main.py:74 - loglevel=logging.INFO\nINFO 2016-11-23 20:25:18,907 DataCleaner.py:39 - Data cleanup thread started\nINFO 2016-11-23 20:25:18,908 DataCleaner.py:120 - Data cleanup started\nINFO 2016-11-23 20:25:18,909 DataCleaner.py:122 - Data cleanup finished\nINFO 2016-11-23 20:25:18,930 PingPortListener.py:50 - Ping port listener started on port: 8670\nINFO 2016-11-23 20:25:18,931 main.py:289 - Connecting to Ambari server at https://IPADDRESS.us-west-2.compute.internal:8440 (172.31.37.172)\nINFO 2016-11-23 20:25:18,931 NetUtil.py:59 - Connecting to https://IPADDRESS.us-west-2.compute.internal:8440/ca\nERROR 2016-11-23 20:25:18,983 NetUtil.py:77 - [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)\nERROR 2016-11-23 20:25:18,983 NetUtil.py:78 - SSLError: Failed to connect. Please check openssl library versions. \nRefer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.\nWARNING 2016-11-23 20:25:18,983 NetUtil.py:105 - Server at https://IPADDRESS.us-west-2.compute.internal:8440 is not reachable, sleeping for 10 seconds...\n', None)\n\n\nConnection to IPADDRESS.us-west-2.compute.internal closed.\nSSH command execution finished\nhost=IPADDRESS.us-west-2.compute.internal, exitcode=0\nCommand end time 2016-11-23 20:25:21\n\n\nRegistering with the server...\nRegistration with the server failed.\n</pre><p>I think it is something basic, but I was not able to solve.</p><p>Any advice?</p><p>Thank you</p>","tags":["hadoop","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-24 10:55:25.0","id":68279,"title":"Ingestion vs stream processing","body":"<p>Hi All,</p><p>I am having difficulty in understanding the use cases when we should go for stream processing vs ingestion</p><p>something like storm vs nifi or spark streaming vs nifi</p><p>some text answers seem to be if we want to aggregation, windowing operation etc. we should go for stream processing.</p><p>Other than the internal architecture differences, can someone please give an example of something with real data which can be done in storm/spark streaming compared to nifi or flume.</p><p>Thanks,</p><p>Avijeet</p>","tags":["stream-processing","data-ingestion"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-11-23 21:25:50.0","id":68175,"title":"RANGER installation giving warnings","body":"<p>I have installed Ranger with SOLR support but the installation is giving warnings as follows :</p><p>\"curl\" has never worked on my cluster since I am behind proxy and I have tried every possible solution available on web but it wont work. so is Curl mandatory for Ranger ?  that would be a disaster</p><pre>Traceback (most recent call last):\n  File \"/var/lib/ambari-agent/cache/common-services/RANGER/0.4.0/package/scripts/service_check.py\", line 49, in &lt;module&gt;\n    RangerServiceCheck().execute()\n  File \"/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\", line 280, in execute\n    method(env)\n  File \"/var/lib/ambari-agent/cache/common-services/RANGER/0.4.0/package/scripts/service_check.py\", line 34, in service_check\n    self.check_ranger_admin_service(params.ranger_external_url, params.upgrade_marker_file)\n  File \"/var/lib/ambari-agent/cache/common-services/RANGER/0.4.0/package/scripts/service_check.py\", line 43, in check_ranger_admin_service\n    logoutput=True)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/base.py\", line 155, in __init__\n    self.env.run()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 160, in run\n    self.run_action(resource, action)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/environment.py\", line 124, in run_action\n    provider_action()\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py\", line 273, in action_run\n    tries=self.resource.tries, try_sleep=self.resource.try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 71, in inner\n    result = function(command, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 93, in checked_call\n    tries=tries, try_sleep=try_sleep)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 141, in _call_wrapper\n    result = _call(command, **kwargs_copy)\n  File \"/usr/lib/python2.6/site-packages/resource_management/core/shell.py\", line 294, in _call\n    raise Fail(err_msg)\nresource_management.core.exceptions.Fail: Execution of 'curl -s -o /dev/null -w'%{http_code}' --negotiate -u: -k http://hadoop1.tolls.dot.state.fl.us:6080/login.jsp | grep 200' returned 1.\n\n\n</pre>","tags":["hadoop","hdp-2.5.0","Ambari","SOLR"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-24 20:41:19.0","id":68347,"title":"hdp 2.5 sandbox URL not working","body":"<p>I Installed the hdp 2.5 sandbox on VMware. When i launch the hortonworks sandbox session in browse http://192.168.85.134:8888/ its doesn't work. I tried following (1) added the hostname sandbox.hortonworks.com in the windows hosts file. it does not work. However, when at command prompt, i ping the IP 192.168.85.134 i am able to reach the sandbox (2) i am also able to login with SSH client on the sandbox. </p><p>Main issue in on the browser I am able to launch the sandbox session. kindly advice.</p>","tags":["hdp-2.5.0","Sandbox"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-24 04:02:03.0","id":68223,"title":"how can i get metric kafka data message in for each topic per second?","body":"<p>hi anybody im still newbie at grafana\ncan anyone teach me how to get metric data from each topic in kafka and visualize in grafana??\nthanks before</p>","tags":["Kafka","grafana"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-25 12:28:52.0","id":68413,"title":"Access denied while creating Hive Table","body":"<p>Hi, </p><p>I am creating external hive table , but suddenly I am getting below error message. </p><p><img src=\"/storage/attachments/9774-satjs.png\" alt=\"\"></p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-25 00:56:38.0","id":68319,"title":"Is HDP 2.5 with Amabari 2.4.1 supported on Ubuntu 14.04.5?","body":"<p>\n\tI have been trying to install various versions of HDP on Ubuntu 14.04.5 and have been facing issues.</p><p>\n\tI need help with:</p><p>\n\tCreating Ambari and HDP repo on ubuntu14. I have tried to follow the various docs, I have placed the \"untared\" HDP & HDP-UTILS in my Apache root(<strong>/var/www/html</strong>) and I am able to view them when I browse <strong>&lt;ServerIP&gt;/hdp</strong> (HDP2.5 & HDP-UTILS) and <strong>&lt;ServerIP&gt;/Ambari</strong>(Ambari 2.4.1).</p><p>The <a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.1.0/bk_ambari-installation/content/preparing_the_ambari_repository_configuration_file.html\">documents</a> say \"Place the <strong>ambari.repo</strong> file on the machine you plan to use for the Ambari\n              Server\". Then it says <strong>For Debain/Ubuntu: </strong><code>/etc/apt/sources.list.d/ambari.list</code></p><p>\n\tI am not sure what is to be done. I changed the path in the <em>ambari.list </em>to point to &lt;ServerIP&gt;/Ambari. But my installation fails.</p><ol><li>Am I missing any step? I am unable to follow what is to be done on Ubuntu14 after reading through: \n<a href=\"http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.1.0/bk_ambari-installation/content/preparing_the_ambari_repository_configuration_file.html\">http://docs.hortonworks.com/HDPDocuments/Ambari-2.4.1.0/bk_ambari-installation/content/preparing_the_ambari_repository_configuration_file.html</a>\n<p>Where am I supposed to place (this is ambari.repo file I assume):</p><pre>[Updates-Ambari-2.4.1.0]\nname=Ambari-2.4.1.0-Updates\nbaseurl=...</pre></li><li> The next step says \"Edit the <code>/etc/yum/pluginconf.d/priorities.conf</code> file\" this file doesn't exist in Ubuntu!</li></ol><p>I have earlier installed HDP 2.2 with Ambari 2.1, I felt it was much easier to install HDP2.2 (with internet). Now I am trying to install this on VMs without Internet access. Could somebody help me with setting up the repo so that I can complete the HDP2.5 install on Ubuntu 14.04.</p>","tags":["Ambari","hdp-2.5.0","ubuntu","installation"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-25 07:22:46.0","id":68379,"title":"Is there any way to add workflow feature on nifi dataflow","body":"<p>\n\tHI</p><p>\n\tWe are working on HDF nifi cluster. We would like to have some workflow feature on HDF.  e.g. If I can execute the oozie task after the data was imported to hdfs through nifi. </p><p><a href=\"/storage/attachments/9789-nifi-dataflow.jpg\">nifi-dataflow.jpg</a></p><p>Is there any idea to implement the behavior.  </p><p>Thanks</p><p>Paul</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-23 22:14:51.0","id":68198,"title":"Data Push to Azure Event Hub","body":"<p>I am trying to push data to Azure Event Hub using PutAzureEventHub processor in NiFi but the data just queues up.</p><p>Excerpt from the nifi-app.log :</p><pre>\"o.a.n.controller.StandardProcessorNode\n Timed out while waiting for OnScheduled of 'PutAzureEventHub' processor\n to finish. An attempt is made to cancel the task\nvia Thread.interrupt(). However it does not guarantee that the task will\n be canceled since the code inside current OnScheduled operation may \nhave been written to ignore interrupts which may result in a runaway \nthread. This could lead to more issues, eventually\nrequiring NiFi to be restarted. This is usually a bug in the target \nProcessor 'PutAzureEventHub[id=438643cd-0158-1000-f73f-69442635b0d0]' \nthat needs to be documented, reported and eventually fixed.\"</pre><p>Please guide.</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-23 23:34:53.0","id":68199,"title":"HDPCD: Java Exam Environment Issue","body":"<p>Hello Team,</p><p>Before 2-3 days I had appeared for HDPCD: java exam. Initially there was problem with VM connectivity, but proctor resolved it from his end. I wanted to analyse data set for exam but it was not available on VM. When I tried to do hadoop fs -ls , Its was giving error \"Can Not Connect to Name Node: \n  I immediately informed proctor, but he  is not ready to help & said he can not help for the exam content. I thought of finishing the coding part first & then check, \nAfter creating jar, I again tried to connect to  cluster but its in vain,  I tried to ssh the name node, I tried to connect to Ambari, Even I tried to run start-all-services.sh but was of no use & giving same error.\nI informed proctor, multiple times but he was not ready to help & informed me that he will send report to Hortonwork about discussion.  \nEven though I have enough time to test & validate, I could not test & validate my work because of these environment issues..</p><p>I had already sent mail to certification alias i.e. certification@hortonworks.com, but still there is no response. Can some one guide me about this</p><p>Thanks in advance</p>","tags":["hdpcd","examlocal"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-11-24 13:03:15.0","id":68291,"title":"How could I use pandas library in Pyspark in ZEPPELIN?","body":"<p>Hi everyone!</p><p>I am working with the Pyspark interpreter in a Zeppelin notebook, and I want to use \"pandas\" library functionalites, but when I try this command:</p><p>    import pandas as pd</p><p>I get the next error message:</p><p>    Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2633231603377305574.py\", line 239, in &lt;module&gt; eval(compiledCode)\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nImportError: No module named pandas</p><p>I have already installed Pandas in my Virtual Machine where Zeppelin is running, and restart ambari-server as it's explained in the next post:</p><p>http://stackoverflow.com/questions/39221959/zeppelin-unable-to-import-pandas-numpy-scipy/39254183#39254183</p><p>How could I do?</p>","tags":["import","pyspark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-24 17:38:40.0","id":68320,"title":"Ambari+HDP installation on Ubuntu Documents not complete","body":"<p>Ubuntu Installation is not properly documented in Hortonworks docs. Many docs don't even mention Ubuntu OS.\nIs HW trying to avoid ubuntu support?\n</p>","tags":["ubuntu"],"track_id":82,"track_name":"Design & Architecture"}
{"creation_date":"2016-11-25 00:33:57.0","id":68361,"title":"Fail to install kerberos client in sandbox 2.5","body":"<p>I try to enable kerberos in sandbox 2.5, but fais with the following error. I check this file, it indeed doesn't exist.</p><pre>Caught an exception while executing custom service command: &lt;class 'ambari_agent.AgentException.AgentException'&gt;: 'Script /var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py does not exist'; 'Script /var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py does not exist'</pre>","tags":["Sandbox"],"track_id":62,"track_name":"Security"}
{"creation_date":"2016-11-25 10:52:47.0","id":68397,"title":"Whether flume reads timestamp from the particular digits onwards?","body":"<p>Hi All,</p><p>I am in a situation whether data is continuously being loaded into a particular folder based on date-wise. Filename would be like </p><p>21511182016 - where 215 is the folder name followed by timestamp in the order of MM/DD/YYYY.</p><p>I have followed this configuration for log data.</p><p># a source for for error log file</p><p>agent1.sources = tailAccessSource tailErrorSource</p><p># I define one sink\nagent1.sinks = hdfsSink\n# I define one channel\nagent1.channels = memChannel01\n \n# Bind the source and sink to the channel\n# Both sources will use the memory channel \nagent1.sources.tailAccessSource.channels = memChannel01\nagent1.sources.tailErrorSource.channels = memChannel01\nagent1.sinks.hdfsSink.channel = memChannel01\n \n \n# Define the type and options for each sources\nagent1.sources.tailAccessSource.type = exec\n<strong>agent1.sources.tailAccessSource.command = tail -F /var/log/httpd/access_log</strong>\n \nagent1.sources.tailErrorSource.type = exec\n<strong>agent1.sources.tailErrorSource.command = tail -F /var/log/httpd/error_log</strong>\n \n# Define the type and options for the channel\nagent1.channels.memChannel01.type = memory\nagent1.channels.memChannel01.capacity = 100000\nagent1.channels.memChannel01.transactionCapacity = 10000\n \n \n# Define the type and options for the sink\n# Note: namenode is the hostname the hadoop namenode server\n#  flume/data-example.1/ is the directory where the apache logs will be stored\nagent1.sinks.hdfsSink.type = hdfs\nagent1.sinks.hdfsSink.hdfs.path = hdfs://namenode/flume/data-example.1/\nagent1.sinks.hdfsSink.hdfs.fileType = DataStream\nagent1.sinks.hdfsSink.hdfs.rollCount = 0\nagent1.sinks.hdfsSink.hdfs.rollSize = 0\nagent1.sinks.hdfsSink.hdfs.rollInterval = 60.</p><p>Here we are <strong>Hard Coding the Files inside a folder,\n</strong></p><p>Now how flume should take the filenames automatically from the folder with time stamp present. </p><p>Eg)  21511182016 is the <strong>Filename.\n\n</strong></p><p><strong>agent1.sources.tailAccessSource.command = tail -F /var/log/httpd/access_log (Here the filename is hard coded)\n</strong></p><p>\n<strong></strong></p><p>\n<strong></strong></p><p><strong>I need flume to take the files automatically from a folder from FTP Server Source.</strong></p><p>Anyone tried </p><p><strong>tail -F /var/log/httpd/access_log \n</strong></p><p>with Source from FTP Server logs (text files).</p><p>Please do the needful. </p><p>Thanks in Advance<strong>\n</strong></p>","tags":["logs","flume-1.6"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-23 22:01:35.0","id":68177,"title":"NiFi processor putHiveStreaming exception.","body":"<p>I am getting following exception while invoking the putHiveStreaming processor.</p><p>016-11-23 04:16:43,024 ERROR [Timer-Driven Process Thread-4] o.a.n.processors.hive.PutHiveStreaming PutHiveStreaming[id=8f50a88f-0158-1000-4941-4b83f9b57917] Hive Streaming connect/write error, flow file will be penalized and routed to retry\n2016-11-23 04:16:43,025 ERROR [Timer-Driven Process Thread-4] o.a.n.processors.hive.PutHiveStreaming\norg.apache.nifi.util.hive.HiveWriter$ConnectFailure: Failed connecting to EndPoint {metaStoreUri='thrift://sandbox.hortonworks.com:9083', database='default', table='store_order', partitionVals=[] }\n        at org.apache.nifi.util.hive.HiveWriter.&lt;init&gt;(HiveWriter.java:80) ~[nifi-hive-processors-1.0.0.2.0.0.0-579.jar:1.0.0.2.0.0.0-579]\n        at org.apache.nifi.util.hive.HiveUtils.makeHiveWriter(HiveUtils.java:45) ~[nifi-hive-processors-1.0.0.2.0.0.0-579.jar:1.0.0.2.0.0.0-579]\n        at org.apache.nifi.processors.hive.PutHiveStreaming.makeHiveWriter(PutHiveStreaming.java:827) ~[nifi-hive-processors-1.0.0.2.0.0.0-579.jar:1.0.0.2.0.0.0-579]\n        at org.apache.nifi.processors.hive.PutHiveStreaming.getOrCreateWriter(PutHiveStreaming.java:738) ~[nifi-hive-processors-1.0.0.2.0.0.0-579.jar:1.0.0.2.0.0.0-579]</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-24 19:30:01.0","id":68339,"title":"Ambari Server setup Java","body":"<p>While setting up Ambari Server, why doesn't ambari pick up the JAVA_HOME path from the $PATH?\nWhy does it ask for a new JDK install?\n\nWhen I specify :</p><pre>ambari-server setup –j /opt/java/</pre><p>It later throws an error</p><pre>ERROR: Exiting with exit code 1.REASON: Downloading or installing JDK failed: 'Fatal exception: Java home path or java binary file is unavailable. Please put correct path to java home., exit code 1'. Exiting.  </pre>","tags":["ambari-server","java"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-25 08:38:30.0","id":68384,"title":"NiFi :Is it possible to collect file via remote directory (from Windows)?","body":"<p>NiFi installed in a linux (Ubuntu Server).</p><p>Is it possible to collect file from a remote directory (ex. windows)? </p><p>In our case, we're collecting data from different servers (SunOS, Win XP, unix, linux).</p>","tags":["Nifi"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-23 17:44:48.0","id":68142,"title":"hive contrib versions","body":"<p>Hello,</p><p>I need to add a version of hive contrib jar into hive, and the version of my hive is different from the latest available jar in maven. The jar that I have is:</p><p>// https://mvnrepository.com/artifact/org.apache.hive/hive-contrib\ncompile group: 'org.apache.hive', name: 'hive-contrib', version: '1.2.1000.2.4.2.11-1'</p><p>The version of my hive is:</p><p>hive --version</p><p>Hive 1.2.1000.2.5.0.1-60</p><p>Subversion git://c66-slave-ff632c10-2/grid/0/jenkins/workspace/HDP-parallel-centos6/SOURCES/hive -r d06374085c979d5fb87ace79fe1afd51f7d9b64b</p><p>Compiled by jenkins on Tue Oct 11 22:49:50 UTC 2016</p><p>From source with checksum 7926a8417dccffc24fcb29de27df368e</p><p>Would this cause any incompatibilities? How do I get the latest version of contrib jar that would work?</p><p>Thanks</p>","tags":["Hive"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-11-24 09:15:03.0","id":68262,"title":"I need to know how to use regex for new line in pig latin","body":"<p>I am using catalana log . my input has is like below lines. for date and others i have no problem but I need to read neext line which is start after INFO . i tried alot but i do not how to bring next line .I have used \\\\n and \\\\r but they did not work.</p><p>my regex is like this .</p><p>A= LOAD 'catalina.log' USING TextLoader AS (line:chararray);</p><p> B = FOREACH A GENERATE FLATTEN(REGEX_EXTRACT_ALL(line, \n '^([a-zA-z]{3}\\\\s[0-9]{1,2},\\\\s[0-9]{4}\\\\s[0-9]{1,2}:[0-9]{2}:[0-9]{2}\\\\s[A-Z]{2})\\\\n+(.*)INFO:(.*)));</p><p>DUMP B;</p><p>input :\nNov 3, 2016 11:00:06 AM org.apache.catalina.startup.HostConfig deployDescriptor</p><p>INFO: Deploying configuration descriptor host-manager.xmlF</p><p>output: Nov 3, 2016 11:00:06 AM org.apache.catalina.startup.HostConfig deployDescriptor</p>","tags":["Pig"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-24 09:38:54.0","id":68255,"title":"Got 401 - Full authentication is required to access this resource when post to ATLAS API","body":"<p>Hi guys, I 've got another question for Atlas, I finally installed and got Atlas 0.7 run on my server, but when I tried to post new types to Atlas, it got a 401 which is Full authentication is required to access this resource, do you know what should I do to get the admission to access the api? </p><p>Qinglin</p>","tags":["atlas-api"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-11-24 20:09:55.0","id":68335,"title":"Twitter Data is in HIVE - How do I visualise it?","body":"<p>Hi</p><p>I am currently storing live twitter feeds via flume in to Hive. Next I'd like to visualise the data in real-time. How best can I achieve this and what visualisation tool do you recommend I use (as a beginner). I've attempted to do this in Tableau desktop, but wasn't able to populate graphs in real time and had issues with formatting the twitter time/date stamp in tableau.</p><p>Thanks!</p>","tags":["Hive"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-24 00:16:13.0","id":68203,"title":"hive.merge.tezfiles and Parquet","body":"<p>Does hive.merge.tezfiles applies when using parquet data in Hive?</p>","tags":["Hive","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-11-24 17:40:35.0","id":68329,"title":"Is there a way to save a model in PYSPARK (python) 1.6.0 to HDFS ?","body":"<p>Hi,</p><p>I need to save a model in python spark 1.6.0. I know save()/load functions are available in 2.0 but I'm not in a position to upgrade our HDP cluster at this current time and need a hack.</p><p>I know Scala 1.6 does support saving of models. Is there some way I could share my model object from python to scala. Im using zeppelin so this might work between paragraphs? </p><p>Any help much appreciated</p>","tags":["Spark","pyspark"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-11-24 09:20:09.0","id":68263,"title":"Change log4j format for the ambari-worker","body":"<p>Dear all,</p><p>I would like to change the output format for the log4j Ambari Agent Service. In the server I just changed the pattern in a properties file, but I can see the agent does not have that (or I cannot find it). What would be the way to change it?</p><p>Thanks!</p>","tags":["logs","Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-24 01:29:54.0","id":68205,"title":"After a period of time enable kerberos from ambari  kinit Password incorrect","body":"<p>Ambari 2.1.1 ,after a period of time enable kerberos, it will appear occasionally questions like:</p><p>start a service in Ambari web but curl namenode error, since kinit password incorrect hdfs.headless.keytab;</p><p>or /etc/security/keytabs/rm.service.keytab Checksum failed . At last ,I found principal and keytab of each service password incorrect</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-11-25 10:34:40.0","id":68396,"title":"Invalid scalar projection in pig","body":"<p>grunt&gt; uniqcnt_1 = foreach uniq_sym_1 generate COUNT(uniq_sym_1.symbol); </p><pre>2016-11-25 00:35:49,902 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: Pig script failed to parse: &lt;line 10, column 46&gt; Invalid scalar projection: uniq_sym_1 Details at logfile: /home/naresh/Work1/hadoop-1.2.1/bin/pig_1480051207097.log </pre><p>grunt&gt; describe uniq_sym_1; </p><p>uniq_sym_1: {{(symbol: bytearray)}} </p>","tags":["data-processing","HDFS"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-11-24 15:37:33.0","id":68312,"title":"Does TailFile Processor support Expression Language?","body":"<p>The situation I have is the need to keep a NoSQL data store in sync with a legacy database with minimal intrusiveness on the operational legacy environment. Only selected entries in the legacy database need to be propagated to the NoSQL database with some data transformation/enrichment.</p><p>I'm considering using NiFi for this scenario. My thought is to use TailFile processor to 'tail' the legacy database log file and have it act only on certain file entries. Conceptually, the filtering could be done via the Expression Language. Does NiFi TailFile Processor support Expression Language to filter the file entries of interest? If not, how else can this be done? </p><p>Thanks for your help in advance!</p>","tags":["tailfile"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-02 18:25:38.0","id":69697,"title":"Getting error \"User session not found 403\" when using zeppelin with livy","body":"<p>So we have a kerberized cluster and have integrated ldaps with zeppelin. But when i try to use livy i get the following error</p><p>ERROR [2016-12-01 10:00:02,442] ({pool-2-thread-4} LivyHelper.java[createSession]:128) - Error getting session for user\njava.lang.Exception: Cannot start  spark.\n        at org.apache.zeppelin.livy.LivyHelper.createSession(LivyHelper.java:117)\n        at org.apache.zeppelin.livy.LivySparkSQLInterpreter.interpret(LivySparkSQLInterpreter.java:62)\n        at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n        at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n        at org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n        at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nERROR [2016-12-01 10:00:02,442] ({pool-2-thread-4} LivySparkSQLInterpreter.java[interpret]:70) - Exception in LivySparkSQLInterpreter while interpret\njava.lang.Exception: Cannot start  spark.</p><p>in zeppelin interpreter logs.</p><p>Also when i dont enable ldap for zeppelin (that is i log in as anonymous), I can see that job is been submitted to yarn but it fails as user is  `zeppelin-clusterName`. But when i use ldap with zeppelin it is not even getting submit to yarn.</p>","tags":["zeppelin"],"track_id":85,"track_name":"DS, Analytics & Spark"}
{"creation_date":"2016-12-02 16:57:34.0","id":69687,"title":"HDP 2.5 - Storm process taking up all system memory and utilizing 100% of a CPU core.","body":"<p>HDP 2.5, Ambari 2.4.1.0-22 running on CentOS 7.2.</p><p>This is after a fresh install, one of my data nodes keeps having this issue: <a href=\"https://community.hortonworks.com/storage/attachments/10005-capture.png\">capture.png</a></p><p>The process is running under the storm user, and it's running \"python splitsentence.py\", as you can see it's utilizing almost all of the memory on the system. When I stop All of the components on this host, this process does not get killed. What HDP process is launching this? How can I prevent this from happening?</p><p>Data node is only running HDFS / Flume / RegionServer / Metrics Monitor / Supervisor / NodeManager.</p><p>Thanks</p>","tags":["hdp-2.5.0","Ambari","datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-02 23:47:49.0","id":69750,"title":"Minifi to Kafka","body":"<p>Do I need to use Nifi with Minifi or could i have a Minifi agent send directly to Kafka?</p>","tags":["minifi","Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-03 17:56:02.0","id":69813,"title":"How to start more than one Spark thrift server using Ambari","body":"<p>We have a HDP-2.4 cluster and Spark thrift server is installed on two nodes. When I start Spark thrift servers from Ambari, It starts one of them which is installed on the same node where Hive is installed and another fails. When I looked at the log file of the failed node, it says failed to start thrift server on the host where Hive is installed.</p><p>Should Hive and Spark thrift servers installed on the same node?</p><p>Can't we run more than one thrift server?</p>","tags":["spark-streaming","Spark","spark-sql"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-03 20:02:01.0","id":69830,"title":"HDP 2.3,  Ambari 2.1.2 All hosts show heartbeat lost","body":"<p>Hi,  I have 11 hosts and all of them are showing heartbeat lost and the icons are Yellow.  I restarted the ambari-server and agents on all the hosts but the hosts are still in this state.   Any idea what is going on.</p>","tags":["hdp-2.3.0"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-04 02:06:02.0","id":69841,"title":"HDP 2.3, Ambari 2.1.2 both namenodes go to standby mode","body":"<p>Hi,  Both my namenodes go into standby mode.  I tried stopping the zkfailovercontrollers, namenodes and starting them back up again.  This usually resolves the issue but in this case they always go into standby mode.  Any idea why this is happening.</p>","tags":["ambari-2.1.2"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-02 17:22:48.0","id":69699,"title":"Hive's taking to much time. It is normal?","body":"<p>Hi,</p><p>I'm trying to do a select count(*) from tweets; \nusing Hive CLI. But as you can see, it is taking to much time (i guess)</p><p>It is normal?</p><p>Any tip?</p><p><img src=\"/storage/attachments/10007-hive-sql.png\"></p><p>Regards</p><p><img alt=\"\"></p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-02 21:36:17.0","id":69735,"title":"Hive vertex error when I insert into orc zlib compression","body":"<p>Hi All,</p><p>I have a insert statement that works when I do insert into an uncompressed ORC table but when I insert it into a ORC compressed Zlib it throws a vertex error.. is there any way around this?</p><p>Here is the error attached</p><p><a href=\"/storage/attachments/10046-hive-error.txt\">hive-error.txt</a></p>","tags":["Hive","orc"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-02 18:28:04.0","id":69702,"title":"Is it needed Hive metaserver as well as Hive server2 on same node?","body":"","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-02 19:46:29.0","id":69683,"title":"I have build a cluster on Azure","body":"<p>I have build a cluster on Azure with hdp, but why i am getting the following error or alert:</p><p>HDFS CRITICALDataNode Web UI Connection failed to http://icsworkerphwx04.companies.com:1022 \n(Execution of 'curl -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/ed31ed55-af1f-46bf-9948-e9050869fd94 -c \n/var/lib/ambari-agent/tmp/cookies/ed31ed55-af1f-46bf-9948-e9050869fd94 -w '%{http_code}' \nhttp://icsworkerphwx04.companies.com:1022 \n--connect-timeout 5 --max-time 7 -o /dev/null 1&gt;/tmp/tmp2EVDyq 2&gt;\n/tmp/tmpihp2g9' returned 28. \n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total \nSpent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:02 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:04 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:15 --:--:-- 0 curl: (28) Resolving timed out after 5518 milliseconds 000) Cluster: icsprd Host: icsworkerphwx04.companies.com YARN CRITICAL NodeManager Web UI Connection failed to http://icsworkerphwx04.companies.com:8042 (Execution of 'curl -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/63c30616-b284-4d4b-b5b7-023886543501 -c /var/lib/ambari-agent/tmp/cookies/63c30616-b284-4d4b-b5b7-023886543501 -w '%{http_code}' http://icsworkerphwx04.companies.com:8042 --connect-timeout 5 --max-time 7 -o /dev/null 1&gt;/tmp/tmppj9zYW 2&gt;/tmp/tmpTyxeHN' returned 28. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:02 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:04 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:15 --:--:-- 0 curl: (28) Resolving timed out after 5519 milliseconds 000) Cluster: icsprd Host: icsworkerphwx04.companies.com CRITICALNodeManager Health Connection failed to http://icsworkerphwx04.companies.com:8042/ws/v1/node/info (Traceback (most recent call last): File \"/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/alerts/alert_nodemanager_health.py\", line 166, in execute connection_timeout=curl_connection_timeout, kinit_timer_ms = kinit_timer_ms) File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/curl_krb_request.py\", line 186, in curl_krb_request user=user, env=kerberos_env) File \"/usr/lib/python2.6/site-packages/resource_management/libraries/functions/get_user_call_output.py\", line 61, in get_user_call_output raise Fail(err_msg) Fail: Execution of 'curl -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/a83d3ded-b926-49d8-b2d5-46b2cbe09048 -c /var/lib/ambari-agent/tmp/cookies/a83d3ded-b926-49d8-b2d5-46b2cbe09048 http://icsworkerphwx04.companies.com:8042/ws/v1/node/info --connect-timeout 5 --max-time 7 1&gt;/tmp/tmpuKO4vq 2&gt;/tmp/tmpwQM8LB' returned 28. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:02 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:04 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:15 --:--:-- 0 curl: (28) Resolving timed out after 5517 milliseconds ) Cluster: icsprd Host: icsworkerphwx04.companies.com\n@\n</p><p>can any one please help me to find the exact error or help me to resolved the above problem. </p>","tags":["hdp-2.4.0","datanode"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-02 19:50:54.0","id":69725,"title":"error while running spark shell that uses tez as hive execution engine","body":"<p>java.lang.ClassNotFoundException: org.apache.tez.dag.api.SessionNotRunning</p>","tags":["Tez"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-03 07:46:30.0","id":69781,"title":"Error Whilst executing reposync -r Updates-ambari-2.4.2.0 and or trying to untar ambari-2.4.2.0-centos7.tar.gz","body":"<p>Hi</p><p>I am getting the below issues  while creating a local repository for the latest version of Ambari . Am I missing something or its an repository issue. Also getting error if I try to download the tar file and untar it. Tried this is many different ways, but still getting the same issues.</p><p>[root@hdprepo centos7]# reposync -r Updates-ambari-2.4.2.0\nwarning: /var/www/html/ambari/centos7/Updates-ambari-2.4.2.0/ambari/ambari-agent-2.4.2.0-136.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 07513cad: NOKEY |  27 MB  00:04:52 ETA \nPublic key for ambari-agent-2.4.2.0-136.x86_64.rpm is not installed\n(1/12): ambari-agent-2.4.2.0-136.x86_64.rpm                                                                                                                   |  22 MB  00:00:07    \n(2/12): ambari-infra-solr-client-2.4.2.0-136.x86_64.rpm                                                                                                       | 5.5 MB  00:00:01    \nambari-logsearch-logfeeder-2.4 FAILED                                           [======                                                            ] 5.8 MB/s | 123 MB  00:03:25 ETA \nambari-logsearch-logfeeder-2.4 FAILED                                           [====                                                              ]  0.0 B/s |  85 MB  --:--:-- ETA \n(3/12): ambari-logsearch-portal-2.4.2.0-136.x86_64.rpm                                                                                                        |  36 MB  00:00:16    \nambari-infra-solr-2.4.2.0-136. FAILED                                           [=========                                                         ] 2.4 MB/s | 189 MB  00:07:46 ETA \nambari-infra-solr-2.4.2.0-136. FAILED                                           [=========                                                         ] 2.4 MB/s | 189 MB  00:07:46 ETA \n(4/12): ambari-metrics-collector-2.4.2.0-136.x86_64.rpm                                                                                                       | 223 MB  00:01:04    \n(5/12): ambari-metrics-common-2.4.2.0-136.noarch.rpm                                                                                                          | 2.4 kB  00:00:00    \n(6/12): ambari-metrics-grafana-2.4.2.0-136.x86_64.rpm                                                                                                         |  18 MB  00:00:04    \n(7/12): ambari-metrics-hadoop-sink-2.4.2.0-136.x86_64.rpm                                                                                                     | 4.4 MB  00:00:00    \n(8/12): ambari-metrics-monitor-2.4.2.0-136.x86_64.rpm                                                                                                         | 177 kB  00:00:00    \nambari-server-2.4.2.0-136.x86_ FAILED                                           [===============================================-                  ] 2.8 MB/s | 954 MB  00:02:10 ETA \nambari-server-2.4.2.0-136.x86_ FAILED                                           [===============================================-                  ] 2.8 MB/s | 954 MB  00:02:10 ETA \nsmartsense-hst-1.3.1.0-136.x86 FAILED                                           [=========================                                         ] 4.8 MB/s | 504 MB  00:02:50 ETA \nsmartsense-hst-1.3.1.0-136.x86 FAILED                                           [=========================                                         ] 4.8 MB/s | 504 MB  00:02:50 ETA \nsmartsense-hst-1.3.1.0-136.x86_64: [Errno 256] No more mirrors to try.      38% [=========================                                         ] 4.8 MB/s | 504 MB  00:02:50 ETA \nambari-server-2.4.2.0-136.x86_64: [Errno 256] No more mirrors to try.\nambari-logsearch-logfeeder-2.4.2.0-136.x86_64: [Errno 256] No more mirrors to try.\nambari-infra-solr-2.4.2.0-136.x86_64: [Errno 256] No more mirrors to try.</p><p>UNTAR ERROR</p><p>---------------------------------------------------------------------------------------------------------------------------------------------------------------------</p><p>[root@hdprepo Downloads]# tar -xzvf ambari-2.4.2.0-centos7.tar.gz \nAMBARI-2.4.2.0/\nAMBARI-2.4.2.0/setup_repo.sh\nAMBARI-2.4.2.0/centos7/\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/fc65a16ed6652689c8a14d158afc7507694d9beb30e959987232b995ba179c88-other.sqlite.bz2\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/10706b5ef12a1b2fc74fdf374221144d1ff5e931292aa7d697c75b7abd81a5e8-primary.xml.gz\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/repomd.xml\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/c643fa22aee035e3169569ec749c2bfd20ecb9d598f0fa3e3447a9a30bddbc8a-other.xml.gz\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/83d9c101351ce11322ba57006eca3ceee022095557feb87468ebd2504222d81a-filelists.sqlite.bz2\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/f7e2733f04a9c2cbc906cf6149a0bf8ecf36fab0174f14e4c9687ef552928563-filelists.xml.gz\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/repodata/95f327e2b87eb11db7f242387a2229d71c10d62a20ed67d7ee91508b0d76d091-primary.sqlite.bz2\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/RPM-GPG-KEY/\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/build_metadata.txt\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/build.id\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/changelog.txt\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/smartsense/\nAMBARI-2.4.2.0/centos7/2.4.2.0-136/smartsense/smartsense-hst-1.3.1.0-136.x86_64.rpm\ngzip: stdin: invalid compressed data--format violated\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now</p>","tags":["Ambari"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-12-03 02:29:28.0","id":69761,"title":"Kerberized HDP 2.4 - getting error in using Hive View on Ambari","body":"<p>Hi All,</p><p>I've a kerberized HDP 2.4 - and i've created user - hive_user1</p><p>I logon to Hive View on Ambari, and fire simple query - select * from test</p><p>The error i get is shown below -</p><p>-------------------------------</p><p>Error while compiling statement: FAILED: HiveAccessControlException \nPermission denied: user [hdfs] does not have [SELECT] privilege on \n[default/test/sno] [ERROR_STATUS]</p><p>-----------------------------</p><p>Pls note - i've disabled Global access to Hive, but given access to user - hive_user1</p><p>But somehow, it is using hdfs user to access Hive.</p><p>Any ideas on this ?</p><p>attached is the screenshot of the user, and the error obtained.<a href=\"/storage/attachments/10010-screen-shot-2016-12-02-at-61927-pm.png\">\n</a></p><p><a href=\"/storage/attachments/10010-screen-shot-2016-12-02-at-61927-pm.png\">screen-shot-2016-12-02-at-61927-pm.png</a><a href=\"/storage/attachments/10051-screen-shot-2016-12-02-at-62808-pm.png\">\n</a></p><p><a href=\"/storage/attachments/10051-screen-shot-2016-12-02-at-62808-pm.png\">screen-shot-2016-12-02-at-62808-pm.png</a>   </p>\n","tags":["hiveserver2","Hive","security","hive-views"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-03 10:18:38.0","id":69777,"title":"MiNiFi RPG HTTP Proxy property is not automatically generated?","body":"<p>Hi all,</p><p>I try a simple MiNiFi case to get file and then transmit the file to RPG.</p><p><img src=\"/storage/attachments/10049-minifi-flow.jpg\"></p><p>The RPG is configurated with HTTP Proxy Server properties.</p><p><img src=\"/storage/attachments/10050-rpg-config.jpg\"></p><p>But when I transform template xml into MiNiFi config YAML by minifi-toolkit.</p><p><img src=\"/storage/attachments/10062-template.jpg\"></p><p>The HTTP Proxy Server properties is not automatically generated in the Remote Process Groups section of YAML.</p><p><img src=\"/storage/attachments/10061-yml.jpg\"></p><p>The attached minifi.zip is the template & config.yml</p><p><a href=\"/storage/attachments/10064-minifi.zip\">minifi.zip</a>\n</p><p>Thanks for your help.</p>","tags":["minifi","minifi-yml"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-03 17:29:36.0","id":69808,"title":"What happens if one of the Spark task fails while inserting data into Hive","body":"<p>I came across a situation when inserting data into hive table from another table. The query was processed using two MR jobs. one got successful and another failed. I could see, few records are inserted into the target table. It was obvious to me since there were two MR jobs processed independently and  it is not transactional based.</p><p>I am trying to understand what happens if the same occurs while inserting data into Hive using Spark. If one of the executor/task fails and it reached retry limit, will it completely terminate the job or partial data get inserted into the table?</p><p> Thanks in advance.</p>","tags":["spark-sql","Hive","spark-streaming"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-02 18:50:06.0","id":69706,"title":"Kafka producer fail to retrieve data","body":"<p>bin/kafka-console-producer.sh --broker-list *.*.*.*:6667 --topic test4 --security-protocol SASL_PLAINTEXT</p><p>test</p><p>[2016-12-02 13:36:09,138] WARN Error while fetching metadata [{TopicMetadata for topic test4 -&gt;\nNo partition metadata for topic test4 due to kafka.common.LeaderNotAvailableException}] for topic [test4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)\n[2016-12-02 13:36:09,139] ERROR Failed to send requests for topics test4 with correlation ids in [0,8] (kafka.producer.async.DefaultEventHandler)\n[2016-12-02 13:36:09,140] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread)\nkafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.\n        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:91)\n        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105)\n        at kafka.producer.async.ProducerSendThread$anonfun$processEvents$3.apply(ProducerSendThread.scala:88)\n        at kafka.producer.async.ProducerSendThread$anonfun$processEvents$3.apply(ProducerSendThread.scala:68)\n        at scala.collection.immutable.Stream.foreach(Stream.scala:547)\n        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67)\n        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45)\n</p><p>Getting following error while running with my user. This works fine with kafka user. </p><p>PS - We have enabled kerberos on this cluster</p>","tags":["Kafka"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-02 19:45:51.0","id":69607,"title":"Ambari server start giving Error java process died with exitcode 255","body":"<p>Trying to install ambari in my local centos7 machine.</p><p>I have followed the hortonworks document step by step.</p><p>when i run the command i.e</p><p>ambari-server start</p><p>it giving me bolow error.</p><p>Starting ambari-server\nAmbari Server running with administrator privileges.\nOrganizing resource files at /var/lib/ambari-server/resources...\nServer PID at: /var/run/ambari-server/ambari-server.pid\nServer out at: /var/log/ambari-server/ambari-server.out\nServer log at: /var/log/ambari-server/ambari-server.log\nWaiting for server start.........\n<strong>ERROR: Exiting with exit code -1. \nREASON: Ambari Server java process died with exitcode 255. Check /var/log/ambari-server/ambari-server.out for more information.</strong></p><p>I have checked into <strong>/var/log/ambari-server/ambari-server.out </strong>file, it contains</p><pre>[EL Warning]: metadata: 2016-12-02 12:53:02.301--ServerSession(799570413)--The reference column name [resource_type_id] mapped on the element [field permissions] does not correspond to a valid id or basic field/column on the mapping reference. Will use referenced column name as provided.</pre><p><strong></strong></p><p>and also i have checked the logs in /var/logs/ambari-server/ambari-server.log file</p><p>it contains</p><p>02 Dec 2016 12:53:00,195  INFO [main] ControllerModule:185 - Detected POSTGRES as the database type from the JDBC URL\n02 Dec 2016 12:53:00,643  INFO [main] ControllerModule:558 - Binding and registering notification dispatcher class org.apache.ambari.server.notifications.dispatchers.AlertScriptDispatcher\n02 Dec 2016 12:53:00,647  INFO [main] ControllerModule:558 - Binding and registering notification dispatcher class org.apache.ambari.server.notifications.dispatchers.EmailDispatcher\n02 Dec 2016 12:53:00,684  INFO [main] ControllerModule:558 - Binding and registering notification dispatcher class org.apache.ambari.server.notifications.dispatchers.SNMPDispatcher\n02 Dec 2016 12:53:01,911  INFO [main] AmbariServer:705 - Getting the controller\n02 Dec 2016 12:53:02,614  INFO [main] StackManager:107 - Initializing the stack manager...\n02 Dec 2016 12:53:02,614  INFO [main] StackManager:267 - Validating stack directory /var/lib/ambari-server/resources/stacks ...\n02 Dec 2016 12:53:02,614  INFO [main] StackManager:243 - Validating common services directory /var/lib/ambari-server/resources/common-services ...\n02 Dec 2016 12:53:02,888<strong> ERROR [main] AmbariServer:717 - Failed to run the Ambari Server</strong>\ncom.google.inject.ProvisionException: Guice provision errors:</p><p>1) <strong>Error injecting constructor, org.apache.ambari.server.AmbariException: Stack Definition Service at '/var/lib/ambari-server/resources/common-services/HAWQ/2.0.0/metainfo.xml' doesn't contain a metainfo.xml file</strong>\n  at org.apache.ambari.server.stack.StackManager.&lt;init&gt;(StackManager.java:105)\n  while locating org.apache.ambari.server.stack.StackManager annotated with interface com.google.inject.assistedinject.Assisted\n  at org.apache.ambari.server.api.services.AmbariMetaInfo.init(AmbariMetaInfo.java:242)\n  at org.apache.ambari.server.api.services.AmbariMetaInfo.class(AmbariMetaInfo.java:124)\n  while locating org.apache.ambari.server.api.services.AmbariMetaInfo\n  for field at org.apache.ambari.server.controller.AmbariServer.ambariMetaInfo(AmbariServer.java:138)\n  at org.apache.ambari.server.controller.AmbariServer.class(AmbariServer.java:138)\n  while locating org.apache.ambari.server.controller.AmbariServer</p><p>1 error\n   at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:987)\n   at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1013)\n   at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:710)\n<strong>Caused by: org.apache.ambari.server.AmbariException: Stack Definition Service at '/var/lib/ambari-server/resources/common-services/HAWQ/2.0.0/metainfo.xml' doesn't contain a metainfo.xml file</strong>\n   at org.apache.ambari.server.stack.ServiceDirectory.parseMetaInfoFile(ServiceDirectory.java:209)\n   at org.apache.ambari.server.stack.CommonServiceDirectory.parsePath(CommonServiceDirectory.java:71)\n   at org.apache.ambari.server.stack.ServiceDirectory.&lt;init&gt;(ServiceDirectory.java:106)\n   at org.apache.ambari.server.stack.CommonServiceDirectory.&lt;init&gt;(CommonServiceDirectory.java:43)\n   at org.apache.ambari.server.stack.StackManager.parseCommonServicesDirectory(StackManager.java:301)\n   at org.apache.ambari.server.stack.StackManager.&lt;init&gt;(StackManager.java:115)\n   at org.apache.ambari.server.stack.StackManager$$FastClassByGuice$$33e4ffe0.newInstance(&lt;generated&gt;)\n   at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40)\n   at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:60)\n   at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)\n   at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)\n   at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978)\n   at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031)\n   at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974)\n   at com.google.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:632)\n   at com.sun.proxy.$Proxy25.create(Unknown Source)\n   at org.apache.ambari.server.api.services.AmbariMetaInfo.init(AmbariMetaInfo.java:246)\n   at org.apache.ambari.server.api.services.AmbariMetaInfo$$FastClassByGuice$$202844bc.invoke(&lt;generated&gt;)\n   at com.google.inject.internal.cglib.reflect.$FastMethod.invoke(FastMethod.java:53)\n   at com.google.inject.internal.SingleMethodInjector$1.invoke(SingleMethodInjector.java:56)\n   at com.google.inject.internal.SingleMethodInjector.inject(SingleMethodInjector.java:90)\n   at com.google.inject.internal.MembersInjectorImpl.injectMembers(MembersInjectorImpl.java:110)\n   at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:94)\n   at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)\n   at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n   at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031)\n   at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n   at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n   at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:40)\n   at com.google.inject.internal.SingleFieldInjector.inject(SingleFieldInjector.java:53)\n   at com.google.inject.internal.MembersInjectorImpl.injectMembers(MembersInjectorImpl.java:110)\n   at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:94)\n   at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)\n   at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\n   at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031)\n   at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n   at com.google.inject.Scopes$1$1.get(Scopes.java:65)\n   at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:40)\n   at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978)\n   at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1024)\n   at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974)\n   ... 2 more</p><p>Please suggest me.</p><p>Mohan.V</p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-02 22:18:45.0","id":69748,"title":"How to add HBASE to Hortonworks Data Cloud?","body":"<p>Hi,</p><p>I created Hortonworks Cloud using the AWS Marketplace. The install works great, but I wanted to try HBASE and the Ambari View doesn't have the HBASE option. Is there an add-on component that we need to add/install to enable HBASE?</p><p>Thanks!</p><p><a href=\"/storage/attachments/10047-screen-shot-2016-12-02-at-21405-pm.png\">screen-shot-2016-12-02-at-21405-pm.png</a></p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-03 04:27:10.0","id":69768,"title":"Launching Phoenix issue","body":"<p>I downloaded apache phoenix using ambari but I can't launch it. I can launch the Hbase using hbase shell but I can't launch bin/sqlline.py ... it says no such file or directory... It's HDP 2.5</p>","tags":["Hbase","Phoenix"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-04 08:55:05.0","id":69849,"title":"HDP 2.5 Ambari Dashboard doesn't accept any credentials","body":"<p>All the other components (welcome page, shell, Zepellin, etc) are working after downloading and launching HDP 2.5 on Virtual Box. Only Ambari Dashboard is prompting me user id and password after navigating to its page i.e. 127.0.0.1:8080 but never goes beyond that. </p><p>I have powered off VM for couple of times but to my dismay it never got resolved. </p><p>Even restarting ambari-server & ambari-agent has not worked</p><p>OR using IP mentioned in eth0 section hasnt worked either. Please let me know what am i missing?</p>","tags":["hdp-2.5.0"],"track_id":81,"track_name":"Sandbox & Learning"}
{"creation_date":"2016-12-02 17:37:53.0","id":69692,"title":"Creating a topic in kakfa fails with \"ERROR kafka.admin.AdminOperationException: replication factor: 2 larger than available brokers: 0\"","body":"<pre> HDP version : 2.5</pre><p>Here are the steps which I have followed :</p><p>1. Installed kafka brokers on 2 nodes in my cluster.</p><p>2. Tried to create a topic , Created successfully </p><p>3. I tried to change the zookeeper root directory for kafka by following below steps :</p><p>     &gt; In ambari &gt; Kafka &gt; Configs &gt; changed value of zookeeper.connect  as follows :</p><pre>node1.example.com:2181,node2.example.com:2181,node1.example.com:2181/kafka</pre><p>4. Saved the changes and manually created  /kafka in zookeeper using zkCli.sh</p><p>5. Restarted kafka brokers. After restart znodes were created in /kafka in zookeeper.</p><p>6. Now when I try to create a topic it fails with below stack trace :</p><pre>Error while executing topic command : replication factor: 2 larger than available brokers: 0\n[2016-12-02 16:13:12,851] ERROR kafka.admin.AdminOperationException: replication factor: 2 larger than available brokers: 0\nat kafka.admin.AdminUtils$.assignReplicasToBrokers(AdminUtils.scala:117)\nat kafka.admin.AdminUtils$.createTopic(AdminUtils.scala:403)\nat kafka.admin.TopicCommand$.createTopic(TopicCommand.scala:110)\nat kafka.admin.TopicCommand$.main(TopicCommand.scala:61)\nat kafka.admin.TopicCommand.main(TopicCommand.scala)</pre><p>I can confirm that all znodes are created correctly under /kafka, brokers are up and running fine. I suspect that for some reason kafka admin or kafka controller(not sure who looks at the brokers znode value) is not able to identify live brokers because it tries to find the broker ids in previous zookeeper root dir i.e /brokers/ids/ which is empty.</p><p>In order to confirm if this is the scenario, I reverted the changes which were made. </p><pre> &gt; Changed the zookeeper root directory to default i.e &lt;node1.example.com:2181,node2.example.com:2181,node1.example.com:2181&gt;</pre><p>&gt; Restarted zookeeper broker </p><p>&gt; Confirmed that all znodes are with correct value (/controller_epoch) (/brokers/ids/1004 /brokers/ids/1006)</p><p>Now I tried to create a topic which got created without any issues. </p><p>Then I deleted both the broker IDS in /brokers/ids and tried to create a topic. It failed with same error i,e :</p><pre>Error while executing topic command : replication factor: 2 larger than available brokers: 0 [2016-12-02 16:13:12,851] ERROR</pre><p>Can some one please help me to understand this behaviour  ?</p>","tags":["zookeeper"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-12-02 19:50:22.0","id":69728,"title":"When i type yarn application -list , it shows a application list even with status KILLED, what is the best command to clear this list where application status is mentioned as \"KILLED\"?","body":"","tags":["YARN","yarn-scheduler"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-03 02:21:26.0","id":69760,"title":"Hive query with where clasue throwing error using Squirrel SQL client","body":"<p>When I run a simple query with WHERE clause from hive, it returns result. But, the same query using Squirrel client throws error. If I remove where clause, it works on SquirreL and Hive both.  Any idea?</p><pre>On SquirreL SQL Client:\nselect * from my_table where col_id=11500 limit 5;\n\nError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\nSQLState:  08S01\nErrorCode: 1\n</pre><pre>hive&gt; select * from table where col_id=11500 limit 5;\n2013-07-01\n 01:15:00  2013-07-01 06:15:00  20130701  11500  5864449886  957877905  \n17.493334  0  17.493334  17.493334  3.936  0  3.936  47  4  4  \n2013-10-30 16:21:08.93995  NULL  NULL  NULL\n2013-07-02 01:15:00  \n2013-07-02 06:15:00  20130702  11500  5864449886  957877905  14.364444  0\n  14.364444  16.517917  3.232  0  3.232  47  4  4  2013-10-30 \n16:21:36.220502  NULL  NULL  NULL\n2013-07-03 01:15:00  2013-07-03 \n06:15:00  20130703  11500  5864449886  957877905  13.853334  0  \n13.853334  17.220324  3.117  0  3.117  47  4  4  2013-10-30 \n16:22:23.973718  NULL  NULL  NULL\n2013-07-04 01:15:00  2013-07-04 \n06:15:00  20130704  11500  5864449886  957877905  12.426666  0  \n12.426666  19.591296  2.796  0  2.796  47  4  4  2013-10-30 \n16:23:08.96686  NULL  NULL  NULL\n2013-07-05 01:15:00  2013-07-05 \n06:15:00  20130705  11500  5864449886  957877905  19.328889  0  \n19.328889  18.618565  4.349  0  4.349  47  4  4  2013-10-30 \n16:23:57.512115  NULL  NULL  NULL\nTime taken: 31.885 seconds, Fetched: 5 row(s)\n</pre>","tags":["error","Hive","Tez"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-12-03 07:14:35.0","id":69774,"title":"HDInsight & NiFi","body":"<p>Hi ,</p><p>I'm new to HDIndight and trying to explore it. I have a scenario that need to get the realtime seonsor's data and analyze them.</p><p>I would like to use NiFi for the data ingestion and pass it Kafka and then Spark on HDInsight </p><p>Can you please advice how can I integrate Nifi with HDInsight ?</p><p>Thanks a lot,</p><p>SJ</p>","tags":["Nifi"],"track_id":61,"track_name":"Cloud & Operations"}
{"creation_date":"2016-12-02 20:11:53.0","id":69739,"title":"Oozie Hive2 action in kerberized cluster Any ideas for hiding or encrypting the Hive MySQL repository database password in the Workflow.xml","body":"","tags":["Oozie"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-02 23:31:58.0","id":69749,"title":"Ambari 2.4.2 doesn't have HDP2.5.3.0","body":"<p>Hi,</p><p>I installed the latest Ambari 2.4.2, but I can't find the latest HDP 2.5.3. It still use HDP2.5.0.</p><p>Anyone knows how to add HDP2.5.3 into Ambari 2.4.2?</p><p>Regards,</p><p>Wendell</p>","tags":["Ambari"],"track_id":65,"track_name":"Hadoop Core"}
{"creation_date":"2016-12-03 04:34:05.0","id":69763,"title":"Where is import-hive.sh in HDP 2.5.3","body":"<p>Installing Atlas with HDP 2.5.3, but could not locate import-hive.sh</p><p>It used to be under either folder</p><pre>/usr/hdp/current/atlas-server/bin/\n/usr/hdp/current/atlas-server/hook-bin/</pre><p>But in HDP 2.5.3, it is no longer there</p><pre>[root@qwang-hdp3 ~]# ll /usr/hdp/2.5.3.0-37/atlas/bin\ntotal 56\n-rwxr-xr-x. 1 root root  1394 Nov 30 04:50 atlas_admin.py\n-rwxr-xr-x. 1 root root  2848 Nov 30 04:50 atlas_client_cmdline.py\n-rwxr-xr-x. 1 root root 16576 Nov 30 04:50 atlas_config.py\n-rwxr-xr-x. 1 root root  1491 Nov 30 04:50 atlas_kafka_setup_hook.py\n-rwxr-xr-x. 1 root root  1481 Nov 30 04:50 atlas_kafka_setup.py\n-rwxr-xr-x. 1 root root  6093 Nov 30 04:50 atlas_start.py\n-rwxr-xr-x. 1 root root  2266 Nov 30 04:50 atlas_stop.py\n-rwxr-xr-x. 1 root root  1965 Nov 30 04:50 cputil.py\n-rwxr-xr-x. 1 root root  1574 Nov 30 04:50 quick_start.py\n</pre><p>Without the script, no Hive schema is synced in atlas after install</p>","tags":["hdp-2.5.3","Atlas"],"track_id":64,"track_name":"Governance & Lifecycle"}
{"creation_date":"2016-12-03 16:45:54.0","id":69799,"title":"I am getting below error while creating external table","body":"<p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:hdfs://quickstart.cloudera:8020/user/cloudera/emp1/emp.txt is not a directory or unable to create one)</p>","tags":["Hive"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-02 19:58:23.0","id":69727,"title":"Oozie Hive2 action in kerberized cluster","body":"<p> Any ideas for hiding or encrypting  the Hive MySQL repository database password in the Workflow.xml</p>","tags":["Oozie","hive2"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-02 21:23:56.0","id":69734,"title":"NiFi-1.0.0 GenerateFlowFile processor creates duplicates when coordinator and primary node are different","body":"<p>Today I noticed in NiFi-1.0.0 that all of my GenerateFlowFile processors that were set to run on primary node were  generating flow files twice instead of only the primary node generating 1 flow file. After a lot of verifying I noticed that in the cluster the coordinator node and primary node were different and not the same node. Till previously whenever zookeeper elected the coordinator and primary node, one node had always  been both the primary and coordinator node. So I restarted all the nodes in the cluster but one so that the one remaining would be both the primary and coordinator and that seemed to fix the issue. Because of this I am wondering if it is possible that NiFi-1.0.0 counts the primary and coordinator as two primary nodes when they are different? Has anyone encountered this before? Also I know that NiFi-1.1.0 came out last week so has this issue been fixed there?</p>","tags":["nifi-processor"],"track_id":63,"track_name":"Data Ingestion & Streaming"}
{"creation_date":"2016-12-02 22:13:00.0","id":69744,"title":"Connecting to Hive from SquirreL SQL client","body":"<p><a rel=\"user\" href=\"/users/336/nshawa.html\" nodeid=\"336\">@Ned Shawa</a></p><p>I followed these steps, but still unable to connect: </p><p>https://community.hortonworks.com/articles/3043/connecting-to-hive-thrift-server-on-hortonworks-us.html</p><p>I did following:</p><p>1. Copied all required jar files from the cluster to Squirrel lib directory:</p><p>commons-logging*.jar</p><p>hadoop-common-*.jar</p><p>hive-exec-*.jar</p><p>hive-jdbc-*.jar</p><p>httpclient-*.jar</p><p>httpcore-*.jar</p><p>libthrift-*.jar</p><p>ojdbc7.jar</p><p>sfl4j-api-*.jar</p><p>sfl4j-log4j12-*.jar</p><p>2. Created a new driver using above jar files.</p><p>3. Created a new alias using the new driver with following URL: jdbc:hive2://&lt;HiveServer2 SERVER&gt;:10000/default</p><p>UserName: &lt;Hive Oracle User Name&gt;</p><p>Password: &lt;Hive Oracle Password&gt;</p><p>Still, I'm seeing following error:</p><p>===========================================================</p><p>java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/MetaException\n   at java.util.concurrent.FutureTask.report(Unknown Source)\n   at java.util.concurrent.FutureTask.get(Unknown Source)\n   at net.sourceforge.squirrel_sql.client.mainframe.action.OpenConnectionCommand.awaitConnection(OpenConnectionCommand.java:132)\n   at net.sourceforge.squirrel_sql.client.mainframe.action.OpenConnectionCommand.access$100(OpenConnectionCommand.java:45)\n   at net.sourceforge.squirrel_sql.client.mainframe.action.OpenConnectionCommand$2.run(OpenConnectionCommand.java:115)\n   at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n   at java.util.concurrent.FutureTask.run(Unknown Source)\n   at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n   at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n   at java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/MetaException\n   at net.sourceforge.squirrel_sql.client.mainframe.action.OpenConnectionCommand.executeConnect(OpenConnectionCommand.java:175)\n   at net.sourceforge.squirrel_sql.client.mainframe.action.OpenConnectionCommand.access$000(OpenConnectionCommand.java:45)\n   at net.sourceforge.squirrel_sql.client.mainframe.action.OpenConnectionCommand$1.run(OpenConnectionCommand.java:104)\n   ... 5 more\nCaused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/MetaException\n   at org.apache.hive.jdbc.HiveConnection.createBinaryTransport(HiveConnection.java:456)\n   at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:182)\n   at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:155)\n   at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)\n   at net.sourceforge.squirrel_sql.fw.sql.SQLDriverManager.getConnection(SQLDriverManager.java:133)\n   at net.sourceforge.squirrel_sql.client.mainframe.action.OpenConnectionCommand.executeConnect(OpenConnectionCommand.java:167)\n   ... 7 more\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.metastore.api.MetaException\n   at java.net.URLClassLoader.findClass(Unknown Source)\n   at java.lang.ClassLoader.loadClass(Unknown Source)\n   at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\n   at java.lang.ClassLoader.loadClass(Unknown Source)\n   ... 13 more</p>","tags":["Hive"],"track_id":84,"track_name":"Solutions"}
{"creation_date":"2016-12-03 04:22:06.0","id":69762,"title":"Couldn't resolve host 'raw.githubusercontent.com'","body":"<p>While taking Hbase tutorial from link: http://hortonworks.com/hadoop-tutorial/introduction-apache-hbase-concepts-apache-phoenix-new-backup-restore-utility-hbase/ </p><p>and while downloading data file (dataset) using command:</p><p>curl -o ~/data.csv https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/data.csv</p><p>I am getting below error message: </p><p>curl: (6) Couldn't resolve host 'raw.githubusercontent.com'</p><p>Could someone tell how to proceed ahead</p><p></p>","tags":["Hbase"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-03 21:30:50.0","id":69820,"title":"Falcon extension libraries missing. HDP 2.5","body":"<p>I am trying to mirror set of directories from HDP 2.3.4 to HDP 2.5 and I am using Falcon's HDFS-Mirroring extension to do this. However, when I run the job it fails. When I took a close look at the error, it seems like the extension libraries which are supposed to be under HDFS path \"/apps/falcon/extensions/hdfs-mirroring/libs/runtime\" are missing. The same is true of all falcon extensions. I tried to copy this from the instance where falcon server is running, but the corresponding folders (Ex. /usr/hdp/2.5.0.0-1245/falcon/extensions/hdfs-mirroring/libs/runtime) are also empty. </p><p>Where can I find the relevant jar files so that I can copy them into my local instance as well as HDFS?</p>","tags":["mirroring"],"track_id":66,"track_name":"Data Processing"}
{"creation_date":"2016-12-04 16:23:31.0","id":69852,"title":"hdfs data disk size is exceeding 90% threshold while rest of the disks(in the same server) are about 55%","body":"<p>Hi all,</p><p>Can you please guide me how to troubleshoot why one of the disks in one data node is exceeding its size than the others. </p><p>Filesystem  Size  Used Avail Use% Mounted on </p><p>/dev/mapper/VolGroup00-LogVol00  1.8T  3.1G  1.7T  1% \n/\ntmpfs  127G  264K  127G  1% /dev/shm\n\n/dev/sda1  485M  38M  422M  9% /boot\n\n/dev/sdb1  1.8T  956G  785G  55% /data01 \n/dev/sdc1  1.8T  964G  777G  56% /data02\n\n/dev/sdd1  1.8T  960G  781G  56% /data03\n\n/dev/sde1  1.8T  931G  810G  54% /data04\n\n/dev/sdf1  1.8T  962G  779G  56% /data05\n\n/dev/sdg1  1.8T  944G  796G  55% /data06\n\n/dev/sdh1  1.8T  945G  796G  55% /data07\n\n/dev/sdi1  1.8T  1.6T  192G  90% /data08\n\n/dev/sdj1  1.8T  934G  806G  54% /data09\n\n/dev/sdk1  1.8T  930G  811G  54% /data10\n\n/dev/sdl1  1.8T  940G  800G  55% /data11\n\n/dev/mapper/VolGroup00-LogVol04  5.0G  696M  4.0G  15% /home \n/dev/mapper/VolGroup00-LogVol03  9.9G  470M  8.9G  5% /opt\n\n/dev/mapper/VolGroup00-LogVol05  5.0G  150M  4.6G  4% /tmp\n\n/dev/mapper/VolGroup00-LogVol02  20G  1.9G  17G  11% /var</p>","tags":["hadoop","HDFS","size"],"track_id":65,"track_name":"Hadoop Core"}
